{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ae2ecd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Modus library development build. Latest git commit:\n",
      "\n",
      "\tcommit 4333b3b218e30e0aa8b967eefee693e890be1cea\n",
      "\tAuthor: henry.tang <henry.tang@modusasset.com.hk>\n",
      "\tDate:   Wed May 25 13:23:00 2022 +0800\n",
      "\t\n",
      "\t    Add iterassetname\n",
      "\t\n",
      "Found - ../../../TEST_DATA.csv !\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import modus.dna as md\n",
    "from modus.dna.constants import DO_NOT_LOG_COLUMNS\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Activation, Dense, LSTM, Dropout, GlobalMaxPooling1D, Bidirectional, GRU, Lambda\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau \n",
    "from keras.layers.core import Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.optimizers import adam_v2 # not sure if this is the same\n",
    "from keras.metrics import categorical_crossentropy, BinaryAccuracy, PrecisionAtRecall, CategoricalAccuracy, Precision, Recall\n",
    "from keras.losses import SparseCategoricalCrossentropy, BinaryCrossentropy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import keras_tuner as kt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def find_parent_dirs(filename, directory = None, max_depth = 10, verbose = 0):\n",
    "    if max_depth <= 0:\n",
    "        return None\n",
    "    print(f\"Searching {directory}\") if verbose else None\n",
    "    print(os.listdir(directory)) if verbose else None\n",
    "    if filename in os.listdir(directory):\n",
    "        print(f\"Found - {directory+filename} !\")\n",
    "        return directory + filename\n",
    "    return find_parent_dirs(filename, '../' + (directory if directory else ''), max_depth = max_depth - 1, verbose = verbose)\n",
    "\n",
    "nearest_excel_file = find_parent_dirs('TEST_DATA.csv')\n",
    "\n",
    "d1 = md.read_bloomberg_csv(nearest_excel_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cad841bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.to_csv('DATA.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93688b6",
   "metadata": {},
   "source": [
    "# CCMP INDEX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69194150",
   "metadata": {},
   "source": [
    "### Delete Duplicate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9339d567",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = d1.loc[:,~d1.columns.duplicated()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee446ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSFT debuted on the 14th March 1986\n",
    "d1 = d1[d1.index >= '1986-06-30']\n",
    "# d1 = d1[d1.index < '2020-12-31']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6462336f",
   "metadata": {},
   "source": [
    "### Add RSI as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00a0dfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = d1.loc[:, (slice(None), 'PX_LAST')]\n",
    "\n",
    "class RSI():\n",
    "    def __init__(self,v,days=14):\n",
    "        self.px_last = v\n",
    "        self.days = days\n",
    "        self.diff = self.px_last.diff(periods=1)\n",
    "        self.up = self.diff.clip(lower=0)\n",
    "        self.down = -1*self.diff.clip(upper=0)\n",
    "        self.ema_up = self.up.ewm(com=self.days-1, adjust=False).mean()\n",
    "        self.ema_down = self.down.ewm(com=self.days-1, adjust=False).mean()\n",
    "        self.rs = self.ema_up/self.ema_down\n",
    "        self.RSI = 100 - (100/(1+self.rs))\n",
    "        \n",
    "RSI_df = pd.DataFrame()\n",
    "for i in range(len(df.columns)):\n",
    "    X = pd.DataFrame(df.iloc[:,i])\n",
    "    Asset14 = RSI(X,14)\n",
    "    Asset30 = RSI(X,30)\n",
    "    Asset14 = Asset14.RSI.rename(columns={'PX_LAST':'RSI_14D'})\n",
    "    Asset30 = Asset30.RSI.rename(columns={'PX_LAST':'RSI_30D'})\n",
    "    d1 = pd.concat([d1, Asset14], axis = 1)\n",
    "    d1 = pd.concat([d1, Asset30], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fcab34",
   "metadata": {},
   "source": [
    "### Add Bollinger Band as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47efdcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------#\n",
    "TA = 'CCMP Index' # <-- remember to change the target asset name\n",
    "#---------------------------------------------------------------#\n",
    "\n",
    "Target_Last, Target_High, Target_Low = d1.loc[:, ([TA], ['PX_LAST'])], d1.loc[:, ([TA], ['PX_HIGH'])], d1.loc[:, ([TA], ['PX_LOW'])]\n",
    "Data_target = pd.concat([Target_Last, Target_High, Target_Low], axis = 1)\n",
    "\n",
    "class BOLL():\n",
    "    def __init__(self,v,days=20  ):\n",
    "        self.d_target = v\n",
    "        self.days = days        \n",
    "        self.avg = self.d_target.mean(axis=1)\n",
    "        self.stdev = self.avg.rolling(self.days).std(ddof=0)\n",
    "        self.middle = self.avg.rolling(self.days).mean()\n",
    "        self.high = self.middle + 2*self.stdev\n",
    "        self.low = self.middle - 2*self.stdev\n",
    "BOLL_df = pd.DataFrame()\n",
    "target = BOLL(Data_target,20)\n",
    "d_B = pd.DataFrame()\n",
    "d_B[(TA,'BB_H_DEV')] = d1[TA, 'PX_LAST']/target.high - 1\n",
    "d_B[(TA,'BB_M_DEV')] = d1[TA, 'PX_LAST']/target.middle - 1\n",
    "d_B[(TA,'BB_L_DEV')] = d1[TA, 'PX_LAST']/target.low - 1\n",
    "\n",
    "d1 = pd.concat([d1, d_B], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70ef8c7",
   "metadata": {},
   "source": [
    "### Add VIX TECHNICAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87c0d126",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------#\n",
    "TA = 'VIX Index' # <-- remember to change the target asset name\n",
    "#---------------------------------------------------------------#\n",
    "import talib \n",
    "\n",
    "Target_Open, Target_Last, Target_High, Target_Low = d1.loc[:, ([TA], ['PX_OPEN'])], d1.loc[:, ([TA], ['PX_LAST'])], d1.loc[:, ([TA], ['PX_HIGH'])], d1.loc[:, ([TA], ['PX_LOW'])]\n",
    "Data_target = pd.concat([Target_Open, Target_Last, Target_High, Target_Low], axis = 1)\n",
    "\n",
    "SMA20 = talib.SMA(Data_target[TA,'PX_LAST'], timeperiod=20)\n",
    "SMA10 = talib.SMA(Data_target[TA,'PX_LAST'], timeperiod=10)\n",
    "SMA5 = talib.SMA(Data_target[TA,'PX_LAST'], timeperiod=5)\n",
    "\n",
    "d_TA = pd.DataFrame()\n",
    "d_TA[(TA,'SMA5')] = SMA5\n",
    "d_TA[(TA,'SMA10')] = SMA10\n",
    "d_TA[(TA,'SMA20')] = SMA20\n",
    "\n",
    "d1 = pd.concat([d1, d_TA], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe8bb03",
   "metadata": {},
   "source": [
    "### Add TA-LIB III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a5bef12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------#\n",
    "TA = 'CCMP Index' # <-- remember to change the target asset name\n",
    "#---------------------------------------------------------------#\n",
    "import talib\n",
    "\n",
    "Target_Open, Target_Last, Target_High, Target_Low = d1.loc[:, ([TA], ['PX_OPEN'])], d1.loc[:, ([TA], ['PX_LAST'])], d1.loc[:, ([TA], ['PX_HIGH'])], d1.loc[:, ([TA], ['PX_LOW'])]\n",
    "Data_target = pd.concat([Target_Open, Target_Last, Target_High, Target_Low], axis = 1)\n",
    "\n",
    "\n",
    "SHOOT_STAR = talib.CDLSHOOTINGSTAR(Data_target[TA,'PX_OPEN'], \n",
    "                                     Data_target[TA,'PX_HIGH'], \n",
    "                                     Data_target[TA,'PX_LOW'], \n",
    "                                     Data_target[TA,'PX_LAST'])\n",
    "\n",
    "DOJI_STAR = talib.CDLDOJISTAR(Data_target[TA,'PX_OPEN'], \n",
    "                             Data_target[TA,'PX_HIGH'], \n",
    "                             Data_target[TA,'PX_LOW'], \n",
    "                             Data_target[TA,'PX_LAST'])\n",
    "\n",
    "INV_HAM = talib.CDLINVERTEDHAMMER(Data_target[TA,'PX_OPEN'], \n",
    "                                 Data_target[TA,'PX_HIGH'], \n",
    "                                 Data_target[TA,'PX_LOW'], \n",
    "                                 Data_target[TA,'PX_LAST'])\n",
    "\n",
    "EV_STAR = talib.CDLEVENINGSTAR(Data_target[TA,'PX_OPEN'], \n",
    "                                 Data_target[TA,'PX_HIGH'], \n",
    "                                 Data_target[TA,'PX_LOW'], \n",
    "                                 Data_target[TA,'PX_LAST'], penetration = 0)\n",
    "\n",
    "AB_BABY = talib.CDLABANDONEDBABY(Data_target[TA,'PX_OPEN'], \n",
    "                                 Data_target[TA,'PX_HIGH'], \n",
    "                                 Data_target[TA,'PX_LOW'], \n",
    "                                 Data_target[TA,'PX_LAST'], penetration = 0)\n",
    "\n",
    "LINE_STRIKE = talib.CDL3LINESTRIKE(Data_target[TA,'PX_OPEN'], \n",
    "                                 Data_target[TA,'PX_HIGH'], \n",
    "                                 Data_target[TA,'PX_LOW'], \n",
    "                                 Data_target[TA,'PX_LAST'])\n",
    "\n",
    "BLK_CROWS = talib.CDL3BLACKCROWS(Data_target[TA,'PX_OPEN'], \n",
    "                                 Data_target[TA,'PX_HIGH'], \n",
    "                                 Data_target[TA,'PX_LOW'], \n",
    "                                 Data_target[TA,'PX_LAST'])\n",
    "\n",
    "GAP_CROWS = talib.CDLUPSIDEGAP2CROWS(Data_target[TA,'PX_OPEN'], \n",
    "                                 Data_target[TA,'PX_HIGH'], \n",
    "                                 Data_target[TA,'PX_LOW'], \n",
    "                                 Data_target[TA,'PX_LAST'])\n",
    "\n",
    "# ADX - Average Directional Movement Index\n",
    "ADX = talib.ADX(Data_target[TA,'PX_HIGH'],\n",
    "                Data_target[TA,'PX_LOW'],\n",
    "                Data_target[TA,'PX_LAST'],\n",
    "                timeperiod = 14)\n",
    "\n",
    "# HT_TRENDMODE - Hilbert Transform - Trend vs Cycle Mode\n",
    "HT_TRENDMODE = talib.HT_TRENDMODE(Data_target[TA,'PX_LAST'])\n",
    "\n",
    "# Simple Moving Average\n",
    "SMA10 = talib.SMA(Data_target[TA,'PX_LAST'], timeperiod=10)\n",
    "SMA20 = talib.SMA(Data_target[TA,'PX_LAST'], timeperiod=20)\n",
    "\n",
    "d_TA = pd.DataFrame()\n",
    "d_TA[(TA,'SMA10')] = SMA10\n",
    "d_TA[(TA,'SMA20')] = SMA20\n",
    "d_TA[(TA,'SHOOT_STAR')] = SHOOT_STAR\n",
    "d_TA[(TA,'DOJI_STAR')] = DOJI_STAR\n",
    "d_TA[(TA,'INV_HAM')] = INV_HAM\n",
    "d_TA[(TA,'EV_STAR')] = EV_STAR\n",
    "d_TA[(TA,'AB_BABY')] = AB_BABY\n",
    "d_TA[(TA,'LINE_STRIKE')] = LINE_STRIKE\n",
    "d_TA[(TA,'BLK_CROWS')] = BLK_CROWS\n",
    "d_TA[(TA,'GAP_CROWS')] = GAP_CROWS\n",
    "d_TA[(TA,'ADX')] = ADX\n",
    "d_TA[(TA,'HT_TRENDMODE')] = HT_TRENDMODE\n",
    "\n",
    "d1 = pd.concat([d1, d_TA], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86930bf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('VIX Index', 'BEST_PX_BPS_RATIO') is dropped as all entries are nan.\n",
      "('VIX Index', 'PX_VOLUME') is dropped as all entries are nan.\n",
      "('VIX Index', 'BEST_ESTIMATE_FCF') is dropped as all entries are nan.\n",
      "('VIX Index', 'BEST_PE_NXT_YR') is dropped as all entries are nan.\n",
      "('VIX Index', 'BEST_ANALYST_RATING') is dropped as all entries are nan.\n",
      "('AUDJPY Curncy', 'BEST_PX_BPS_RATIO') is dropped as all entries are nan.\n",
      "('AUDJPY Curncy', 'PX_VOLUME') is dropped as all entries are nan.\n",
      "('AUDJPY Curncy', 'BEST_ESTIMATE_FCF') is dropped as all entries are nan.\n",
      "('AUDJPY Curncy', 'BEST_PE_NXT_YR') is dropped as all entries are nan.\n",
      "('AUDJPY Curncy', 'BEST_ANALYST_RATING') is dropped as all entries are nan.\n",
      "('CCMP Index', 'BEST_ESTIMATE_FCF') is dropped as all entries are nan.\n",
      "('CCMP Index', 'BEST_PE_NXT_YR') is dropped as all entries are nan.\n",
      "('CCMP Index', 'BEST_ANALYST_RATING') is dropped as all entries are nan.\n",
      "('DXY Curncy', 'BEST_PX_BPS_RATIO') is dropped as all entries are nan.\n",
      "('DXY Curncy', 'PX_VOLUME') is dropped as all entries are nan.\n",
      "('DXY Curncy', 'BEST_ESTIMATE_FCF') is dropped as all entries are nan.\n",
      "('DXY Curncy', 'BEST_PE_NXT_YR') is dropped as all entries are nan.\n",
      "('DXY Curncy', 'BEST_ANALYST_RATING') is dropped as all entries are nan.\n",
      "('USGG10YR Index', 'BEST_PX_BPS_RATIO') is dropped as all entries are nan.\n",
      "('USGG10YR Index', 'BEST_ESTIMATE_FCF') is dropped as all entries are nan.\n",
      "('USGG10YR Index', 'BEST_PE_NXT_YR') is dropped as all entries are nan.\n",
      "('USGG10YR Index', 'BEST_ANALYST_RATING') is dropped as all entries are nan.\n",
      "('USGG2YR Index', 'BEST_PX_BPS_RATIO') is dropped as all entries are nan.\n",
      "('USGG2YR Index', 'BEST_ESTIMATE_FCF') is dropped as all entries are nan.\n",
      "('USGG2YR Index', 'BEST_PE_NXT_YR') is dropped as all entries are nan.\n",
      "('USGG2YR Index', 'BEST_ANALYST_RATING') is dropped as all entries are nan.\n",
      "('AAPL US Equity', 'BEST_PX_BPS_RATIO') is dropped as there are 93.3% nan's\n",
      "('AAPL US Equity', 'BEST_ESTIMATE_FCF') is dropped as there are 93.4% nan's\n",
      "('AAPL US Equity', 'BEST_PE_NXT_YR') is dropped as there are 89.1% nan's\n",
      "('AAPL US Equity', 'BEST_ANALYST_RATING') is dropped as there are 72.7% nan's\n",
      "('AMD US Equity', 'BEST_PX_BPS_RATIO') is dropped as there are 94.0% nan's\n",
      "('AMD US Equity', 'BEST_ESTIMATE_FCF') is dropped as there are 94.7% nan's\n",
      "('AMD US Equity', 'BEST_PE_NXT_YR') is dropped as there are 92.1% nan's\n",
      "('AMD US Equity', 'BEST_ANALYST_RATING') is dropped as there are 41.1% nan's\n",
      "('AMZN US Equity', 'PX_LAST') is dropped as there are 30.4% nan's\n",
      "('AMZN US Equity', 'PX_OPEN') is dropped as there are 30.4% nan's\n",
      "('AMZN US Equity', 'PX_HIGH') is dropped as there are 30.4% nan's\n",
      "('AMZN US Equity', 'PX_LOW') is dropped as there are 30.4% nan's\n",
      "('AMZN US Equity', 'BEST_PX_BPS_RATIO') is dropped as there are 94.0% nan's\n",
      "('AMZN US Equity', 'PX_VOLUME') is dropped as there are 30.4% nan's\n",
      "('AMZN US Equity', 'BEST_ESTIMATE_FCF') is dropped as there are 93.8% nan's\n",
      "('AMZN US Equity', 'BEST_PE_NXT_YR') is dropped as there are 83.9% nan's\n",
      "('AMZN US Equity', 'BEST_ANALYST_RATING') is dropped as there are 63.6% nan's\n",
      "('NVDA US Equity', 'PX_LAST') is dropped as there are 35.2% nan's\n",
      "('NVDA US Equity', 'PX_OPEN') is dropped as there are 35.2% nan's\n",
      "('NVDA US Equity', 'PX_HIGH') is dropped as there are 35.2% nan's\n",
      "('NVDA US Equity', 'PX_LOW') is dropped as there are 35.2% nan's\n",
      "('NVDA US Equity', 'BEST_PX_BPS_RATIO') is dropped as there are 94.1% nan's\n",
      "('NVDA US Equity', 'PX_VOLUME') is dropped as there are 35.2% nan's\n",
      "('NVDA US Equity', 'BEST_ESTIMATE_FCF') is dropped as there are 94.2% nan's\n",
      "('NVDA US Equity', 'BEST_PE_NXT_YR') is dropped as there are 93.4% nan's\n",
      "('NVDA US Equity', 'BEST_ANALYST_RATING') is dropped as there are 42.1% nan's\n",
      "('VIX Index', 'PX_LAST') is dropped as there are 9.8% nan's\n",
      "('VIX Index', 'PX_OPEN') is dropped as there are 15.4% nan's\n",
      "('VIX Index', 'PX_HIGH') is dropped as there are 15.4% nan's\n",
      "('VIX Index', 'PX_LOW') is dropped as there are 15.4% nan's\n",
      "('MSFT US Equity', 'BEST_PX_BPS_RATIO') is dropped as there are 92.5% nan's\n",
      "('MSFT US Equity', 'BEST_ESTIMATE_FCF') is dropped as there are 93.4% nan's\n",
      "('MSFT US Equity', 'BEST_PE_NXT_YR') is dropped as there are 92.4% nan's\n",
      "('MSFT US Equity', 'BEST_ANALYST_RATING') is dropped as there are 55.8% nan's\n",
      "('CCMP Index', 'BEST_PX_BPS_RATIO') is dropped as there are 53.9% nan's\n",
      "('CCMP Index', 'PX_VOLUME') is dropped as there are 23.8% nan's\n",
      "('GOOGL US Equity', 'PX_LAST') is dropped as there are 50.8% nan's\n",
      "('GOOGL US Equity', 'PX_OPEN') is dropped as there are 50.8% nan's\n",
      "('GOOGL US Equity', 'PX_HIGH') is dropped as there are 50.8% nan's\n",
      "('GOOGL US Equity', 'PX_LOW') is dropped as there are 50.8% nan's\n",
      "('GOOGL US Equity', 'BEST_PX_BPS_RATIO') is dropped as there are 94.0% nan's\n",
      "('GOOGL US Equity', 'PX_VOLUME') is dropped as there are 50.8% nan's\n",
      "('GOOGL US Equity', 'BEST_ESTIMATE_FCF') is dropped as there are 95.1% nan's\n",
      "('GOOGL US Equity', 'BEST_PE_NXT_YR') is dropped as there are 89.0% nan's\n",
      "('GOOGL US Equity', 'BEST_ANALYST_RATING') is dropped as there are 55.4% nan's\n",
      "('USGG10YR Index', 'PX_VOLUME') is dropped as there are 77.5% nan's\n",
      "('USGG2YR Index', 'PX_OPEN') is dropped as there are 34.4% nan's\n",
      "('USGG2YR Index', 'PX_HIGH') is dropped as there are 34.4% nan's\n",
      "('USGG2YR Index', 'PX_LOW') is dropped as there are 34.4% nan's\n",
      "('USGG2YR Index', 'PX_VOLUME') is dropped as there are 77.5% nan's\n",
      "('AMZN US Equity', 'RSI_14D') is dropped as there are 30.4% nan's\n",
      "('AMZN US Equity', 'RSI_30D') is dropped as there are 30.4% nan's\n",
      "('NVDA US Equity', 'RSI_14D') is dropped as there are 35.2% nan's\n",
      "('NVDA US Equity', 'RSI_30D') is dropped as there are 35.2% nan's\n",
      "('VIX Index', 'RSI_14D') is dropped as there are 9.8% nan's\n",
      "('VIX Index', 'RSI_30D') is dropped as there are 9.8% nan's\n",
      "('GOOGL US Equity', 'RSI_14D') is dropped as there are 50.8% nan's\n",
      "('GOOGL US Equity', 'RSI_30D') is dropped as there are 50.8% nan's\n",
      "('VIX Index', 'SMA5') is dropped as there are 9.9% nan's\n",
      "('VIX Index', 'SMA10') is dropped as there are 9.9% nan's\n",
      "('VIX Index', 'SMA20') is dropped as there are 10.0% nan's\n",
      "('CCMP Index', 'SHOOT_STAR') is dropped as there are 98.5% 0's\n",
      "('CCMP Index', 'DOJI_STAR') is dropped as there are 97.9% 0's\n",
      "('CCMP Index', 'INV_HAM') is dropped as there are 99.1% 0's\n",
      "('CCMP Index', 'EV_STAR') is dropped as there are 99.3% 0's\n",
      "('CCMP Index', 'AB_BABY') is dropped as there are 100.0% 0's\n",
      "('CCMP Index', 'LINE_STRIKE') is dropped as there are 99.9% 0's\n",
      "('CCMP Index', 'BLK_CROWS') is dropped as there are 100.0% 0's\n",
      "('CCMP Index', 'GAP_CROWS') is dropped as there are 99.9% 0's\n",
      "('CCMP Index', 'HT_TRENDMODE') is dropped as there are 25.3% 0's\n"
     ]
    }
   ],
   "source": [
    "# drop all columns that has no data\n",
    "for col_name in d1.columns[(d1.isna()).all()]:\n",
    "    print(f\"{col_name} is dropped as all entries are nan.\")\n",
    "d1.drop(d1.columns[(d1.isna()).all()], axis=1, inplace=True)\n",
    "\n",
    "# drop all columns that has \"NA\" more than the designated % \n",
    "drop_pct = 0.050\n",
    "for col_name, nan_pct in (d1.isna().sum() / len(d1)).iteritems():\n",
    "    if nan_pct > drop_pct:\n",
    "        print(f\"{col_name} is dropped as there are {nan_pct*100:.1f}% nan's\")\n",
    "d1.drop(d1.columns[\n",
    "    d1.isna().sum() / len(d1) > drop_pct\n",
    "], axis=1, inplace=True)\n",
    "\n",
    "for col_name, zero_pct in ((d1.fillna(0) == 0).sum() / len(d1)).iteritems():\n",
    "    if zero_pct > drop_pct:\n",
    "        print(f\"{col_name} is dropped as there are {zero_pct*100:.1f}% 0's\")\n",
    "d1.drop(d1.columns[\n",
    "    (d1.fillna(0) == 0).sum() / len(d1) == 1\n",
    "], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24a452dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col_name, nan_pct in (d1.isna().sum() / len(d1)).iteritems():\n",
    "    if nan_pct > 0.05:\n",
    "        print(f\"{str(col_name):30}  {nan_pct*100:.1f}% nan's\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb3132af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='None-None'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(rc = {'figure.figsize':(40,10)})\n",
    "sns.heatmap(d1.isnull(),yticklabels=False, cmap = 'viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8dae3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AAPL US Equity', 'PX_LAST'), ('AAPL US Equity', 'PX_OPEN'), ('AAPL US Equity', 'PX_HIGH'), ('AAPL US Equity', 'PX_LOW'), ('AAPL US Equity', 'PX_VOLUME'), ('AMD US Equity', 'PX_LAST'), ('AMD US Equity', 'PX_OPEN'), ('AMD US Equity', 'PX_HIGH'), ('AMD US Equity', 'PX_LOW'), ('AMD US Equity', 'PX_VOLUME'), ('MSFT US Equity', 'PX_LAST'), ('MSFT US Equity', 'PX_OPEN'), ('MSFT US Equity', 'PX_HIGH'), ('MSFT US Equity', 'PX_LOW'), ('MSFT US Equity', 'PX_VOLUME'), ('AUDJPY Curncy', 'PX_LAST'), ('AUDJPY Curncy', 'PX_OPEN'), ('AUDJPY Curncy', 'PX_HIGH'), ('AUDJPY Curncy', 'PX_LOW'), ('CCMP Index', 'PX_LAST'), ('CCMP Index', 'PX_OPEN'), ('CCMP Index', 'PX_HIGH'), ('CCMP Index', 'PX_LOW'), ('DXY Curncy', 'PX_LAST'), ('DXY Curncy', 'PX_OPEN'), ('DXY Curncy', 'PX_HIGH'), ('DXY Curncy', 'PX_LOW'), ('USGG10YR Index', 'PX_LAST'), ('USGG10YR Index', 'PX_OPEN'), ('USGG10YR Index', 'PX_HIGH'), ('USGG10YR Index', 'PX_LOW'), ('USGG2YR Index', 'PX_LAST'), ('AAPL US Equity', 'RSI_14D'), ('AAPL US Equity', 'RSI_30D'), ('AMD US Equity', 'RSI_14D'), ('AMD US Equity', 'RSI_30D'), ('MSFT US Equity', 'RSI_14D'), ('MSFT US Equity', 'RSI_30D'), ('AUDJPY Curncy', 'RSI_14D'), ('AUDJPY Curncy', 'RSI_30D'), ('CCMP Index', 'RSI_14D'), ('CCMP Index', 'RSI_30D'), ('DXY Curncy', 'RSI_14D'), ('DXY Curncy', 'RSI_30D'), ('USGG10YR Index', 'RSI_14D'), ('USGG10YR Index', 'RSI_30D'), ('USGG2YR Index', 'RSI_14D'), ('USGG2YR Index', 'RSI_30D'), ('CCMP Index', 'BB_H_DEV'), ('CCMP Index', 'BB_M_DEV'), ('CCMP Index', 'BB_L_DEV'), ('CCMP Index', 'SMA10'), ('CCMP Index', 'SMA20'), ('CCMP Index', 'SHOOT_STAR'), ('CCMP Index', 'DOJI_STAR'), ('CCMP Index', 'INV_HAM'), ('CCMP Index', 'EV_STAR'), ('CCMP Index', 'AB_BABY'), ('CCMP Index', 'LINE_STRIKE'), ('CCMP Index', 'BLK_CROWS'), ('CCMP Index', 'GAP_CROWS'), ('CCMP Index', 'ADX'), ('CCMP Index', 'HT_TRENDMODE')]\n"
     ]
    }
   ],
   "source": [
    "print(list(d1.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acffa8aa",
   "metadata": {},
   "source": [
    "### Create the column with the return of CCMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d4b3036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1986-06-30      405.51\n",
       "1986-07-01      407.61\n",
       "1986-07-02      409.48\n",
       "1986-07-03      411.16\n",
       "1986-07-04      411.16\n",
       "                ...   \n",
       "2022-03-18    13893.84\n",
       "2022-03-21    13838.46\n",
       "2022-03-22    14108.82\n",
       "2022-03-23    13922.60\n",
       "2022-03-24    13922.60\n",
       "Name: (CCMP Index, PX_LAST), Length: 9324, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the PX_LAST(Last Price) of CCMP Index\n",
    "d1[('CCMP Index','PX_LAST')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c98a5475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1986-06-30      407.61\n",
       "1986-07-01      409.48\n",
       "1986-07-02      411.16\n",
       "1986-07-03      411.16\n",
       "1986-07-04      400.96\n",
       "                ...   \n",
       "2022-03-18    13838.46\n",
       "2022-03-21    14108.82\n",
       "2022-03-22    13922.60\n",
       "2022-03-23    13922.60\n",
       "2022-03-24         NaN\n",
       "Name: (CCMP Index, PX_LAST), Length: 9324, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This shifts the price from time \"t+1\" to time \"t\"\n",
    "d1[('CCMP Index','PX_LAST')].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8571dc84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1986-06-30         NaN\n",
       "1986-07-01      405.51\n",
       "1986-07-02      407.61\n",
       "1986-07-03      409.48\n",
       "1986-07-04      411.16\n",
       "                ...   \n",
       "2022-03-18    13614.78\n",
       "2022-03-21    13893.84\n",
       "2022-03-22    13838.46\n",
       "2022-03-23    14108.82\n",
       "2022-03-24    13922.60\n",
       "Name: (CCMP Index, PX_LAST), Length: 9324, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This shifts the price from time \"t\" to time \"t+1\"\n",
    "d1[('CCMP Index','PX_LAST')].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4bb09dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This 'CCMP_Daily return' records the  return of the stock  = price @ t / price @ t-1 \n",
    "d1['CCMP_Daily return'] = d1[('CCMP Index','PX_LAST')]/d1[('CCMP Index','PX_LAST')].shift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c512a4fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1986-06-30         NaN\n",
       "1986-07-01    0.005165\n",
       "1986-07-02    0.004577\n",
       "1986-07-03    0.004094\n",
       "1986-07-04    0.000000\n",
       "                ...   \n",
       "2022-03-18    0.020290\n",
       "2022-03-21   -0.003994\n",
       "2022-03-22    0.019348\n",
       "2022-03-23   -0.013287\n",
       "2022-03-24    0.000000\n",
       "Name: CCMP_Daily return, Length: 9324, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This calculates the log return of the data.\n",
    "np.log(d1['CCMP_Daily return'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c303d7cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1986-06-30         NaN\n",
       "1986-07-01    0.005165\n",
       "1986-07-02    0.004577\n",
       "1986-07-03    0.004094\n",
       "1986-07-04    0.000000\n",
       "                ...   \n",
       "2022-03-18    0.020290\n",
       "2022-03-21   -0.003994\n",
       "2022-03-22    0.019348\n",
       "2022-03-23   -0.013287\n",
       "2022-03-24    0.000000\n",
       "Name: (CCMP Index, PX_LAST), Length: 9324, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This also calculates the log return of the data.\n",
    "np.log(d1['CCMP Index','PX_LAST'])-np.log(d1[('CCMP Index','PX_LAST')].shift(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "147bae7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.04764934088446 %\n"
     ]
    }
   ],
   "source": [
    "#The mean daily return of CCMP\n",
    "print(np.mean(d1['CCMP_Daily return'])*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7203707",
   "metadata": {},
   "source": [
    "We attempt to enforce the $\\mu_1$ to hover around the following value (only on the training set to make sure no information outside of the training set is given to the training parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "276c98c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003792902674989007"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est_mu1 = np.mean(np.log(d1['CCMP_Daily return'] ))\n",
    "est_mu1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8c06c0",
   "metadata": {},
   "source": [
    "Take a log of the PX_LAST of the target asset as well as the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e9685fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1['AMD_log_PX_LAST'] = np.log(d1['AMD US Equity','PX_LAST'])\n",
    "d1['AUDJPY_log_PX_LAST'] = np.log(d1['AUDJPY Curncy','PX_LAST'])\n",
    "d1['MSFT_log_PX_LAST'] = np.log(d1['MSFT US Equity','PX_LAST'])\n",
    "d1['AAPL_log_PX_LAST'] = np.log(d1['AAPL US Equity','PX_LAST'])\n",
    "d1['USGG10YR_log_PX_LAST'] = np.log(d1['USGG10YR Index','PX_LAST'])\n",
    "d1['USGG2YR_log_PX_LAST'] = np.log(d1['USGG2YR Index','PX_LAST'])\n",
    "\n",
    "d1['CCMP_log_PX_LAST'] = np.log(d1['CCMP Index','PX_LAST'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1aa9ad",
   "metadata": {},
   "source": [
    "Target would be the log return of CCMP sharep price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef806b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1['AMD_log_diff'] = d1['AMD_log_PX_LAST'].diff()\n",
    "d1['AUDJPY_log_diff'] = d1['AUDJPY_log_PX_LAST'].diff()\n",
    "d1['MSFT_log_diff'] = d1['MSFT_log_PX_LAST'].diff()\n",
    "d1['AAPL_log_diff'] = d1['AAPL_log_PX_LAST'].diff()\n",
    "d1['USGG10YR_log_diff'] = d1['USGG10YR_log_PX_LAST'].diff()\n",
    "d1['USGG2YR_log_diff'] = d1['USGG2YR_log_PX_LAST'].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "075a53a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,11):\n",
    "    d1[f'CCMP_log_diff_{i}d'] = d1['CCMP_log_PX_LAST'].diff(i)\n",
    "    d1[f'CCMP_log_diff_{i}d'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e3f9668",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AAPL US Equity', 'PX_LAST'), ('AAPL US Equity', 'PX_OPEN'), ('AAPL US Equity', 'PX_HIGH'), ('AAPL US Equity', 'PX_LOW'), ('AAPL US Equity', 'PX_VOLUME'), ('AMD US Equity', 'PX_LAST'), ('AMD US Equity', 'PX_OPEN'), ('AMD US Equity', 'PX_HIGH'), ('AMD US Equity', 'PX_LOW'), ('AMD US Equity', 'PX_VOLUME'), ('MSFT US Equity', 'PX_LAST'), ('MSFT US Equity', 'PX_OPEN'), ('MSFT US Equity', 'PX_HIGH'), ('MSFT US Equity', 'PX_LOW'), ('MSFT US Equity', 'PX_VOLUME'), ('AUDJPY Curncy', 'PX_LAST'), ('AUDJPY Curncy', 'PX_OPEN'), ('AUDJPY Curncy', 'PX_HIGH'), ('AUDJPY Curncy', 'PX_LOW'), ('CCMP Index', 'PX_LAST'), ('CCMP Index', 'PX_OPEN'), ('CCMP Index', 'PX_HIGH'), ('CCMP Index', 'PX_LOW'), ('DXY Curncy', 'PX_LAST'), ('DXY Curncy', 'PX_OPEN'), ('DXY Curncy', 'PX_HIGH'), ('DXY Curncy', 'PX_LOW'), ('USGG10YR Index', 'PX_LAST'), ('USGG10YR Index', 'PX_OPEN'), ('USGG10YR Index', 'PX_HIGH'), ('USGG10YR Index', 'PX_LOW'), ('USGG2YR Index', 'PX_LAST'), ('AAPL US Equity', 'RSI_14D'), ('AAPL US Equity', 'RSI_30D'), ('AMD US Equity', 'RSI_14D'), ('AMD US Equity', 'RSI_30D'), ('MSFT US Equity', 'RSI_14D'), ('MSFT US Equity', 'RSI_30D'), ('AUDJPY Curncy', 'RSI_14D'), ('AUDJPY Curncy', 'RSI_30D'), ('CCMP Index', 'RSI_14D'), ('CCMP Index', 'RSI_30D'), ('DXY Curncy', 'RSI_14D'), ('DXY Curncy', 'RSI_30D'), ('USGG10YR Index', 'RSI_14D'), ('USGG10YR Index', 'RSI_30D'), ('USGG2YR Index', 'RSI_14D'), ('USGG2YR Index', 'RSI_30D'), ('CCMP Index', 'BB_H_DEV'), ('CCMP Index', 'BB_M_DEV'), ('CCMP Index', 'BB_L_DEV'), ('CCMP Index', 'SMA10'), ('CCMP Index', 'SMA20'), ('CCMP Index', 'SHOOT_STAR'), ('CCMP Index', 'DOJI_STAR'), ('CCMP Index', 'INV_HAM'), ('CCMP Index', 'EV_STAR'), ('CCMP Index', 'AB_BABY'), ('CCMP Index', 'LINE_STRIKE'), ('CCMP Index', 'BLK_CROWS'), ('CCMP Index', 'GAP_CROWS'), ('CCMP Index', 'ADX'), ('CCMP Index', 'HT_TRENDMODE'), ('CCMP_Daily return', ''), ('AMD_log_PX_LAST', ''), ('AUDJPY_log_PX_LAST', ''), ('MSFT_log_PX_LAST', ''), ('AAPL_log_PX_LAST', ''), ('USGG10YR_log_PX_LAST', ''), ('USGG2YR_log_PX_LAST', ''), ('CCMP_log_PX_LAST', ''), ('AMD_log_diff', ''), ('AUDJPY_log_diff', ''), ('MSFT_log_diff', ''), ('AAPL_log_diff', ''), ('USGG10YR_log_diff', ''), ('USGG2YR_log_diff', ''), ('CCMP_log_diff_1d', ''), ('CCMP_log_diff_2d', ''), ('CCMP_log_diff_3d', ''), ('CCMP_log_diff_4d', ''), ('CCMP_log_diff_5d', ''), ('CCMP_log_diff_6d', ''), ('CCMP_log_diff_7d', ''), ('CCMP_log_diff_8d', ''), ('CCMP_log_diff_9d', ''), ('CCMP_log_diff_10d', '')]\n"
     ]
    }
   ],
   "source": [
    "print(list(d1.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca003286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1986-06-30         NaN\n",
       "1986-07-01         NaN\n",
       "1986-07-02         NaN\n",
       "1986-07-03         NaN\n",
       "1986-07-04         NaN\n",
       "1986-07-07         NaN\n",
       "1986-07-08         NaN\n",
       "1986-07-09         NaN\n",
       "1986-07-10         NaN\n",
       "1986-07-11         NaN\n",
       "1986-07-14   -0.052422\n",
       "1986-07-15   -0.073566\n",
       "1986-07-16   -0.075428\n",
       "1986-07-17   -0.074636\n",
       "1986-07-18   -0.075999\n",
       "Name: CCMP_log_diff_10d, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " d1[f'CCMP_log_diff_10d'].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56b1d94c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(1,11): \n",
    "    plt.hist(d1[f'CCMP_log_diff_{i}d'], bins=100, color = 'lightblue', ec = 'skyblue')\n",
    "    plt.ylim((0, d1[f'CCMP_log_diff_{i}d'].count()/5))\n",
    "    plt.title(f'{i}days')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fd7d5df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We would need to shift one time step backward \n",
    "d1['target'] = d1['CCMP_log_diff_1d'].shift(-1)\n",
    "d1['target10'] = d1['CCMP_log_diff_10d'].shift(-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000053ca",
   "metadata": {},
   "source": [
    "The first roll of \"MSFT_log_diff\" would be NA as there would be no value to subtract from, yet we will fill it with zero for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e5cff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1['CCMP_log_diff_1d'] = d1['CCMP_log_diff_1d'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a0a35c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2022-03-18    0.020290\n",
       " 2022-03-21   -0.003994\n",
       " 2022-03-22    0.019348\n",
       " 2022-03-23   -0.013287\n",
       " 2022-03-24    0.000000\n",
       " Name: CCMP_log_diff_1d, dtype: float64,\n",
       " 2022-03-18   -0.003994\n",
       " 2022-03-21    0.019348\n",
       " 2022-03-22   -0.013287\n",
       " 2022-03-23    0.000000\n",
       " 2022-03-24         NaN\n",
       " Name: target, dtype: float64,\n",
       " 2022-03-09    0.049097\n",
       " 2022-03-10    0.058617\n",
       " 2022-03-11         NaN\n",
       " 2022-03-14         NaN\n",
       " 2022-03-15         NaN\n",
       " 2022-03-16         NaN\n",
       " 2022-03-17         NaN\n",
       " 2022-03-18         NaN\n",
       " 2022-03-21         NaN\n",
       " 2022-03-22         NaN\n",
       " 2022-03-23         NaN\n",
       " 2022-03-24         NaN\n",
       " Name: target10, dtype: float64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1['CCMP_log_diff_1d'].tail(), d1['target'].tail(), d1['target10'].tail(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa90b31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have to drop the last 10 rows as d1['target10'] has no value for the last 10 row.\n",
    "d1.drop(d1.tail(10).index, inplace = True)\n",
    "\n",
    "# Have to drop the first row as d1['NVDA_log_diff_1d'] has no value on the first row. \n",
    "# If we are to deal with the k-days as d1['NVDA_log_diff_{k}d'] has no value on the first {k} rows. \n",
    "# We would have to drop more rows.\n",
    "# But for NN of 1 day, we should have taken away the first roll.\n",
    "# We might have to drop all the 10 rows in the front if appropriate.\n",
    "\n",
    "d1.drop(d1.head(1).index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa90ed31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(d1['CCMP_log_diff_1d'], bins=5000, color = 'lightblue', ec = 'skyblue')\n",
    "plt.ylim((0, 30))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72129108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(d1['target'], bins=100, color = 'lightblue', ec = 'skyblue')\n",
    "plt.ylim((0, 1000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5bd26270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AAPL US Equity', 'PX_LAST'), ('AAPL US Equity', 'PX_OPEN'), ('AAPL US Equity', 'PX_HIGH'), ('AAPL US Equity', 'PX_LOW'), ('AAPL US Equity', 'PX_VOLUME'), ('AMD US Equity', 'PX_LAST'), ('AMD US Equity', 'PX_OPEN'), ('AMD US Equity', 'PX_HIGH'), ('AMD US Equity', 'PX_LOW'), ('AMD US Equity', 'PX_VOLUME'), ('MSFT US Equity', 'PX_LAST'), ('MSFT US Equity', 'PX_OPEN'), ('MSFT US Equity', 'PX_HIGH'), ('MSFT US Equity', 'PX_LOW'), ('MSFT US Equity', 'PX_VOLUME'), ('AUDJPY Curncy', 'PX_LAST'), ('AUDJPY Curncy', 'PX_OPEN'), ('AUDJPY Curncy', 'PX_HIGH'), ('AUDJPY Curncy', 'PX_LOW'), ('CCMP Index', 'PX_LAST'), ('CCMP Index', 'PX_OPEN'), ('CCMP Index', 'PX_HIGH'), ('CCMP Index', 'PX_LOW'), ('DXY Curncy', 'PX_LAST'), ('DXY Curncy', 'PX_OPEN'), ('DXY Curncy', 'PX_HIGH'), ('DXY Curncy', 'PX_LOW'), ('USGG10YR Index', 'PX_LAST'), ('USGG10YR Index', 'PX_OPEN'), ('USGG10YR Index', 'PX_HIGH'), ('USGG10YR Index', 'PX_LOW'), ('USGG2YR Index', 'PX_LAST'), ('AAPL US Equity', 'RSI_14D'), ('AAPL US Equity', 'RSI_30D'), ('AMD US Equity', 'RSI_14D'), ('AMD US Equity', 'RSI_30D'), ('MSFT US Equity', 'RSI_14D'), ('MSFT US Equity', 'RSI_30D'), ('AUDJPY Curncy', 'RSI_14D'), ('AUDJPY Curncy', 'RSI_30D'), ('CCMP Index', 'RSI_14D'), ('CCMP Index', 'RSI_30D'), ('DXY Curncy', 'RSI_14D'), ('DXY Curncy', 'RSI_30D'), ('USGG10YR Index', 'RSI_14D'), ('USGG10YR Index', 'RSI_30D'), ('USGG2YR Index', 'RSI_14D'), ('USGG2YR Index', 'RSI_30D'), ('CCMP Index', 'BB_H_DEV'), ('CCMP Index', 'BB_M_DEV'), ('CCMP Index', 'BB_L_DEV'), ('CCMP Index', 'SMA10'), ('CCMP Index', 'SMA20'), ('CCMP Index', 'SHOOT_STAR'), ('CCMP Index', 'DOJI_STAR'), ('CCMP Index', 'INV_HAM'), ('CCMP Index', 'EV_STAR'), ('CCMP Index', 'AB_BABY'), ('CCMP Index', 'LINE_STRIKE'), ('CCMP Index', 'BLK_CROWS'), ('CCMP Index', 'GAP_CROWS'), ('CCMP Index', 'ADX'), ('CCMP Index', 'HT_TRENDMODE'), ('CCMP_Daily return', ''), ('AMD_log_PX_LAST', ''), ('AUDJPY_log_PX_LAST', ''), ('MSFT_log_PX_LAST', ''), ('AAPL_log_PX_LAST', ''), ('USGG10YR_log_PX_LAST', ''), ('USGG2YR_log_PX_LAST', ''), ('CCMP_log_PX_LAST', ''), ('AMD_log_diff', ''), ('AUDJPY_log_diff', ''), ('MSFT_log_diff', ''), ('AAPL_log_diff', ''), ('USGG10YR_log_diff', ''), ('USGG2YR_log_diff', ''), ('CCMP_log_diff_1d', ''), ('CCMP_log_diff_2d', ''), ('CCMP_log_diff_3d', ''), ('CCMP_log_diff_4d', ''), ('CCMP_log_diff_5d', ''), ('CCMP_log_diff_6d', ''), ('CCMP_log_diff_7d', ''), ('CCMP_log_diff_8d', ''), ('CCMP_log_diff_9d', ''), ('CCMP_log_diff_10d', ''), ('target', ''), ('target10', '')]\n"
     ]
    }
   ],
   "source": [
    "print(list(d1.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb3793cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9313,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of the target10.\n",
    "d1.target10.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa2103cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the data by applying standard scaler\n",
    "SC1 = StandardScaler()\n",
    "SC2 = StandardScaler()\n",
    "SC3 = StandardScaler()\n",
    "\n",
    "d2 = pd.DataFrame()\n",
    "d2['CCMP_log_PX_LAST'] = d1['CCMP_log_PX_LAST']\n",
    "d2['AMD_log_PX_LAST'] = d1['AMD_log_PX_LAST']\n",
    "d2['AUDJPY_log_PX_LAST'] = d1['AUDJPY_log_PX_LAST']\n",
    "d2['AAPL_log_PX_LAST'] = d1['AAPL_log_PX_LAST']\n",
    "d2['MSFT_log_PX_LAST'] = d1['MSFT_log_PX_LAST']\n",
    "d2['USGG10YR_log_PX_LAST'] = d1['USGG10YR_log_PX_LAST']\n",
    "d2['USGG2YR_log_PX_LAST'] = d1['USGG2YR_log_PX_LAST']\n",
    "d2['CCMP_log_diff'] = d1['CCMP_log_diff_1d']\n",
    "d2['AMD_log_diff'] = d1['AMD_log_diff']\n",
    "d2['MSFT_log_diff'] = d1['MSFT_log_diff']\n",
    "d2['AAPL_log_diff'] = d1['AAPL_log_diff']\n",
    "d2['USGG10YR_log_diff'] = d1['USGG10YR_log_diff']\n",
    "d2['USGG2YR_log_diff'] = d1['USGG2YR_log_diff']\n",
    "d2['AUDJPY_log_diff'] = d1['AUDJPY_log_diff']\n",
    "\n",
    "new_d1 = d2\n",
    "# new_d1 = np.array(d2).reshape(-1,1)\n",
    "# target = np.array(d1['NVDA_log_diff']).reshape(-1,1)\n",
    "\n",
    "# Have to convert the dataframe that we are dealing with to numpy array.\n",
    "new_d1 = np.array(new_d1)\n",
    "\n",
    "train_days = int(new_d1.shape[0]/10)\n",
    "\n",
    "# Validation set and Test set are set to be the 10% of the total dataset\n",
    "new_d1[:-train_days] = SC1.fit_transform(new_d1[:-train_days])\n",
    "new_d1[-train_days:] = SC1.transform(new_d1[-train_days:])\n",
    "\n",
    "target = np.array(d1[('target')])\n",
    "target = target.reshape(-1,1)\n",
    "target[:-train_days] = SC2.fit_transform(target[:-train_days])\n",
    "target[-train_days:] = SC2.transform(target[-train_days:])\n",
    "\n",
    "target10 = np.array(d1[('target10')])\n",
    "target10 = target10.reshape(-1,1)\n",
    "target10[:-train_days] = SC3.fit_transform(target10[:-train_days])\n",
    "target10[-train_days:] = SC3.transform(target10[-train_days:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652b66a9",
   "metadata": {},
   "source": [
    "### Check the scaled value of est_mu1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c21cbce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02565305]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the mean of the scaled \"target\" of the training set.\n",
    "# So we would likely have $\\mu_1$ constrained to hover around this value... \n",
    "SC2.transform(np.array(np.mean(target[:-train_days])).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0864c90b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9313, 14), (9313, 1))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the shape of target complies the shape of the feature in terms of number of time steps.\n",
    "new_d1.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c7f06695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the values into d_r\n",
    "\n",
    "d2['CCMP_log_diff'] = d1['CCMP_log_diff_1d']\n",
    "d2['AMD_log_diff'] = d1['AMD_log_diff']\n",
    "d2['MSFT_log_diff'] = d1['MSFT_log_diff']\n",
    "d2['AAPL_log_diff'] = d1['AAPL_log_diff']\n",
    "d2['USGG10YR_log_diff'] = d1['USGG10YR_log_diff']\n",
    "d2['USGG2YR_log_diff'] = d1['USGG2YR_log_diff']\n",
    "d2['AUDJPY_log_diff'] = d1['AUDJPY_log_diff']\n",
    "d_r = d2\n",
    "d_r['target'] = d1['target']\n",
    "d_r['target10'] = d1['target10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7b30a63e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9313, 16)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76f85f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.63136156e+00,  2.58017663e-01,  1.54602135e+00, ...,\n",
       "         2.37855284e-01, -1.07578964e-03, -2.62095367e+00],\n",
       "       [-1.62594138e+00,  2.58017663e-01,  1.53122995e+00, ...,\n",
       "         8.37802629e-02,  3.61332611e-03, -2.37915381e-01],\n",
       "       [-1.62109299e+00,  2.20951048e-01,  1.20406908e+00, ...,\n",
       "        -6.09660228e-01, -4.78351747e-01, -5.34642496e+00],\n",
       "       ...,\n",
       "       [ 2.44989746e+00,  3.70875674e+00, -3.46858220e-02, ...,\n",
       "         2.50929727e+00,  9.79610577e-01, -3.35460431e-01],\n",
       "       [ 2.49172069e+00,  3.78341096e+00,  1.92268720e-02, ...,\n",
       "         3.55693825e+00,  1.57461253e+00,  8.85675604e-01],\n",
       "       [ 2.48044784e+00,  3.72160398e+00,  6.61347309e-02, ...,\n",
       "         1.06711294e+00,  3.13936202e-01,  7.71118099e-01]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e68fb5a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9313, 16), (9313, 14))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_r.shape, new_d1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752c1064",
   "metadata": {},
   "source": [
    "The first 14 columns of the features would be the features for us to predict the target. All data are stored in the DataFrame d_r nice and neatly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dbc783d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CCMP_log_PX_LAST', 'AMD_log_PX_LAST', 'AUDJPY_log_PX_LAST', 'AAPL_log_PX_LAST', 'MSFT_log_PX_LAST', 'USGG10YR_log_PX_LAST', 'USGG2YR_log_PX_LAST', 'CCMP_log_diff', 'AMD_log_diff', 'MSFT_log_diff', 'AAPL_log_diff', 'USGG10YR_log_diff', 'USGG2YR_log_diff', 'AUDJPY_log_diff', 'target', 'target10']\n"
     ]
    }
   ],
   "source": [
    "print(list(d_r.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "10c5ed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = []\n",
    "trainY = []\n",
    "var_trainY = []\n",
    "\n",
    "n_future = 1\n",
    "n_past = 45\n",
    "\n",
    "for i in range(n_past,d_r.shape[0]-n_future+1):\n",
    "    trainX.append(new_d1[i-n_past:i,0:new_d1.shape[1]])\n",
    "    trainY.append(target[i-1,0])\n",
    "    var_trainY.append(target10[i-n_past:i-1].var()) # This calculate the past 44 days variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3f4f8589",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX,trainY, var_trainY = np.array(trainX),np.array(trainY), np.array(var_trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5337be9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9268, 45, 14), (9268,), (9268,))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape, trainY.shape, var_trainY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2f9e1cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Activation, Dense, LSTM, Dropout, GlobalMaxPooling1D\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import adam_v2 # not sure if this is the same\n",
    "from keras.metrics import categorical_crossentropy, BinaryAccuracy, CategoricalAccuracy, Precision, Recall\n",
    "from keras.losses import SparseCategoricalCrossentropy, BinaryCrossentropy, MeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "868dfe73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.12\n",
      "\n",
      "Keras version 2.9.0\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "import keras; \n",
    "print(\"\")\n",
    "print('Keras version',keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d7210935",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training_Set = 8337\n",
      "Validation_Set = 466\n",
      "Test_Set =  465\n",
      "X_val shape: (466, 45, 14)\n",
      "X_rtest shape: (465, 45, 14)\n",
      "Y_val shape: (466, 1)\n",
      "varY_val shape: (466, 1)\n",
      "Y_rtest shape: (465, 1)\n",
      "varY_rtest shape: (465, 1)\n"
     ]
    }
   ],
   "source": [
    "n_val = int(trainY.shape[0] - trainY[:-train_days].shape[0])\n",
    "n_rtest = int(n_val/2)\n",
    "print('Training_Set =', trainX.shape[0]-n_val)\n",
    "print('Validation_Set =',n_val - n_rtest)\n",
    "print(\"Test_Set = \", n_rtest)\n",
    "X_train, Y_train, varY_train = trainX[:-n_val], trainY[:-n_val], var_trainY[:-n_val]\n",
    "Y_train = Y_train.reshape(-1,1)\n",
    "varY_train = varY_train.reshape(-1,1)\n",
    "X_val, Y_val, varY_val = trainX[-n_val:-n_rtest], trainY[-n_val:-n_rtest], var_trainY[-n_val:-n_rtest]\n",
    "Y_val = Y_val.reshape(-1,1)\n",
    "varY_val = varY_val.reshape(-1,1)\n",
    "X_rtest, Y_rtest, varY_rtest = trainX[-n_rtest:], trainY[-n_rtest:], var_trainY[-n_rtest:]\n",
    "Y_rtest = Y_rtest.reshape(-1,1)\n",
    "varY_rtest = varY_rtest.reshape(-1,1)\n",
    "print('X_val shape:',X_val.shape)\n",
    "print('X_rtest shape:',X_rtest.shape)\n",
    "print('Y_val shape:',Y_val.reshape(-1,1).shape)\n",
    "print('varY_val shape:',varY_val.reshape(-1,1).shape)\n",
    "print('Y_rtest shape:',Y_rtest.reshape(-1,1).shape)\n",
    "print('varY_rtest shape:',varY_rtest.reshape(-1,1).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f388d6",
   "metadata": {},
   "source": [
    "# MODEL 4 - TWO GAUSSIAN MIXTURE MODEL for CCMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "07403def",
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(layer, self).__init__()\n",
    "        \n",
    "        \n",
    "    def Normal_Dist_Layer_2(x):\n",
    "        \"\"\"\n",
    "        Lambda function for generating Normal Distribution parameters\n",
    "        mu and sigma2 from a Dense(2) output.\n",
    "        Assumes tensorflow 2 backend.\n",
    "\n",
    "        Usage\n",
    "        -----\n",
    "        outputs = Dense(4)(final_layer)\n",
    "        distribution_outputs = Lambda(Normal_Dist_Layer_2)(outputs)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : tf.Tensor\n",
    "            output tensor of Dense layer\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        out_tensor : tf.Tensor\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the number of dimensions of the input\n",
    "        num_dims = len(x.get_shape())\n",
    "\n",
    "        # Separate the parameters\n",
    "        mu1,sigma2_1,mu2,sigma2_2, w1 = tf.unstack(x, num=5, axis=-1)\n",
    "\n",
    "        # Add one dimension to make the right shape\n",
    "        mu1 = tf.expand_dims(mu1, -1)\n",
    "        sigma2_1 = tf.expand_dims(sigma2_1, -1)\n",
    "        mu2 = tf.expand_dims(mu2, -1)\n",
    "        sigma2_2 = tf.expand_dims(sigma2_2, -1)    \n",
    "        w1 = tf.expand_dims(w1, -1)\n",
    "\n",
    "        # Apply a softplus to make variance 1 and variance 2 both positive\n",
    "        # But other than that, there are no other restrictions\n",
    "        sigma2_1 = tf.keras.activations.softplus(sigma2_1)\n",
    "        sigma2_2 = tf.keras.activations.softplus(sigma2_2)\n",
    "\n",
    "        # We restrict mu1 to be very close to the simple mean return of the whole training period. \n",
    "        # We aim to find the 2nd component, which could explain the abnormal log return.\n",
    "        # Would it make more sense to convert this to a more dynamic process?\n",
    "        # May be exponentially weighted? Consider the mean return of the window that we are observing?\n",
    "\n",
    "        mu1 = -0.02565305+0.002565305*tf.keras.activations.sigmoid(mu1)*1.2\n",
    "        w1 = 0.8+0.2*tf.keras.activations.sigmoid(w1)\n",
    "\n",
    "        # Join back together again\n",
    "        out_tensor = tf.stack((mu1,sigma2_1,mu2,sigma2_2, w1), axis=num_dims-1)\n",
    "\n",
    "        return out_tensor\n",
    "    \n",
    "    def negative_log_likelihood_loss_2(y_true,y_pred):\n",
    "        \"\"\"\n",
    "        Negative log-likelihood loss function.\n",
    "        Assumes tensorflow backend.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : tf.Tensor\n",
    "            Ground truth values of predicted variable.\n",
    "        y_pred : tf.Tensor\n",
    "            mu and sigma2 values of predicted distribution.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        nll : tf.Tensor\n",
    "            Negative log likelihood.\n",
    "        \"\"\"\n",
    "\n",
    "        # Separate the parameters\n",
    "        mu1, sigma2_1, mu2, sigma2_2, w1 = tf.unstack(y_pred, num=5, axis=-1)\n",
    "\n",
    "        # Add one dimension to make the right shape\n",
    "        mu1 = tf.expand_dims(mu1, -1)\n",
    "        sigma2_1 = tf.expand_dims(sigma2_1, -1)\n",
    "        mu2 = tf.expand_dims(mu2, -1)\n",
    "        sigma2_2 = tf.expand_dims(sigma2_2, -1)\n",
    "        w1 = tf.expand_dims(w1, -1)\n",
    "\n",
    "        # Calculate the negative log likelihood\n",
    "        nll = (-tf.math.log(w1)\n",
    "               +0.5*(tf.math.log(sigma2_1)\n",
    "                     +tf.math.square(y_true-mu1)/sigma2_1)\n",
    "               -tf.math.log(1-w1)\n",
    "               +0.5*(tf.math.log(sigma2_2)\n",
    "                     +tf.math.square(y_true-mu2)/sigma2_2)\n",
    "              )-tf.math.log(sigma2_2/sigma2_1)+k*sigma2_2\n",
    "\n",
    "        return tf.reduce_mean(nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "79638f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cbfa71d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best val_loss So Far: 3.230719486872355\n",
    "# Total elapsed time: 06h 40m 27s\n",
    "\n",
    "# Search: Running Trial #23\n",
    "\n",
    "# Value             |Best Value So Far |Hyperparameter\n",
    "# 8                 |24                |Input_Layer_LSTM_unit\n",
    "# 0                 |0.4               |Dropout_1\n",
    "# 3                 |1                 |n_layers\n",
    "# 8                 |24                |LSTM_0_units\n",
    "# 0.2               |0.4               |Dropout_0\n",
    "# 24                |32                |Final_Lstm_layer\n",
    "# 40                |40                |Final_Dense\n",
    "# 16                |16                |LSTM_1_units\n",
    "# 8                 |32                |LSTM_2_units\n",
    "# 0.2               |0                 |Dropout_2\n",
    "\n",
    "\n",
    "# def build_model(hp):\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(hp.Int('Input_Layer_LSTM_unit',min_value=8,max_value=32,step=8),\n",
    "#                    return_sequences=True, \n",
    "#                    input_shape=(X_train.shape[1],X_train.shape[2]),\n",
    "#                   ))\n",
    "#     model.add(Dropout(hp.Float('Dropout_1',min_value=0,max_value=0.5,step=0.1)))\n",
    "#     for i in range(hp.Int('n_layers', 1, 3)):\n",
    "#         model.add(LSTM(hp.Int(f'LSTM_{i}_units',min_value=8,max_value=32,step=8),kernel_initializer=glorot_uniform(),\n",
    "#                        return_sequences=True))\n",
    "#         model.add(Dropout(hp.Float(f'Dropout_{i}',min_value=0,max_value=0.5,step=0.1)))\n",
    "    \n",
    "#     model.add(LSTM(hp.Int('Final_Lstm_layer',min_value=8,max_value=32,step=8)))\n",
    "#     model.add(Dense(hp.Int('Final_Dense',min_value=16,max_value=64,step=8), activation='relu'))\n",
    "#     model.add(Dense(5))\n",
    "#     model.add(Lambda(Normal_Dist_Layer_2))\n",
    "\n",
    "#     model.compile(loss = negative_log_likelihood_loss_2, \n",
    "#                    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n",
    "#                    )\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00859ace",
   "metadata": {},
   "source": [
    "# TRAINING OF MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36e1c3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 45, 24)       3744        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 45, 24)       0           ['lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 45, 16)       2624        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 45, 16)       0           ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  (None, 32)           6272        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 40)           1320        ['lstm_2[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 5)            205         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " tf.unstack (TFOpLambda)        [(None,),            0           ['dense_1[0][0]']                \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims (TFOpLambda)    (None, 1)            0           ['tf.unstack[0][0]']             \n",
      "                                                                                                  \n",
      " tf.math.sigmoid (TFOpLambda)   (None, 1)            0           ['tf.expand_dims[0][0]']         \n",
      "                                                                                                  \n",
      " tf.expand_dims_4 (TFOpLambda)  (None, 1)            0           ['tf.unstack[0][4]']             \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLambda)  (None, 1)            0           ['tf.math.sigmoid[0][0]']        \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_1 (TFOpLambda)  (None, 1)           0           ['tf.expand_dims_4[0][0]']       \n",
      "                                                                                                  \n",
      " tf.math.multiply_1 (TFOpLambda  (None, 1)           0           ['tf.math.multiply[0][0]']       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_1 (TFOpLambda)  (None, 1)            0           ['tf.unstack[0][1]']             \n",
      "                                                                                                  \n",
      " tf.expand_dims_3 (TFOpLambda)  (None, 1)            0           ['tf.unstack[0][3]']             \n",
      "                                                                                                  \n",
      " tf.math.multiply_2 (TFOpLambda  (None, 1)           0           ['tf.math.sigmoid_1[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 1)           0           ['tf.math.multiply_1[0][0]']     \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.softplus (TFOpLambda)  (None, 1)            0           ['tf.expand_dims_1[0][0]']       \n",
      "                                                                                                  \n",
      " tf.expand_dims_2 (TFOpLambda)  (None, 1)            0           ['tf.unstack[0][2]']             \n",
      "                                                                                                  \n",
      " tf.math.softplus_1 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_3[0][0]']       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 1)           0           ['tf.math.multiply_2[0][0]']     \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.stack (TFOpLambda)          (None, 5, 1)         0           ['tf.__operators__.add[0][0]',   \n",
      "                                                                  'tf.math.softplus[0][0]',       \n",
      "                                                                  'tf.expand_dims_2[0][0]',       \n",
      "                                                                  'tf.math.softplus_1[0][0]',     \n",
      "                                                                  'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _1_CCMP_t+1_0.05\n",
      "Epoch 1/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 3.2990\n",
      "Epoch 1: val_loss improved from inf to 4.08692, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.05.hdf5\n",
      "66/66 [==============================] - 9s 56ms/step - loss: 3.2990 - val_loss: 4.0869 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 2.6931\n",
      "Epoch 2: val_loss improved from 4.08692 to 3.37117, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.05.hdf5\n",
      "66/66 [==============================] - 2s 38ms/step - loss: 2.6931 - val_loss: 3.3712 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.6670\n",
      "Epoch 3: val_loss improved from 3.37117 to 2.58768, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.05.hdf5\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 1.6651 - val_loss: 2.5877 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0811\n",
      "Epoch 4: val_loss improved from 2.58768 to 2.26307, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.05.hdf5\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 1.0813 - val_loss: 2.2631 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8310\n",
      "Epoch 5: val_loss did not improve from 2.26307\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.8310 - val_loss: 2.6099 - lr: 1.0000e-04\n",
      "Epoch 6/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6994\n",
      "Epoch 6: val_loss did not improve from 2.26307\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.6994 - val_loss: 2.6484 - lr: 9.9000e-05\n",
      "Epoch 7/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6458\n",
      "Epoch 7: val_loss did not improve from 2.26307\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.6488 - val_loss: 2.4109 - lr: 9.8010e-05\n",
      "Epoch 8/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5928\n",
      "Epoch 8: val_loss improved from 2.26307 to 1.94493, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.05.hdf5\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.5907 - val_loss: 1.9449 - lr: 9.7030e-05\n",
      "Epoch 9/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5519\n",
      "Epoch 9: val_loss did not improve from 1.94493\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.5519 - val_loss: 2.2340 - lr: 9.7030e-05\n",
      "Epoch 10/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5146\n",
      "Epoch 10: val_loss did not improve from 1.94493\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.5126 - val_loss: 1.9688 - lr: 9.6060e-05\n",
      "Epoch 11/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4914\n",
      "Epoch 11: val_loss improved from 1.94493 to 1.87330, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.05.hdf5\n",
      "66/66 [==============================] - 5s 69ms/step - loss: 0.4904 - val_loss: 1.8733 - lr: 9.5099e-05\n",
      "Epoch 12/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4638\n",
      "Epoch 12: val_loss improved from 1.87330 to 1.85780, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.05.hdf5\n",
      "66/66 [==============================] - 4s 58ms/step - loss: 0.4638 - val_loss: 1.8578 - lr: 9.5099e-05\n",
      "Epoch 13/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4509\n",
      "Epoch 13: val_loss did not improve from 1.85780\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.4509 - val_loss: 1.9031 - lr: 9.5099e-05\n",
      "Epoch 14/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4339\n",
      "Epoch 14: val_loss improved from 1.85780 to 1.80354, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.05.hdf5\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.4339 - val_loss: 1.8035 - lr: 9.4148e-05\n",
      "Epoch 15/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4345\n",
      "Epoch 15: val_loss improved from 1.80354 to 1.62794, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.05.hdf5\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.4331 - val_loss: 1.6279 - lr: 9.4148e-05\n",
      "Epoch 16/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4209\n",
      "Epoch 16: val_loss did not improve from 1.62794\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 3s 38ms/step - loss: 0.4209 - val_loss: 1.8023 - lr: 9.4148e-05\n",
      "Epoch 17/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4067\n",
      "Epoch 17: val_loss did not improve from 1.62794\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 2s 38ms/step - loss: 0.4067 - val_loss: 1.6644 - lr: 9.3207e-05\n",
      "Epoch 18/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3964\n",
      "Epoch 18: val_loss did not improve from 1.62794\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3985 - val_loss: 1.8768 - lr: 9.2274e-05\n",
      "Epoch 19/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4051\n",
      "Epoch 19: val_loss improved from 1.62794 to 1.54528, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.05.hdf5\n",
      "66/66 [==============================] - 3s 38ms/step - loss: 0.4053 - val_loss: 1.5453 - lr: 9.1352e-05\n",
      "Epoch 20/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4150\n",
      "Epoch 20: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 2s 38ms/step - loss: 0.4146 - val_loss: 1.6439 - lr: 9.1352e-05\n",
      "Epoch 21/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3875\n",
      "Epoch 21: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.3875 - val_loss: 1.7158 - lr: 9.0438e-05\n",
      "Epoch 22/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3898\n",
      "Epoch 22: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3876 - val_loss: 1.7561 - lr: 8.9534e-05\n",
      "Epoch 23/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3836\n",
      "Epoch 23: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3837 - val_loss: 1.6485 - lr: 8.8638e-05\n",
      "Epoch 24/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3787\n",
      "Epoch 24: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 3s 38ms/step - loss: 0.3776 - val_loss: 1.7451 - lr: 8.7752e-05\n",
      "Epoch 25/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3660\n",
      "Epoch 25: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 3s 38ms/step - loss: 0.3649 - val_loss: 1.9303 - lr: 8.6875e-05\n",
      "Epoch 26/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3583\n",
      "Epoch 26: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 3s 38ms/step - loss: 0.3598 - val_loss: 1.6937 - lr: 8.6006e-05\n",
      "Epoch 27/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3582\n",
      "Epoch 27: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 2s 38ms/step - loss: 0.3594 - val_loss: 1.5945 - lr: 8.5146e-05\n",
      "Epoch 28/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3641\n",
      "Epoch 28: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 2s 38ms/step - loss: 0.3632 - val_loss: 1.5749 - lr: 8.4294e-05\n",
      "Epoch 29/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3536\n",
      "Epoch 29: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 3s 38ms/step - loss: 0.3539 - val_loss: 1.7305 - lr: 8.3451e-05\n",
      "Epoch 30/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3374\n",
      "Epoch 30: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3390 - val_loss: 1.8564 - lr: 8.2617e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3459\n",
      "Epoch 31: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3443 - val_loss: 1.9220 - lr: 8.1791e-05\n",
      "Epoch 32/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3498\n",
      "Epoch 32: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3488 - val_loss: 1.8346 - lr: 8.0973e-05\n",
      "Epoch 33/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3384\n",
      "Epoch 33: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3385 - val_loss: 1.8760 - lr: 8.0163e-05\n",
      "Epoch 34/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3362\n",
      "Epoch 34: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3371 - val_loss: 1.9292 - lr: 7.9361e-05\n",
      "Epoch 35/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3292\n",
      "Epoch 35: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3310 - val_loss: 1.7754 - lr: 7.8568e-05\n",
      "Epoch 36/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3289\n",
      "Epoch 36: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3264 - val_loss: 1.9921 - lr: 7.7782e-05\n",
      "Epoch 37/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3280\n",
      "Epoch 37: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 3s 38ms/step - loss: 0.3274 - val_loss: 1.9070 - lr: 7.7004e-05\n",
      "Epoch 38/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3182\n",
      "Epoch 38: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.3200 - val_loss: 1.8414 - lr: 7.6234e-05\n",
      "Epoch 39/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3280\n",
      "Epoch 39: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.3280 - val_loss: 1.6717 - lr: 7.5472e-05\n",
      "Epoch 40/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3243\n",
      "Epoch 40: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3233 - val_loss: 1.6892 - lr: 7.4717e-05\n",
      "Epoch 41/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3174\n",
      "Epoch 41: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 3s 38ms/step - loss: 0.3172 - val_loss: 1.7753 - lr: 7.3970e-05\n",
      "Epoch 42/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3203\n",
      "Epoch 42: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3196 - val_loss: 1.6862 - lr: 7.3230e-05\n",
      "Epoch 43/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3278\n",
      "Epoch 43: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 2s 38ms/step - loss: 0.3258 - val_loss: 1.6807 - lr: 7.2498e-05\n",
      "Epoch 44/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3136\n",
      "Epoch 44: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3127 - val_loss: 1.6839 - lr: 7.1773e-05\n",
      "Epoch 45/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3055\n",
      "Epoch 45: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3055 - val_loss: 1.9358 - lr: 7.1055e-05\n",
      "Epoch 46/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3115\n",
      "Epoch 46: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3126 - val_loss: 1.8147 - lr: 7.0345e-05\n",
      "Epoch 47/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3003\n",
      "Epoch 47: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 2s 38ms/step - loss: 0.3001 - val_loss: 1.7672 - lr: 6.9641e-05\n",
      "Epoch 48/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2998\n",
      "Epoch 48: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.2977 - val_loss: 1.8405 - lr: 6.8945e-05\n",
      "Epoch 49/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3075\n",
      "Epoch 49: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3069 - val_loss: 1.8641 - lr: 6.8255e-05\n",
      "Epoch 50/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2991\n",
      "Epoch 50: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3003 - val_loss: 1.6958 - lr: 6.7573e-05\n",
      "Epoch 51/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3038\n",
      "Epoch 51: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 2s 38ms/step - loss: 0.3034 - val_loss: 1.6926 - lr: 6.6897e-05\n",
      "Epoch 52/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3028\n",
      "Epoch 52: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 2s 38ms/step - loss: 0.3013 - val_loss: 1.6327 - lr: 6.6228e-05\n",
      "Epoch 53/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2985\n",
      "Epoch 53: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 2s 38ms/step - loss: 0.2974 - val_loss: 1.6706 - lr: 6.5566e-05\n",
      "Epoch 54/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2962\n",
      "Epoch 54: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.2978 - val_loss: 1.7868 - lr: 6.4910e-05\n",
      "Epoch 55/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2907\n",
      "Epoch 55: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 2s 38ms/step - loss: 0.2907 - val_loss: 1.8258 - lr: 6.4261e-05\n",
      "Epoch 56/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2892\n",
      "Epoch 56: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.2891 - val_loss: 1.8285 - lr: 6.3619e-05\n",
      "Epoch 57/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2848\n",
      "Epoch 57: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 2s 38ms/step - loss: 0.2859 - val_loss: 1.6507 - lr: 6.2982e-05\n",
      "Epoch 58/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2860\n",
      "Epoch 58: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.2860 - val_loss: 1.8782 - lr: 6.2353e-05\n",
      "Epoch 59/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2922\n",
      "Epoch 59: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 2s 38ms/step - loss: 0.2946 - val_loss: 1.7506 - lr: 6.1729e-05\n",
      "Epoch 60/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2916\n",
      "Epoch 60: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 2s 38ms/step - loss: 0.2916 - val_loss: 1.6393 - lr: 6.1112e-05\n",
      "Epoch 61/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2817\n",
      "Epoch 61: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 2s 38ms/step - loss: 0.2817 - val_loss: 1.7381 - lr: 6.0501e-05\n",
      "Epoch 62/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2798\n",
      "Epoch 62: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 3s 38ms/step - loss: 0.2798 - val_loss: 1.7221 - lr: 5.9896e-05\n",
      "Epoch 63/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2762\n",
      "Epoch 63: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 5.8703650938696225e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.2767 - val_loss: 1.5481 - lr: 5.9297e-05\n",
      "Epoch 64/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2804\n",
      "Epoch 64: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 5.811661550978897e-05.\n",
      "66/66 [==============================] - 2s 38ms/step - loss: 0.2796 - val_loss: 1.5559 - lr: 5.8704e-05\n",
      "Epoch 65/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2860\n",
      "Epoch 65: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 5.753545068728272e-05.\n",
      "66/66 [==============================] - 2s 38ms/step - loss: 0.2860 - val_loss: 1.7168 - lr: 5.8117e-05\n",
      "Epoch 66/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2706\n",
      "Epoch 66: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 5.6960095243994146e-05.\n",
      "66/66 [==============================] - 2s 38ms/step - loss: 0.2688 - val_loss: 1.8067 - lr: 5.7535e-05\n",
      "Epoch 67/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2736\n",
      "Epoch 67: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 5.639049515593797e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.2736 - val_loss: 1.6495 - lr: 5.6960e-05\n",
      "Epoch 68/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2824\n",
      "Epoch 68: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 5.5826589195930865e-05.\n",
      "66/66 [==============================] - 3s 38ms/step - loss: 0.2813 - val_loss: 1.6618 - lr: 5.6390e-05\n",
      "Epoch 69/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2782\n",
      "Epoch 69: val_loss did not improve from 1.54528\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 5.526832333998754e-05.\n",
      "66/66 [==============================] - 2s 38ms/step - loss: 0.2763 - val_loss: 1.6632 - lr: 5.5827e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  (None, 45, 24)       3744        ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 45, 24)       0           ['lstm_3[0][0]']                 \n",
      "                                                                                                  \n",
      " lstm_4 (LSTM)                  (None, 45, 16)       2624        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 45, 16)       0           ['lstm_4[0][0]']                 \n",
      "                                                                                                  \n",
      " lstm_5 (LSTM)                  (None, 32)           6272        ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 40)           1320        ['lstm_5[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 5)            205         ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " tf.unstack_1 (TFOpLambda)      [(None,),            0           ['dense_3[0][0]']                \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_5 (TFOpLambda)  (None, 1)            0           ['tf.unstack_1[0][0]']           \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_2 (TFOpLambda)  (None, 1)           0           ['tf.expand_dims_5[0][0]']       \n",
      "                                                                                                  \n",
      " tf.expand_dims_9 (TFOpLambda)  (None, 1)            0           ['tf.unstack_1[0][4]']           \n",
      "                                                                                                  \n",
      " tf.math.multiply_3 (TFOpLambda  (None, 1)           0           ['tf.math.sigmoid_2[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_3 (TFOpLambda)  (None, 1)           0           ['tf.expand_dims_9[0][0]']       \n",
      "                                                                                                  \n",
      " tf.math.multiply_4 (TFOpLambda  (None, 1)           0           ['tf.math.multiply_3[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_6 (TFOpLambda)  (None, 1)            0           ['tf.unstack_1[0][1]']           \n",
      "                                                                                                  \n",
      " tf.expand_dims_8 (TFOpLambda)  (None, 1)            0           ['tf.unstack_1[0][3]']           \n",
      "                                                                                                  \n",
      " tf.math.multiply_5 (TFOpLambda  (None, 1)           0           ['tf.math.sigmoid_3[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 1)           0           ['tf.math.multiply_4[0][0]']     \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.math.softplus_2 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_6[0][0]']       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_7 (TFOpLambda)  (None, 1)            0           ['tf.unstack_1[0][2]']           \n",
      "                                                                                                  \n",
      " tf.math.softplus_3 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_8[0][0]']       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 1)           0           ['tf.math.multiply_5[0][0]']     \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.stack_1 (TFOpLambda)        (None, 5, 1)         0           ['tf.__operators__.add_2[0][0]', \n",
      "                                                                  'tf.math.softplus_2[0][0]',     \n",
      "                                                                  'tf.expand_dims_7[0][0]',       \n",
      "                                                                  'tf.math.softplus_3[0][0]',     \n",
      "                                                                  'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _1_CCMP_t+1_0.06\n",
      "Epoch 1/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 3.4329\n",
      "Epoch 1: val_loss improved from inf to 4.27615, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 9s 53ms/step - loss: 3.4329 - val_loss: 4.2762 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 2.9323\n",
      "Epoch 2: val_loss improved from 4.27615 to 3.31542, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 2.9323 - val_loss: 3.3154 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.7162\n",
      "Epoch 3: val_loss improved from 3.31542 to 2.65336, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 1.7153 - val_loss: 2.6534 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.2049\n",
      "Epoch 4: val_loss improved from 2.65336 to 2.58688, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.06.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 2s 38ms/step - loss: 1.2076 - val_loss: 2.5869 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9939\n",
      "Epoch 5: val_loss improved from 2.58688 to 2.57049, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 2s 38ms/step - loss: 0.9934 - val_loss: 2.5705 - lr: 1.0000e-04\n",
      "Epoch 6/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8188\n",
      "Epoch 6: val_loss improved from 2.57049 to 2.46208, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.8214 - val_loss: 2.4621 - lr: 1.0000e-04\n",
      "Epoch 7/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7461\n",
      "Epoch 7: val_loss improved from 2.46208 to 2.40096, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.7468 - val_loss: 2.4010 - lr: 1.0000e-04\n",
      "Epoch 8/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6798\n",
      "Epoch 8: val_loss improved from 2.40096 to 2.28133, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.6798 - val_loss: 2.2813 - lr: 1.0000e-04\n",
      "Epoch 9/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6431\n",
      "Epoch 9: val_loss did not improve from 2.28133\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.6447 - val_loss: 2.3539 - lr: 1.0000e-04\n",
      "Epoch 10/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6080\n",
      "Epoch 10: val_loss did not improve from 2.28133\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.6050 - val_loss: 2.3940 - lr: 9.9000e-05\n",
      "Epoch 11/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5908\n",
      "Epoch 11: val_loss improved from 2.28133 to 2.19625, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 3s 38ms/step - loss: 0.5905 - val_loss: 2.1963 - lr: 9.8010e-05\n",
      "Epoch 12/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5691\n",
      "Epoch 12: val_loss improved from 2.19625 to 2.18892, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 3s 38ms/step - loss: 0.5718 - val_loss: 2.1889 - lr: 9.8010e-05\n",
      "Epoch 13/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5700\n",
      "Epoch 13: val_loss did not improve from 2.18892\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.5684 - val_loss: 2.1941 - lr: 9.8010e-05\n",
      "Epoch 14/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5303\n",
      "Epoch 14: val_loss improved from 2.18892 to 2.18756, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.5303 - val_loss: 2.1876 - lr: 9.7030e-05\n",
      "Epoch 15/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5442\n",
      "Epoch 15: val_loss improved from 2.18756 to 2.09651, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.5422 - val_loss: 2.0965 - lr: 9.7030e-05\n",
      "Epoch 16/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5237\n",
      "Epoch 16: val_loss improved from 2.09651 to 1.99702, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.5220 - val_loss: 1.9970 - lr: 9.7030e-05\n",
      "Epoch 17/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5056\n",
      "Epoch 17: val_loss did not improve from 1.99702\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.5042 - val_loss: 2.1315 - lr: 9.7030e-05\n",
      "Epoch 18/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5065\n",
      "Epoch 18: val_loss did not improve from 1.99702\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 3s 38ms/step - loss: 0.5074 - val_loss: 2.1309 - lr: 9.6060e-05\n",
      "Epoch 19/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5061\n",
      "Epoch 19: val_loss did not improve from 1.99702\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.5103 - val_loss: 2.0613 - lr: 9.5099e-05\n",
      "Epoch 20/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5014\n",
      "Epoch 20: val_loss did not improve from 1.99702\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.5000 - val_loss: 2.0145 - lr: 9.4148e-05\n",
      "Epoch 21/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5003\n",
      "Epoch 21: val_loss did not improve from 1.99702\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.5000 - val_loss: 2.1153 - lr: 9.3207e-05\n",
      "Epoch 22/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4815\n",
      "Epoch 22: val_loss did not improve from 1.99702\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.4809 - val_loss: 2.1713 - lr: 9.2274e-05\n",
      "Epoch 23/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4712\n",
      "Epoch 23: val_loss did not improve from 1.99702\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.4721 - val_loss: 2.1952 - lr: 9.1352e-05\n",
      "Epoch 24/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4792\n",
      "Epoch 24: val_loss did not improve from 1.99702\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.4790 - val_loss: 2.1960 - lr: 9.0438e-05\n",
      "Epoch 25/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4672\n",
      "Epoch 25: val_loss did not improve from 1.99702\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 3s 38ms/step - loss: 0.4683 - val_loss: 2.0119 - lr: 8.9534e-05\n",
      "Epoch 26/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4715\n",
      "Epoch 26: val_loss did not improve from 1.99702\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.4701 - val_loss: 2.0986 - lr: 8.8638e-05\n",
      "Epoch 27/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4624\n",
      "Epoch 27: val_loss did not improve from 1.99702\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.4610 - val_loss: 2.1053 - lr: 8.7752e-05\n",
      "Epoch 28/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4771\n",
      "Epoch 28: val_loss did not improve from 1.99702\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.4776 - val_loss: 2.0513 - lr: 8.6875e-05\n",
      "Epoch 29/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4678\n",
      "Epoch 29: val_loss did not improve from 1.99702\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.4675 - val_loss: 2.0449 - lr: 8.6006e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4533\n",
      "Epoch 30: val_loss did not improve from 1.99702\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.4511 - val_loss: 2.1555 - lr: 8.5146e-05\n",
      "Epoch 31/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4399\n",
      "Epoch 31: val_loss improved from 1.99702 to 1.96547, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 2s 38ms/step - loss: 0.4408 - val_loss: 1.9655 - lr: 8.4294e-05\n",
      "Epoch 32/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4651\n",
      "Epoch 32: val_loss did not improve from 1.96547\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.4630 - val_loss: 2.0282 - lr: 8.4294e-05\n",
      "Epoch 33/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4471\n",
      "Epoch 33: val_loss did not improve from 1.96547\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.4440 - val_loss: 2.0144 - lr: 8.3451e-05\n",
      "Epoch 34/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4313\n",
      "Epoch 34: val_loss did not improve from 1.96547\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.4307 - val_loss: 1.9864 - lr: 8.2617e-05\n",
      "Epoch 35/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4400\n",
      "Epoch 35: val_loss did not improve from 1.96547\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.4400 - val_loss: 2.1059 - lr: 8.1791e-05\n",
      "Epoch 36/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4397\n",
      "Epoch 36: val_loss did not improve from 1.96547\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.4368 - val_loss: 2.0724 - lr: 8.0973e-05\n",
      "Epoch 37/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4414\n",
      "Epoch 37: val_loss did not improve from 1.96547\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 2s 38ms/step - loss: 0.4404 - val_loss: 2.0064 - lr: 8.0163e-05\n",
      "Epoch 38/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4335\n",
      "Epoch 38: val_loss improved from 1.96547 to 1.90692, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.4335 - val_loss: 1.9069 - lr: 7.9361e-05\n",
      "Epoch 39/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4325\n",
      "Epoch 39: val_loss did not improve from 1.90692\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.4322 - val_loss: 2.0786 - lr: 7.9361e-05\n",
      "Epoch 40/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4193\n",
      "Epoch 40: val_loss did not improve from 1.90692\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.4223 - val_loss: 1.9874 - lr: 7.8568e-05\n",
      "Epoch 41/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4272\n",
      "Epoch 41: val_loss did not improve from 1.90692\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 2s 38ms/step - loss: 0.4285 - val_loss: 2.1633 - lr: 7.7782e-05\n",
      "Epoch 42/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4162\n",
      "Epoch 42: val_loss improved from 1.90692 to 1.86342, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 2s 38ms/step - loss: 0.4148 - val_loss: 1.8634 - lr: 7.7004e-05\n",
      "Epoch 43/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4279\n",
      "Epoch 43: val_loss improved from 1.86342 to 1.81166, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.4307 - val_loss: 1.8117 - lr: 7.7004e-05\n",
      "Epoch 44/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4247\n",
      "Epoch 44: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.4253 - val_loss: 1.9687 - lr: 7.7004e-05\n",
      "Epoch 45/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4298\n",
      "Epoch 45: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 4s 55ms/step - loss: 0.4298 - val_loss: 2.0612 - lr: 7.6234e-05\n",
      "Epoch 46/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4118\n",
      "Epoch 46: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 6s 84ms/step - loss: 0.4118 - val_loss: 2.0413 - lr: 7.5472e-05\n",
      "Epoch 47/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4041\n",
      "Epoch 47: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 0.4041 - val_loss: 2.1946 - lr: 7.4717e-05\n",
      "Epoch 48/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4173\n",
      "Epoch 48: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 4s 64ms/step - loss: 0.4171 - val_loss: 2.0214 - lr: 7.3970e-05\n",
      "Epoch 49/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4218\n",
      "Epoch 49: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 4s 55ms/step - loss: 0.4186 - val_loss: 2.0915 - lr: 7.3230e-05\n",
      "Epoch 50/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4048\n",
      "Epoch 50: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.4042 - val_loss: 2.0865 - lr: 7.2498e-05\n",
      "Epoch 51/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4073\n",
      "Epoch 51: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.4043 - val_loss: 2.0389 - lr: 7.1773e-05\n",
      "Epoch 52/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4021\n",
      "Epoch 52: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.4021 - val_loss: 1.9989 - lr: 7.1055e-05\n",
      "Epoch 53/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3908\n",
      "Epoch 53: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.3950 - val_loss: 2.0614 - lr: 7.0345e-05\n",
      "Epoch 54/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4002\n",
      "Epoch 54: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.4002 - val_loss: 2.0603 - lr: 6.9641e-05\n",
      "Epoch 55/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3899\n",
      "Epoch 55: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.3899 - val_loss: 2.0329 - lr: 6.8945e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3984\n",
      "Epoch 56: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.3971 - val_loss: 1.8992 - lr: 6.8255e-05\n",
      "Epoch 57/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4007\n",
      "Epoch 57: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 4s 60ms/step - loss: 0.4013 - val_loss: 1.9700 - lr: 6.7573e-05\n",
      "Epoch 58/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3999\n",
      "Epoch 58: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.3974 - val_loss: 1.8818 - lr: 6.6897e-05\n",
      "Epoch 59/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4008\n",
      "Epoch 59: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 2s 34ms/step - loss: 0.3995 - val_loss: 2.0379 - lr: 6.6228e-05\n",
      "Epoch 60/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4042\n",
      "Epoch 60: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 2s 34ms/step - loss: 0.4052 - val_loss: 1.8273 - lr: 6.5566e-05\n",
      "Epoch 61/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3880\n",
      "Epoch 61: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.3888 - val_loss: 2.0352 - lr: 6.4910e-05\n",
      "Epoch 62/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3910\n",
      "Epoch 62: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.3910 - val_loss: 1.9755 - lr: 6.4261e-05\n",
      "Epoch 63/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3931\n",
      "Epoch 63: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.3958 - val_loss: 2.0393 - lr: 6.3619e-05\n",
      "Epoch 64/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3899\n",
      "Epoch 64: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.3883 - val_loss: 1.9226 - lr: 6.2982e-05\n",
      "Epoch 65/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3840\n",
      "Epoch 65: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3842 - val_loss: 1.9943 - lr: 6.2353e-05\n",
      "Epoch 66/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3895\n",
      "Epoch 66: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3895 - val_loss: 1.9374 - lr: 6.1729e-05\n",
      "Epoch 67/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3866\n",
      "Epoch 67: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3866 - val_loss: 2.0485 - lr: 6.1112e-05\n",
      "Epoch 68/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3882\n",
      "Epoch 68: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3867 - val_loss: 1.9141 - lr: 6.0501e-05\n",
      "Epoch 69/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3724\n",
      "Epoch 69: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3723 - val_loss: 2.0739 - lr: 5.9896e-05\n",
      "Epoch 70/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3836\n",
      "Epoch 70: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 5.8703650938696225e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3808 - val_loss: 2.2026 - lr: 5.9297e-05\n",
      "Epoch 71/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3870\n",
      "Epoch 71: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 5.811661550978897e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3857 - val_loss: 2.0082 - lr: 5.8704e-05\n",
      "Epoch 72/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3750\n",
      "Epoch 72: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 5.753545068728272e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3749 - val_loss: 2.0124 - lr: 5.8117e-05\n",
      "Epoch 73/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3714\n",
      "Epoch 73: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 5.6960095243994146e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3721 - val_loss: 1.9636 - lr: 5.7535e-05\n",
      "Epoch 74/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3770\n",
      "Epoch 74: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 5.639049515593797e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3760 - val_loss: 2.0764 - lr: 5.6960e-05\n",
      "Epoch 75/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3656\n",
      "Epoch 75: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 5.5826589195930865e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3670 - val_loss: 1.9568 - lr: 5.6390e-05\n",
      "Epoch 76/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3766\n",
      "Epoch 76: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 5.526832333998754e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3785 - val_loss: 1.8757 - lr: 5.5827e-05\n",
      "Epoch 77/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3825\n",
      "Epoch 77: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 5.471563996252371e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3813 - val_loss: 1.9088 - lr: 5.5268e-05\n",
      "Epoch 78/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3700\n",
      "Epoch 78: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 5.416848503955407e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3696 - val_loss: 1.8758 - lr: 5.4716e-05\n",
      "Epoch 79/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3632\n",
      "Epoch 79: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 5.362680094549433e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3689 - val_loss: 1.9778 - lr: 5.4168e-05\n",
      "Epoch 80/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3734\n",
      "Epoch 80: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 5.309053365635918e-05.\n",
      "66/66 [==============================] - 2s 38ms/step - loss: 0.3712 - val_loss: 1.9120 - lr: 5.3627e-05\n",
      "Epoch 81/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3741\n",
      "Epoch 81: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 5.2559629148163366e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3731 - val_loss: 1.8668 - lr: 5.3091e-05\n",
      "Epoch 82/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3653\n",
      "Epoch 82: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 5.2034033396921584e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3628 - val_loss: 2.1040 - lr: 5.2560e-05\n",
      "Epoch 83/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3760\n",
      "Epoch 83: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 5.1513692378648555e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3742 - val_loss: 1.9660 - lr: 5.2034e-05\n",
      "Epoch 84/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3689\n",
      "Epoch 84: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 5.099855567095801e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3676 - val_loss: 1.9216 - lr: 5.1514e-05\n",
      "Epoch 85/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3675\n",
      "Epoch 85: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 5.048856924986467e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3675 - val_loss: 1.9957 - lr: 5.0999e-05\n",
      "Epoch 86/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3773\n",
      "Epoch 86: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 4.998368269298226e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3763 - val_loss: 1.8766 - lr: 5.0489e-05\n",
      "Epoch 87/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3573\n",
      "Epoch 87: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 4.9483845577924514e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3599 - val_loss: 1.9220 - lr: 4.9984e-05\n",
      "Epoch 88/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3623\n",
      "Epoch 88: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 4.898900748230517e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3650 - val_loss: 1.8821 - lr: 4.9484e-05\n",
      "Epoch 89/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3731\n",
      "Epoch 89: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 4.849911798373796e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3721 - val_loss: 1.9281 - lr: 4.8989e-05\n",
      "Epoch 90/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3494\n",
      "Epoch 90: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 4.801412665983662e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3502 - val_loss: 1.9599 - lr: 4.8499e-05\n",
      "Epoch 91/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3701\n",
      "Epoch 91: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 4.75339866898139e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3711 - val_loss: 1.8537 - lr: 4.8014e-05\n",
      "Epoch 92/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3550\n",
      "Epoch 92: val_loss did not improve from 1.81166\n",
      "\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 4.705864765128354e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3579 - val_loss: 1.8532 - lr: 4.7534e-05\n",
      "Epoch 93/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3590\n",
      "Epoch 93: val_loss improved from 1.81166 to 1.74882, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 3s 38ms/step - loss: 0.3590 - val_loss: 1.7488 - lr: 4.7059e-05\n",
      "Epoch 94/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3486\n",
      "Epoch 94: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 4.658806272345828e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3510 - val_loss: 1.8181 - lr: 4.7059e-05\n",
      "Epoch 95/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3474\n",
      "Epoch 95: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 4.612218148395186e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3472 - val_loss: 1.8748 - lr: 4.6588e-05\n",
      "Epoch 96/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3626\n",
      "Epoch 96: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 4.566096071357606e-05.\n",
      "66/66 [==============================] - 2s 38ms/step - loss: 0.3626 - val_loss: 1.8202 - lr: 4.6122e-05\n",
      "Epoch 97/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3496\n",
      "Epoch 97: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 4.52043499899446e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3493 - val_loss: 1.8283 - lr: 4.5661e-05\n",
      "Epoch 98/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3523\n",
      "Epoch 98: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 4.475230609386927e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3523 - val_loss: 1.8292 - lr: 4.5204e-05\n",
      "Epoch 99/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3596\n",
      "Epoch 99: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 4.43047822045628e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3592 - val_loss: 1.8237 - lr: 4.4752e-05\n",
      "Epoch 100/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3479\n",
      "Epoch 100: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 4.386173510283697e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3490 - val_loss: 1.8770 - lr: 4.4305e-05\n",
      "Epoch 101/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3516\n",
      "Epoch 101: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 4.342311796790454e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3502 - val_loss: 1.8694 - lr: 4.3862e-05\n",
      "Epoch 102/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3559\n",
      "Epoch 102: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 4.298888758057728e-05.\n",
      "66/66 [==============================] - 2s 38ms/step - loss: 0.3569 - val_loss: 1.8419 - lr: 4.3423e-05\n",
      "Epoch 103/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3551\n",
      "Epoch 103: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 4.2558997120067946e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3547 - val_loss: 1.8858 - lr: 4.2989e-05\n",
      "Epoch 104/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3590\n",
      "Epoch 104: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 4.2133406968787315e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3600 - val_loss: 1.9122 - lr: 4.2559e-05\n",
      "Epoch 105/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3495\n",
      "Epoch 105: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 4.1712073907547166e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.3495 - val_loss: 1.8908 - lr: 4.2133e-05\n",
      "Epoch 106/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3485\n",
      "Epoch 106: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 4.1294954717159274e-05.\n",
      "66/66 [==============================] - 2s 38ms/step - loss: 0.3482 - val_loss: 1.9523 - lr: 4.1712e-05\n",
      "Epoch 107/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3486\n",
      "Epoch 107: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 4.08820061784354e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3491 - val_loss: 1.8894 - lr: 4.1295e-05\n",
      "Epoch 108/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3547\n",
      "Epoch 108: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 4.0473185072187336e-05.\n",
      "66/66 [==============================] - 3s 38ms/step - loss: 0.3547 - val_loss: 1.8266 - lr: 4.0882e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3570\n",
      "Epoch 109: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 4.0068451780825854e-05.\n",
      "66/66 [==============================] - 2s 38ms/step - loss: 0.3576 - val_loss: 1.9329 - lr: 4.0473e-05\n",
      "Epoch 110/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3450\n",
      "Epoch 110: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 3.966776668676175e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.3469 - val_loss: 1.8845 - lr: 4.0068e-05\n",
      "Epoch 111/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3368\n",
      "Epoch 111: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 3.927109017240582e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.3368 - val_loss: 1.8986 - lr: 3.9668e-05\n",
      "Epoch 112/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3506\n",
      "Epoch 112: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 3.887837901856983e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.3506 - val_loss: 1.8332 - lr: 3.9271e-05\n",
      "Epoch 113/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3422\n",
      "Epoch 113: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 3.848959360766457e-05.\n",
      "66/66 [==============================] - 4s 61ms/step - loss: 0.3422 - val_loss: 1.8531 - lr: 3.8878e-05\n",
      "Epoch 114/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3489\n",
      "Epoch 114: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 3.810469792369986e-05.\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.3463 - val_loss: 1.7586 - lr: 3.8490e-05\n",
      "Epoch 115/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3596\n",
      "Epoch 115: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 3.772365234908648e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.3596 - val_loss: 1.7574 - lr: 3.8105e-05\n",
      "Epoch 116/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3552\n",
      "Epoch 116: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 3.734641726623522e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.3542 - val_loss: 1.7777 - lr: 3.7724e-05\n",
      "Epoch 117/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3490\n",
      "Epoch 117: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 3.6972953057556876e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.3490 - val_loss: 1.8222 - lr: 3.7346e-05\n",
      "Epoch 118/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3417\n",
      "Epoch 118: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 3.660322370706126e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.3401 - val_loss: 1.8044 - lr: 3.6973e-05\n",
      "Epoch 119/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3492\n",
      "Epoch 119: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 3.623719319875818e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.3475 - val_loss: 1.8485 - lr: 3.6603e-05\n",
      "Epoch 120/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3328\n",
      "Epoch 120: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 3.5874821915058416e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.3353 - val_loss: 1.7493 - lr: 3.6237e-05\n",
      "Epoch 121/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3493\n",
      "Epoch 121: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 3.551607383997179e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.3481 - val_loss: 1.8041 - lr: 3.5875e-05\n",
      "Epoch 122/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3454\n",
      "Epoch 122: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 3.516091295750812e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3447 - val_loss: 1.8012 - lr: 3.5516e-05\n",
      "Epoch 123/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3405\n",
      "Epoch 123: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 123: ReduceLROnPlateau reducing learning rate to 3.480930325167719e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3388 - val_loss: 1.7722 - lr: 3.5161e-05\n",
      "Epoch 124/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3400\n",
      "Epoch 124: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 124: ReduceLROnPlateau reducing learning rate to 3.446120870648883e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3394 - val_loss: 1.8081 - lr: 3.4809e-05\n",
      "Epoch 125/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3364\n",
      "Epoch 125: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 3.4116596907551866e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3364 - val_loss: 1.7827 - lr: 3.4461e-05\n",
      "Epoch 126/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3348\n",
      "Epoch 126: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 3.37754318388761e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3356 - val_loss: 1.7568 - lr: 3.4117e-05\n",
      "Epoch 127/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3445\n",
      "Epoch 127: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 127: ReduceLROnPlateau reducing learning rate to 3.3437677484471354e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3439 - val_loss: 1.8277 - lr: 3.3775e-05\n",
      "Epoch 128/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3278\n",
      "Epoch 128: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 128: ReduceLROnPlateau reducing learning rate to 3.310330142994644e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3297 - val_loss: 1.8345 - lr: 3.3438e-05\n",
      "Epoch 129/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3255\n",
      "Epoch 129: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 129: ReduceLROnPlateau reducing learning rate to 3.277226765931118e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3269 - val_loss: 1.8028 - lr: 3.3103e-05\n",
      "Epoch 130/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3394\n",
      "Epoch 130: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 130: ReduceLROnPlateau reducing learning rate to 3.24445437581744e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.3394 - val_loss: 1.8770 - lr: 3.2772e-05\n",
      "Epoch 131/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3471\n",
      "Epoch 131: val_loss did not improve from 1.74882\n",
      "\n",
      "Epoch 131: ReduceLROnPlateau reducing learning rate to 3.212009731214493e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3445 - val_loss: 1.8175 - lr: 3.2445e-05\n",
      "Epoch 132/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3461\n",
      "Epoch 132: val_loss improved from 1.74882 to 1.73821, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.3441 - val_loss: 1.7382 - lr: 3.2120e-05\n",
      "Epoch 133/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3385\n",
      "Epoch 133: val_loss did not improve from 1.73821\n",
      "\n",
      "Epoch 133: ReduceLROnPlateau reducing learning rate to 3.17988959068316e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.3409 - val_loss: 1.7641 - lr: 3.2120e-05\n",
      "Epoch 134/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3329\n",
      "Epoch 134: val_loss did not improve from 1.73821\n",
      "\n",
      "Epoch 134: ReduceLROnPlateau reducing learning rate to 3.1480907127843236e-05.\n",
      "66/66 [==============================] - 4s 59ms/step - loss: 0.3345 - val_loss: 1.8805 - lr: 3.1799e-05\n",
      "Epoch 135/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3399\n",
      "Epoch 135: val_loss did not improve from 1.73821\n",
      "\n",
      "Epoch 135: ReduceLROnPlateau reducing learning rate to 3.116609856078867e-05.\n",
      "66/66 [==============================] - 4s 61ms/step - loss: 0.3379 - val_loss: 1.8355 - lr: 3.1481e-05\n",
      "Epoch 136/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3474\n",
      "Epoch 136: val_loss did not improve from 1.73821\n",
      "\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 3.085443779127672e-05.\n",
      "66/66 [==============================] - 4s 57ms/step - loss: 0.3486 - val_loss: 1.7656 - lr: 3.1166e-05\n",
      "Epoch 137/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3265\n",
      "Epoch 137: val_loss did not improve from 1.73821\n",
      "\n",
      "Epoch 137: ReduceLROnPlateau reducing learning rate to 3.054589240491623e-05.\n",
      "66/66 [==============================] - 3s 52ms/step - loss: 0.3253 - val_loss: 1.7947 - lr: 3.0854e-05\n",
      "Epoch 138/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3375\n",
      "Epoch 138: val_loss did not improve from 1.73821\n",
      "\n",
      "Epoch 138: ReduceLROnPlateau reducing learning rate to 3.024043358891504e-05.\n",
      "66/66 [==============================] - 3s 53ms/step - loss: 0.3366 - val_loss: 1.7853 - lr: 3.0546e-05\n",
      "Epoch 139/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3330\n",
      "Epoch 139: val_loss improved from 1.73821 to 1.73360, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.3330 - val_loss: 1.7336 - lr: 3.0240e-05\n",
      "Epoch 140/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3339\n",
      "Epoch 140: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 140: ReduceLROnPlateau reducing learning rate to 2.9938028928881975e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.3339 - val_loss: 1.8122 - lr: 3.0240e-05\n",
      "Epoch 141/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3372\n",
      "Epoch 141: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 141: ReduceLROnPlateau reducing learning rate to 2.963864781122538e-05.\n",
      "66/66 [==============================] - 2s 34ms/step - loss: 0.3355 - val_loss: 1.7870 - lr: 2.9938e-05\n",
      "Epoch 142/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3381\n",
      "Epoch 142: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 142: ReduceLROnPlateau reducing learning rate to 2.9342261423153105e-05.\n",
      "66/66 [==============================] - 2s 34ms/step - loss: 0.3368 - val_loss: 1.8434 - lr: 2.9639e-05\n",
      "Epoch 143/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3353\n",
      "Epoch 143: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 143: ReduceLROnPlateau reducing learning rate to 2.9048839151073478e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.3330 - val_loss: 1.7544 - lr: 2.9342e-05\n",
      "Epoch 144/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3345\n",
      "Epoch 144: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 144: ReduceLROnPlateau reducing learning rate to 2.875835038139485e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.3326 - val_loss: 1.7878 - lr: 2.9049e-05\n",
      "Epoch 145/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3353\n",
      "Epoch 145: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 145: ReduceLROnPlateau reducing learning rate to 2.8470766301325058e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3362 - val_loss: 1.8063 - lr: 2.8758e-05\n",
      "Epoch 146/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3290\n",
      "Epoch 146: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 146: ReduceLROnPlateau reducing learning rate to 2.8186058098071954e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3292 - val_loss: 1.8637 - lr: 2.8471e-05\n",
      "Epoch 147/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3326\n",
      "Epoch 147: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 147: ReduceLROnPlateau reducing learning rate to 2.7904196958843385e-05.\n",
      "66/66 [==============================] - 3s 38ms/step - loss: 0.3318 - val_loss: 1.7738 - lr: 2.8186e-05\n",
      "Epoch 148/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3272\n",
      "Epoch 148: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 148: ReduceLROnPlateau reducing learning rate to 2.7625155871646713e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.3261 - val_loss: 1.8454 - lr: 2.7904e-05\n",
      "Epoch 149/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3431\n",
      "Epoch 149: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 149: ReduceLROnPlateau reducing learning rate to 2.734890422289027e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3422 - val_loss: 1.7467 - lr: 2.7625e-05\n",
      "Epoch 150/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3263\n",
      "Epoch 150: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 150: ReduceLROnPlateau reducing learning rate to 2.7075415000581416e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3286 - val_loss: 1.8438 - lr: 2.7349e-05\n",
      "Epoch 151/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3361\n",
      "Epoch 151: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 151: ReduceLROnPlateau reducing learning rate to 2.6804661192727508e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3354 - val_loss: 1.7489 - lr: 2.7075e-05\n",
      "Epoch 152/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3359\n",
      "Epoch 152: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 152: ReduceLROnPlateau reducing learning rate to 2.6536613986536395e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3336 - val_loss: 1.7749 - lr: 2.6805e-05\n",
      "Epoch 153/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3341\n",
      "Epoch 153: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 153: ReduceLROnPlateau reducing learning rate to 2.6271248170814942e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3341 - val_loss: 1.7656 - lr: 2.6537e-05\n",
      "Epoch 154/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3295\n",
      "Epoch 154: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 154: ReduceLROnPlateau reducing learning rate to 2.6008534932770998e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3287 - val_loss: 1.7864 - lr: 2.6271e-05\n",
      "Epoch 155/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3171\n",
      "Epoch 155: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 155: ReduceLROnPlateau reducing learning rate to 2.5748449061211432e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.3171 - val_loss: 1.8234 - lr: 2.6009e-05\n",
      "Epoch 156/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3308\n",
      "Epoch 156: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 156: ReduceLROnPlateau reducing learning rate to 2.5490965344943107e-05.\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.3308 - val_loss: 1.7425 - lr: 2.5748e-05\n",
      "Epoch 157/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3181\n",
      "Epoch 157: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 157: ReduceLROnPlateau reducing learning rate to 2.523605497117387e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.3181 - val_loss: 1.8026 - lr: 2.5491e-05\n",
      "Epoch 158/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3130\n",
      "Epoch 158: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 158: ReduceLROnPlateau reducing learning rate to 2.4983694529510104e-05.\n",
      "66/66 [==============================] - 4s 59ms/step - loss: 0.3120 - val_loss: 1.8266 - lr: 2.5236e-05\n",
      "Epoch 159/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3229\n",
      "Epoch 159: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 159: ReduceLROnPlateau reducing learning rate to 2.4733857007959158e-05.\n",
      "66/66 [==============================] - 4s 53ms/step - loss: 0.3250 - val_loss: 1.8491 - lr: 2.4984e-05\n",
      "Epoch 160/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3327\n",
      "Epoch 160: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 160: ReduceLROnPlateau reducing learning rate to 2.4486518996127415e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.3327 - val_loss: 1.8876 - lr: 2.4734e-05\n",
      "Epoch 161/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/66 [============================>.] - ETA: 0s - loss: 0.3363\n",
      "Epoch 161: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 161: ReduceLROnPlateau reducing learning rate to 2.424165348202223e-05.\n",
      "66/66 [==============================] - 2s 34ms/step - loss: 0.3399 - val_loss: 1.7987 - lr: 2.4487e-05\n",
      "Epoch 162/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3311\n",
      "Epoch 162: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 162: ReduceLROnPlateau reducing learning rate to 2.3999237055249977e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.3303 - val_loss: 1.7564 - lr: 2.4242e-05\n",
      "Epoch 163/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3303\n",
      "Epoch 163: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 163: ReduceLROnPlateau reducing learning rate to 2.375924450461753e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.3282 - val_loss: 1.7619 - lr: 2.3999e-05\n",
      "Epoch 164/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3326\n",
      "Epoch 164: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 164: ReduceLROnPlateau reducing learning rate to 2.3521652419731254e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.3320 - val_loss: 1.7792 - lr: 2.3759e-05\n",
      "Epoch 165/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3231\n",
      "Epoch 165: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 165: ReduceLROnPlateau reducing learning rate to 2.3286435589398024e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.3283 - val_loss: 1.7793 - lr: 2.3522e-05\n",
      "Epoch 166/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3408\n",
      "Epoch 166: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 166: ReduceLROnPlateau reducing learning rate to 2.3053570603224217e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3399 - val_loss: 1.8036 - lr: 2.3286e-05\n",
      "Epoch 167/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3244\n",
      "Epoch 167: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 167: ReduceLROnPlateau reducing learning rate to 2.2823034050816205e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3242 - val_loss: 1.8456 - lr: 2.3054e-05\n",
      "Epoch 168/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3274\n",
      "Epoch 168: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 168: ReduceLROnPlateau reducing learning rate to 2.2594804322579874e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3274 - val_loss: 1.7889 - lr: 2.2823e-05\n",
      "Epoch 169/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3297\n",
      "Epoch 169: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 169: ReduceLROnPlateau reducing learning rate to 2.2368856207322096e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3297 - val_loss: 1.8049 - lr: 2.2595e-05\n",
      "Epoch 170/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3334\n",
      "Epoch 170: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 170: ReduceLROnPlateau reducing learning rate to 2.2145168095448753e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3334 - val_loss: 1.7342 - lr: 2.2369e-05\n",
      "Epoch 171/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3196\n",
      "Epoch 171: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 171: ReduceLROnPlateau reducing learning rate to 2.1923716576566222e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3182 - val_loss: 1.7802 - lr: 2.2145e-05\n",
      "Epoch 172/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3263\n",
      "Epoch 172: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 172: ReduceLROnPlateau reducing learning rate to 2.1704480041080387e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3276 - val_loss: 1.7655 - lr: 2.1924e-05\n",
      "Epoch 173/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3264\n",
      "Epoch 173: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 173: ReduceLROnPlateau reducing learning rate to 2.1487435078597626e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3241 - val_loss: 1.7560 - lr: 2.1704e-05\n",
      "Epoch 174/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3287\n",
      "Epoch 174: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 174: ReduceLROnPlateau reducing learning rate to 2.1272560079523828e-05.\n",
      "66/66 [==============================] - 3s 38ms/step - loss: 0.3285 - val_loss: 1.7607 - lr: 2.1487e-05\n",
      "Epoch 175/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3393\n",
      "Epoch 175: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 175: ReduceLROnPlateau reducing learning rate to 2.1059835235064383e-05.\n",
      "66/66 [==============================] - 5s 69ms/step - loss: 0.3393 - val_loss: 1.7419 - lr: 2.1273e-05\n",
      "Epoch 176/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3270\n",
      "Epoch 176: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 176: ReduceLROnPlateau reducing learning rate to 2.084923713482567e-05.\n",
      "66/66 [==============================] - 5s 83ms/step - loss: 0.3270 - val_loss: 1.7502 - lr: 2.1060e-05\n",
      "Epoch 177/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3231\n",
      "Epoch 177: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 177: ReduceLROnPlateau reducing learning rate to 2.0640744169213575e-05.\n",
      "66/66 [==============================] - 4s 64ms/step - loss: 0.3208 - val_loss: 1.7599 - lr: 2.0849e-05\n",
      "Epoch 178/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3117\n",
      "Epoch 178: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 178: ReduceLROnPlateau reducing learning rate to 2.0434336529433495e-05.\n",
      "66/66 [==============================] - 4s 55ms/step - loss: 0.3122 - val_loss: 1.7504 - lr: 2.0641e-05\n",
      "Epoch 179/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3274\n",
      "Epoch 179: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 179: ReduceLROnPlateau reducing learning rate to 2.0229992605891313e-05.\n",
      "66/66 [==============================] - 3s 52ms/step - loss: 0.3274 - val_loss: 1.7373 - lr: 2.0434e-05\n",
      "Epoch 180/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3214\n",
      "Epoch 180: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 180: ReduceLROnPlateau reducing learning rate to 2.0027692589792422e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.3214 - val_loss: 1.7455 - lr: 2.0230e-05\n",
      "Epoch 181/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3194\n",
      "Epoch 181: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 181: ReduceLROnPlateau reducing learning rate to 1.9827414871542714e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.3188 - val_loss: 1.7423 - lr: 2.0028e-05\n",
      "Epoch 182/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3225\n",
      "Epoch 182: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 182: ReduceLROnPlateau reducing learning rate to 1.9629141443147093e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.3232 - val_loss: 1.7400 - lr: 1.9827e-05\n",
      "Epoch 183/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3264\n",
      "Epoch 183: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 183: ReduceLROnPlateau reducing learning rate to 1.943285069501144e-05.\n",
      "66/66 [==============================] - 2s 34ms/step - loss: 0.3260 - val_loss: 1.7515 - lr: 1.9629e-05\n",
      "Epoch 184/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3227\n",
      "Epoch 184: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 184: ReduceLROnPlateau reducing learning rate to 1.9238522818341152e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.3207 - val_loss: 1.7856 - lr: 1.9433e-05\n",
      "Epoch 185/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3276\n",
      "Epoch 185: val_loss did not improve from 1.73360\n",
      "\n",
      "Epoch 185: ReduceLROnPlateau reducing learning rate to 1.9046138004341628e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.3280 - val_loss: 1.7356 - lr: 1.9239e-05\n",
      "Epoch 186/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3251\n",
      "Epoch 186: val_loss improved from 1.73360 to 1.72345, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3254 - val_loss: 1.7235 - lr: 1.9046e-05\n",
      "Epoch 187/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3345\n",
      "Epoch 187: val_loss improved from 1.72345 to 1.72296, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3337 - val_loss: 1.7230 - lr: 1.9046e-05\n",
      "Epoch 188/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3162\n",
      "Epoch 188: val_loss improved from 1.72296 to 1.72138, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.3145 - val_loss: 1.7214 - lr: 1.9046e-05\n",
      "Epoch 189/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3188\n",
      "Epoch 189: val_loss improved from 1.72138 to 1.71987, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3202 - val_loss: 1.7199 - lr: 1.9046e-05\n",
      "Epoch 190/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3247\n",
      "Epoch 190: val_loss improved from 1.71987 to 1.71708, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3246 - val_loss: 1.7171 - lr: 1.9046e-05\n",
      "Epoch 191/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3268\n",
      "Epoch 191: val_loss did not improve from 1.71708\n",
      "\n",
      "Epoch 191: ReduceLROnPlateau reducing learning rate to 1.885567644421826e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3282 - val_loss: 1.7258 - lr: 1.9046e-05\n",
      "Epoch 192/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3192\n",
      "Epoch 192: val_loss did not improve from 1.71708\n",
      "\n",
      "Epoch 192: ReduceLROnPlateau reducing learning rate to 1.8667120129975956e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3197 - val_loss: 1.7353 - lr: 1.8856e-05\n",
      "Epoch 193/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3203\n",
      "Epoch 193: val_loss did not improve from 1.71708\n",
      "\n",
      "Epoch 193: ReduceLROnPlateau reducing learning rate to 1.8480449252820108e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3184 - val_loss: 1.7196 - lr: 1.8667e-05\n",
      "Epoch 194/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3155\n",
      "Epoch 194: val_loss did not improve from 1.71708\n",
      "\n",
      "Epoch 194: ReduceLROnPlateau reducing learning rate to 1.8295644003956113e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3157 - val_loss: 1.7296 - lr: 1.8480e-05\n",
      "Epoch 195/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3156\n",
      "Epoch 195: val_loss did not improve from 1.71708\n",
      "\n",
      "Epoch 195: ReduceLROnPlateau reducing learning rate to 1.8112688176188387e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3168 - val_loss: 1.7474 - lr: 1.8296e-05\n",
      "Epoch 196/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3280\n",
      "Epoch 196: val_loss did not improve from 1.71708\n",
      "\n",
      "Epoch 196: ReduceLROnPlateau reducing learning rate to 1.793156196072232e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.3282 - val_loss: 1.7595 - lr: 1.8113e-05\n",
      "Epoch 197/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3218\n",
      "Epoch 197: val_loss did not improve from 1.71708\n",
      "\n",
      "Epoch 197: ReduceLROnPlateau reducing learning rate to 1.775224554876331e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.3218 - val_loss: 1.7310 - lr: 1.7932e-05\n",
      "Epoch 198/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3224\n",
      "Epoch 198: val_loss did not improve from 1.71708\n",
      "\n",
      "Epoch 198: ReduceLROnPlateau reducing learning rate to 1.7574722733115777e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.3224 - val_loss: 1.7444 - lr: 1.7752e-05\n",
      "Epoch 199/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3256\n",
      "Epoch 199: val_loss did not improve from 1.71708\n",
      "\n",
      "Epoch 199: ReduceLROnPlateau reducing learning rate to 1.739897550578462e-05.\n",
      "66/66 [==============================] - 4s 60ms/step - loss: 0.3271 - val_loss: 1.7642 - lr: 1.7575e-05\n",
      "Epoch 200/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3215\n",
      "Epoch 200: val_loss did not improve from 1.71708\n",
      "\n",
      "Epoch 200: ReduceLROnPlateau reducing learning rate to 1.7224985858774742e-05.\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.3203 - val_loss: 1.7479 - lr: 1.7399e-05\n",
      "Epoch 201/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3231\n",
      "Epoch 201: val_loss did not improve from 1.71708\n",
      "\n",
      "Epoch 201: ReduceLROnPlateau reducing learning rate to 1.7052735784091056e-05.\n",
      "66/66 [==============================] - 2s 34ms/step - loss: 0.3238 - val_loss: 1.7340 - lr: 1.7225e-05\n",
      "Epoch 202/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3240\n",
      "Epoch 202: val_loss did not improve from 1.71708\n",
      "\n",
      "Epoch 202: ReduceLROnPlateau reducing learning rate to 1.6882209074537968e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.3236 - val_loss: 1.7607 - lr: 1.7053e-05\n",
      "Epoch 203/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3103\n",
      "Epoch 203: val_loss did not improve from 1.71708\n",
      "\n",
      "Epoch 203: ReduceLROnPlateau reducing learning rate to 1.6713387722120388e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.3086 - val_loss: 1.7972 - lr: 1.6882e-05\n",
      "Epoch 204/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3146\n",
      "Epoch 204: val_loss did not improve from 1.71708\n",
      "\n",
      "Epoch 204: ReduceLROnPlateau reducing learning rate to 1.6546253718843217e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.3150 - val_loss: 1.7277 - lr: 1.6713e-05\n",
      "Epoch 205/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3160\n",
      "Epoch 205: val_loss improved from 1.71708 to 1.71086, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3192 - val_loss: 1.7109 - lr: 1.6546e-05\n",
      "Epoch 206/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3136\n",
      "Epoch 206: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 206: ReduceLROnPlateau reducing learning rate to 1.6380790857510874e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3133 - val_loss: 1.7680 - lr: 1.6546e-05\n",
      "Epoch 207/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3312\n",
      "Epoch 207: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 207: ReduceLROnPlateau reducing learning rate to 1.621698293092777e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3312 - val_loss: 1.7333 - lr: 1.6381e-05\n",
      "Epoch 208/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3207\n",
      "Epoch 208: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 208: ReduceLROnPlateau reducing learning rate to 1.605481373189832e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3204 - val_loss: 1.7683 - lr: 1.6217e-05\n",
      "Epoch 209/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3052\n",
      "Epoch 209: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 209: ReduceLROnPlateau reducing learning rate to 1.589426525242743e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.3074 - val_loss: 1.7273 - lr: 1.6055e-05\n",
      "Epoch 210/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3223\n",
      "Epoch 210: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 210: ReduceLROnPlateau reducing learning rate to 1.5735323086119023e-05.\n",
      "66/66 [==============================] - 4s 64ms/step - loss: 0.3223 - val_loss: 1.7157 - lr: 1.5894e-05\n",
      "Epoch 211/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3230\n",
      "Epoch 211: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 211: ReduceLROnPlateau reducing learning rate to 1.5577969224978006e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.3233 - val_loss: 1.7156 - lr: 1.5735e-05\n",
      "Epoch 212/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3107\n",
      "Epoch 212: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 212: ReduceLROnPlateau reducing learning rate to 1.5422189262608297e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3103 - val_loss: 1.7800 - lr: 1.5578e-05\n",
      "Epoch 213/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3327\n",
      "Epoch 213: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 213: ReduceLROnPlateau reducing learning rate to 1.5267966991814318e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3345 - val_loss: 1.7421 - lr: 1.5422e-05\n",
      "Epoch 214/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3204\n",
      "Epoch 214: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 214: ReduceLROnPlateau reducing learning rate to 1.511528800619999e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3209 - val_loss: 1.7512 - lr: 1.5268e-05\n",
      "Epoch 215/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3153\n",
      "Epoch 215: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 215: ReduceLROnPlateau reducing learning rate to 1.496413519816997e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3181 - val_loss: 1.7336 - lr: 1.5115e-05\n",
      "Epoch 216/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3181\n",
      "Epoch 216: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 216: ReduceLROnPlateau reducing learning rate to 1.4814494161328184e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3185 - val_loss: 1.7472 - lr: 1.4964e-05\n",
      "Epoch 217/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3147\n",
      "Epoch 217: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 217: ReduceLROnPlateau reducing learning rate to 1.4666349588878802e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3157 - val_loss: 1.7467 - lr: 1.4814e-05\n",
      "Epoch 218/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3102\n",
      "Epoch 218: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 218: ReduceLROnPlateau reducing learning rate to 1.4519686174025991e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3087 - val_loss: 1.7624 - lr: 1.4666e-05\n",
      "Epoch 219/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3216\n",
      "Epoch 219: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 219: ReduceLROnPlateau reducing learning rate to 1.4374489510373678e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3193 - val_loss: 1.7699 - lr: 1.4520e-05\n",
      "Epoch 220/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3159\n",
      "Epoch 220: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 220: ReduceLROnPlateau reducing learning rate to 1.423074429112603e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.3164 - val_loss: 1.7367 - lr: 1.4374e-05\n",
      "Epoch 221/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3047\n",
      "Epoch 221: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 221: ReduceLROnPlateau reducing learning rate to 1.4088437010286726e-05.\n",
      "66/66 [==============================] - 4s 65ms/step - loss: 0.3047 - val_loss: 1.7467 - lr: 1.4231e-05\n",
      "Epoch 222/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3251\n",
      "Epoch 222: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 222: ReduceLROnPlateau reducing learning rate to 1.3947552361059934e-05.\n",
      "66/66 [==============================] - 5s 81ms/step - loss: 0.3251 - val_loss: 1.7444 - lr: 1.4088e-05\n",
      "Epoch 223/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3120\n",
      "Epoch 223: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 223: ReduceLROnPlateau reducing learning rate to 1.3808076837449334e-05.\n",
      "66/66 [==============================] - 4s 65ms/step - loss: 0.3113 - val_loss: 1.7572 - lr: 1.3948e-05\n",
      "Epoch 224/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3191\n",
      "Epoch 224: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 224: ReduceLROnPlateau reducing learning rate to 1.3669996033058851e-05.\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.3182 - val_loss: 1.7497 - lr: 1.3808e-05\n",
      "Epoch 225/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3104\n",
      "Epoch 225: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 225: ReduceLROnPlateau reducing learning rate to 1.3533296441892163e-05.\n",
      "66/66 [==============================] - 3s 52ms/step - loss: 0.3133 - val_loss: 1.7664 - lr: 1.3670e-05\n",
      "Epoch 226/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3075\n",
      "Epoch 226: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 226: ReduceLROnPlateau reducing learning rate to 1.3397963657553191e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.3076 - val_loss: 1.7527 - lr: 1.3533e-05\n",
      "Epoch 227/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3112\n",
      "Epoch 227: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 227: ReduceLROnPlateau reducing learning rate to 1.3263984174045618e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.3107 - val_loss: 1.7412 - lr: 1.3398e-05\n",
      "Epoch 228/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3136\n",
      "Epoch 228: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 228: ReduceLROnPlateau reducing learning rate to 1.3131344485373119e-05.\n",
      "66/66 [==============================] - 3s 52ms/step - loss: 0.3132 - val_loss: 1.7444 - lr: 1.3264e-05\n",
      "Epoch 229/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3110\n",
      "Epoch 229: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 229: ReduceLROnPlateau reducing learning rate to 1.3000031085539376e-05.\n",
      "66/66 [==============================] - 3s 52ms/step - loss: 0.3110 - val_loss: 1.7670 - lr: 1.3131e-05\n",
      "Epoch 230/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3104\n",
      "Epoch 230: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 230: ReduceLROnPlateau reducing learning rate to 1.2870030468548066e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.3138 - val_loss: 1.7230 - lr: 1.3000e-05\n",
      "Epoch 231/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3155\n",
      "Epoch 231: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 231: ReduceLROnPlateau reducing learning rate to 1.2741330028802622e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.3155 - val_loss: 1.7544 - lr: 1.2870e-05\n",
      "Epoch 232/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3150\n",
      "Epoch 232: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 232: ReduceLROnPlateau reducing learning rate to 1.2613917160706478e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.3162 - val_loss: 1.7442 - lr: 1.2741e-05\n",
      "Epoch 233/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3129\n",
      "Epoch 233: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 233: ReduceLROnPlateau reducing learning rate to 1.2487778358263313e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.3129 - val_loss: 1.7405 - lr: 1.2614e-05\n",
      "Epoch 234/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3106\n",
      "Epoch 234: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 234: ReduceLROnPlateau reducing learning rate to 1.236290101587656e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.3109 - val_loss: 1.7644 - lr: 1.2488e-05\n",
      "Epoch 235/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3170\n",
      "Epoch 235: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 235: ReduceLROnPlateau reducing learning rate to 1.2239271627549898e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.3170 - val_loss: 1.7339 - lr: 1.2363e-05\n",
      "Epoch 236/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3251\n",
      "Epoch 236: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 236: ReduceLROnPlateau reducing learning rate to 1.2116878488086512e-05.\n",
      "66/66 [==============================] - 2s 34ms/step - loss: 0.3225 - val_loss: 1.7549 - lr: 1.2239e-05\n",
      "Epoch 237/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3198\n",
      "Epoch 237: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 237: ReduceLROnPlateau reducing learning rate to 1.1995709892289596e-05.\n",
      "66/66 [==============================] - 2s 34ms/step - loss: 0.3206 - val_loss: 1.7584 - lr: 1.2117e-05\n",
      "Epoch 238/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3058\n",
      "Epoch 238: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 238: ReduceLROnPlateau reducing learning rate to 1.187575323456258e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.3061 - val_loss: 1.8011 - lr: 1.1996e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 239/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3161\n",
      "Epoch 239: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 239: ReduceLROnPlateau reducing learning rate to 1.1756995909308897e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.3147 - val_loss: 1.7710 - lr: 1.1876e-05\n",
      "Epoch 240/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3151\n",
      "Epoch 240: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 240: ReduceLROnPlateau reducing learning rate to 1.1639426211331737e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.3158 - val_loss: 1.7793 - lr: 1.1757e-05\n",
      "Epoch 241/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3156\n",
      "Epoch 241: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 241: ReduceLROnPlateau reducing learning rate to 1.1523031535034534e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.3156 - val_loss: 1.7517 - lr: 1.1639e-05\n",
      "Epoch 242/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3142\n",
      "Epoch 242: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 242: ReduceLROnPlateau reducing learning rate to 1.1407801075620227e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3136 - val_loss: 1.7528 - lr: 1.1523e-05\n",
      "Epoch 243/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3130\n",
      "Epoch 243: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 243: ReduceLROnPlateau reducing learning rate to 1.1293723127892008e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3134 - val_loss: 1.7457 - lr: 1.1408e-05\n",
      "Epoch 244/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3019\n",
      "Epoch 244: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 244: ReduceLROnPlateau reducing learning rate to 1.1180785986653063e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3026 - val_loss: 1.7690 - lr: 1.1294e-05\n",
      "Epoch 245/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3148\n",
      "Epoch 245: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 245: ReduceLROnPlateau reducing learning rate to 1.1068977946706581e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3144 - val_loss: 1.7407 - lr: 1.1181e-05\n",
      "Epoch 246/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3172\n",
      "Epoch 246: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 246: ReduceLROnPlateau reducing learning rate to 1.0958288203255507e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3154 - val_loss: 1.7640 - lr: 1.1069e-05\n",
      "Epoch 247/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3178\n",
      "Epoch 247: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 247: ReduceLROnPlateau reducing learning rate to 1.0848705051103024e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3188 - val_loss: 1.7476 - lr: 1.0958e-05\n",
      "Epoch 248/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3085\n",
      "Epoch 248: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 248: ReduceLROnPlateau reducing learning rate to 1.074021768545208e-05.\n",
      "66/66 [==============================] - 3s 38ms/step - loss: 0.3087 - val_loss: 1.7568 - lr: 1.0849e-05\n",
      "Epoch 249/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3062\n",
      "Epoch 249: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 249: ReduceLROnPlateau reducing learning rate to 1.0632815301505616e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3111 - val_loss: 1.7434 - lr: 1.0740e-05\n",
      "Epoch 250/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3013\n",
      "Epoch 250: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 250: ReduceLROnPlateau reducing learning rate to 1.0526487094466574e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3013 - val_loss: 1.7807 - lr: 1.0633e-05\n",
      "Epoch 251/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3077\n",
      "Epoch 251: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 251: ReduceLROnPlateau reducing learning rate to 1.04212222595379e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3094 - val_loss: 1.7781 - lr: 1.0526e-05\n",
      "Epoch 252/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3052\n",
      "Epoch 252: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 252: ReduceLROnPlateau reducing learning rate to 1.0317009991922532e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3045 - val_loss: 1.7740 - lr: 1.0421e-05\n",
      "Epoch 253/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3068\n",
      "Epoch 253: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 253: ReduceLROnPlateau reducing learning rate to 1.0213839486823417e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3067 - val_loss: 1.7796 - lr: 1.0317e-05\n",
      "Epoch 254/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3182\n",
      "Epoch 254: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 254: ReduceLROnPlateau reducing learning rate to 1.0111700839843252e-05.\n",
      "66/66 [==============================] - 2s 38ms/step - loss: 0.3182 - val_loss: 1.7737 - lr: 1.0214e-05\n",
      "Epoch 255/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3164\n",
      "Epoch 255: val_loss did not improve from 1.71086\n",
      "\n",
      "Epoch 255: ReduceLROnPlateau reducing learning rate to 1.0010584146584733e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.3151 - val_loss: 1.7744 - lr: 1.0112e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_6 (LSTM)                  (None, 45, 24)       3744        ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 45, 24)       0           ['lstm_6[0][0]']                 \n",
      "                                                                                                  \n",
      " lstm_7 (LSTM)                  (None, 45, 16)       2624        ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 45, 16)       0           ['lstm_7[0][0]']                 \n",
      "                                                                                                  \n",
      " lstm_8 (LSTM)                  (None, 32)           6272        ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 40)           1320        ['lstm_8[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 5)            205         ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " tf.unstack_2 (TFOpLambda)      [(None,),            0           ['dense_5[0][0]']                \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_10 (TFOpLambda)  (None, 1)           0           ['tf.unstack_2[0][0]']           \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_4 (TFOpLambda)  (None, 1)           0           ['tf.expand_dims_10[0][0]']      \n",
      "                                                                                                  \n",
      " tf.expand_dims_14 (TFOpLambda)  (None, 1)           0           ['tf.unstack_2[0][4]']           \n",
      "                                                                                                  \n",
      " tf.math.multiply_6 (TFOpLambda  (None, 1)           0           ['tf.math.sigmoid_4[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_5 (TFOpLambda)  (None, 1)           0           ['tf.expand_dims_14[0][0]']      \n",
      "                                                                                                  \n",
      " tf.math.multiply_7 (TFOpLambda  (None, 1)           0           ['tf.math.multiply_6[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_11 (TFOpLambda)  (None, 1)           0           ['tf.unstack_2[0][1]']           \n",
      "                                                                                                  \n",
      " tf.expand_dims_13 (TFOpLambda)  (None, 1)           0           ['tf.unstack_2[0][3]']           \n",
      "                                                                                                  \n",
      " tf.math.multiply_8 (TFOpLambda  (None, 1)           0           ['tf.math.sigmoid_5[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 1)           0           ['tf.math.multiply_7[0][0]']     \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.math.softplus_4 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_11[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_12 (TFOpLambda)  (None, 1)           0           ['tf.unstack_2[0][2]']           \n",
      "                                                                                                  \n",
      " tf.math.softplus_5 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_13[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 1)           0           ['tf.math.multiply_8[0][0]']     \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.stack_2 (TFOpLambda)        (None, 5, 1)         0           ['tf.__operators__.add_4[0][0]', \n",
      "                                                                  'tf.math.softplus_4[0][0]',     \n",
      "                                                                  'tf.expand_dims_12[0][0]',      \n",
      "                                                                  'tf.math.softplus_5[0][0]',     \n",
      "                                                                  'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _1_CCMP_t+1_0.07\n",
      "Epoch 1/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 3.4218\n",
      "Epoch 1: val_loss improved from inf to 4.06911, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 9s 54ms/step - loss: 3.4307 - val_loss: 4.0691 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 2.9259\n",
      "Epoch 2: val_loss improved from 4.06911 to 3.52646, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 2.9259 - val_loss: 3.5265 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.7879\n",
      "Epoch 3: val_loss improved from 3.52646 to 3.15903, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 1.7861 - val_loss: 3.1590 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.2383\n",
      "Epoch 4: val_loss improved from 3.15903 to 2.73489, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.07.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 3s 38ms/step - loss: 1.2385 - val_loss: 2.7349 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0106\n",
      "Epoch 5: val_loss did not improve from 2.73489\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 1.0078 - val_loss: 2.8754 - lr: 1.0000e-04\n",
      "Epoch 6/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8855\n",
      "Epoch 6: val_loss did not improve from 2.73489\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.8871 - val_loss: 2.9368 - lr: 9.9000e-05\n",
      "Epoch 7/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7943\n",
      "Epoch 7: val_loss did not improve from 2.73489\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.7938 - val_loss: 3.1375 - lr: 9.8010e-05\n",
      "Epoch 8/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7473\n",
      "Epoch 8: val_loss did not improve from 2.73489\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.7479 - val_loss: 3.1873 - lr: 9.7030e-05\n",
      "Epoch 9/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7057\n",
      "Epoch 9: val_loss did not improve from 2.73489\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.7047 - val_loss: 3.4986 - lr: 9.6060e-05\n",
      "Epoch 10/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6536\n",
      "Epoch 10: val_loss did not improve from 2.73489\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.6516 - val_loss: 3.3182 - lr: 9.5099e-05\n",
      "Epoch 11/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6601\n",
      "Epoch 11: val_loss did not improve from 2.73489\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.6602 - val_loss: 3.2630 - lr: 9.4148e-05\n",
      "Epoch 12/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6315\n",
      "Epoch 12: val_loss did not improve from 2.73489\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.6307 - val_loss: 3.2639 - lr: 9.3207e-05\n",
      "Epoch 13/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6138\n",
      "Epoch 13: val_loss did not improve from 2.73489\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.6139 - val_loss: 3.5086 - lr: 9.2274e-05\n",
      "Epoch 14/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6029\n",
      "Epoch 14: val_loss did not improve from 2.73489\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 2s 38ms/step - loss: 0.6034 - val_loss: 3.2208 - lr: 9.1352e-05\n",
      "Epoch 15/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5943\n",
      "Epoch 15: val_loss did not improve from 2.73489\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.5943 - val_loss: 3.2624 - lr: 9.0438e-05\n",
      "Epoch 16/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5874\n",
      "Epoch 16: val_loss did not improve from 2.73489\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 3s 38ms/step - loss: 0.5874 - val_loss: 3.0446 - lr: 8.9534e-05\n",
      "Epoch 17/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5797\n",
      "Epoch 17: val_loss did not improve from 2.73489\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.5797 - val_loss: 2.9500 - lr: 8.8638e-05\n",
      "Epoch 18/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5706\n",
      "Epoch 18: val_loss did not improve from 2.73489\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.5706 - val_loss: 3.0281 - lr: 8.7752e-05\n",
      "Epoch 19/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5648\n",
      "Epoch 19: val_loss did not improve from 2.73489\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 5s 68ms/step - loss: 0.5652 - val_loss: 3.0028 - lr: 8.6875e-05\n",
      "Epoch 20/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5442\n",
      "Epoch 20: val_loss did not improve from 2.73489\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.5460 - val_loss: 2.9275 - lr: 8.6006e-05\n",
      "Epoch 21/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5606\n",
      "Epoch 21: val_loss did not improve from 2.73489\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.5600 - val_loss: 2.9894 - lr: 8.5146e-05\n",
      "Epoch 22/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5546\n",
      "Epoch 22: val_loss did not improve from 2.73489\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.5553 - val_loss: 2.9311 - lr: 8.4294e-05\n",
      "Epoch 23/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5325\n",
      "Epoch 23: val_loss did not improve from 2.73489\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.5354 - val_loss: 2.9670 - lr: 8.3451e-05\n",
      "Epoch 24/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5393\n",
      "Epoch 24: val_loss did not improve from 2.73489\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.5411 - val_loss: 2.7539 - lr: 8.2617e-05\n",
      "Epoch 25/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5394\n",
      "Epoch 25: val_loss did not improve from 2.73489\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.5392 - val_loss: 2.9973 - lr: 8.1791e-05\n",
      "Epoch 26/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5299\n",
      "Epoch 26: val_loss did not improve from 2.73489\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.5322 - val_loss: 2.8534 - lr: 8.0973e-05\n",
      "Epoch 27/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5312\n",
      "Epoch 27: val_loss did not improve from 2.73489\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.5295 - val_loss: 2.8109 - lr: 8.0163e-05\n",
      "Epoch 28/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5212\n",
      "Epoch 28: val_loss did not improve from 2.73489\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.5212 - val_loss: 2.9398 - lr: 7.9361e-05\n",
      "Epoch 29/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5289\n",
      "Epoch 29: val_loss did not improve from 2.73489\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.5266 - val_loss: 2.8707 - lr: 7.8568e-05\n",
      "Epoch 30/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5119\n",
      "Epoch 30: val_loss did not improve from 2.73489\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.5119 - val_loss: 2.9021 - lr: 7.7782e-05\n",
      "Epoch 31/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5093\n",
      "Epoch 31: val_loss improved from 2.73489 to 2.68686, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.5136 - val_loss: 2.6869 - lr: 7.7004e-05\n",
      "Epoch 32/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5216\n",
      "Epoch 32: val_loss did not improve from 2.68686\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.5210 - val_loss: 2.7699 - lr: 7.7004e-05\n",
      "Epoch 33/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5194\n",
      "Epoch 33: val_loss improved from 2.68686 to 2.66001, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.5205 - val_loss: 2.6600 - lr: 7.6234e-05\n",
      "Epoch 34/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5103\n",
      "Epoch 34: val_loss did not improve from 2.66001\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 2s 33ms/step - loss: 0.5090 - val_loss: 2.8228 - lr: 7.6234e-05\n",
      "Epoch 35/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4943\n",
      "Epoch 35: val_loss did not improve from 2.66001\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 2s 33ms/step - loss: 0.4943 - val_loss: 2.7240 - lr: 7.5472e-05\n",
      "Epoch 36/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4960\n",
      "Epoch 36: val_loss did not improve from 2.66001\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 2s 34ms/step - loss: 0.4977 - val_loss: 2.7507 - lr: 7.4717e-05\n",
      "Epoch 37/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5178\n",
      "Epoch 37: val_loss did not improve from 2.66001\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 2s 34ms/step - loss: 0.5162 - val_loss: 2.6680 - lr: 7.3970e-05\n",
      "Epoch 38/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4893\n",
      "Epoch 38: val_loss improved from 2.66001 to 2.54899, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.4886 - val_loss: 2.5490 - lr: 7.3230e-05\n",
      "Epoch 39/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5057\n",
      "Epoch 39: val_loss improved from 2.54899 to 2.48283, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.5049 - val_loss: 2.4828 - lr: 7.3230e-05\n",
      "Epoch 40/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5016\n",
      "Epoch 40: val_loss did not improve from 2.48283\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.5003 - val_loss: 2.5878 - lr: 7.3230e-05\n",
      "Epoch 41/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4864\n",
      "Epoch 41: val_loss did not improve from 2.48283\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.4890 - val_loss: 2.5887 - lr: 7.2498e-05\n",
      "Epoch 42/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4909\n",
      "Epoch 42: val_loss did not improve from 2.48283\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.4918 - val_loss: 2.5876 - lr: 7.1773e-05\n",
      "Epoch 43/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4886\n",
      "Epoch 43: val_loss improved from 2.48283 to 2.33194, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4888 - val_loss: 2.3319 - lr: 7.1055e-05\n",
      "Epoch 44/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4849\n",
      "Epoch 44: val_loss did not improve from 2.33194\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.4831 - val_loss: 2.5263 - lr: 7.1055e-05\n",
      "Epoch 45/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4778\n",
      "Epoch 45: val_loss improved from 2.33194 to 2.28238, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4790 - val_loss: 2.2824 - lr: 7.0345e-05\n",
      "Epoch 46/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4807\n",
      "Epoch 46: val_loss improved from 2.28238 to 2.26742, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4808 - val_loss: 2.2674 - lr: 7.0345e-05\n",
      "Epoch 47/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4756\n",
      "Epoch 47: val_loss did not improve from 2.26742\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.4782 - val_loss: 2.3102 - lr: 7.0345e-05\n",
      "Epoch 48/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4775\n",
      "Epoch 48: val_loss did not improve from 2.26742\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.4755 - val_loss: 2.2749 - lr: 6.9641e-05\n",
      "Epoch 49/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4713\n",
      "Epoch 49: val_loss did not improve from 2.26742\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.4701 - val_loss: 2.4201 - lr: 6.8945e-05\n",
      "Epoch 50/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4697\n",
      "Epoch 50: val_loss improved from 2.26742 to 2.18930, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4699 - val_loss: 2.1893 - lr: 6.8255e-05\n",
      "Epoch 51/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4745\n",
      "Epoch 51: val_loss did not improve from 2.18930\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4757 - val_loss: 2.3116 - lr: 6.8255e-05\n",
      "Epoch 52/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4764\n",
      "Epoch 52: val_loss did not improve from 2.18930\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.4760 - val_loss: 2.2306 - lr: 6.7573e-05\n",
      "Epoch 53/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4716\n",
      "Epoch 53: val_loss improved from 2.18930 to 2.15468, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4687 - val_loss: 2.1547 - lr: 6.6897e-05\n",
      "Epoch 54/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4630\n",
      "Epoch 54: val_loss did not improve from 2.15468\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4624 - val_loss: 2.3252 - lr: 6.6897e-05\n",
      "Epoch 55/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4630\n",
      "Epoch 55: val_loss did not improve from 2.15468\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4647 - val_loss: 2.2157 - lr: 6.6228e-05\n",
      "Epoch 56/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/66 [============================>.] - ETA: 0s - loss: 0.4629\n",
      "Epoch 56: val_loss did not improve from 2.15468\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.4627 - val_loss: 2.1735 - lr: 6.5566e-05\n",
      "Epoch 57/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4647\n",
      "Epoch 57: val_loss improved from 2.15468 to 2.12437, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4652 - val_loss: 2.1244 - lr: 6.4910e-05\n",
      "Epoch 58/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4566\n",
      "Epoch 58: val_loss did not improve from 2.12437\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4555 - val_loss: 2.1556 - lr: 6.4910e-05\n",
      "Epoch 59/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4607\n",
      "Epoch 59: val_loss improved from 2.12437 to 2.09541, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.4579 - val_loss: 2.0954 - lr: 6.4261e-05\n",
      "Epoch 60/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4624\n",
      "Epoch 60: val_loss did not improve from 2.09541\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.4617 - val_loss: 2.1527 - lr: 6.4261e-05\n",
      "Epoch 61/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4527\n",
      "Epoch 61: val_loss improved from 2.09541 to 2.05321, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4522 - val_loss: 2.0532 - lr: 6.3619e-05\n",
      "Epoch 62/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4655\n",
      "Epoch 62: val_loss improved from 2.05321 to 1.99768, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.4657 - val_loss: 1.9977 - lr: 6.3619e-05\n",
      "Epoch 63/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4550\n",
      "Epoch 63: val_loss improved from 1.99768 to 1.79585, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 2s 38ms/step - loss: 0.4554 - val_loss: 1.7958 - lr: 6.3619e-05\n",
      "Epoch 64/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4569\n",
      "Epoch 64: val_loss did not improve from 1.79585\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4560 - val_loss: 1.9341 - lr: 6.3619e-05\n",
      "Epoch 65/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4493\n",
      "Epoch 65: val_loss did not improve from 1.79585\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4559 - val_loss: 1.9282 - lr: 6.2982e-05\n",
      "Epoch 66/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4545\n",
      "Epoch 66: val_loss did not improve from 1.79585\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4521 - val_loss: 1.8806 - lr: 6.2353e-05\n",
      "Epoch 67/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4460\n",
      "Epoch 67: val_loss did not improve from 1.79585\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4469 - val_loss: 1.8948 - lr: 6.1729e-05\n",
      "Epoch 68/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4529\n",
      "Epoch 68: val_loss did not improve from 1.79585\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4528 - val_loss: 1.8095 - lr: 6.1112e-05\n",
      "Epoch 69/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4514\n",
      "Epoch 69: val_loss did not improve from 1.79585\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4498 - val_loss: 1.9422 - lr: 6.0501e-05\n",
      "Epoch 70/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4373\n",
      "Epoch 70: val_loss did not improve from 1.79585\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4426 - val_loss: 1.8277 - lr: 5.9896e-05\n",
      "Epoch 71/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4340\n",
      "Epoch 71: val_loss did not improve from 1.79585\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 5.8703650938696225e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4345 - val_loss: 1.8617 - lr: 5.9297e-05\n",
      "Epoch 72/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4501\n",
      "Epoch 72: val_loss did not improve from 1.79585\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 5.811661550978897e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4483 - val_loss: 1.8737 - lr: 5.8704e-05\n",
      "Epoch 73/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4537\n",
      "Epoch 73: val_loss did not improve from 1.79585\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 5.753545068728272e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4519 - val_loss: 1.8325 - lr: 5.8117e-05\n",
      "Epoch 74/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4446\n",
      "Epoch 74: val_loss improved from 1.79585 to 1.72723, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.4436 - val_loss: 1.7272 - lr: 5.7535e-05\n",
      "Epoch 75/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4461\n",
      "Epoch 75: val_loss did not improve from 1.72723\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 5.6960095243994146e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4475 - val_loss: 1.7598 - lr: 5.7535e-05\n",
      "Epoch 76/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4390\n",
      "Epoch 76: val_loss improved from 1.72723 to 1.70801, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.4440 - val_loss: 1.7080 - lr: 5.6960e-05\n",
      "Epoch 77/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4448\n",
      "Epoch 77: val_loss improved from 1.70801 to 1.64842, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.4457 - val_loss: 1.6484 - lr: 5.6960e-05\n",
      "Epoch 78/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4386\n",
      "Epoch 78: val_loss did not improve from 1.64842\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 5.639049515593797e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4381 - val_loss: 1.6767 - lr: 5.6960e-05\n",
      "Epoch 79/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4340\n",
      "Epoch 79: val_loss did not improve from 1.64842\n",
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 5.5826589195930865e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4341 - val_loss: 1.7117 - lr: 5.6390e-05\n",
      "Epoch 80/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4282\n",
      "Epoch 80: val_loss did not improve from 1.64842\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 5.526832333998754e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4319 - val_loss: 1.7931 - lr: 5.5827e-05\n",
      "Epoch 81/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4403\n",
      "Epoch 81: val_loss improved from 1.64842 to 1.61936, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.07.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 2s 37ms/step - loss: 0.4414 - val_loss: 1.6194 - lr: 5.5268e-05\n",
      "Epoch 82/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4403\n",
      "Epoch 82: val_loss did not improve from 1.61936\n",
      "\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 5.471563996252371e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4393 - val_loss: 1.6498 - lr: 5.5268e-05\n",
      "Epoch 83/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4462\n",
      "Epoch 83: val_loss did not improve from 1.61936\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 5.416848503955407e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4468 - val_loss: 1.6747 - lr: 5.4716e-05\n",
      "Epoch 84/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4212\n",
      "Epoch 84: val_loss did not improve from 1.61936\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 5.362680094549433e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4204 - val_loss: 1.7199 - lr: 5.4168e-05\n",
      "Epoch 85/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4361\n",
      "Epoch 85: val_loss did not improve from 1.61936\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 5.309053365635918e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4349 - val_loss: 1.7521 - lr: 5.3627e-05\n",
      "Epoch 86/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4307\n",
      "Epoch 86: val_loss did not improve from 1.61936\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 5.2559629148163366e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4312 - val_loss: 1.7020 - lr: 5.3091e-05\n",
      "Epoch 87/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4435\n",
      "Epoch 87: val_loss did not improve from 1.61936\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 5.2034033396921584e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4435 - val_loss: 1.6396 - lr: 5.2560e-05\n",
      "Epoch 88/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4420\n",
      "Epoch 88: val_loss did not improve from 1.61936\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 5.1513692378648555e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4405 - val_loss: 1.6433 - lr: 5.2034e-05\n",
      "Epoch 89/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4375\n",
      "Epoch 89: val_loss did not improve from 1.61936\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 5.099855567095801e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4352 - val_loss: 1.7067 - lr: 5.1514e-05\n",
      "Epoch 90/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4280\n",
      "Epoch 90: val_loss did not improve from 1.61936\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 5.048856924986467e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4287 - val_loss: 1.7074 - lr: 5.0999e-05\n",
      "Epoch 91/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4306\n",
      "Epoch 91: val_loss did not improve from 1.61936\n",
      "\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 4.998368269298226e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4305 - val_loss: 1.6956 - lr: 5.0489e-05\n",
      "Epoch 92/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4247\n",
      "Epoch 92: val_loss did not improve from 1.61936\n",
      "\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 4.9483845577924514e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4226 - val_loss: 1.6305 - lr: 4.9984e-05\n",
      "Epoch 93/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4384\n",
      "Epoch 93: val_loss did not improve from 1.61936\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 4.898900748230517e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4381 - val_loss: 1.6391 - lr: 4.9484e-05\n",
      "Epoch 94/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4259\n",
      "Epoch 94: val_loss improved from 1.61936 to 1.61773, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.4250 - val_loss: 1.6177 - lr: 4.8989e-05\n",
      "Epoch 95/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4144\n",
      "Epoch 95: val_loss did not improve from 1.61773\n",
      "\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 4.849911798373796e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4152 - val_loss: 1.6426 - lr: 4.8989e-05\n",
      "Epoch 96/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4321\n",
      "Epoch 96: val_loss improved from 1.61773 to 1.59113, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.4304 - val_loss: 1.5911 - lr: 4.8499e-05\n",
      "Epoch 97/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4352\n",
      "Epoch 97: val_loss improved from 1.59113 to 1.58846, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.4331 - val_loss: 1.5885 - lr: 4.8499e-05\n",
      "Epoch 98/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4205\n",
      "Epoch 98: val_loss did not improve from 1.58846\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 4.801412665983662e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.4210 - val_loss: 1.6253 - lr: 4.8499e-05\n",
      "Epoch 99/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4275\n",
      "Epoch 99: val_loss improved from 1.58846 to 1.55602, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.4275 - val_loss: 1.5560 - lr: 4.8014e-05\n",
      "Epoch 100/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4246\n",
      "Epoch 100: val_loss did not improve from 1.55602\n",
      "\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 4.75339866898139e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.4246 - val_loss: 1.5673 - lr: 4.8014e-05\n",
      "Epoch 101/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4286\n",
      "Epoch 101: val_loss did not improve from 1.55602\n",
      "\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 4.705864765128354e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.4286 - val_loss: 1.5561 - lr: 4.7534e-05\n",
      "Epoch 102/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4159\n",
      "Epoch 102: val_loss did not improve from 1.55602\n",
      "\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 4.658806272345828e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.4159 - val_loss: 1.6562 - lr: 4.7059e-05\n",
      "Epoch 103/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4230\n",
      "Epoch 103: val_loss did not improve from 1.55602\n",
      "\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 4.612218148395186e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.4230 - val_loss: 1.6155 - lr: 4.6588e-05\n",
      "Epoch 104/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4139\n",
      "Epoch 104: val_loss did not improve from 1.55602\n",
      "\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 4.566096071357606e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.4132 - val_loss: 1.7022 - lr: 4.6122e-05\n",
      "Epoch 105/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4191\n",
      "Epoch 105: val_loss did not improve from 1.55602\n",
      "\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 4.52043499899446e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.4173 - val_loss: 1.7146 - lr: 4.5661e-05\n",
      "Epoch 106/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4273\n",
      "Epoch 106: val_loss did not improve from 1.55602\n",
      "\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 4.475230609386927e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.4273 - val_loss: 1.6161 - lr: 4.5204e-05\n",
      "Epoch 107/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4093\n",
      "Epoch 107: val_loss did not improve from 1.55602\n",
      "\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 4.43047822045628e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 3s 44ms/step - loss: 0.4114 - val_loss: 1.5583 - lr: 4.4752e-05\n",
      "Epoch 108/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4106\n",
      "Epoch 108: val_loss did not improve from 1.55602\n",
      "\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 4.386173510283697e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.4110 - val_loss: 1.6072 - lr: 4.4305e-05\n",
      "Epoch 109/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4094\n",
      "Epoch 109: val_loss did not improve from 1.55602\n",
      "\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 4.342311796790454e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.4095 - val_loss: 1.5988 - lr: 4.3862e-05\n",
      "Epoch 110/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4159\n",
      "Epoch 110: val_loss did not improve from 1.55602\n",
      "\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 4.298888758057728e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.4157 - val_loss: 1.6004 - lr: 4.3423e-05\n",
      "Epoch 111/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4133\n",
      "Epoch 111: val_loss did not improve from 1.55602\n",
      "\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 4.2558997120067946e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.4119 - val_loss: 1.5887 - lr: 4.2989e-05\n",
      "Epoch 112/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4193\n",
      "Epoch 112: val_loss did not improve from 1.55602\n",
      "\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 4.2133406968787315e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.4193 - val_loss: 1.6327 - lr: 4.2559e-05\n",
      "Epoch 113/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4141\n",
      "Epoch 113: val_loss did not improve from 1.55602\n",
      "\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 4.1712073907547166e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.4141 - val_loss: 1.6741 - lr: 4.2133e-05\n",
      "Epoch 114/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4132\n",
      "Epoch 114: val_loss did not improve from 1.55602\n",
      "\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 4.1294954717159274e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.4135 - val_loss: 1.5615 - lr: 4.1712e-05\n",
      "Epoch 115/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4260\n",
      "Epoch 115: val_loss improved from 1.55602 to 1.54094, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.4260 - val_loss: 1.5409 - lr: 4.1295e-05\n",
      "Epoch 116/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4116\n",
      "Epoch 116: val_loss improved from 1.54094 to 1.49129, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.4090 - val_loss: 1.4913 - lr: 4.1295e-05\n",
      "Epoch 117/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4163\n",
      "Epoch 117: val_loss did not improve from 1.49129\n",
      "\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 4.08820061784354e-05.\n",
      "66/66 [==============================] - 2s 34ms/step - loss: 0.4160 - val_loss: 1.5552 - lr: 4.1295e-05\n",
      "Epoch 118/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4102\n",
      "Epoch 118: val_loss did not improve from 1.49129\n",
      "\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 4.0473185072187336e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.4108 - val_loss: 1.6393 - lr: 4.0882e-05\n",
      "Epoch 119/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4122\n",
      "Epoch 119: val_loss did not improve from 1.49129\n",
      "\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 4.0068451780825854e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.4144 - val_loss: 1.4948 - lr: 4.0473e-05\n",
      "Epoch 120/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4079\n",
      "Epoch 120: val_loss did not improve from 1.49129\n",
      "\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 3.966776668676175e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.4086 - val_loss: 1.6033 - lr: 4.0068e-05\n",
      "Epoch 121/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4082\n",
      "Epoch 121: val_loss did not improve from 1.49129\n",
      "\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 3.927109017240582e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.4102 - val_loss: 1.5689 - lr: 3.9668e-05\n",
      "Epoch 122/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4182\n",
      "Epoch 122: val_loss did not improve from 1.49129\n",
      "\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 3.887837901856983e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.4178 - val_loss: 1.4928 - lr: 3.9271e-05\n",
      "Epoch 123/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4194\n",
      "Epoch 123: val_loss did not improve from 1.49129\n",
      "\n",
      "Epoch 123: ReduceLROnPlateau reducing learning rate to 3.848959360766457e-05.\n",
      "66/66 [==============================] - 2s 38ms/step - loss: 0.4185 - val_loss: 1.5416 - lr: 3.8878e-05\n",
      "Epoch 124/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4189\n",
      "Epoch 124: val_loss did not improve from 1.49129\n",
      "\n",
      "Epoch 124: ReduceLROnPlateau reducing learning rate to 3.810469792369986e-05.\n",
      "66/66 [==============================] - 3s 38ms/step - loss: 0.4185 - val_loss: 1.5263 - lr: 3.8490e-05\n",
      "Epoch 125/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4079\n",
      "Epoch 125: val_loss did not improve from 1.49129\n",
      "\n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 3.772365234908648e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4078 - val_loss: 1.6036 - lr: 3.8105e-05\n",
      "Epoch 126/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4140\n",
      "Epoch 126: val_loss did not improve from 1.49129\n",
      "\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 3.734641726623522e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4122 - val_loss: 1.5919 - lr: 3.7724e-05\n",
      "Epoch 127/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4216\n",
      "Epoch 127: val_loss did not improve from 1.49129\n",
      "\n",
      "Epoch 127: ReduceLROnPlateau reducing learning rate to 3.6972953057556876e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.4216 - val_loss: 1.5240 - lr: 3.7346e-05\n",
      "Epoch 128/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3993\n",
      "Epoch 128: val_loss did not improve from 1.49129\n",
      "\n",
      "Epoch 128: ReduceLROnPlateau reducing learning rate to 3.660322370706126e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3994 - val_loss: 1.5574 - lr: 3.6973e-05\n",
      "Epoch 129/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4010\n",
      "Epoch 129: val_loss did not improve from 1.49129\n",
      "\n",
      "Epoch 129: ReduceLROnPlateau reducing learning rate to 3.623719319875818e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4011 - val_loss: 1.5416 - lr: 3.6603e-05\n",
      "Epoch 130/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4081\n",
      "Epoch 130: val_loss did not improve from 1.49129\n",
      "\n",
      "Epoch 130: ReduceLROnPlateau reducing learning rate to 3.5874821915058416e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4081 - val_loss: 1.5386 - lr: 3.6237e-05\n",
      "Epoch 131/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4136\n",
      "Epoch 131: val_loss did not improve from 1.49129\n",
      "\n",
      "Epoch 131: ReduceLROnPlateau reducing learning rate to 3.551607383997179e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4119 - val_loss: 1.5745 - lr: 3.5875e-05\n",
      "Epoch 132/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4041\n",
      "Epoch 132: val_loss did not improve from 1.49129\n",
      "\n",
      "Epoch 132: ReduceLROnPlateau reducing learning rate to 3.516091295750812e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4048 - val_loss: 1.5561 - lr: 3.5516e-05\n",
      "Epoch 133/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4071\n",
      "Epoch 133: val_loss did not improve from 1.49129\n",
      "\n",
      "Epoch 133: ReduceLROnPlateau reducing learning rate to 3.480930325167719e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4059 - val_loss: 1.5716 - lr: 3.5161e-05\n",
      "Epoch 134/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4015\n",
      "Epoch 134: val_loss did not improve from 1.49129\n",
      "\n",
      "Epoch 134: ReduceLROnPlateau reducing learning rate to 3.446120870648883e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4010 - val_loss: 1.5901 - lr: 3.4809e-05\n",
      "Epoch 135/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4011\n",
      "Epoch 135: val_loss did not improve from 1.49129\n",
      "\n",
      "Epoch 135: ReduceLROnPlateau reducing learning rate to 3.4116596907551866e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.4011 - val_loss: 1.5689 - lr: 3.4461e-05\n",
      "Epoch 136/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4024\n",
      "Epoch 136: val_loss did not improve from 1.49129\n",
      "\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 3.37754318388761e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4038 - val_loss: 1.5617 - lr: 3.4117e-05\n",
      "Epoch 137/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4077\n",
      "Epoch 137: val_loss did not improve from 1.49129\n",
      "\n",
      "Epoch 137: ReduceLROnPlateau reducing learning rate to 3.3437677484471354e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4072 - val_loss: 1.5270 - lr: 3.3775e-05\n",
      "Epoch 138/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4000\n",
      "Epoch 138: val_loss improved from 1.49129 to 1.45840, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.4015 - val_loss: 1.4584 - lr: 3.3438e-05\n",
      "Epoch 139/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4086\n",
      "Epoch 139: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 139: ReduceLROnPlateau reducing learning rate to 3.310330142994644e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4068 - val_loss: 1.5336 - lr: 3.3438e-05\n",
      "Epoch 140/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4022\n",
      "Epoch 140: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 140: ReduceLROnPlateau reducing learning rate to 3.277226765931118e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4041 - val_loss: 1.5210 - lr: 3.3103e-05\n",
      "Epoch 141/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3968\n",
      "Epoch 141: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 141: ReduceLROnPlateau reducing learning rate to 3.24445437581744e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4012 - val_loss: 1.4674 - lr: 3.2772e-05\n",
      "Epoch 142/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4000\n",
      "Epoch 142: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 142: ReduceLROnPlateau reducing learning rate to 3.212009731214493e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3986 - val_loss: 1.5511 - lr: 3.2445e-05\n",
      "Epoch 143/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3926\n",
      "Epoch 143: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 143: ReduceLROnPlateau reducing learning rate to 3.17988959068316e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3919 - val_loss: 1.5343 - lr: 3.2120e-05\n",
      "Epoch 144/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3833\n",
      "Epoch 144: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 144: ReduceLROnPlateau reducing learning rate to 3.1480907127843236e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3896 - val_loss: 1.5728 - lr: 3.1799e-05\n",
      "Epoch 145/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3994\n",
      "Epoch 145: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 145: ReduceLROnPlateau reducing learning rate to 3.116609856078867e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3997 - val_loss: 1.5778 - lr: 3.1481e-05\n",
      "Epoch 146/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3895\n",
      "Epoch 146: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 146: ReduceLROnPlateau reducing learning rate to 3.085443779127672e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3937 - val_loss: 1.4985 - lr: 3.1166e-05\n",
      "Epoch 147/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4030\n",
      "Epoch 147: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 147: ReduceLROnPlateau reducing learning rate to 3.054589240491623e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4040 - val_loss: 1.5844 - lr: 3.0854e-05\n",
      "Epoch 148/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4190\n",
      "Epoch 148: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 148: ReduceLROnPlateau reducing learning rate to 3.024043358891504e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4182 - val_loss: 1.5057 - lr: 3.0546e-05\n",
      "Epoch 149/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4046\n",
      "Epoch 149: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 149: ReduceLROnPlateau reducing learning rate to 2.9938028928881975e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.4037 - val_loss: 1.4970 - lr: 3.0240e-05\n",
      "Epoch 150/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3978\n",
      "Epoch 150: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 150: ReduceLROnPlateau reducing learning rate to 2.963864781122538e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3968 - val_loss: 1.4860 - lr: 2.9938e-05\n",
      "Epoch 151/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3908\n",
      "Epoch 151: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 151: ReduceLROnPlateau reducing learning rate to 2.9342261423153105e-05.\n",
      "66/66 [==============================] - 2s 38ms/step - loss: 0.3933 - val_loss: 1.5736 - lr: 2.9639e-05\n",
      "Epoch 152/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4096\n",
      "Epoch 152: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 152: ReduceLROnPlateau reducing learning rate to 2.9048839151073478e-05.\n",
      "66/66 [==============================] - 4s 59ms/step - loss: 0.4096 - val_loss: 1.5607 - lr: 2.9342e-05\n",
      "Epoch 153/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3990\n",
      "Epoch 153: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 153: ReduceLROnPlateau reducing learning rate to 2.875835038139485e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.3990 - val_loss: 1.5490 - lr: 2.9049e-05\n",
      "Epoch 154/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4008\n",
      "Epoch 154: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 154: ReduceLROnPlateau reducing learning rate to 2.8470766301325058e-05.\n",
      "66/66 [==============================] - 4s 63ms/step - loss: 0.4011 - val_loss: 1.4916 - lr: 2.8758e-05\n",
      "Epoch 155/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3911\n",
      "Epoch 155: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 155: ReduceLROnPlateau reducing learning rate to 2.8186058098071954e-05.\n",
      "66/66 [==============================] - 4s 53ms/step - loss: 0.3909 - val_loss: 1.5057 - lr: 2.8471e-05\n",
      "Epoch 156/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4027\n",
      "Epoch 156: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 156: ReduceLROnPlateau reducing learning rate to 2.7904196958843385e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.4028 - val_loss: 1.5421 - lr: 2.8186e-05\n",
      "Epoch 157/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4014\n",
      "Epoch 157: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 157: ReduceLROnPlateau reducing learning rate to 2.7625155871646713e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.3987 - val_loss: 1.4799 - lr: 2.7904e-05\n",
      "Epoch 158/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3944\n",
      "Epoch 158: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 158: ReduceLROnPlateau reducing learning rate to 2.734890422289027e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.3959 - val_loss: 1.5231 - lr: 2.7625e-05\n",
      "Epoch 159/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3975\n",
      "Epoch 159: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 159: ReduceLROnPlateau reducing learning rate to 2.7075415000581416e-05.\n",
      "66/66 [==============================] - 3s 38ms/step - loss: 0.3975 - val_loss: 1.5418 - lr: 2.7349e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3973\n",
      "Epoch 160: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 160: ReduceLROnPlateau reducing learning rate to 2.6804661192727508e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.3956 - val_loss: 1.5018 - lr: 2.7075e-05\n",
      "Epoch 161/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3948\n",
      "Epoch 161: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 161: ReduceLROnPlateau reducing learning rate to 2.6536613986536395e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.3955 - val_loss: 1.4987 - lr: 2.6805e-05\n",
      "Epoch 162/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3839\n",
      "Epoch 162: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 162: ReduceLROnPlateau reducing learning rate to 2.6271248170814942e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.3856 - val_loss: 1.5151 - lr: 2.6537e-05\n",
      "Epoch 163/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4010\n",
      "Epoch 163: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 163: ReduceLROnPlateau reducing learning rate to 2.6008534932770998e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.4007 - val_loss: 1.5395 - lr: 2.6271e-05\n",
      "Epoch 164/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4006\n",
      "Epoch 164: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 164: ReduceLROnPlateau reducing learning rate to 2.5748449061211432e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.4032 - val_loss: 1.5060 - lr: 2.6009e-05\n",
      "Epoch 165/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3969\n",
      "Epoch 165: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 165: ReduceLROnPlateau reducing learning rate to 2.5490965344943107e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.3978 - val_loss: 1.5044 - lr: 2.5748e-05\n",
      "Epoch 166/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3911\n",
      "Epoch 166: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 166: ReduceLROnPlateau reducing learning rate to 2.523605497117387e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.3904 - val_loss: 1.5650 - lr: 2.5491e-05\n",
      "Epoch 167/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3959\n",
      "Epoch 167: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 167: ReduceLROnPlateau reducing learning rate to 2.4983694529510104e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.3969 - val_loss: 1.5300 - lr: 2.5236e-05\n",
      "Epoch 168/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3847\n",
      "Epoch 168: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 168: ReduceLROnPlateau reducing learning rate to 2.4733857007959158e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.3847 - val_loss: 1.5210 - lr: 2.4984e-05\n",
      "Epoch 169/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3954\n",
      "Epoch 169: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 169: ReduceLROnPlateau reducing learning rate to 2.4486518996127415e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.3951 - val_loss: 1.5258 - lr: 2.4734e-05\n",
      "Epoch 170/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3948\n",
      "Epoch 170: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 170: ReduceLROnPlateau reducing learning rate to 2.424165348202223e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.3956 - val_loss: 1.4721 - lr: 2.4487e-05\n",
      "Epoch 171/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3981\n",
      "Epoch 171: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 171: ReduceLROnPlateau reducing learning rate to 2.3999237055249977e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.3968 - val_loss: 1.4635 - lr: 2.4242e-05\n",
      "Epoch 172/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3805\n",
      "Epoch 172: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 172: ReduceLROnPlateau reducing learning rate to 2.375924450461753e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.3818 - val_loss: 1.4986 - lr: 2.3999e-05\n",
      "Epoch 173/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3862\n",
      "Epoch 173: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 173: ReduceLROnPlateau reducing learning rate to 2.3521652419731254e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.3872 - val_loss: 1.5452 - lr: 2.3759e-05\n",
      "Epoch 174/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3937\n",
      "Epoch 174: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 174: ReduceLROnPlateau reducing learning rate to 2.3286435589398024e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.3925 - val_loss: 1.4727 - lr: 2.3522e-05\n",
      "Epoch 175/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3951\n",
      "Epoch 175: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 175: ReduceLROnPlateau reducing learning rate to 2.3053570603224217e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.3965 - val_loss: 1.4855 - lr: 2.3286e-05\n",
      "Epoch 176/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3882\n",
      "Epoch 176: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 176: ReduceLROnPlateau reducing learning rate to 2.2823034050816205e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.3882 - val_loss: 1.4855 - lr: 2.3054e-05\n",
      "Epoch 177/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3869\n",
      "Epoch 177: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 177: ReduceLROnPlateau reducing learning rate to 2.2594804322579874e-05.\n",
      "66/66 [==============================] - 2s 34ms/step - loss: 0.3860 - val_loss: 1.4945 - lr: 2.2823e-05\n",
      "Epoch 178/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3911\n",
      "Epoch 178: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 178: ReduceLROnPlateau reducing learning rate to 2.2368856207322096e-05.\n",
      "66/66 [==============================] - 2s 34ms/step - loss: 0.3945 - val_loss: 1.5111 - lr: 2.2595e-05\n",
      "Epoch 179/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4027\n",
      "Epoch 179: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 179: ReduceLROnPlateau reducing learning rate to 2.2145168095448753e-05.\n",
      "66/66 [==============================] - 2s 34ms/step - loss: 0.4031 - val_loss: 1.4629 - lr: 2.2369e-05\n",
      "Epoch 180/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3988\n",
      "Epoch 180: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 180: ReduceLROnPlateau reducing learning rate to 2.1923716576566222e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.3995 - val_loss: 1.4624 - lr: 2.2145e-05\n",
      "Epoch 181/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3999\n",
      "Epoch 181: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 181: ReduceLROnPlateau reducing learning rate to 2.1704480041080387e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.3990 - val_loss: 1.4702 - lr: 2.1924e-05\n",
      "Epoch 182/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3888\n",
      "Epoch 182: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 182: ReduceLROnPlateau reducing learning rate to 2.1487435078597626e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.3883 - val_loss: 1.4816 - lr: 2.1704e-05\n",
      "Epoch 183/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3875\n",
      "Epoch 183: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 183: ReduceLROnPlateau reducing learning rate to 2.1272560079523828e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.3864 - val_loss: 1.5068 - lr: 2.1487e-05\n",
      "Epoch 184/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3820\n",
      "Epoch 184: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 184: ReduceLROnPlateau reducing learning rate to 2.1059835235064383e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.3858 - val_loss: 1.4929 - lr: 2.1273e-05\n",
      "Epoch 185/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3820\n",
      "Epoch 185: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 185: ReduceLROnPlateau reducing learning rate to 2.084923713482567e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.3809 - val_loss: 1.5275 - lr: 2.1060e-05\n",
      "Epoch 186/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3887\n",
      "Epoch 186: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 186: ReduceLROnPlateau reducing learning rate to 2.0640744169213575e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3895 - val_loss: 1.4874 - lr: 2.0849e-05\n",
      "Epoch 187/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3899\n",
      "Epoch 187: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 187: ReduceLROnPlateau reducing learning rate to 2.0434336529433495e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.3892 - val_loss: 1.5131 - lr: 2.0641e-05\n",
      "Epoch 188/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3857\n",
      "Epoch 188: val_loss did not improve from 1.45840\n",
      "\n",
      "Epoch 188: ReduceLROnPlateau reducing learning rate to 2.0229992605891313e-05.\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.3839 - val_loss: 1.4908 - lr: 2.0434e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_9 (LSTM)                  (None, 45, 24)       3744        ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 45, 24)       0           ['lstm_9[0][0]']                 \n",
      "                                                                                                  \n",
      " lstm_10 (LSTM)                 (None, 45, 16)       2624        ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 45, 16)       0           ['lstm_10[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_11 (LSTM)                 (None, 32)           6272        ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 40)           1320        ['lstm_11[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 5)            205         ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " tf.unstack_3 (TFOpLambda)      [(None,),            0           ['dense_7[0][0]']                \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_15 (TFOpLambda)  (None, 1)           0           ['tf.unstack_3[0][0]']           \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_6 (TFOpLambda)  (None, 1)           0           ['tf.expand_dims_15[0][0]']      \n",
      "                                                                                                  \n",
      " tf.expand_dims_19 (TFOpLambda)  (None, 1)           0           ['tf.unstack_3[0][4]']           \n",
      "                                                                                                  \n",
      " tf.math.multiply_9 (TFOpLambda  (None, 1)           0           ['tf.math.sigmoid_6[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_7 (TFOpLambda)  (None, 1)           0           ['tf.expand_dims_19[0][0]']      \n",
      "                                                                                                  \n",
      " tf.math.multiply_10 (TFOpLambd  (None, 1)           0           ['tf.math.multiply_9[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_16 (TFOpLambda)  (None, 1)           0           ['tf.unstack_3[0][1]']           \n",
      "                                                                                                  \n",
      " tf.expand_dims_18 (TFOpLambda)  (None, 1)           0           ['tf.unstack_3[0][3]']           \n",
      "                                                                                                  \n",
      " tf.math.multiply_11 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_7[0][0]']      \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 1)           0           ['tf.math.multiply_10[0][0]']    \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.math.softplus_6 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_16[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_17 (TFOpLambda)  (None, 1)           0           ['tf.unstack_3[0][2]']           \n",
      "                                                                                                  \n",
      " tf.math.softplus_7 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_18[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 1)           0           ['tf.math.multiply_11[0][0]']    \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.stack_3 (TFOpLambda)        (None, 5, 1)         0           ['tf.__operators__.add_6[0][0]', \n",
      "                                                                  'tf.math.softplus_6[0][0]',     \n",
      "                                                                  'tf.expand_dims_17[0][0]',      \n",
      "                                                                  'tf.math.softplus_7[0][0]',     \n",
      "                                                                  'tf.__operators__.add_7[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _1_CCMP_t+1_0.08\n",
      "Epoch 1/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 3.4266\n",
      "Epoch 1: val_loss improved from inf to 4.08609, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 8s 53ms/step - loss: 3.4254 - val_loss: 4.0861 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 2.8672\n",
      "Epoch 2: val_loss improved from 4.08609 to 3.22137, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 2.8643 - val_loss: 3.2214 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.8015\n",
      "Epoch 3: val_loss improved from 3.22137 to 2.64323, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 1.8090 - val_loss: 2.6432 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.3316\n",
      "Epoch 4: val_loss improved from 2.64323 to 2.24666, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.08.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 2s 37ms/step - loss: 1.3316 - val_loss: 2.2467 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0892\n",
      "Epoch 5: val_loss improved from 2.24666 to 1.88371, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 1.0870 - val_loss: 1.8837 - lr: 1.0000e-04\n",
      "Epoch 6/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9657\n",
      "Epoch 6: val_loss improved from 1.88371 to 1.75450, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.9645 - val_loss: 1.7545 - lr: 1.0000e-04\n",
      "Epoch 7/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8739\n",
      "Epoch 7: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.8724 - val_loss: 2.2317 - lr: 1.0000e-04\n",
      "Epoch 8/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8269\n",
      "Epoch 8: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.8269 - val_loss: 2.2052 - lr: 9.9000e-05\n",
      "Epoch 9/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7788\n",
      "Epoch 9: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.7810 - val_loss: 2.3072 - lr: 9.8010e-05\n",
      "Epoch 10/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7438\n",
      "Epoch 10: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.7442 - val_loss: 2.7592 - lr: 9.7030e-05\n",
      "Epoch 11/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7408\n",
      "Epoch 11: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.7392 - val_loss: 2.6233 - lr: 9.6060e-05\n",
      "Epoch 12/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6970\n",
      "Epoch 12: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.7013 - val_loss: 2.7732 - lr: 9.5099e-05\n",
      "Epoch 13/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7001\n",
      "Epoch 13: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.6983 - val_loss: 3.1007 - lr: 9.4148e-05\n",
      "Epoch 14/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7015\n",
      "Epoch 14: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.7031 - val_loss: 2.4706 - lr: 9.3207e-05\n",
      "Epoch 15/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6841\n",
      "Epoch 15: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.6827 - val_loss: 2.5546 - lr: 9.2274e-05\n",
      "Epoch 16/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6655\n",
      "Epoch 16: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.6678 - val_loss: 2.8457 - lr: 9.1352e-05\n",
      "Epoch 17/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6589\n",
      "Epoch 17: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.6612 - val_loss: 3.0002 - lr: 9.0438e-05\n",
      "Epoch 18/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6506\n",
      "Epoch 18: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.6532 - val_loss: 2.6956 - lr: 8.9534e-05\n",
      "Epoch 19/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6506\n",
      "Epoch 19: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.6505 - val_loss: 2.5438 - lr: 8.8638e-05\n",
      "Epoch 20/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6296\n",
      "Epoch 20: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.6325 - val_loss: 3.1169 - lr: 8.7752e-05\n",
      "Epoch 21/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6265\n",
      "Epoch 21: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.6265 - val_loss: 2.6445 - lr: 8.6875e-05\n",
      "Epoch 22/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6303\n",
      "Epoch 22: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.6309 - val_loss: 2.7479 - lr: 8.6006e-05\n",
      "Epoch 23/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6216\n",
      "Epoch 23: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.6204 - val_loss: 2.7923 - lr: 8.5146e-05\n",
      "Epoch 24/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6239\n",
      "Epoch 24: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.6266 - val_loss: 2.8492 - lr: 8.4294e-05\n",
      "Epoch 25/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6169\n",
      "Epoch 25: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.6154 - val_loss: 3.1793 - lr: 8.3451e-05\n",
      "Epoch 26/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6226\n",
      "Epoch 26: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.6226 - val_loss: 3.0812 - lr: 8.2617e-05\n",
      "Epoch 27/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6059\n",
      "Epoch 27: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 3s 38ms/step - loss: 0.6059 - val_loss: 3.1457 - lr: 8.1791e-05\n",
      "Epoch 28/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6009\n",
      "Epoch 28: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.5985 - val_loss: 2.8101 - lr: 8.0973e-05\n",
      "Epoch 29/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6046\n",
      "Epoch 29: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.6035 - val_loss: 3.0819 - lr: 8.0163e-05\n",
      "Epoch 30/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6005\n",
      "Epoch 30: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.5981 - val_loss: 2.7188 - lr: 7.9361e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5939\n",
      "Epoch 31: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.5939 - val_loss: 3.0487 - lr: 7.8568e-05\n",
      "Epoch 32/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5941\n",
      "Epoch 32: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.5942 - val_loss: 2.7745 - lr: 7.7782e-05\n",
      "Epoch 33/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5974\n",
      "Epoch 33: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 2s 36ms/step - loss: 0.5965 - val_loss: 2.8621 - lr: 7.7004e-05\n",
      "Epoch 34/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5988\n",
      "Epoch 34: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.6000 - val_loss: 2.8291 - lr: 7.6234e-05\n",
      "Epoch 35/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5789\n",
      "Epoch 35: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.5785 - val_loss: 2.7954 - lr: 7.5472e-05\n",
      "Epoch 36/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5827\n",
      "Epoch 36: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.5827 - val_loss: 2.7838 - lr: 7.4717e-05\n",
      "Epoch 37/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5761\n",
      "Epoch 37: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.5754 - val_loss: 3.1778 - lr: 7.3970e-05\n",
      "Epoch 38/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5737\n",
      "Epoch 38: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.5760 - val_loss: 2.9052 - lr: 7.3230e-05\n",
      "Epoch 39/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5770\n",
      "Epoch 39: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.5763 - val_loss: 3.3993 - lr: 7.2498e-05\n",
      "Epoch 40/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5721\n",
      "Epoch 40: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.5746 - val_loss: 2.7117 - lr: 7.1773e-05\n",
      "Epoch 41/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5626\n",
      "Epoch 41: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.5619 - val_loss: 2.6123 - lr: 7.1055e-05\n",
      "Epoch 42/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5661\n",
      "Epoch 42: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 3s 38ms/step - loss: 0.5661 - val_loss: 2.6174 - lr: 7.0345e-05\n",
      "Epoch 43/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5761\n",
      "Epoch 43: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.5743 - val_loss: 2.4978 - lr: 6.9641e-05\n",
      "Epoch 44/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5635\n",
      "Epoch 44: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.5649 - val_loss: 2.8518 - lr: 6.8945e-05\n",
      "Epoch 45/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5565\n",
      "Epoch 45: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 4s 64ms/step - loss: 0.5565 - val_loss: 2.8143 - lr: 6.8255e-05\n",
      "Epoch 46/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5618\n",
      "Epoch 46: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 4s 55ms/step - loss: 0.5624 - val_loss: 2.7446 - lr: 6.7573e-05\n",
      "Epoch 47/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5545\n",
      "Epoch 47: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 4s 55ms/step - loss: 0.5533 - val_loss: 2.5452 - lr: 6.6897e-05\n",
      "Epoch 48/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5568\n",
      "Epoch 48: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 4s 58ms/step - loss: 0.5561 - val_loss: 2.5522 - lr: 6.6228e-05\n",
      "Epoch 49/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5403\n",
      "Epoch 49: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 4s 57ms/step - loss: 0.5406 - val_loss: 2.6681 - lr: 6.5566e-05\n",
      "Epoch 50/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5416\n",
      "Epoch 50: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 4s 55ms/step - loss: 0.5421 - val_loss: 2.6426 - lr: 6.4910e-05\n",
      "Epoch 51/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5474\n",
      "Epoch 51: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 4s 55ms/step - loss: 0.5460 - val_loss: 2.5559 - lr: 6.4261e-05\n",
      "Epoch 52/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5606\n",
      "Epoch 52: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.5617 - val_loss: 2.7022 - lr: 6.3619e-05\n",
      "Epoch 53/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5408\n",
      "Epoch 53: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.5397 - val_loss: 2.7127 - lr: 6.2982e-05\n",
      "Epoch 54/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5409\n",
      "Epoch 54: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 3s 52ms/step - loss: 0.5403 - val_loss: 2.6654 - lr: 6.2353e-05\n",
      "Epoch 55/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5492\n",
      "Epoch 55: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 4s 56ms/step - loss: 0.5497 - val_loss: 2.6829 - lr: 6.1729e-05\n",
      "Epoch 56/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5442\n",
      "Epoch 56: val_loss did not improve from 1.75450\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 3s 52ms/step - loss: 0.5447 - val_loss: 2.5596 - lr: 6.1112e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_12 (LSTM)                 (None, 45, 24)       3744        ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 45, 24)       0           ['lstm_12[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_13 (LSTM)                 (None, 45, 16)       2624        ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 45, 16)       0           ['lstm_13[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_14 (LSTM)                 (None, 32)           6272        ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 40)           1320        ['lstm_14[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 5)            205         ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " tf.unstack_4 (TFOpLambda)      [(None,),            0           ['dense_9[0][0]']                \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_20 (TFOpLambda)  (None, 1)           0           ['tf.unstack_4[0][0]']           \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_8 (TFOpLambda)  (None, 1)           0           ['tf.expand_dims_20[0][0]']      \n",
      "                                                                                                  \n",
      " tf.expand_dims_24 (TFOpLambda)  (None, 1)           0           ['tf.unstack_4[0][4]']           \n",
      "                                                                                                  \n",
      " tf.math.multiply_12 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_8[0][0]']      \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_9 (TFOpLambda)  (None, 1)           0           ['tf.expand_dims_24[0][0]']      \n",
      "                                                                                                  \n",
      " tf.math.multiply_13 (TFOpLambd  (None, 1)           0           ['tf.math.multiply_12[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_21 (TFOpLambda)  (None, 1)           0           ['tf.unstack_4[0][1]']           \n",
      "                                                                                                  \n",
      " tf.expand_dims_23 (TFOpLambda)  (None, 1)           0           ['tf.unstack_4[0][3]']           \n",
      "                                                                                                  \n",
      " tf.math.multiply_14 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_9[0][0]']      \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_8 (TFOpLa  (None, 1)           0           ['tf.math.multiply_13[0][0]']    \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.math.softplus_8 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_21[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_22 (TFOpLambda)  (None, 1)           0           ['tf.unstack_4[0][2]']           \n",
      "                                                                                                  \n",
      " tf.math.softplus_9 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_23[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_9 (TFOpLa  (None, 1)           0           ['tf.math.multiply_14[0][0]']    \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.stack_4 (TFOpLambda)        (None, 5, 1)         0           ['tf.__operators__.add_8[0][0]', \n",
      "                                                                  'tf.math.softplus_8[0][0]',     \n",
      "                                                                  'tf.expand_dims_22[0][0]',      \n",
      "                                                                  'tf.math.softplus_9[0][0]',     \n",
      "                                                                  'tf.__operators__.add_9[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _1_CCMP_t+1_0.09\n",
      "Epoch 1/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 3.3823\n",
      "Epoch 1: val_loss improved from inf to 4.04708, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 10s 64ms/step - loss: 3.3803 - val_loss: 4.0471 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 2.7220\n",
      "Epoch 2: val_loss improved from 4.04708 to 3.36762, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 3s 38ms/step - loss: 2.7220 - val_loss: 3.3676 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.7688\n",
      "Epoch 3: val_loss improved from 3.36762 to 2.57924, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 1.7713 - val_loss: 2.5792 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.2849\n",
      "Epoch 4: val_loss improved from 2.57924 to 2.17672, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.09.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 2s 38ms/step - loss: 1.2845 - val_loss: 2.1767 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1268\n",
      "Epoch 5: val_loss improved from 2.17672 to 1.95885, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 1.1267 - val_loss: 1.9588 - lr: 1.0000e-04\n",
      "Epoch 6/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0199\n",
      "Epoch 6: val_loss improved from 1.95885 to 1.85305, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 3s 38ms/step - loss: 1.0211 - val_loss: 1.8530 - lr: 1.0000e-04\n",
      "Epoch 7/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9460\n",
      "Epoch 7: val_loss improved from 1.85305 to 1.69187, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 3s 38ms/step - loss: 0.9437 - val_loss: 1.6919 - lr: 1.0000e-04\n",
      "Epoch 8/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9180\n",
      "Epoch 8: val_loss improved from 1.69187 to 1.65181, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.9152 - val_loss: 1.6518 - lr: 1.0000e-04\n",
      "Epoch 9/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8936\n",
      "Epoch 9: val_loss improved from 1.65181 to 1.64220, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.8948 - val_loss: 1.6422 - lr: 1.0000e-04\n",
      "Epoch 10/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8481\n",
      "Epoch 10: val_loss improved from 1.64220 to 1.61454, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.8481 - val_loss: 1.6145 - lr: 1.0000e-04\n",
      "Epoch 11/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8133\n",
      "Epoch 11: val_loss improved from 1.61454 to 1.60382, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.8113 - val_loss: 1.6038 - lr: 1.0000e-04\n",
      "Epoch 12/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7785\n",
      "Epoch 12: val_loss did not improve from 1.60382\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.7759 - val_loss: 1.6072 - lr: 1.0000e-04\n",
      "Epoch 13/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7508\n",
      "Epoch 13: val_loss improved from 1.60382 to 1.52861, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.7511 - val_loss: 1.5286 - lr: 9.9000e-05\n",
      "Epoch 14/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7382\n",
      "Epoch 14: val_loss improved from 1.52861 to 1.43872, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.7365 - val_loss: 1.4387 - lr: 9.9000e-05\n",
      "Epoch 15/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7372\n",
      "Epoch 15: val_loss did not improve from 1.43872\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.7361 - val_loss: 1.5890 - lr: 9.9000e-05\n",
      "Epoch 16/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7177\n",
      "Epoch 16: val_loss did not improve from 1.43872\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.7180 - val_loss: 1.5669 - lr: 9.8010e-05\n",
      "Epoch 17/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7109\n",
      "Epoch 17: val_loss did not improve from 1.43872\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.7116 - val_loss: 1.5009 - lr: 9.7030e-05\n",
      "Epoch 18/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7016\n",
      "Epoch 18: val_loss did not improve from 1.43872\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.7009 - val_loss: 1.5184 - lr: 9.6060e-05\n",
      "Epoch 19/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6992\n",
      "Epoch 19: val_loss did not improve from 1.43872\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.7010 - val_loss: 1.4920 - lr: 9.5099e-05\n",
      "Epoch 20/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6957\n",
      "Epoch 20: val_loss did not improve from 1.43872\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.6950 - val_loss: 1.4522 - lr: 9.4148e-05\n",
      "Epoch 21/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6949\n",
      "Epoch 21: val_loss improved from 1.43872 to 1.43714, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.6925 - val_loss: 1.4371 - lr: 9.3207e-05\n",
      "Epoch 22/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6869\n",
      "Epoch 22: val_loss improved from 1.43714 to 1.39512, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.6869 - val_loss: 1.3951 - lr: 9.3207e-05\n",
      "Epoch 23/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6810\n",
      "Epoch 23: val_loss did not improve from 1.39512\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.6814 - val_loss: 1.5185 - lr: 9.3207e-05\n",
      "Epoch 24/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6861\n",
      "Epoch 24: val_loss did not improve from 1.39512\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.6871 - val_loss: 1.5060 - lr: 9.2274e-05\n",
      "Epoch 25/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6651\n",
      "Epoch 25: val_loss did not improve from 1.39512\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.6631 - val_loss: 1.4748 - lr: 9.1352e-05\n",
      "Epoch 26/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6719\n",
      "Epoch 26: val_loss did not improve from 1.39512\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.6698 - val_loss: 1.4211 - lr: 9.0438e-05\n",
      "Epoch 27/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6589\n",
      "Epoch 27: val_loss did not improve from 1.39512\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.6617 - val_loss: 1.4000 - lr: 8.9534e-05\n",
      "Epoch 28/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6647\n",
      "Epoch 28: val_loss did not improve from 1.39512\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.6647 - val_loss: 1.5143 - lr: 8.8638e-05\n",
      "Epoch 29/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6575\n",
      "Epoch 29: val_loss did not improve from 1.39512\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 3s 40ms/step - loss: 0.6576 - val_loss: 1.4352 - lr: 8.7752e-05\n",
      "Epoch 30/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6350\n",
      "Epoch 30: val_loss improved from 1.39512 to 1.38919, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.6390 - val_loss: 1.3892 - lr: 8.6875e-05\n",
      "Epoch 31/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6615\n",
      "Epoch 31: val_loss improved from 1.38919 to 1.35720, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.6594 - val_loss: 1.3572 - lr: 8.6875e-05\n",
      "Epoch 32/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6511\n",
      "Epoch 32: val_loss improved from 1.35720 to 1.33539, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.6497 - val_loss: 1.3354 - lr: 8.6875e-05\n",
      "Epoch 33/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6516\n",
      "Epoch 33: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.6519 - val_loss: 1.4783 - lr: 8.6875e-05\n",
      "Epoch 34/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6429\n",
      "Epoch 34: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.6421 - val_loss: 1.3974 - lr: 8.6006e-05\n",
      "Epoch 35/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6431\n",
      "Epoch 35: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.6417 - val_loss: 1.5275 - lr: 8.5146e-05\n",
      "Epoch 36/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6392\n",
      "Epoch 36: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.6380 - val_loss: 1.3908 - lr: 8.4294e-05\n",
      "Epoch 37/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6526\n",
      "Epoch 37: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.6529 - val_loss: 1.4414 - lr: 8.3451e-05\n",
      "Epoch 38/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6302\n",
      "Epoch 38: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.6308 - val_loss: 1.4897 - lr: 8.2617e-05\n",
      "Epoch 39/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6335\n",
      "Epoch 39: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.6347 - val_loss: 1.4812 - lr: 8.1791e-05\n",
      "Epoch 40/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6308\n",
      "Epoch 40: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.6302 - val_loss: 1.4914 - lr: 8.0973e-05\n",
      "Epoch 41/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6236\n",
      "Epoch 41: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.6236 - val_loss: 1.4569 - lr: 8.0163e-05\n",
      "Epoch 42/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6220\n",
      "Epoch 42: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.6220 - val_loss: 1.4474 - lr: 7.9361e-05\n",
      "Epoch 43/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6170\n",
      "Epoch 43: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.6180 - val_loss: 1.5018 - lr: 7.8568e-05\n",
      "Epoch 44/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6237\n",
      "Epoch 44: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.6227 - val_loss: 1.4504 - lr: 7.7782e-05\n",
      "Epoch 45/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6172\n",
      "Epoch 45: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.6163 - val_loss: 1.5395 - lr: 7.7004e-05\n",
      "Epoch 46/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6045\n",
      "Epoch 46: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.6111 - val_loss: 1.4033 - lr: 7.6234e-05\n",
      "Epoch 47/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6287\n",
      "Epoch 47: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.6279 - val_loss: 1.4340 - lr: 7.5472e-05\n",
      "Epoch 48/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6080\n",
      "Epoch 48: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.6072 - val_loss: 1.5318 - lr: 7.4717e-05\n",
      "Epoch 49/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6088\n",
      "Epoch 49: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.6081 - val_loss: 1.5130 - lr: 7.3970e-05\n",
      "Epoch 50/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6114\n",
      "Epoch 50: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.6105 - val_loss: 1.4926 - lr: 7.3230e-05\n",
      "Epoch 51/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5998\n",
      "Epoch 51: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.5998 - val_loss: 1.5246 - lr: 7.2498e-05\n",
      "Epoch 52/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6077\n",
      "Epoch 52: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.6080 - val_loss: 1.6217 - lr: 7.1773e-05\n",
      "Epoch 53/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6025\n",
      "Epoch 53: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.6006 - val_loss: 1.4749 - lr: 7.1055e-05\n",
      "Epoch 54/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5954\n",
      "Epoch 54: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.5942 - val_loss: 1.5685 - lr: 7.0345e-05\n",
      "Epoch 55/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6064\n",
      "Epoch 55: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.6064 - val_loss: 1.5214 - lr: 6.9641e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6033\n",
      "Epoch 56: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.6052 - val_loss: 1.5732 - lr: 6.8945e-05\n",
      "Epoch 57/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5996\n",
      "Epoch 57: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.5988 - val_loss: 1.6203 - lr: 6.8255e-05\n",
      "Epoch 58/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5912\n",
      "Epoch 58: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.5918 - val_loss: 1.6541 - lr: 6.7573e-05\n",
      "Epoch 59/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5943\n",
      "Epoch 59: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.5943 - val_loss: 1.6830 - lr: 6.6897e-05\n",
      "Epoch 60/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5888\n",
      "Epoch 60: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.5859 - val_loss: 1.5898 - lr: 6.6228e-05\n",
      "Epoch 61/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5974\n",
      "Epoch 61: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.5974 - val_loss: 1.5790 - lr: 6.5566e-05\n",
      "Epoch 62/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5797\n",
      "Epoch 62: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.5797 - val_loss: 1.5128 - lr: 6.4910e-05\n",
      "Epoch 63/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5828\n",
      "Epoch 63: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.5829 - val_loss: 1.7385 - lr: 6.4261e-05\n",
      "Epoch 64/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5894\n",
      "Epoch 64: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.5891 - val_loss: 1.6753 - lr: 6.3619e-05\n",
      "Epoch 65/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5923\n",
      "Epoch 65: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.5916 - val_loss: 1.6307 - lr: 6.2982e-05\n",
      "Epoch 66/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5868\n",
      "Epoch 66: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.5850 - val_loss: 1.7286 - lr: 6.2353e-05\n",
      "Epoch 67/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5744\n",
      "Epoch 67: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.5741 - val_loss: 1.6560 - lr: 6.1729e-05\n",
      "Epoch 68/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5896\n",
      "Epoch 68: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.5898 - val_loss: 1.6608 - lr: 6.1112e-05\n",
      "Epoch 69/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5834\n",
      "Epoch 69: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.5834 - val_loss: 1.6464 - lr: 6.0501e-05\n",
      "Epoch 70/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5844\n",
      "Epoch 70: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.5839 - val_loss: 1.6499 - lr: 5.9896e-05\n",
      "Epoch 71/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5863\n",
      "Epoch 71: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 5.8703650938696225e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.5863 - val_loss: 1.5071 - lr: 5.9297e-05\n",
      "Epoch 72/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5854\n",
      "Epoch 72: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 5.811661550978897e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.5854 - val_loss: 1.5530 - lr: 5.8704e-05\n",
      "Epoch 73/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5762\n",
      "Epoch 73: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 5.753545068728272e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.5761 - val_loss: 1.5535 - lr: 5.8117e-05\n",
      "Epoch 74/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5933\n",
      "Epoch 74: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 5.6960095243994146e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.5917 - val_loss: 1.6681 - lr: 5.7535e-05\n",
      "Epoch 75/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5650\n",
      "Epoch 75: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 5.639049515593797e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.5650 - val_loss: 1.7560 - lr: 5.6960e-05\n",
      "Epoch 76/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5739\n",
      "Epoch 76: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 5.5826589195930865e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.5739 - val_loss: 1.6839 - lr: 5.6390e-05\n",
      "Epoch 77/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5758\n",
      "Epoch 77: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 5.526832333998754e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.5758 - val_loss: 1.6199 - lr: 5.5827e-05\n",
      "Epoch 78/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5795\n",
      "Epoch 78: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 5.471563996252371e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.5795 - val_loss: 1.6922 - lr: 5.5268e-05\n",
      "Epoch 79/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5743\n",
      "Epoch 79: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 5.416848503955407e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.5743 - val_loss: 1.7691 - lr: 5.4716e-05\n",
      "Epoch 80/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5746\n",
      "Epoch 80: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 5.362680094549433e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.5746 - val_loss: 1.5198 - lr: 5.4168e-05\n",
      "Epoch 81/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5806\n",
      "Epoch 81: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 5.309053365635918e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.5791 - val_loss: 1.5741 - lr: 5.3627e-05\n",
      "Epoch 82/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5697\n",
      "Epoch 82: val_loss did not improve from 1.33539\n",
      "\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 5.2559629148163366e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.5716 - val_loss: 1.6361 - lr: 5.3091e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_15 (LSTM)                 (None, 45, 24)       3744        ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 45, 24)       0           ['lstm_15[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_16 (LSTM)                 (None, 45, 16)       2624        ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 45, 16)       0           ['lstm_16[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_17 (LSTM)                 (None, 32)           6272        ['dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 40)           1320        ['lstm_17[0][0]']                \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 5)            205         ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " tf.unstack_5 (TFOpLambda)      [(None,),            0           ['dense_11[0][0]']               \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_25 (TFOpLambda)  (None, 1)           0           ['tf.unstack_5[0][0]']           \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_10 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_25[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_29 (TFOpLambda)  (None, 1)           0           ['tf.unstack_5[0][4]']           \n",
      "                                                                                                  \n",
      " tf.math.multiply_15 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_10[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_11 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_29[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_16 (TFOpLambd  (None, 1)           0           ['tf.math.multiply_15[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_26 (TFOpLambda)  (None, 1)           0           ['tf.unstack_5[0][1]']           \n",
      "                                                                                                  \n",
      " tf.expand_dims_28 (TFOpLambda)  (None, 1)           0           ['tf.unstack_5[0][3]']           \n",
      "                                                                                                  \n",
      " tf.math.multiply_17 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_11[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_10 (TFOpL  (None, 1)           0           ['tf.math.multiply_16[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.softplus_10 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_26[0][0]']      \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_27 (TFOpLambda)  (None, 1)           0           ['tf.unstack_5[0][2]']           \n",
      "                                                                                                  \n",
      " tf.math.softplus_11 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_28[0][0]']      \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_11 (TFOpL  (None, 1)           0           ['tf.math.multiply_17[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack_5 (TFOpLambda)        (None, 5, 1)         0           ['tf.__operators__.add_10[0][0]',\n",
      "                                                                  'tf.math.softplus_10[0][0]',    \n",
      "                                                                  'tf.expand_dims_27[0][0]',      \n",
      "                                                                  'tf.math.softplus_11[0][0]',    \n",
      "                                                                  'tf.__operators__.add_11[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _1_CCMP_t+1_0.1\n",
      "Epoch 1/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 3.3823\n",
      "Epoch 1: val_loss improved from inf to 3.99867, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 10s 55ms/step - loss: 3.3811 - val_loss: 3.9987 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 2.8334\n",
      "Epoch 2: val_loss improved from 3.99867 to 3.18324, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 2.8299 - val_loss: 3.1832 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.9769\n",
      "Epoch 3: val_loss improved from 3.18324 to 2.87941, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 1.9759 - val_loss: 2.8794 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/66 [============================>.] - ETA: 0s - loss: 1.4570\n",
      "Epoch 4: val_loss improved from 2.87941 to 2.74685, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 1.4545 - val_loss: 2.7468 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1810\n",
      "Epoch 5: val_loss improved from 2.74685 to 2.73838, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 1.1814 - val_loss: 2.7384 - lr: 1.0000e-04\n",
      "Epoch 6/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0552\n",
      "Epoch 6: val_loss did not improve from 2.73838\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 1.0582 - val_loss: 2.8714 - lr: 1.0000e-04\n",
      "Epoch 7/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9851\n",
      "Epoch 7: val_loss did not improve from 2.73838\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.9860 - val_loss: 3.1280 - lr: 9.9000e-05\n",
      "Epoch 8/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9554\n",
      "Epoch 8: val_loss did not improve from 2.73838\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.9547 - val_loss: 3.1787 - lr: 9.8010e-05\n",
      "Epoch 9/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9367\n",
      "Epoch 9: val_loss did not improve from 2.73838\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.9359 - val_loss: 3.0042 - lr: 9.7030e-05\n",
      "Epoch 10/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9046\n",
      "Epoch 10: val_loss improved from 2.73838 to 2.70905, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.9033 - val_loss: 2.7091 - lr: 9.6060e-05\n",
      "Epoch 11/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8975\n",
      "Epoch 11: val_loss improved from 2.70905 to 2.65010, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8976 - val_loss: 2.6501 - lr: 9.6060e-05\n",
      "Epoch 12/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8701\n",
      "Epoch 12: val_loss improved from 2.65010 to 2.47835, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8688 - val_loss: 2.4784 - lr: 9.6060e-05\n",
      "Epoch 13/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8559\n",
      "Epoch 13: val_loss did not improve from 2.47835\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.8559 - val_loss: 2.5072 - lr: 9.6060e-05\n",
      "Epoch 14/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8406\n",
      "Epoch 14: val_loss did not improve from 2.47835\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8396 - val_loss: 2.8335 - lr: 9.5099e-05\n",
      "Epoch 15/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8209\n",
      "Epoch 15: val_loss improved from 2.47835 to 2.41094, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.8192 - val_loss: 2.4109 - lr: 9.4148e-05\n",
      "Epoch 16/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8258\n",
      "Epoch 16: val_loss did not improve from 2.41094\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8252 - val_loss: 2.4475 - lr: 9.4148e-05\n",
      "Epoch 17/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8179\n",
      "Epoch 17: val_loss did not improve from 2.41094\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.8198 - val_loss: 2.4594 - lr: 9.3207e-05\n",
      "Epoch 18/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7963\n",
      "Epoch 18: val_loss improved from 2.41094 to 2.14177, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.7962 - val_loss: 2.1418 - lr: 9.2274e-05\n",
      "Epoch 19/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7942\n",
      "Epoch 19: val_loss improved from 2.14177 to 1.96053, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.7942 - val_loss: 1.9605 - lr: 9.2274e-05\n",
      "Epoch 20/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7844\n",
      "Epoch 20: val_loss did not improve from 1.96053\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.7844 - val_loss: 2.0621 - lr: 9.2274e-05\n",
      "Epoch 21/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7750\n",
      "Epoch 21: val_loss did not improve from 1.96053\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.7741 - val_loss: 2.0911 - lr: 9.1352e-05\n",
      "Epoch 22/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7657\n",
      "Epoch 22: val_loss did not improve from 1.96053\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.7666 - val_loss: 2.0287 - lr: 9.0438e-05\n",
      "Epoch 23/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7604\n",
      "Epoch 23: val_loss did not improve from 1.96053\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.7693 - val_loss: 2.0723 - lr: 8.9534e-05\n",
      "Epoch 24/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7504\n",
      "Epoch 24: val_loss improved from 1.96053 to 1.92279, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7482 - val_loss: 1.9228 - lr: 8.8638e-05\n",
      "Epoch 25/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7486\n",
      "Epoch 25: val_loss did not improve from 1.92279\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.7486 - val_loss: 1.9688 - lr: 8.8638e-05\n",
      "Epoch 26/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7449\n",
      "Epoch 26: val_loss did not improve from 1.92279\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.7439 - val_loss: 2.0440 - lr: 8.7752e-05\n",
      "Epoch 27/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7452\n",
      "Epoch 27: val_loss did not improve from 1.92279\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.7483 - val_loss: 1.9403 - lr: 8.6875e-05\n",
      "Epoch 28/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7244\n",
      "Epoch 28: val_loss improved from 1.92279 to 1.74525, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.7254 - val_loss: 1.7453 - lr: 8.6006e-05\n",
      "Epoch 29/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7310\n",
      "Epoch 29: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.7306 - val_loss: 2.0414 - lr: 8.6006e-05\n",
      "Epoch 30/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7379\n",
      "Epoch 30: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7363 - val_loss: 2.0184 - lr: 8.5146e-05\n",
      "Epoch 31/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7295\n",
      "Epoch 31: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.7291 - val_loss: 1.9047 - lr: 8.4294e-05\n",
      "Epoch 32/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7265\n",
      "Epoch 32: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7265 - val_loss: 1.9362 - lr: 8.3451e-05\n",
      "Epoch 33/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7112\n",
      "Epoch 33: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.7149 - val_loss: 1.8072 - lr: 8.2617e-05\n",
      "Epoch 34/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7207\n",
      "Epoch 34: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.7188 - val_loss: 2.1059 - lr: 8.1791e-05\n",
      "Epoch 35/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7244\n",
      "Epoch 35: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.7244 - val_loss: 1.9490 - lr: 8.0973e-05\n",
      "Epoch 36/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7216\n",
      "Epoch 36: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.7213 - val_loss: 1.9044 - lr: 8.0163e-05\n",
      "Epoch 37/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7150\n",
      "Epoch 37: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.7136 - val_loss: 1.9227 - lr: 7.9361e-05\n",
      "Epoch 38/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7003\n",
      "Epoch 38: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.7035 - val_loss: 2.0813 - lr: 7.8568e-05\n",
      "Epoch 39/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7076\n",
      "Epoch 39: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.7064 - val_loss: 1.8910 - lr: 7.7782e-05\n",
      "Epoch 40/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6981\n",
      "Epoch 40: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.6990 - val_loss: 1.9512 - lr: 7.7004e-05\n",
      "Epoch 41/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6928\n",
      "Epoch 41: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.6928 - val_loss: 1.9771 - lr: 7.6234e-05\n",
      "Epoch 42/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6970\n",
      "Epoch 42: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.6970 - val_loss: 1.9600 - lr: 7.5472e-05\n",
      "Epoch 43/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6999\n",
      "Epoch 43: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.6995 - val_loss: 2.0627 - lr: 7.4717e-05\n",
      "Epoch 44/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6930\n",
      "Epoch 44: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.6930 - val_loss: 2.0107 - lr: 7.3970e-05\n",
      "Epoch 45/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6850\n",
      "Epoch 45: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.6842 - val_loss: 2.0035 - lr: 7.3230e-05\n",
      "Epoch 46/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6853\n",
      "Epoch 46: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.6853 - val_loss: 1.8689 - lr: 7.2498e-05\n",
      "Epoch 47/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6855\n",
      "Epoch 47: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.6855 - val_loss: 2.0277 - lr: 7.1773e-05\n",
      "Epoch 48/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6883\n",
      "Epoch 48: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.6919 - val_loss: 2.0037 - lr: 7.1055e-05\n",
      "Epoch 49/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6819\n",
      "Epoch 49: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.6825 - val_loss: 1.9315 - lr: 7.0345e-05\n",
      "Epoch 50/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6866\n",
      "Epoch 50: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.6878 - val_loss: 2.0208 - lr: 6.9641e-05\n",
      "Epoch 51/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6753\n",
      "Epoch 51: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.6741 - val_loss: 2.1628 - lr: 6.8945e-05\n",
      "Epoch 52/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6865\n",
      "Epoch 52: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.6865 - val_loss: 1.9006 - lr: 6.8255e-05\n",
      "Epoch 53/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6825\n",
      "Epoch 53: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.6839 - val_loss: 2.0560 - lr: 6.7573e-05\n",
      "Epoch 54/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6774\n",
      "Epoch 54: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.6783 - val_loss: 2.1376 - lr: 6.6897e-05\n",
      "Epoch 55/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6692\n",
      "Epoch 55: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 3s 43ms/step - loss: 0.6688 - val_loss: 2.0085 - lr: 6.6228e-05\n",
      "Epoch 56/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6830\n",
      "Epoch 56: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.6830 - val_loss: 1.8416 - lr: 6.5566e-05\n",
      "Epoch 57/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6566\n",
      "Epoch 57: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.6596 - val_loss: 1.8898 - lr: 6.4910e-05\n",
      "Epoch 58/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6751\n",
      "Epoch 58: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.6738 - val_loss: 2.2571 - lr: 6.4261e-05\n",
      "Epoch 59/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6763\n",
      "Epoch 59: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.6741 - val_loss: 2.0724 - lr: 6.3619e-05\n",
      "Epoch 60/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6827\n",
      "Epoch 60: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 4s 56ms/step - loss: 0.6909 - val_loss: 2.0741 - lr: 6.2982e-05\n",
      "Epoch 61/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6654\n",
      "Epoch 61: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.6675 - val_loss: 2.1777 - lr: 6.2353e-05\n",
      "Epoch 62/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6756\n",
      "Epoch 62: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.6756 - val_loss: 1.9512 - lr: 6.1729e-05\n",
      "Epoch 63/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6599\n",
      "Epoch 63: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.6599 - val_loss: 2.1332 - lr: 6.1112e-05\n",
      "Epoch 64/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6635\n",
      "Epoch 64: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.6624 - val_loss: 2.1072 - lr: 6.0501e-05\n",
      "Epoch 65/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6792\n",
      "Epoch 65: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.6769 - val_loss: 2.1088 - lr: 5.9896e-05\n",
      "Epoch 66/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6582\n",
      "Epoch 66: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 5.8703650938696225e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.6591 - val_loss: 2.1713 - lr: 5.9297e-05\n",
      "Epoch 67/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6672\n",
      "Epoch 67: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 5.811661550978897e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.6672 - val_loss: 2.1535 - lr: 5.8704e-05\n",
      "Epoch 68/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6617\n",
      "Epoch 68: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 5.753545068728272e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.6624 - val_loss: 2.1123 - lr: 5.8117e-05\n",
      "Epoch 69/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6694\n",
      "Epoch 69: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 5.6960095243994146e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.6690 - val_loss: 2.1517 - lr: 5.7535e-05\n",
      "Epoch 70/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6526\n",
      "Epoch 70: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 5.639049515593797e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.6526 - val_loss: 2.2062 - lr: 5.6960e-05\n",
      "Epoch 71/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6543\n",
      "Epoch 71: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 5.5826589195930865e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.6539 - val_loss: 2.0382 - lr: 5.6390e-05\n",
      "Epoch 72/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6476\n",
      "Epoch 72: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 5.526832333998754e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.6476 - val_loss: 2.1001 - lr: 5.5827e-05\n",
      "Epoch 73/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6652\n",
      "Epoch 73: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 5.471563996252371e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.6643 - val_loss: 2.1310 - lr: 5.5268e-05\n",
      "Epoch 74/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6561\n",
      "Epoch 74: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 5.416848503955407e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.6561 - val_loss: 2.0290 - lr: 5.4716e-05\n",
      "Epoch 75/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6600\n",
      "Epoch 75: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 5.362680094549433e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.6600 - val_loss: 2.1615 - lr: 5.4168e-05\n",
      "Epoch 76/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6647\n",
      "Epoch 76: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 5.309053365635918e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.6647 - val_loss: 2.1849 - lr: 5.3627e-05\n",
      "Epoch 77/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6507\n",
      "Epoch 77: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 5.2559629148163366e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.6507 - val_loss: 2.1491 - lr: 5.3091e-05\n",
      "Epoch 78/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6498\n",
      "Epoch 78: val_loss did not improve from 1.74525\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 5.2034033396921584e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.6484 - val_loss: 2.1656 - lr: 5.2560e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_18 (LSTM)                 (None, 45, 24)       3744        ['input_7[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 45, 24)       0           ['lstm_18[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_19 (LSTM)                 (None, 45, 16)       2624        ['dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 45, 16)       0           ['lstm_19[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_20 (LSTM)                 (None, 32)           6272        ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 40)           1320        ['lstm_20[0][0]']                \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 5)            205         ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " tf.unstack_6 (TFOpLambda)      [(None,),            0           ['dense_13[0][0]']               \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_30 (TFOpLambda)  (None, 1)           0           ['tf.unstack_6[0][0]']           \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_12 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_30[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_34 (TFOpLambda)  (None, 1)           0           ['tf.unstack_6[0][4]']           \n",
      "                                                                                                  \n",
      " tf.math.multiply_18 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_12[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_13 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_34[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_19 (TFOpLambd  (None, 1)           0           ['tf.math.multiply_18[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_31 (TFOpLambda)  (None, 1)           0           ['tf.unstack_6[0][1]']           \n",
      "                                                                                                  \n",
      " tf.expand_dims_33 (TFOpLambda)  (None, 1)           0           ['tf.unstack_6[0][3]']           \n",
      "                                                                                                  \n",
      " tf.math.multiply_20 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_13[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_12 (TFOpL  (None, 1)           0           ['tf.math.multiply_19[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.softplus_12 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_31[0][0]']      \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_32 (TFOpLambda)  (None, 1)           0           ['tf.unstack_6[0][2]']           \n",
      "                                                                                                  \n",
      " tf.math.softplus_13 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_33[0][0]']      \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_13 (TFOpL  (None, 1)           0           ['tf.math.multiply_20[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack_6 (TFOpLambda)        (None, 5, 1)         0           ['tf.__operators__.add_12[0][0]',\n",
      "                                                                  'tf.math.softplus_12[0][0]',    \n",
      "                                                                  'tf.expand_dims_32[0][0]',      \n",
      "                                                                  'tf.math.softplus_13[0][0]',    \n",
      "                                                                  'tf.__operators__.add_13[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _1_CCMP_t+1_0.11\n",
      "Epoch 1/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 3.4690\n",
      "Epoch 1: val_loss improved from inf to 3.98745, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.11.hdf5\n",
      "66/66 [==============================] - 9s 59ms/step - loss: 3.4677 - val_loss: 3.9875 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 2.9080\n",
      "Epoch 2: val_loss improved from 3.98745 to 3.16870, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.11.hdf5\n",
      "66/66 [==============================] - 3s 52ms/step - loss: 2.9080 - val_loss: 3.1687 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.8111\n",
      "Epoch 3: val_loss improved from 3.16870 to 2.62642, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.11.hdf5\n",
      "66/66 [==============================] - 4s 56ms/step - loss: 1.8100 - val_loss: 2.6264 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/66 [============================>.] - ETA: 0s - loss: 1.3428\n",
      "Epoch 4: val_loss improved from 2.62642 to 2.31316, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.11.hdf5\n",
      "66/66 [==============================] - 4s 61ms/step - loss: 1.3453 - val_loss: 2.3132 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1675\n",
      "Epoch 5: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 4s 53ms/step - loss: 1.1657 - val_loss: 2.5569 - lr: 1.0000e-04\n",
      "Epoch 6/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0900\n",
      "Epoch 6: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 3s 52ms/step - loss: 1.0888 - val_loss: 2.7137 - lr: 9.9000e-05\n",
      "Epoch 7/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0395\n",
      "Epoch 7: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 4s 53ms/step - loss: 1.0420 - val_loss: 2.6141 - lr: 9.8010e-05\n",
      "Epoch 8/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0033\n",
      "Epoch 8: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 4s 53ms/step - loss: 1.0027 - val_loss: 2.8683 - lr: 9.7030e-05\n",
      "Epoch 9/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9706\n",
      "Epoch 9: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 3s 53ms/step - loss: 0.9713 - val_loss: 2.6404 - lr: 9.6060e-05\n",
      "Epoch 10/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9424\n",
      "Epoch 10: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.9412 - val_loss: 3.2469 - lr: 9.5099e-05\n",
      "Epoch 11/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9309\n",
      "Epoch 11: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 3s 53ms/step - loss: 0.9302 - val_loss: 2.9250 - lr: 9.4148e-05\n",
      "Epoch 12/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9075\n",
      "Epoch 12: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 3s 53ms/step - loss: 0.9075 - val_loss: 2.9352 - lr: 9.3207e-05\n",
      "Epoch 13/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8923\n",
      "Epoch 13: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 3s 52ms/step - loss: 0.8932 - val_loss: 3.0545 - lr: 9.2274e-05\n",
      "Epoch 14/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8651\n",
      "Epoch 14: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 4s 57ms/step - loss: 0.8718 - val_loss: 2.5457 - lr: 9.1352e-05\n",
      "Epoch 15/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8762\n",
      "Epoch 15: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.8762 - val_loss: 3.0049 - lr: 9.0438e-05\n",
      "Epoch 16/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8456\n",
      "Epoch 16: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.8456 - val_loss: 2.8967 - lr: 8.9534e-05\n",
      "Epoch 17/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8457\n",
      "Epoch 17: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8469 - val_loss: 3.1850 - lr: 8.8638e-05\n",
      "Epoch 18/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8385\n",
      "Epoch 18: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.8380 - val_loss: 2.7519 - lr: 8.7752e-05\n",
      "Epoch 19/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8470\n",
      "Epoch 19: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8470 - val_loss: 2.7739 - lr: 8.6875e-05\n",
      "Epoch 20/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8274\n",
      "Epoch 20: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8274 - val_loss: 3.0311 - lr: 8.6006e-05\n",
      "Epoch 21/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8161\n",
      "Epoch 21: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8150 - val_loss: 2.7898 - lr: 8.5146e-05\n",
      "Epoch 22/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8127\n",
      "Epoch 22: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.8127 - val_loss: 2.4636 - lr: 8.4294e-05\n",
      "Epoch 23/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8121\n",
      "Epoch 23: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8132 - val_loss: 2.6653 - lr: 8.3451e-05\n",
      "Epoch 24/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7973\n",
      "Epoch 24: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7962 - val_loss: 2.9508 - lr: 8.2617e-05\n",
      "Epoch 25/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7909\n",
      "Epoch 25: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7909 - val_loss: 2.9193 - lr: 8.1791e-05\n",
      "Epoch 26/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8025\n",
      "Epoch 26: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.8025 - val_loss: 2.8260 - lr: 8.0973e-05\n",
      "Epoch 27/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7915\n",
      "Epoch 27: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7934 - val_loss: 2.6148 - lr: 8.0163e-05\n",
      "Epoch 28/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7990\n",
      "Epoch 28: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7996 - val_loss: 2.8160 - lr: 7.9361e-05\n",
      "Epoch 29/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7835\n",
      "Epoch 29: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7835 - val_loss: 2.6124 - lr: 7.8568e-05\n",
      "Epoch 30/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7848\n",
      "Epoch 30: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 3s 46ms/step - loss: 0.7838 - val_loss: 2.8600 - lr: 7.7782e-05\n",
      "Epoch 31/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7782\n",
      "Epoch 31: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.7782 - val_loss: 3.0233 - lr: 7.7004e-05\n",
      "Epoch 32/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7639\n",
      "Epoch 32: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7639 - val_loss: 2.6573 - lr: 7.6234e-05\n",
      "Epoch 33/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7763\n",
      "Epoch 33: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7737 - val_loss: 2.5420 - lr: 7.5472e-05\n",
      "Epoch 34/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7777\n",
      "Epoch 34: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7777 - val_loss: 2.7004 - lr: 7.4717e-05\n",
      "Epoch 35/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7789\n",
      "Epoch 35: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7781 - val_loss: 2.7388 - lr: 7.3970e-05\n",
      "Epoch 36/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7675\n",
      "Epoch 36: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7653 - val_loss: 2.5696 - lr: 7.3230e-05\n",
      "Epoch 37/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7609\n",
      "Epoch 37: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7602 - val_loss: 2.8195 - lr: 7.2498e-05\n",
      "Epoch 38/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7689\n",
      "Epoch 38: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.7745 - val_loss: 2.7868 - lr: 7.1773e-05\n",
      "Epoch 39/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7698\n",
      "Epoch 39: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7694 - val_loss: 2.8215 - lr: 7.1055e-05\n",
      "Epoch 40/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7555\n",
      "Epoch 40: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7555 - val_loss: 2.6737 - lr: 7.0345e-05\n",
      "Epoch 41/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7413\n",
      "Epoch 41: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7413 - val_loss: 3.0373 - lr: 6.9641e-05\n",
      "Epoch 42/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7637\n",
      "Epoch 42: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7658 - val_loss: 2.7866 - lr: 6.8945e-05\n",
      "Epoch 43/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7445\n",
      "Epoch 43: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7445 - val_loss: 2.7350 - lr: 6.8255e-05\n",
      "Epoch 44/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7589\n",
      "Epoch 44: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7590 - val_loss: 2.7051 - lr: 6.7573e-05\n",
      "Epoch 45/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7548\n",
      "Epoch 45: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7550 - val_loss: 2.5574 - lr: 6.6897e-05\n",
      "Epoch 46/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7542\n",
      "Epoch 46: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7547 - val_loss: 2.5805 - lr: 6.6228e-05\n",
      "Epoch 47/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7426\n",
      "Epoch 47: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.7439 - val_loss: 2.7800 - lr: 6.5566e-05\n",
      "Epoch 48/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7523\n",
      "Epoch 48: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 4s 59ms/step - loss: 0.7547 - val_loss: 2.6394 - lr: 6.4910e-05\n",
      "Epoch 49/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7519\n",
      "Epoch 49: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 4s 56ms/step - loss: 0.7504 - val_loss: 2.6671 - lr: 6.4261e-05\n",
      "Epoch 50/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7447\n",
      "Epoch 50: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.7452 - val_loss: 2.5156 - lr: 6.3619e-05\n",
      "Epoch 51/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7444\n",
      "Epoch 51: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 3s 52ms/step - loss: 0.7449 - val_loss: 2.6373 - lr: 6.2982e-05\n",
      "Epoch 52/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7397\n",
      "Epoch 52: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 3s 52ms/step - loss: 0.7384 - val_loss: 2.4521 - lr: 6.2353e-05\n",
      "Epoch 53/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7487\n",
      "Epoch 53: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 3s 52ms/step - loss: 0.7468 - val_loss: 2.8150 - lr: 6.1729e-05\n",
      "Epoch 54/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7411\n",
      "Epoch 54: val_loss did not improve from 2.31316\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.7405 - val_loss: 2.7393 - lr: 6.1112e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_8 (InputLayer)           [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_21 (LSTM)                 (None, 45, 24)       3744        ['input_8[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 45, 24)       0           ['lstm_21[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_22 (LSTM)                 (None, 45, 16)       2624        ['dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 45, 16)       0           ['lstm_22[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_23 (LSTM)                 (None, 32)           6272        ['dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 40)           1320        ['lstm_23[0][0]']                \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 5)            205         ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " tf.unstack_7 (TFOpLambda)      [(None,),            0           ['dense_15[0][0]']               \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_35 (TFOpLambda)  (None, 1)           0           ['tf.unstack_7[0][0]']           \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_14 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_35[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_39 (TFOpLambda)  (None, 1)           0           ['tf.unstack_7[0][4]']           \n",
      "                                                                                                  \n",
      " tf.math.multiply_21 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_14[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_15 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_39[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_22 (TFOpLambd  (None, 1)           0           ['tf.math.multiply_21[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_36 (TFOpLambda)  (None, 1)           0           ['tf.unstack_7[0][1]']           \n",
      "                                                                                                  \n",
      " tf.expand_dims_38 (TFOpLambda)  (None, 1)           0           ['tf.unstack_7[0][3]']           \n",
      "                                                                                                  \n",
      " tf.math.multiply_23 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_15[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_14 (TFOpL  (None, 1)           0           ['tf.math.multiply_22[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.softplus_14 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_36[0][0]']      \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_37 (TFOpLambda)  (None, 1)           0           ['tf.unstack_7[0][2]']           \n",
      "                                                                                                  \n",
      " tf.math.softplus_15 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_38[0][0]']      \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_15 (TFOpL  (None, 1)           0           ['tf.math.multiply_23[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack_7 (TFOpLambda)        (None, 5, 1)         0           ['tf.__operators__.add_14[0][0]',\n",
      "                                                                  'tf.math.softplus_14[0][0]',    \n",
      "                                                                  'tf.expand_dims_37[0][0]',      \n",
      "                                                                  'tf.math.softplus_15[0][0]',    \n",
      "                                                                  'tf.__operators__.add_15[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _1_CCMP_t+1_0.12\n",
      "Epoch 1/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 3.4365\n",
      "Epoch 1: val_loss improved from inf to 4.24260, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 9s 58ms/step - loss: 3.4365 - val_loss: 4.2426 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 2.7882\n",
      "Epoch 2: val_loss improved from 4.24260 to 3.55317, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 2.7852 - val_loss: 3.5532 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.8047\n",
      "Epoch 3: val_loss improved from 3.55317 to 3.14874, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 4s 61ms/step - loss: 1.8038 - val_loss: 3.1487 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - ETA: 0s - loss: 1.4193\n",
      "Epoch 4: val_loss improved from 3.14874 to 2.99376, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 4s 57ms/step - loss: 1.4193 - val_loss: 2.9938 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.2480\n",
      "Epoch 5: val_loss did not improve from 2.99376\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 4s 53ms/step - loss: 1.2473 - val_loss: 3.2835 - lr: 1.0000e-04\n",
      "Epoch 6/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1494\n",
      "Epoch 6: val_loss did not improve from 2.99376\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 4s 55ms/step - loss: 1.1485 - val_loss: 3.3242 - lr: 9.9000e-05\n",
      "Epoch 7/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0925\n",
      "Epoch 7: val_loss did not improve from 2.99376\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 4s 55ms/step - loss: 1.0907 - val_loss: 3.3299 - lr: 9.8010e-05\n",
      "Epoch 8/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0347\n",
      "Epoch 8: val_loss did not improve from 2.99376\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 1.0349 - val_loss: 3.1201 - lr: 9.7030e-05\n",
      "Epoch 9/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9892\n",
      "Epoch 9: val_loss did not improve from 2.99376\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 4s 55ms/step - loss: 0.9878 - val_loss: 3.4974 - lr: 9.6060e-05\n",
      "Epoch 10/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9743\n",
      "Epoch 10: val_loss did not improve from 2.99376\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.9748 - val_loss: 3.1162 - lr: 9.5099e-05\n",
      "Epoch 11/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9297\n",
      "Epoch 11: val_loss did not improve from 2.99376\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.9293 - val_loss: 3.4036 - lr: 9.4148e-05\n",
      "Epoch 12/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9222\n",
      "Epoch 12: val_loss improved from 2.99376 to 2.99260, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 4s 55ms/step - loss: 0.9216 - val_loss: 2.9926 - lr: 9.3207e-05\n",
      "Epoch 13/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9019\n",
      "Epoch 13: val_loss improved from 2.99260 to 2.73908, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 4s 55ms/step - loss: 0.9041 - val_loss: 2.7391 - lr: 9.3207e-05\n",
      "Epoch 14/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9032\n",
      "Epoch 14: val_loss did not improve from 2.73908\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 4s 53ms/step - loss: 0.9012 - val_loss: 3.1101 - lr: 9.3207e-05\n",
      "Epoch 15/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8881\n",
      "Epoch 15: val_loss did not improve from 2.73908\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.8853 - val_loss: 2.8548 - lr: 9.2274e-05\n",
      "Epoch 16/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8693\n",
      "Epoch 16: val_loss did not improve from 2.73908\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.8701 - val_loss: 2.9306 - lr: 9.1352e-05\n",
      "Epoch 17/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8610\n",
      "Epoch 17: val_loss did not improve from 2.73908\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 4s 57ms/step - loss: 0.8630 - val_loss: 3.0442 - lr: 9.0438e-05\n",
      "Epoch 18/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8561\n",
      "Epoch 18: val_loss improved from 2.73908 to 2.73495, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 4s 56ms/step - loss: 0.8568 - val_loss: 2.7350 - lr: 8.9534e-05\n",
      "Epoch 19/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8640\n",
      "Epoch 19: val_loss did not improve from 2.73495\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 4s 57ms/step - loss: 0.8639 - val_loss: 2.7922 - lr: 8.9534e-05\n",
      "Epoch 20/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8419\n",
      "Epoch 20: val_loss improved from 2.73495 to 2.63800, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.8419 - val_loss: 2.6380 - lr: 8.8638e-05\n",
      "Epoch 21/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8444\n",
      "Epoch 21: val_loss did not improve from 2.63800\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.8457 - val_loss: 2.7831 - lr: 8.8638e-05\n",
      "Epoch 22/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8459\n",
      "Epoch 22: val_loss did not improve from 2.63800\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8457 - val_loss: 2.8732 - lr: 8.7752e-05\n",
      "Epoch 23/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8328\n",
      "Epoch 23: val_loss did not improve from 2.63800\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8337 - val_loss: 2.6943 - lr: 8.6875e-05\n",
      "Epoch 24/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8244\n",
      "Epoch 24: val_loss did not improve from 2.63800\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8269 - val_loss: 2.7649 - lr: 8.6006e-05\n",
      "Epoch 25/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8306\n",
      "Epoch 25: val_loss improved from 2.63800 to 2.59394, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.8309 - val_loss: 2.5939 - lr: 8.5146e-05\n",
      "Epoch 26/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8150\n",
      "Epoch 26: val_loss did not improve from 2.59394\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.8150 - val_loss: 2.9649 - lr: 8.5146e-05\n",
      "Epoch 27/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8173\n",
      "Epoch 27: val_loss did not improve from 2.59394\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.8173 - val_loss: 2.6877 - lr: 8.4294e-05\n",
      "Epoch 28/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8125\n",
      "Epoch 28: val_loss improved from 2.59394 to 2.46967, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8134 - val_loss: 2.4697 - lr: 8.3451e-05\n",
      "Epoch 29/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8164\n",
      "Epoch 29: val_loss did not improve from 2.46967\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8144 - val_loss: 2.4862 - lr: 8.3451e-05\n",
      "Epoch 30/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8072\n",
      "Epoch 30: val_loss improved from 2.46967 to 2.46247, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8083 - val_loss: 2.4625 - lr: 8.2617e-05\n",
      "Epoch 31/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8069\n",
      "Epoch 31: val_loss did not improve from 2.46247\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.8075 - val_loss: 2.5812 - lr: 8.2617e-05\n",
      "Epoch 32/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7991\n",
      "Epoch 32: val_loss did not improve from 2.46247\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7982 - val_loss: 2.5285 - lr: 8.1791e-05\n",
      "Epoch 33/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7947\n",
      "Epoch 33: val_loss did not improve from 2.46247\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7938 - val_loss: 2.7418 - lr: 8.0973e-05\n",
      "Epoch 34/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8027\n",
      "Epoch 34: val_loss did not improve from 2.46247\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8023 - val_loss: 2.6241 - lr: 8.0163e-05\n",
      "Epoch 35/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7981\n",
      "Epoch 35: val_loss did not improve from 2.46247\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7978 - val_loss: 2.4965 - lr: 7.9361e-05\n",
      "Epoch 36/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7871\n",
      "Epoch 36: val_loss did not improve from 2.46247\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7865 - val_loss: 2.6995 - lr: 7.8568e-05\n",
      "Epoch 37/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7961\n",
      "Epoch 37: val_loss did not improve from 2.46247\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7961 - val_loss: 2.6700 - lr: 7.7782e-05\n",
      "Epoch 38/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7798\n",
      "Epoch 38: val_loss improved from 2.46247 to 2.42016, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7798 - val_loss: 2.4202 - lr: 7.7004e-05\n",
      "Epoch 39/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7869\n",
      "Epoch 39: val_loss did not improve from 2.42016\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7864 - val_loss: 2.4998 - lr: 7.7004e-05\n",
      "Epoch 40/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7879\n",
      "Epoch 40: val_loss did not improve from 2.42016\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7865 - val_loss: 2.8953 - lr: 7.6234e-05\n",
      "Epoch 41/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7811\n",
      "Epoch 41: val_loss did not improve from 2.42016\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7817 - val_loss: 2.4627 - lr: 7.5472e-05\n",
      "Epoch 42/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7943\n",
      "Epoch 42: val_loss did not improve from 2.42016\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7919 - val_loss: 2.4854 - lr: 7.4717e-05\n",
      "Epoch 43/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7747\n",
      "Epoch 43: val_loss did not improve from 2.42016\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7758 - val_loss: 2.5097 - lr: 7.3970e-05\n",
      "Epoch 44/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7781\n",
      "Epoch 44: val_loss did not improve from 2.42016\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7781 - val_loss: 2.4795 - lr: 7.3230e-05\n",
      "Epoch 45/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7793\n",
      "Epoch 45: val_loss improved from 2.42016 to 2.31241, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7776 - val_loss: 2.3124 - lr: 7.2498e-05\n",
      "Epoch 46/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7737\n",
      "Epoch 46: val_loss did not improve from 2.31241\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7737 - val_loss: 2.6763 - lr: 7.2498e-05\n",
      "Epoch 47/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7705\n",
      "Epoch 47: val_loss did not improve from 2.31241\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7713 - val_loss: 2.3243 - lr: 7.1773e-05\n",
      "Epoch 48/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7748\n",
      "Epoch 48: val_loss did not improve from 2.31241\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7746 - val_loss: 2.4205 - lr: 7.1055e-05\n",
      "Epoch 49/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7646\n",
      "Epoch 49: val_loss did not improve from 2.31241\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7646 - val_loss: 2.7505 - lr: 7.0345e-05\n",
      "Epoch 50/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7657\n",
      "Epoch 50: val_loss did not improve from 2.31241\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.7657 - val_loss: 2.4375 - lr: 6.9641e-05\n",
      "Epoch 51/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7670\n",
      "Epoch 51: val_loss did not improve from 2.31241\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7673 - val_loss: 2.4508 - lr: 6.8945e-05\n",
      "Epoch 52/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7644\n",
      "Epoch 52: val_loss did not improve from 2.31241\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7644 - val_loss: 2.3947 - lr: 6.8255e-05\n",
      "Epoch 53/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7563\n",
      "Epoch 53: val_loss did not improve from 2.31241\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7566 - val_loss: 2.4718 - lr: 6.7573e-05\n",
      "Epoch 54/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7563\n",
      "Epoch 54: val_loss did not improve from 2.31241\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7563 - val_loss: 2.5348 - lr: 6.6897e-05\n",
      "Epoch 55/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7728\n",
      "Epoch 55: val_loss did not improve from 2.31241\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7728 - val_loss: 2.4819 - lr: 6.6228e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7696\n",
      "Epoch 56: val_loss did not improve from 2.31241\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7683 - val_loss: 2.4385 - lr: 6.5566e-05\n",
      "Epoch 57/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7613\n",
      "Epoch 57: val_loss did not improve from 2.31241\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7652 - val_loss: 2.5385 - lr: 6.4910e-05\n",
      "Epoch 58/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7596\n",
      "Epoch 58: val_loss did not improve from 2.31241\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.7601 - val_loss: 2.5371 - lr: 6.4261e-05\n",
      "Epoch 59/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7645\n",
      "Epoch 59: val_loss did not improve from 2.31241\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7645 - val_loss: 2.4532 - lr: 6.3619e-05\n",
      "Epoch 60/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7661\n",
      "Epoch 60: val_loss did not improve from 2.31241\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7649 - val_loss: 2.3669 - lr: 6.2982e-05\n",
      "Epoch 61/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7596\n",
      "Epoch 61: val_loss did not improve from 2.31241\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7602 - val_loss: 2.4453 - lr: 6.2353e-05\n",
      "Epoch 62/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7526\n",
      "Epoch 62: val_loss did not improve from 2.31241\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7536 - val_loss: 2.6381 - lr: 6.1729e-05\n",
      "Epoch 63/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7529\n",
      "Epoch 63: val_loss did not improve from 2.31241\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7521 - val_loss: 2.5846 - lr: 6.1112e-05\n",
      "Epoch 64/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7512\n",
      "Epoch 64: val_loss improved from 2.31241 to 2.29587, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7523 - val_loss: 2.2959 - lr: 6.0501e-05\n",
      "Epoch 65/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7562\n",
      "Epoch 65: val_loss did not improve from 2.29587\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7565 - val_loss: 2.4748 - lr: 6.0501e-05\n",
      "Epoch 66/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7501\n",
      "Epoch 66: val_loss did not improve from 2.29587\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7501 - val_loss: 2.3838 - lr: 5.9896e-05\n",
      "Epoch 67/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7567\n",
      "Epoch 67: val_loss did not improve from 2.29587\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 5.8703650938696225e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7557 - val_loss: 2.6176 - lr: 5.9297e-05\n",
      "Epoch 68/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7513\n",
      "Epoch 68: val_loss did not improve from 2.29587\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 5.811661550978897e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7513 - val_loss: 2.3938 - lr: 5.8704e-05\n",
      "Epoch 69/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7599\n",
      "Epoch 69: val_loss did not improve from 2.29587\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 5.753545068728272e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7572 - val_loss: 2.4476 - lr: 5.8117e-05\n",
      "Epoch 70/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7556\n",
      "Epoch 70: val_loss did not improve from 2.29587\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 5.6960095243994146e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7544 - val_loss: 2.3091 - lr: 5.7535e-05\n",
      "Epoch 71/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7463\n",
      "Epoch 71: val_loss did not improve from 2.29587\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 5.639049515593797e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7468 - val_loss: 2.5263 - lr: 5.6960e-05\n",
      "Epoch 72/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7454\n",
      "Epoch 72: val_loss did not improve from 2.29587\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 5.5826589195930865e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7424 - val_loss: 2.5381 - lr: 5.6390e-05\n",
      "Epoch 73/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7511\n",
      "Epoch 73: val_loss did not improve from 2.29587\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 5.526832333998754e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7509 - val_loss: 2.4304 - lr: 5.5827e-05\n",
      "Epoch 74/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7498\n",
      "Epoch 74: val_loss did not improve from 2.29587\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 5.471563996252371e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.7498 - val_loss: 2.4673 - lr: 5.5268e-05\n",
      "Epoch 75/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7483\n",
      "Epoch 75: val_loss did not improve from 2.29587\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 5.416848503955407e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7508 - val_loss: 2.4023 - lr: 5.4716e-05\n",
      "Epoch 76/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7509\n",
      "Epoch 76: val_loss did not improve from 2.29587\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 5.362680094549433e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7505 - val_loss: 2.3576 - lr: 5.4168e-05\n",
      "Epoch 77/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7503\n",
      "Epoch 77: val_loss did not improve from 2.29587\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 5.309053365635918e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7498 - val_loss: 2.3758 - lr: 5.3627e-05\n",
      "Epoch 78/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7415\n",
      "Epoch 78: val_loss did not improve from 2.29587\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 5.2559629148163366e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7413 - val_loss: 2.5681 - lr: 5.3091e-05\n",
      "Epoch 79/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7370\n",
      "Epoch 79: val_loss did not improve from 2.29587\n",
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 5.2034033396921584e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7386 - val_loss: 2.4639 - lr: 5.2560e-05\n",
      "Epoch 80/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7426\n",
      "Epoch 80: val_loss did not improve from 2.29587\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 5.1513692378648555e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7426 - val_loss: 2.3465 - lr: 5.2034e-05\n",
      "Epoch 81/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7389\n",
      "Epoch 81: val_loss did not improve from 2.29587\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 5.099855567095801e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7389 - val_loss: 2.4533 - lr: 5.1514e-05\n",
      "Epoch 82/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7394\n",
      "Epoch 82: val_loss did not improve from 2.29587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 5.048856924986467e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.7394 - val_loss: 2.6647 - lr: 5.0999e-05\n",
      "Epoch 83/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7395\n",
      "Epoch 83: val_loss did not improve from 2.29587\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 4.998368269298226e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7382 - val_loss: 2.5708 - lr: 5.0489e-05\n",
      "Epoch 84/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7365\n",
      "Epoch 84: val_loss did not improve from 2.29587\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 4.9483845577924514e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7351 - val_loss: 2.5790 - lr: 4.9984e-05\n",
      "Epoch 85/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7310\n",
      "Epoch 85: val_loss did not improve from 2.29587\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 4.898900748230517e-05.\n",
      "66/66 [==============================] - 4s 55ms/step - loss: 0.7299 - val_loss: 2.5217 - lr: 4.9484e-05\n",
      "Epoch 86/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7357\n",
      "Epoch 86: val_loss did not improve from 2.29587\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 4.849911798373796e-05.\n",
      "66/66 [==============================] - 4s 65ms/step - loss: 0.7354 - val_loss: 2.4459 - lr: 4.8989e-05\n",
      "Epoch 87/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7280\n",
      "Epoch 87: val_loss did not improve from 2.29587\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 4.801412665983662e-05.\n",
      "66/66 [==============================] - 4s 60ms/step - loss: 0.7294 - val_loss: 2.4751 - lr: 4.8499e-05\n",
      "Epoch 88/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7375\n",
      "Epoch 88: val_loss did not improve from 2.29587\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 4.75339866898139e-05.\n",
      "66/66 [==============================] - 4s 57ms/step - loss: 0.7387 - val_loss: 2.3577 - lr: 4.8014e-05\n",
      "Epoch 89/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7383\n",
      "Epoch 89: val_loss did not improve from 2.29587\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 4.705864765128354e-05.\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.7366 - val_loss: 2.4578 - lr: 4.7534e-05\n",
      "Epoch 90/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7316\n",
      "Epoch 90: val_loss did not improve from 2.29587\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 4.658806272345828e-05.\n",
      "66/66 [==============================] - 4s 53ms/step - loss: 0.7337 - val_loss: 2.4452 - lr: 4.7059e-05\n",
      "Epoch 91/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7217\n",
      "Epoch 91: val_loss improved from 2.29587 to 2.26302, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.7207 - val_loss: 2.2630 - lr: 4.6588e-05\n",
      "Epoch 92/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7302\n",
      "Epoch 92: val_loss did not improve from 2.26302\n",
      "\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 4.612218148395186e-05.\n",
      "66/66 [==============================] - 3s 53ms/step - loss: 0.7296 - val_loss: 2.3744 - lr: 4.6588e-05\n",
      "Epoch 93/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7359\n",
      "Epoch 93: val_loss did not improve from 2.26302\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 4.566096071357606e-05.\n",
      "66/66 [==============================] - 4s 53ms/step - loss: 0.7366 - val_loss: 2.3335 - lr: 4.6122e-05\n",
      "Epoch 94/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7339\n",
      "Epoch 94: val_loss did not improve from 2.26302\n",
      "\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 4.52043499899446e-05.\n",
      "66/66 [==============================] - 4s 53ms/step - loss: 0.7324 - val_loss: 2.3056 - lr: 4.5661e-05\n",
      "Epoch 95/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7245\n",
      "Epoch 95: val_loss did not improve from 2.26302\n",
      "\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 4.475230609386927e-05.\n",
      "66/66 [==============================] - 4s 53ms/step - loss: 0.7233 - val_loss: 2.4505 - lr: 4.5204e-05\n",
      "Epoch 96/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7367\n",
      "Epoch 96: val_loss did not improve from 2.26302\n",
      "\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 4.43047822045628e-05.\n",
      "66/66 [==============================] - 4s 56ms/step - loss: 0.7368 - val_loss: 2.3884 - lr: 4.4752e-05\n",
      "Epoch 97/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7344\n",
      "Epoch 97: val_loss did not improve from 2.26302\n",
      "\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 4.386173510283697e-05.\n",
      "66/66 [==============================] - 4s 56ms/step - loss: 0.7351 - val_loss: 2.3523 - lr: 4.4305e-05\n",
      "Epoch 98/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7307\n",
      "Epoch 98: val_loss did not improve from 2.26302\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 4.342311796790454e-05.\n",
      "66/66 [==============================] - 4s 57ms/step - loss: 0.7307 - val_loss: 2.3097 - lr: 4.3862e-05\n",
      "Epoch 99/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7288\n",
      "Epoch 99: val_loss did not improve from 2.26302\n",
      "\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 4.298888758057728e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.7276 - val_loss: 2.5102 - lr: 4.3423e-05\n",
      "Epoch 100/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7280\n",
      "Epoch 100: val_loss did not improve from 2.26302\n",
      "\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 4.2558997120067946e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.7280 - val_loss: 2.4398 - lr: 4.2989e-05\n",
      "Epoch 101/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7252\n",
      "Epoch 101: val_loss did not improve from 2.26302\n",
      "\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 4.2133406968787315e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.7281 - val_loss: 2.5102 - lr: 4.2559e-05\n",
      "Epoch 102/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7172\n",
      "Epoch 102: val_loss did not improve from 2.26302\n",
      "\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 4.1712073907547166e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.7172 - val_loss: 2.3944 - lr: 4.2133e-05\n",
      "Epoch 103/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7216\n",
      "Epoch 103: val_loss did not improve from 2.26302\n",
      "\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 4.1294954717159274e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.7206 - val_loss: 2.4244 - lr: 4.1712e-05\n",
      "Epoch 104/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7247\n",
      "Epoch 104: val_loss improved from 2.26302 to 2.26121, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.7247 - val_loss: 2.2612 - lr: 4.1295e-05\n",
      "Epoch 105/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7190\n",
      "Epoch 105: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 4.08820061784354e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.7181 - val_loss: 2.3765 - lr: 4.1295e-05\n",
      "Epoch 106/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7235\n",
      "Epoch 106: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 4.0473185072187336e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.7215 - val_loss: 2.4940 - lr: 4.0882e-05\n",
      "Epoch 107/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7310\n",
      "Epoch 107: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 4.0068451780825854e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.7310 - val_loss: 2.2735 - lr: 4.0473e-05\n",
      "Epoch 108/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7297\n",
      "Epoch 108: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 3.966776668676175e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 3s 48ms/step - loss: 0.7297 - val_loss: 2.4205 - lr: 4.0068e-05\n",
      "Epoch 109/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7192\n",
      "Epoch 109: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 3.927109017240582e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.7182 - val_loss: 2.2861 - lr: 3.9668e-05\n",
      "Epoch 110/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7257\n",
      "Epoch 110: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 3.887837901856983e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.7257 - val_loss: 2.4720 - lr: 3.9271e-05\n",
      "Epoch 111/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7242\n",
      "Epoch 111: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 3.848959360766457e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.7228 - val_loss: 2.3150 - lr: 3.8878e-05\n",
      "Epoch 112/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7203\n",
      "Epoch 112: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 3.810469792369986e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.7190 - val_loss: 2.3193 - lr: 3.8490e-05\n",
      "Epoch 113/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7164\n",
      "Epoch 113: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 3.772365234908648e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.7164 - val_loss: 2.3429 - lr: 3.8105e-05\n",
      "Epoch 114/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7304\n",
      "Epoch 114: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 3.734641726623522e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.7296 - val_loss: 2.3335 - lr: 3.7724e-05\n",
      "Epoch 115/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7197\n",
      "Epoch 115: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 3.6972953057556876e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.7197 - val_loss: 2.4539 - lr: 3.7346e-05\n",
      "Epoch 116/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7152\n",
      "Epoch 116: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 3.660322370706126e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.7152 - val_loss: 2.3672 - lr: 3.6973e-05\n",
      "Epoch 117/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7230\n",
      "Epoch 117: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 3.623719319875818e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.7244 - val_loss: 2.4283 - lr: 3.6603e-05\n",
      "Epoch 118/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7191\n",
      "Epoch 118: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 3.5874821915058416e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.7191 - val_loss: 2.5076 - lr: 3.6237e-05\n",
      "Epoch 119/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7206\n",
      "Epoch 119: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 3.551607383997179e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.7182 - val_loss: 2.4072 - lr: 3.5875e-05\n",
      "Epoch 120/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7195\n",
      "Epoch 120: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 3.516091295750812e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.7169 - val_loss: 2.3572 - lr: 3.5516e-05\n",
      "Epoch 121/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7248\n",
      "Epoch 121: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 3.480930325167719e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.7245 - val_loss: 2.3693 - lr: 3.5161e-05\n",
      "Epoch 122/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7207\n",
      "Epoch 122: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 3.446120870648883e-05.\n",
      "66/66 [==============================] - 4s 57ms/step - loss: 0.7205 - val_loss: 2.4095 - lr: 3.4809e-05\n",
      "Epoch 123/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7136\n",
      "Epoch 123: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 123: ReduceLROnPlateau reducing learning rate to 3.4116596907551866e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.7136 - val_loss: 2.3827 - lr: 3.4461e-05\n",
      "Epoch 124/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7079\n",
      "Epoch 124: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 124: ReduceLROnPlateau reducing learning rate to 3.37754318388761e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.7073 - val_loss: 2.5125 - lr: 3.4117e-05\n",
      "Epoch 125/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7132\n",
      "Epoch 125: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 3.3437677484471354e-05.\n",
      "66/66 [==============================] - 3s 52ms/step - loss: 0.7135 - val_loss: 2.3273 - lr: 3.3775e-05\n",
      "Epoch 126/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7109\n",
      "Epoch 126: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 3.310330142994644e-05.\n",
      "66/66 [==============================] - 4s 61ms/step - loss: 0.7109 - val_loss: 2.3999 - lr: 3.3438e-05\n",
      "Epoch 127/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7138\n",
      "Epoch 127: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 127: ReduceLROnPlateau reducing learning rate to 3.277226765931118e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.7127 - val_loss: 2.5025 - lr: 3.3103e-05\n",
      "Epoch 128/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7224\n",
      "Epoch 128: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 128: ReduceLROnPlateau reducing learning rate to 3.24445437581744e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.7201 - val_loss: 2.3776 - lr: 3.2772e-05\n",
      "Epoch 129/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7161\n",
      "Epoch 129: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 129: ReduceLROnPlateau reducing learning rate to 3.212009731214493e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.7155 - val_loss: 2.5442 - lr: 3.2445e-05\n",
      "Epoch 130/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7044\n",
      "Epoch 130: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 130: ReduceLROnPlateau reducing learning rate to 3.17988959068316e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.7044 - val_loss: 2.3629 - lr: 3.2120e-05\n",
      "Epoch 131/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7158\n",
      "Epoch 131: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 131: ReduceLROnPlateau reducing learning rate to 3.1480907127843236e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.7155 - val_loss: 2.3611 - lr: 3.1799e-05\n",
      "Epoch 132/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7128\n",
      "Epoch 132: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 132: ReduceLROnPlateau reducing learning rate to 3.116609856078867e-05.\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.7132 - val_loss: 2.5752 - lr: 3.1481e-05\n",
      "Epoch 133/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6975\n",
      "Epoch 133: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 133: ReduceLROnPlateau reducing learning rate to 3.085443779127672e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.6991 - val_loss: 2.4351 - lr: 3.1166e-05\n",
      "Epoch 134/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7158\n",
      "Epoch 134: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 134: ReduceLROnPlateau reducing learning rate to 3.054589240491623e-05.\n",
      "66/66 [==============================] - 4s 56ms/step - loss: 0.7162 - val_loss: 2.3995 - lr: 3.0854e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7113\n",
      "Epoch 135: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 135: ReduceLROnPlateau reducing learning rate to 3.024043358891504e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.7113 - val_loss: 2.3637 - lr: 3.0546e-05\n",
      "Epoch 136/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7079\n",
      "Epoch 136: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 2.9938028928881975e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.7061 - val_loss: 2.3971 - lr: 3.0240e-05\n",
      "Epoch 137/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7064\n",
      "Epoch 137: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 137: ReduceLROnPlateau reducing learning rate to 2.963864781122538e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.7048 - val_loss: 2.3403 - lr: 2.9938e-05\n",
      "Epoch 138/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7030\n",
      "Epoch 138: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 138: ReduceLROnPlateau reducing learning rate to 2.9342261423153105e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.7037 - val_loss: 2.4726 - lr: 2.9639e-05\n",
      "Epoch 139/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7141\n",
      "Epoch 139: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 139: ReduceLROnPlateau reducing learning rate to 2.9048839151073478e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.7132 - val_loss: 2.3839 - lr: 2.9342e-05\n",
      "Epoch 140/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7079\n",
      "Epoch 140: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 140: ReduceLROnPlateau reducing learning rate to 2.875835038139485e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.7103 - val_loss: 2.4038 - lr: 2.9049e-05\n",
      "Epoch 141/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7161\n",
      "Epoch 141: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 141: ReduceLROnPlateau reducing learning rate to 2.8470766301325058e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.7143 - val_loss: 2.4617 - lr: 2.8758e-05\n",
      "Epoch 142/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6941\n",
      "Epoch 142: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 142: ReduceLROnPlateau reducing learning rate to 2.8186058098071954e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.6941 - val_loss: 2.4037 - lr: 2.8471e-05\n",
      "Epoch 143/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6959\n",
      "Epoch 143: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 143: ReduceLROnPlateau reducing learning rate to 2.7904196958843385e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.6959 - val_loss: 2.4056 - lr: 2.8186e-05\n",
      "Epoch 144/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6949\n",
      "Epoch 144: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 144: ReduceLROnPlateau reducing learning rate to 2.7625155871646713e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.6949 - val_loss: 2.4007 - lr: 2.7904e-05\n",
      "Epoch 145/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7123\n",
      "Epoch 145: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 145: ReduceLROnPlateau reducing learning rate to 2.734890422289027e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7123 - val_loss: 2.4049 - lr: 2.7625e-05\n",
      "Epoch 146/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6983\n",
      "Epoch 146: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 146: ReduceLROnPlateau reducing learning rate to 2.7075415000581416e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.6963 - val_loss: 2.3410 - lr: 2.7349e-05\n",
      "Epoch 147/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7068\n",
      "Epoch 147: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 147: ReduceLROnPlateau reducing learning rate to 2.6804661192727508e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7065 - val_loss: 2.3533 - lr: 2.7075e-05\n",
      "Epoch 148/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7097\n",
      "Epoch 148: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 148: ReduceLROnPlateau reducing learning rate to 2.6536613986536395e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7088 - val_loss: 2.4185 - lr: 2.6805e-05\n",
      "Epoch 149/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6983\n",
      "Epoch 149: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 149: ReduceLROnPlateau reducing learning rate to 2.6271248170814942e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.6992 - val_loss: 2.4069 - lr: 2.6537e-05\n",
      "Epoch 150/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7103\n",
      "Epoch 150: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 150: ReduceLROnPlateau reducing learning rate to 2.6008534932770998e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7103 - val_loss: 2.3894 - lr: 2.6271e-05\n",
      "Epoch 151/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6931\n",
      "Epoch 151: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 151: ReduceLROnPlateau reducing learning rate to 2.5748449061211432e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.6982 - val_loss: 2.3200 - lr: 2.6009e-05\n",
      "Epoch 152/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6994\n",
      "Epoch 152: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 152: ReduceLROnPlateau reducing learning rate to 2.5490965344943107e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.6987 - val_loss: 2.4548 - lr: 2.5748e-05\n",
      "Epoch 153/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6963\n",
      "Epoch 153: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 153: ReduceLROnPlateau reducing learning rate to 2.523605497117387e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.6965 - val_loss: 2.3487 - lr: 2.5491e-05\n",
      "Epoch 154/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7026\n",
      "Epoch 154: val_loss did not improve from 2.26121\n",
      "\n",
      "Epoch 154: ReduceLROnPlateau reducing learning rate to 2.4983694529510104e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7026 - val_loss: 2.4547 - lr: 2.5236e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)           [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_24 (LSTM)                 (None, 45, 24)       3744        ['input_9[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 45, 24)       0           ['lstm_24[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_25 (LSTM)                 (None, 45, 16)       2624        ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 45, 16)       0           ['lstm_25[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_26 (LSTM)                 (None, 32)           6272        ['dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 40)           1320        ['lstm_26[0][0]']                \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 5)            205         ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " tf.unstack_8 (TFOpLambda)      [(None,),            0           ['dense_17[0][0]']               \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_40 (TFOpLambda)  (None, 1)           0           ['tf.unstack_8[0][0]']           \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_16 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_40[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_44 (TFOpLambda)  (None, 1)           0           ['tf.unstack_8[0][4]']           \n",
      "                                                                                                  \n",
      " tf.math.multiply_24 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_16[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_17 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_44[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_25 (TFOpLambd  (None, 1)           0           ['tf.math.multiply_24[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_41 (TFOpLambda)  (None, 1)           0           ['tf.unstack_8[0][1]']           \n",
      "                                                                                                  \n",
      " tf.expand_dims_43 (TFOpLambda)  (None, 1)           0           ['tf.unstack_8[0][3]']           \n",
      "                                                                                                  \n",
      " tf.math.multiply_26 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_17[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_16 (TFOpL  (None, 1)           0           ['tf.math.multiply_25[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.softplus_16 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_41[0][0]']      \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_42 (TFOpLambda)  (None, 1)           0           ['tf.unstack_8[0][2]']           \n",
      "                                                                                                  \n",
      " tf.math.softplus_17 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_43[0][0]']      \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_17 (TFOpL  (None, 1)           0           ['tf.math.multiply_26[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack_8 (TFOpLambda)        (None, 5, 1)         0           ['tf.__operators__.add_16[0][0]',\n",
      "                                                                  'tf.math.softplus_16[0][0]',    \n",
      "                                                                  'tf.expand_dims_42[0][0]',      \n",
      "                                                                  'tf.math.softplus_17[0][0]',    \n",
      "                                                                  'tf.__operators__.add_17[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _1_CCMP_t+1_0.13\n",
      "Epoch 1/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 3.4523\n",
      "Epoch 1: val_loss improved from inf to 4.53824, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 9s 62ms/step - loss: 3.4506 - val_loss: 4.5382 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 2.7657\n",
      "Epoch 2: val_loss improved from 4.53824 to 3.61357, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 2.7652 - val_loss: 3.6136 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.8345\n",
      "Epoch 3: val_loss improved from 3.61357 to 3.05554, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 1.8345 - val_loss: 3.0555 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/66 [============================>.] - ETA: 0s - loss: 1.4706\n",
      "Epoch 4: val_loss did not improve from 3.05554\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 1.4709 - val_loss: 3.3773 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.3044\n",
      "Epoch 5: val_loss did not improve from 3.05554\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 1.3041 - val_loss: 3.8896 - lr: 9.9000e-05\n",
      "Epoch 6/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.2198\n",
      "Epoch 6: val_loss did not improve from 3.05554\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 1.2181 - val_loss: 4.2123 - lr: 9.8010e-05\n",
      "Epoch 7/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1777\n",
      "Epoch 7: val_loss did not improve from 3.05554\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 1.1796 - val_loss: 4.1830 - lr: 9.7030e-05\n",
      "Epoch 8/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1364\n",
      "Epoch 8: val_loss did not improve from 3.05554\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 1.1357 - val_loss: 3.8795 - lr: 9.6060e-05\n",
      "Epoch 9/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1116\n",
      "Epoch 9: val_loss did not improve from 3.05554\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 1.1116 - val_loss: 4.2354 - lr: 9.5099e-05\n",
      "Epoch 10/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0909\n",
      "Epoch 10: val_loss did not improve from 3.05554\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 1.0900 - val_loss: 3.9712 - lr: 9.4148e-05\n",
      "Epoch 11/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0705\n",
      "Epoch 11: val_loss did not improve from 3.05554\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 1.0693 - val_loss: 3.9979 - lr: 9.3207e-05\n",
      "Epoch 12/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0678\n",
      "Epoch 12: val_loss did not improve from 3.05554\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 1.0670 - val_loss: 4.2369 - lr: 9.2274e-05\n",
      "Epoch 13/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0628\n",
      "Epoch 13: val_loss did not improve from 3.05554\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 1.0632 - val_loss: 3.4628 - lr: 9.1352e-05\n",
      "Epoch 14/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0453\n",
      "Epoch 14: val_loss did not improve from 3.05554\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 1.0453 - val_loss: 3.4839 - lr: 9.0438e-05\n",
      "Epoch 15/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0390\n",
      "Epoch 15: val_loss did not improve from 3.05554\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 1.0393 - val_loss: 3.4950 - lr: 8.9534e-05\n",
      "Epoch 16/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0444\n",
      "Epoch 16: val_loss did not improve from 3.05554\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 1.0444 - val_loss: 3.6584 - lr: 8.8638e-05\n",
      "Epoch 17/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0349\n",
      "Epoch 17: val_loss did not improve from 3.05554\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 1.0383 - val_loss: 3.4063 - lr: 8.7752e-05\n",
      "Epoch 18/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0285\n",
      "Epoch 18: val_loss did not improve from 3.05554\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 1.0285 - val_loss: 3.1873 - lr: 8.6875e-05\n",
      "Epoch 19/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0139\n",
      "Epoch 19: val_loss did not improve from 3.05554\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 1.0148 - val_loss: 3.3094 - lr: 8.6006e-05\n",
      "Epoch 20/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0183\n",
      "Epoch 20: val_loss did not improve from 3.05554\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 1.0164 - val_loss: 3.1559 - lr: 8.5146e-05\n",
      "Epoch 21/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0030\n",
      "Epoch 21: val_loss improved from 3.05554 to 3.01869, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 1.0036 - val_loss: 3.0187 - lr: 8.4294e-05\n",
      "Epoch 22/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0009\n",
      "Epoch 22: val_loss did not improve from 3.01869\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 1.0015 - val_loss: 3.0259 - lr: 8.4294e-05\n",
      "Epoch 23/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9809\n",
      "Epoch 23: val_loss improved from 3.01869 to 2.93057, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9810 - val_loss: 2.9306 - lr: 8.3451e-05\n",
      "Epoch 24/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9783\n",
      "Epoch 24: val_loss did not improve from 2.93057\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9783 - val_loss: 2.9654 - lr: 8.3451e-05\n",
      "Epoch 25/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9681\n",
      "Epoch 25: val_loss improved from 2.93057 to 2.82840, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9691 - val_loss: 2.8284 - lr: 8.2617e-05\n",
      "Epoch 26/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9672\n",
      "Epoch 26: val_loss improved from 2.82840 to 2.74037, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9635 - val_loss: 2.7404 - lr: 8.2617e-05\n",
      "Epoch 27/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9392\n",
      "Epoch 27: val_loss improved from 2.74037 to 2.67840, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9396 - val_loss: 2.6784 - lr: 8.2617e-05\n",
      "Epoch 28/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9401\n",
      "Epoch 28: val_loss improved from 2.67840 to 2.67539, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 4s 61ms/step - loss: 0.9402 - val_loss: 2.6754 - lr: 8.2617e-05\n",
      "Epoch 29/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9263\n",
      "Epoch 29: val_loss improved from 2.67539 to 2.58844, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.13.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 4s 63ms/step - loss: 0.9249 - val_loss: 2.5884 - lr: 8.2617e-05\n",
      "Epoch 30/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9277\n",
      "Epoch 30: val_loss improved from 2.58844 to 2.55428, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 4s 58ms/step - loss: 0.9264 - val_loss: 2.5543 - lr: 8.2617e-05\n",
      "Epoch 31/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9118\n",
      "Epoch 31: val_loss did not improve from 2.55428\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.9182 - val_loss: 2.6255 - lr: 8.2617e-05\n",
      "Epoch 32/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9145\n",
      "Epoch 32: val_loss did not improve from 2.55428\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.9118 - val_loss: 2.7926 - lr: 8.1791e-05\n",
      "Epoch 33/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9208\n",
      "Epoch 33: val_loss improved from 2.55428 to 2.53972, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.9207 - val_loss: 2.5397 - lr: 8.0973e-05\n",
      "Epoch 34/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8961\n",
      "Epoch 34: val_loss did not improve from 2.53972\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 3s 52ms/step - loss: 0.8972 - val_loss: 2.6328 - lr: 8.0973e-05\n",
      "Epoch 35/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9012\n",
      "Epoch 35: val_loss improved from 2.53972 to 2.48613, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.9011 - val_loss: 2.4861 - lr: 8.0163e-05\n",
      "Epoch 36/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8945\n",
      "Epoch 36: val_loss did not improve from 2.48613\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 3s 52ms/step - loss: 0.8940 - val_loss: 2.5547 - lr: 8.0163e-05\n",
      "Epoch 37/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8853\n",
      "Epoch 37: val_loss did not improve from 2.48613\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 3s 53ms/step - loss: 0.8849 - val_loss: 2.6728 - lr: 7.9361e-05\n",
      "Epoch 38/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8878\n",
      "Epoch 38: val_loss improved from 2.48613 to 2.48267, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.8869 - val_loss: 2.4827 - lr: 7.8568e-05\n",
      "Epoch 39/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8864\n",
      "Epoch 39: val_loss improved from 2.48267 to 2.48078, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.8852 - val_loss: 2.4808 - lr: 7.8568e-05\n",
      "Epoch 40/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8781\n",
      "Epoch 40: val_loss did not improve from 2.48078\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 3s 52ms/step - loss: 0.8773 - val_loss: 2.4932 - lr: 7.8568e-05\n",
      "Epoch 41/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8672\n",
      "Epoch 41: val_loss did not improve from 2.48078\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 3s 53ms/step - loss: 0.8680 - val_loss: 2.5076 - lr: 7.7782e-05\n",
      "Epoch 42/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8765\n",
      "Epoch 42: val_loss did not improve from 2.48078\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 3s 53ms/step - loss: 0.8782 - val_loss: 2.5418 - lr: 7.7004e-05\n",
      "Epoch 43/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8700\n",
      "Epoch 43: val_loss improved from 2.48078 to 2.35757, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.8712 - val_loss: 2.3576 - lr: 7.6234e-05\n",
      "Epoch 44/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8635\n",
      "Epoch 44: val_loss did not improve from 2.35757\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 3s 52ms/step - loss: 0.8627 - val_loss: 2.4936 - lr: 7.6234e-05\n",
      "Epoch 45/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8707\n",
      "Epoch 45: val_loss improved from 2.35757 to 2.35575, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.8697 - val_loss: 2.3558 - lr: 7.5472e-05\n",
      "Epoch 46/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8626\n",
      "Epoch 46: val_loss did not improve from 2.35575\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 3s 52ms/step - loss: 0.8652 - val_loss: 2.3734 - lr: 7.5472e-05\n",
      "Epoch 47/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8574\n",
      "Epoch 47: val_loss improved from 2.35575 to 2.28864, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 3s 53ms/step - loss: 0.8556 - val_loss: 2.2886 - lr: 7.4717e-05\n",
      "Epoch 48/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8669\n",
      "Epoch 48: val_loss did not improve from 2.28864\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 3s 52ms/step - loss: 0.8669 - val_loss: 2.2910 - lr: 7.4717e-05\n",
      "Epoch 49/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8571\n",
      "Epoch 49: val_loss improved from 2.28864 to 2.28496, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.8571 - val_loss: 2.2850 - lr: 7.3970e-05\n",
      "Epoch 50/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8556\n",
      "Epoch 50: val_loss improved from 2.28496 to 2.22498, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 3s 52ms/step - loss: 0.8622 - val_loss: 2.2250 - lr: 7.3970e-05\n",
      "Epoch 51/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8532\n",
      "Epoch 51: val_loss improved from 2.22498 to 2.21488, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 4s 55ms/step - loss: 0.8522 - val_loss: 2.2149 - lr: 7.3970e-05\n",
      "Epoch 52/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8601\n",
      "Epoch 52: val_loss did not improve from 2.21488\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.8601 - val_loss: 2.3504 - lr: 7.3970e-05\n",
      "Epoch 53/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8593\n",
      "Epoch 53: val_loss did not improve from 2.21488\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 4s 56ms/step - loss: 0.8583 - val_loss: 2.3708 - lr: 7.3230e-05\n",
      "Epoch 54/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8375\n",
      "Epoch 54: val_loss improved from 2.21488 to 2.20488, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.13.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 4s 58ms/step - loss: 0.8381 - val_loss: 2.2049 - lr: 7.2498e-05\n",
      "Epoch 55/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8458\n",
      "Epoch 55: val_loss did not improve from 2.20488\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.8450 - val_loss: 2.2787 - lr: 7.2498e-05\n",
      "Epoch 56/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8471\n",
      "Epoch 56: val_loss did not improve from 2.20488\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8471 - val_loss: 2.3435 - lr: 7.1773e-05\n",
      "Epoch 57/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8430\n",
      "Epoch 57: val_loss improved from 2.20488 to 2.18555, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.8439 - val_loss: 2.1855 - lr: 7.1055e-05\n",
      "Epoch 58/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8539\n",
      "Epoch 58: val_loss improved from 2.18555 to 2.14925, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.8525 - val_loss: 2.1492 - lr: 7.1055e-05\n",
      "Epoch 59/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8425\n",
      "Epoch 59: val_loss did not improve from 2.14925\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.8429 - val_loss: 2.1805 - lr: 7.1055e-05\n",
      "Epoch 60/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8359\n",
      "Epoch 60: val_loss improved from 2.14925 to 2.13917, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8390 - val_loss: 2.1392 - lr: 7.0345e-05\n",
      "Epoch 61/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8355\n",
      "Epoch 61: val_loss did not improve from 2.13917\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8405 - val_loss: 2.1910 - lr: 7.0345e-05\n",
      "Epoch 62/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8514\n",
      "Epoch 62: val_loss did not improve from 2.13917\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.8514 - val_loss: 2.1568 - lr: 6.9641e-05\n",
      "Epoch 63/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8328\n",
      "Epoch 63: val_loss improved from 2.13917 to 2.10748, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.8328 - val_loss: 2.1075 - lr: 6.8945e-05\n",
      "Epoch 64/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8368\n",
      "Epoch 64: val_loss did not improve from 2.10748\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8368 - val_loss: 2.2392 - lr: 6.8945e-05\n",
      "Epoch 65/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8253\n",
      "Epoch 65: val_loss improved from 2.10748 to 2.09720, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.8277 - val_loss: 2.0972 - lr: 6.8255e-05\n",
      "Epoch 66/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8294\n",
      "Epoch 66: val_loss did not improve from 2.09720\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8292 - val_loss: 2.2275 - lr: 6.8255e-05\n",
      "Epoch 67/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8318\n",
      "Epoch 67: val_loss improved from 2.09720 to 2.00262, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.8326 - val_loss: 2.0026 - lr: 6.7573e-05\n",
      "Epoch 68/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8247\n",
      "Epoch 68: val_loss did not improve from 2.00262\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.8241 - val_loss: 2.1260 - lr: 6.7573e-05\n",
      "Epoch 69/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8255\n",
      "Epoch 69: val_loss did not improve from 2.00262\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.8244 - val_loss: 2.0805 - lr: 6.6897e-05\n",
      "Epoch 70/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8154\n",
      "Epoch 70: val_loss did not improve from 2.00262\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.8154 - val_loss: 2.1799 - lr: 6.6228e-05\n",
      "Epoch 71/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8351\n",
      "Epoch 71: val_loss did not improve from 2.00262\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.8338 - val_loss: 2.0774 - lr: 6.5566e-05\n",
      "Epoch 72/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8279\n",
      "Epoch 72: val_loss did not improve from 2.00262\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.8279 - val_loss: 2.0988 - lr: 6.4910e-05\n",
      "Epoch 73/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8312\n",
      "Epoch 73: val_loss did not improve from 2.00262\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.8312 - val_loss: 2.0646 - lr: 6.4261e-05\n",
      "Epoch 74/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8152\n",
      "Epoch 74: val_loss did not improve from 2.00262\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.8152 - val_loss: 2.1621 - lr: 6.3619e-05\n",
      "Epoch 75/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8314\n",
      "Epoch 75: val_loss did not improve from 2.00262\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.8302 - val_loss: 2.0433 - lr: 6.2982e-05\n",
      "Epoch 76/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8078\n",
      "Epoch 76: val_loss did not improve from 2.00262\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.8077 - val_loss: 2.1706 - lr: 6.2353e-05\n",
      "Epoch 77/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8274\n",
      "Epoch 77: val_loss did not improve from 2.00262\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.8277 - val_loss: 2.0355 - lr: 6.1729e-05\n",
      "Epoch 78/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8178\n",
      "Epoch 78: val_loss did not improve from 2.00262\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.8189 - val_loss: 2.1028 - lr: 6.1112e-05\n",
      "Epoch 79/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8204\n",
      "Epoch 79: val_loss did not improve from 2.00262\n",
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.8191 - val_loss: 2.0687 - lr: 6.0501e-05\n",
      "Epoch 80/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8144\n",
      "Epoch 80: val_loss improved from 2.00262 to 1.98272, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.8141 - val_loss: 1.9827 - lr: 5.9896e-05\n",
      "Epoch 81/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8030\n",
      "Epoch 81: val_loss did not improve from 1.98272\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.8030 - val_loss: 2.0281 - lr: 5.9896e-05\n",
      "Epoch 82/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8202\n",
      "Epoch 82: val_loss improved from 1.98272 to 1.88188, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.8210 - val_loss: 1.8819 - lr: 5.9297e-05\n",
      "Epoch 83/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8115\n",
      "Epoch 83: val_loss did not improve from 1.88188\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 5.8703650938696225e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.8115 - val_loss: 1.9427 - lr: 5.9297e-05\n",
      "Epoch 84/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8145\n",
      "Epoch 84: val_loss did not improve from 1.88188\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 5.811661550978897e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.8145 - val_loss: 2.0826 - lr: 5.8704e-05\n",
      "Epoch 85/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8081\n",
      "Epoch 85: val_loss did not improve from 1.88188\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 5.753545068728272e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.8081 - val_loss: 1.8876 - lr: 5.8117e-05\n",
      "Epoch 86/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8141\n",
      "Epoch 86: val_loss did not improve from 1.88188\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 5.6960095243994146e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.8143 - val_loss: 1.9384 - lr: 5.7535e-05\n",
      "Epoch 87/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8048\n",
      "Epoch 87: val_loss did not improve from 1.88188\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 5.639049515593797e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.8055 - val_loss: 1.9760 - lr: 5.6960e-05\n",
      "Epoch 88/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8037\n",
      "Epoch 88: val_loss did not improve from 1.88188\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 5.5826589195930865e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.8037 - val_loss: 1.9818 - lr: 5.6390e-05\n",
      "Epoch 89/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8063\n",
      "Epoch 89: val_loss did not improve from 1.88188\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 5.526832333998754e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.8063 - val_loss: 1.9613 - lr: 5.5827e-05\n",
      "Epoch 90/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8109\n",
      "Epoch 90: val_loss did not improve from 1.88188\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 5.471563996252371e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.8090 - val_loss: 1.9247 - lr: 5.5268e-05\n",
      "Epoch 91/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8114\n",
      "Epoch 91: val_loss did not improve from 1.88188\n",
      "\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 5.416848503955407e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8114 - val_loss: 1.9432 - lr: 5.4716e-05\n",
      "Epoch 92/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8065\n",
      "Epoch 92: val_loss did not improve from 1.88188\n",
      "\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 5.362680094549433e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.8065 - val_loss: 1.9331 - lr: 5.4168e-05\n",
      "Epoch 93/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7966\n",
      "Epoch 93: val_loss did not improve from 1.88188\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 5.309053365635918e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7966 - val_loss: 1.9328 - lr: 5.3627e-05\n",
      "Epoch 94/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8060\n",
      "Epoch 94: val_loss did not improve from 1.88188\n",
      "\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 5.2559629148163366e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.8062 - val_loss: 1.9906 - lr: 5.3091e-05\n",
      "Epoch 95/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8034\n",
      "Epoch 95: val_loss did not improve from 1.88188\n",
      "\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 5.2034033396921584e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.8034 - val_loss: 1.9814 - lr: 5.2560e-05\n",
      "Epoch 96/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8054\n",
      "Epoch 96: val_loss did not improve from 1.88188\n",
      "\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 5.1513692378648555e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.8088 - val_loss: 1.9040 - lr: 5.2034e-05\n",
      "Epoch 97/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8005\n",
      "Epoch 97: val_loss did not improve from 1.88188\n",
      "\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 5.099855567095801e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.8029 - val_loss: 1.9754 - lr: 5.1514e-05\n",
      "Epoch 98/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7890\n",
      "Epoch 98: val_loss did not improve from 1.88188\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 5.048856924986467e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7879 - val_loss: 2.0129 - lr: 5.0999e-05\n",
      "Epoch 99/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8006\n",
      "Epoch 99: val_loss did not improve from 1.88188\n",
      "\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 4.998368269298226e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7994 - val_loss: 2.0050 - lr: 5.0489e-05\n",
      "Epoch 100/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7842\n",
      "Epoch 100: val_loss improved from 1.88188 to 1.85312, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.7842 - val_loss: 1.8531 - lr: 4.9984e-05\n",
      "Epoch 101/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7968\n",
      "Epoch 101: val_loss did not improve from 1.85312\n",
      "\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 4.9483845577924514e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7968 - val_loss: 1.8826 - lr: 4.9984e-05\n",
      "Epoch 102/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8011\n",
      "Epoch 102: val_loss did not improve from 1.85312\n",
      "\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 4.898900748230517e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.8011 - val_loss: 1.9842 - lr: 4.9484e-05\n",
      "Epoch 103/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8006\n",
      "Epoch 103: val_loss did not improve from 1.85312\n",
      "\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 4.849911798373796e-05.\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.7987 - val_loss: 1.9312 - lr: 4.8989e-05\n",
      "Epoch 104/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8040\n",
      "Epoch 104: val_loss did not improve from 1.85312\n",
      "\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 4.801412665983662e-05.\n",
      "66/66 [==============================] - 4s 61ms/step - loss: 0.8013 - val_loss: 1.9601 - lr: 4.8499e-05\n",
      "Epoch 105/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7955\n",
      "Epoch 105: val_loss did not improve from 1.85312\n",
      "\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 4.75339866898139e-05.\n",
      "66/66 [==============================] - 4s 57ms/step - loss: 0.7945 - val_loss: 1.9000 - lr: 4.8014e-05\n",
      "Epoch 106/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/66 [============================>.] - ETA: 0s - loss: 0.7961\n",
      "Epoch 106: val_loss did not improve from 1.85312\n",
      "\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 4.705864765128354e-05.\n",
      "66/66 [==============================] - 3s 52ms/step - loss: 0.7946 - val_loss: 1.8540 - lr: 4.7534e-05\n",
      "Epoch 107/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7969\n",
      "Epoch 107: val_loss did not improve from 1.85312\n",
      "\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 4.658806272345828e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.7967 - val_loss: 1.8799 - lr: 4.7059e-05\n",
      "Epoch 108/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7920\n",
      "Epoch 108: val_loss did not improve from 1.85312\n",
      "\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 4.612218148395186e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.7920 - val_loss: 1.9493 - lr: 4.6588e-05\n",
      "Epoch 109/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7809\n",
      "Epoch 109: val_loss improved from 1.85312 to 1.81354, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.7809 - val_loss: 1.8135 - lr: 4.6122e-05\n",
      "Epoch 110/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7890\n",
      "Epoch 110: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 4.566096071357606e-05.\n",
      "66/66 [==============================] - 3s 53ms/step - loss: 0.7882 - val_loss: 1.9599 - lr: 4.6122e-05\n",
      "Epoch 111/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8031\n",
      "Epoch 111: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 4.52043499899446e-05.\n",
      "66/66 [==============================] - 4s 59ms/step - loss: 0.8040 - val_loss: 1.8461 - lr: 4.5661e-05\n",
      "Epoch 112/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7878\n",
      "Epoch 112: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 4.475230609386927e-05.\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.7878 - val_loss: 1.8804 - lr: 4.5204e-05\n",
      "Epoch 113/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7773\n",
      "Epoch 113: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 4.43047822045628e-05.\n",
      "66/66 [==============================] - 4s 53ms/step - loss: 0.7780 - val_loss: 1.8729 - lr: 4.4752e-05\n",
      "Epoch 114/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7939\n",
      "Epoch 114: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 4.386173510283697e-05.\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.7930 - val_loss: 1.9511 - lr: 4.4305e-05\n",
      "Epoch 115/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7891\n",
      "Epoch 115: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 4.342311796790454e-05.\n",
      "66/66 [==============================] - 4s 55ms/step - loss: 0.7895 - val_loss: 1.8763 - lr: 4.3862e-05\n",
      "Epoch 116/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7935\n",
      "Epoch 116: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 4.298888758057728e-05.\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.7926 - val_loss: 1.9482 - lr: 4.3423e-05\n",
      "Epoch 117/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7807\n",
      "Epoch 117: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 4.2558997120067946e-05.\n",
      "66/66 [==============================] - 4s 55ms/step - loss: 0.7796 - val_loss: 1.8493 - lr: 4.2989e-05\n",
      "Epoch 118/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7754\n",
      "Epoch 118: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 4.2133406968787315e-05.\n",
      "66/66 [==============================] - 4s 55ms/step - loss: 0.7779 - val_loss: 1.8754 - lr: 4.2559e-05\n",
      "Epoch 119/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7782\n",
      "Epoch 119: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 4.1712073907547166e-05.\n",
      "66/66 [==============================] - 4s 55ms/step - loss: 0.7787 - val_loss: 1.9005 - lr: 4.2133e-05\n",
      "Epoch 120/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7824\n",
      "Epoch 120: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 4.1294954717159274e-05.\n",
      "66/66 [==============================] - 4s 55ms/step - loss: 0.7833 - val_loss: 1.9694 - lr: 4.1712e-05\n",
      "Epoch 121/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7739\n",
      "Epoch 121: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 4.08820061784354e-05.\n",
      "66/66 [==============================] - 4s 55ms/step - loss: 0.7762 - val_loss: 1.8380 - lr: 4.1295e-05\n",
      "Epoch 122/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7832\n",
      "Epoch 122: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 4.0473185072187336e-05.\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.7824 - val_loss: 1.9063 - lr: 4.0882e-05\n",
      "Epoch 123/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7894\n",
      "Epoch 123: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 123: ReduceLROnPlateau reducing learning rate to 4.0068451780825854e-05.\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.7899 - val_loss: 1.8667 - lr: 4.0473e-05\n",
      "Epoch 124/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7800\n",
      "Epoch 124: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 124: ReduceLROnPlateau reducing learning rate to 3.966776668676175e-05.\n",
      "66/66 [==============================] - 4s 55ms/step - loss: 0.7810 - val_loss: 1.9019 - lr: 4.0068e-05\n",
      "Epoch 125/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7857\n",
      "Epoch 125: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 3.927109017240582e-05.\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.7855 - val_loss: 1.9760 - lr: 3.9668e-05\n",
      "Epoch 126/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7869\n",
      "Epoch 126: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 3.887837901856983e-05.\n",
      "66/66 [==============================] - 4s 55ms/step - loss: 0.7864 - val_loss: 1.8695 - lr: 3.9271e-05\n",
      "Epoch 127/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7745\n",
      "Epoch 127: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 127: ReduceLROnPlateau reducing learning rate to 3.848959360766457e-05.\n",
      "66/66 [==============================] - 4s 56ms/step - loss: 0.7776 - val_loss: 1.9344 - lr: 3.8878e-05\n",
      "Epoch 128/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7812\n",
      "Epoch 128: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 128: ReduceLROnPlateau reducing learning rate to 3.810469792369986e-05.\n",
      "66/66 [==============================] - 4s 56ms/step - loss: 0.7790 - val_loss: 1.9026 - lr: 3.8490e-05\n",
      "Epoch 129/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7808\n",
      "Epoch 129: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 129: ReduceLROnPlateau reducing learning rate to 3.772365234908648e-05.\n",
      "66/66 [==============================] - 4s 59ms/step - loss: 0.7834 - val_loss: 1.8702 - lr: 3.8105e-05\n",
      "Epoch 130/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7831\n",
      "Epoch 130: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 130: ReduceLROnPlateau reducing learning rate to 3.734641726623522e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.7831 - val_loss: 1.9896 - lr: 3.7724e-05\n",
      "Epoch 131/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7865\n",
      "Epoch 131: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 131: ReduceLROnPlateau reducing learning rate to 3.6972953057556876e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.7845 - val_loss: 1.9434 - lr: 3.7346e-05\n",
      "Epoch 132/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7794\n",
      "Epoch 132: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 132: ReduceLROnPlateau reducing learning rate to 3.660322370706126e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.7783 - val_loss: 1.8384 - lr: 3.6973e-05\n",
      "Epoch 133/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7774\n",
      "Epoch 133: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 133: ReduceLROnPlateau reducing learning rate to 3.623719319875818e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.7781 - val_loss: 1.9290 - lr: 3.6603e-05\n",
      "Epoch 134/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7741\n",
      "Epoch 134: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 134: ReduceLROnPlateau reducing learning rate to 3.5874821915058416e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.7776 - val_loss: 1.8921 - lr: 3.6237e-05\n",
      "Epoch 135/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7771\n",
      "Epoch 135: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 135: ReduceLROnPlateau reducing learning rate to 3.551607383997179e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.7775 - val_loss: 1.9358 - lr: 3.5875e-05\n",
      "Epoch 136/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7720\n",
      "Epoch 136: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 3.516091295750812e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7712 - val_loss: 1.9518 - lr: 3.5516e-05\n",
      "Epoch 137/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7688\n",
      "Epoch 137: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 137: ReduceLROnPlateau reducing learning rate to 3.480930325167719e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7675 - val_loss: 1.8937 - lr: 3.5161e-05\n",
      "Epoch 138/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7797\n",
      "Epoch 138: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 138: ReduceLROnPlateau reducing learning rate to 3.446120870648883e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7789 - val_loss: 1.8481 - lr: 3.4809e-05\n",
      "Epoch 139/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7677\n",
      "Epoch 139: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 139: ReduceLROnPlateau reducing learning rate to 3.4116596907551866e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7682 - val_loss: 1.9281 - lr: 3.4461e-05\n",
      "Epoch 140/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7714\n",
      "Epoch 140: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 140: ReduceLROnPlateau reducing learning rate to 3.37754318388761e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7714 - val_loss: 1.8793 - lr: 3.4117e-05\n",
      "Epoch 141/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7738\n",
      "Epoch 141: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 141: ReduceLROnPlateau reducing learning rate to 3.3437677484471354e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7737 - val_loss: 1.8559 - lr: 3.3775e-05\n",
      "Epoch 142/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7798\n",
      "Epoch 142: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 142: ReduceLROnPlateau reducing learning rate to 3.310330142994644e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7812 - val_loss: 1.8456 - lr: 3.3438e-05\n",
      "Epoch 143/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7763\n",
      "Epoch 143: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 143: ReduceLROnPlateau reducing learning rate to 3.277226765931118e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7765 - val_loss: 1.8649 - lr: 3.3103e-05\n",
      "Epoch 144/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7838\n",
      "Epoch 144: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 144: ReduceLROnPlateau reducing learning rate to 3.24445437581744e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.7816 - val_loss: 1.9667 - lr: 3.2772e-05\n",
      "Epoch 145/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7723\n",
      "Epoch 145: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 145: ReduceLROnPlateau reducing learning rate to 3.212009731214493e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7716 - val_loss: 1.8743 - lr: 3.2445e-05\n",
      "Epoch 146/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7671\n",
      "Epoch 146: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 146: ReduceLROnPlateau reducing learning rate to 3.17988959068316e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7671 - val_loss: 1.9316 - lr: 3.2120e-05\n",
      "Epoch 147/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7645\n",
      "Epoch 147: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 147: ReduceLROnPlateau reducing learning rate to 3.1480907127843236e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7641 - val_loss: 1.8606 - lr: 3.1799e-05\n",
      "Epoch 148/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7655\n",
      "Epoch 148: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 148: ReduceLROnPlateau reducing learning rate to 3.116609856078867e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.7668 - val_loss: 1.8859 - lr: 3.1481e-05\n",
      "Epoch 149/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7734\n",
      "Epoch 149: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 149: ReduceLROnPlateau reducing learning rate to 3.085443779127672e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7725 - val_loss: 1.9031 - lr: 3.1166e-05\n",
      "Epoch 150/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7733\n",
      "Epoch 150: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 150: ReduceLROnPlateau reducing learning rate to 3.054589240491623e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7733 - val_loss: 1.8317 - lr: 3.0854e-05\n",
      "Epoch 151/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7692\n",
      "Epoch 151: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 151: ReduceLROnPlateau reducing learning rate to 3.024043358891504e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7681 - val_loss: 1.8497 - lr: 3.0546e-05\n",
      "Epoch 152/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7692\n",
      "Epoch 152: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 152: ReduceLROnPlateau reducing learning rate to 2.9938028928881975e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7711 - val_loss: 1.8314 - lr: 3.0240e-05\n",
      "Epoch 153/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7660\n",
      "Epoch 153: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 153: ReduceLROnPlateau reducing learning rate to 2.963864781122538e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7657 - val_loss: 1.9317 - lr: 2.9938e-05\n",
      "Epoch 154/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7705\n",
      "Epoch 154: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 154: ReduceLROnPlateau reducing learning rate to 2.9342261423153105e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7705 - val_loss: 1.8827 - lr: 2.9639e-05\n",
      "Epoch 155/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7528\n",
      "Epoch 155: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 155: ReduceLROnPlateau reducing learning rate to 2.9048839151073478e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7542 - val_loss: 1.9177 - lr: 2.9342e-05\n",
      "Epoch 156/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7750\n",
      "Epoch 156: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 156: ReduceLROnPlateau reducing learning rate to 2.875835038139485e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7744 - val_loss: 1.9182 - lr: 2.9049e-05\n",
      "Epoch 157/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7705\n",
      "Epoch 157: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 157: ReduceLROnPlateau reducing learning rate to 2.8470766301325058e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7715 - val_loss: 1.8759 - lr: 2.8758e-05\n",
      "Epoch 158/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7665\n",
      "Epoch 158: val_loss did not improve from 1.81354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 158: ReduceLROnPlateau reducing learning rate to 2.8186058098071954e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7652 - val_loss: 1.8742 - lr: 2.8471e-05\n",
      "Epoch 159/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7688\n",
      "Epoch 159: val_loss did not improve from 1.81354\n",
      "\n",
      "Epoch 159: ReduceLROnPlateau reducing learning rate to 2.7904196958843385e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.7683 - val_loss: 1.8996 - lr: 2.8186e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_10 (InputLayer)          [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_27 (LSTM)                 (None, 45, 24)       3744        ['input_10[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 45, 24)       0           ['lstm_27[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_28 (LSTM)                 (None, 45, 16)       2624        ['dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 45, 16)       0           ['lstm_28[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_29 (LSTM)                 (None, 32)           6272        ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 40)           1320        ['lstm_29[0][0]']                \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 5)            205         ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " tf.unstack_9 (TFOpLambda)      [(None,),            0           ['dense_19[0][0]']               \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_45 (TFOpLambda)  (None, 1)           0           ['tf.unstack_9[0][0]']           \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_18 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_45[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_49 (TFOpLambda)  (None, 1)           0           ['tf.unstack_9[0][4]']           \n",
      "                                                                                                  \n",
      " tf.math.multiply_27 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_18[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_19 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_49[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_28 (TFOpLambd  (None, 1)           0           ['tf.math.multiply_27[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_46 (TFOpLambda)  (None, 1)           0           ['tf.unstack_9[0][1]']           \n",
      "                                                                                                  \n",
      " tf.expand_dims_48 (TFOpLambda)  (None, 1)           0           ['tf.unstack_9[0][3]']           \n",
      "                                                                                                  \n",
      " tf.math.multiply_29 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_19[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_18 (TFOpL  (None, 1)           0           ['tf.math.multiply_28[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.softplus_18 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_46[0][0]']      \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_47 (TFOpLambda)  (None, 1)           0           ['tf.unstack_9[0][2]']           \n",
      "                                                                                                  \n",
      " tf.math.softplus_19 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_48[0][0]']      \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_19 (TFOpL  (None, 1)           0           ['tf.math.multiply_29[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack_9 (TFOpLambda)        (None, 5, 1)         0           ['tf.__operators__.add_18[0][0]',\n",
      "                                                                  'tf.math.softplus_18[0][0]',    \n",
      "                                                                  'tf.expand_dims_47[0][0]',      \n",
      "                                                                  'tf.math.softplus_19[0][0]',    \n",
      "                                                                  'tf.__operators__.add_19[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _1_CCMP_t+1_0.14\n",
      "Epoch 1/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 3.4384\n",
      "Epoch 1: val_loss improved from inf to 4.77641, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 9s 60ms/step - loss: 3.4365 - val_loss: 4.7764 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 2.8510\n",
      "Epoch 2: val_loss improved from 4.77641 to 4.49947, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 2.8510 - val_loss: 4.4995 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 2.0679\n",
      "Epoch 3: val_loss improved from 4.49947 to 3.25935, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 2.0705 - val_loss: 3.2594 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - ETA: 0s - loss: 1.6227\n",
      "Epoch 4: val_loss improved from 3.25935 to 2.94380, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 1.6227 - val_loss: 2.9438 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.4038\n",
      "Epoch 5: val_loss improved from 2.94380 to 2.87041, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 1.4038 - val_loss: 2.8704 - lr: 1.0000e-04\n",
      "Epoch 6/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.2940\n",
      "Epoch 6: val_loss improved from 2.87041 to 2.75125, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 1.2965 - val_loss: 2.7512 - lr: 1.0000e-04\n",
      "Epoch 7/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.2377\n",
      "Epoch 7: val_loss improved from 2.75125 to 2.65369, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 1.2399 - val_loss: 2.6537 - lr: 1.0000e-04\n",
      "Epoch 8/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1880\n",
      "Epoch 8: val_loss improved from 2.65369 to 2.59200, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 1.1880 - val_loss: 2.5920 - lr: 1.0000e-04\n",
      "Epoch 9/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1566\n",
      "Epoch 9: val_loss improved from 2.59200 to 2.42643, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 1.1549 - val_loss: 2.4264 - lr: 1.0000e-04\n",
      "Epoch 10/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1352\n",
      "Epoch 10: val_loss improved from 2.42643 to 2.39522, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 1.1366 - val_loss: 2.3952 - lr: 1.0000e-04\n",
      "Epoch 11/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1099\n",
      "Epoch 11: val_loss improved from 2.39522 to 2.39011, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 1.1086 - val_loss: 2.3901 - lr: 1.0000e-04\n",
      "Epoch 12/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0763\n",
      "Epoch 12: val_loss improved from 2.39011 to 2.33061, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 1.0770 - val_loss: 2.3306 - lr: 1.0000e-04\n",
      "Epoch 13/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0700\n",
      "Epoch 13: val_loss did not improve from 2.33061\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 1.0688 - val_loss: 2.3315 - lr: 1.0000e-04\n",
      "Epoch 14/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0664\n",
      "Epoch 14: val_loss improved from 2.33061 to 2.14456, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 1.0662 - val_loss: 2.1446 - lr: 9.9000e-05\n",
      "Epoch 15/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0373\n",
      "Epoch 15: val_loss did not improve from 2.14456\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 1.0373 - val_loss: 2.2457 - lr: 9.9000e-05\n",
      "Epoch 16/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0338\n",
      "Epoch 16: val_loss did not improve from 2.14456\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 1.0305 - val_loss: 2.2528 - lr: 9.8010e-05\n",
      "Epoch 17/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0204\n",
      "Epoch 17: val_loss improved from 2.14456 to 2.04546, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 1.0262 - val_loss: 2.0455 - lr: 9.7030e-05\n",
      "Epoch 18/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0185\n",
      "Epoch 18: val_loss did not improve from 2.04546\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 1.0173 - val_loss: 2.1846 - lr: 9.7030e-05\n",
      "Epoch 19/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9932\n",
      "Epoch 19: val_loss did not improve from 2.04546\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.9956 - val_loss: 2.1250 - lr: 9.6060e-05\n",
      "Epoch 20/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9974\n",
      "Epoch 20: val_loss did not improve from 2.04546\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.9994 - val_loss: 2.0906 - lr: 9.5099e-05\n",
      "Epoch 21/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9869\n",
      "Epoch 21: val_loss did not improve from 2.04546\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.9872 - val_loss: 2.1540 - lr: 9.4148e-05\n",
      "Epoch 22/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9852\n",
      "Epoch 22: val_loss did not improve from 2.04546\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.9870 - val_loss: 2.0737 - lr: 9.3207e-05\n",
      "Epoch 23/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9799\n",
      "Epoch 23: val_loss did not improve from 2.04546\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.9791 - val_loss: 2.0919 - lr: 9.2274e-05\n",
      "Epoch 24/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9619\n",
      "Epoch 24: val_loss improved from 2.04546 to 1.97702, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.9608 - val_loss: 1.9770 - lr: 9.1352e-05\n",
      "Epoch 25/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9548\n",
      "Epoch 25: val_loss did not improve from 1.97702\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.9548 - val_loss: 2.0202 - lr: 9.1352e-05\n",
      "Epoch 26/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9539\n",
      "Epoch 26: val_loss improved from 1.97702 to 1.89374, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.9523 - val_loss: 1.8937 - lr: 9.0438e-05\n",
      "Epoch 27/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9424\n",
      "Epoch 27: val_loss did not improve from 1.89374\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9421 - val_loss: 1.9238 - lr: 9.0438e-05\n",
      "Epoch 28/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9462\n",
      "Epoch 28: val_loss did not improve from 1.89374\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 4s 58ms/step - loss: 0.9475 - val_loss: 1.9093 - lr: 8.9534e-05\n",
      "Epoch 29/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9447\n",
      "Epoch 29: val_loss improved from 1.89374 to 1.88895, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 4s 58ms/step - loss: 0.9454 - val_loss: 1.8890 - lr: 8.8638e-05\n",
      "Epoch 30/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9481\n",
      "Epoch 30: val_loss improved from 1.88895 to 1.83260, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 4s 56ms/step - loss: 0.9488 - val_loss: 1.8326 - lr: 8.8638e-05\n",
      "Epoch 31/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9317\n",
      "Epoch 31: val_loss did not improve from 1.83260\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 3s 52ms/step - loss: 0.9317 - val_loss: 1.8923 - lr: 8.8638e-05\n",
      "Epoch 32/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9306\n",
      "Epoch 32: val_loss improved from 1.83260 to 1.81872, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 4s 53ms/step - loss: 0.9309 - val_loss: 1.8187 - lr: 8.7752e-05\n",
      "Epoch 33/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9290\n",
      "Epoch 33: val_loss improved from 1.81872 to 1.81638, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 4s 60ms/step - loss: 0.9261 - val_loss: 1.8164 - lr: 8.7752e-05\n",
      "Epoch 34/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9310\n",
      "Epoch 34: val_loss did not improve from 1.81638\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.9310 - val_loss: 1.8221 - lr: 8.7752e-05\n",
      "Epoch 35/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9246\n",
      "Epoch 35: val_loss improved from 1.81638 to 1.74982, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 3s 52ms/step - loss: 0.9231 - val_loss: 1.7498 - lr: 8.6875e-05\n",
      "Epoch 36/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9006\n",
      "Epoch 36: val_loss did not improve from 1.74982\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.9000 - val_loss: 1.7773 - lr: 8.6875e-05\n",
      "Epoch 37/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9162\n",
      "Epoch 37: val_loss did not improve from 1.74982\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 4s 57ms/step - loss: 0.9152 - val_loss: 1.7955 - lr: 8.6006e-05\n",
      "Epoch 38/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9197\n",
      "Epoch 38: val_loss improved from 1.74982 to 1.69307, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.9224 - val_loss: 1.6931 - lr: 8.5146e-05\n",
      "Epoch 39/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9101\n",
      "Epoch 39: val_loss did not improve from 1.69307\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 3s 53ms/step - loss: 0.9093 - val_loss: 1.7283 - lr: 8.5146e-05\n",
      "Epoch 40/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8927\n",
      "Epoch 40: val_loss improved from 1.69307 to 1.65584, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.8938 - val_loss: 1.6558 - lr: 8.4294e-05\n",
      "Epoch 41/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9054\n",
      "Epoch 41: val_loss improved from 1.65584 to 1.61679, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.9064 - val_loss: 1.6168 - lr: 8.4294e-05\n",
      "Epoch 42/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9038\n",
      "Epoch 42: val_loss did not improve from 1.61679\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 4s 53ms/step - loss: 0.9025 - val_loss: 1.6783 - lr: 8.4294e-05\n",
      "Epoch 43/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9038\n",
      "Epoch 43: val_loss did not improve from 1.61679\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 4s 56ms/step - loss: 0.9041 - val_loss: 1.6413 - lr: 8.3451e-05\n",
      "Epoch 44/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9035\n",
      "Epoch 44: val_loss did not improve from 1.61679\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.9062 - val_loss: 1.6520 - lr: 8.2617e-05\n",
      "Epoch 45/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8981\n",
      "Epoch 45: val_loss did not improve from 1.61679\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 4s 58ms/step - loss: 0.8984 - val_loss: 1.6378 - lr: 8.1791e-05\n",
      "Epoch 46/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8968\n",
      "Epoch 46: val_loss did not improve from 1.61679\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.8968 - val_loss: 1.6588 - lr: 8.0973e-05\n",
      "Epoch 47/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8782\n",
      "Epoch 47: val_loss did not improve from 1.61679\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.8779 - val_loss: 1.6330 - lr: 8.0163e-05\n",
      "Epoch 48/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8860\n",
      "Epoch 48: val_loss improved from 1.61679 to 1.59754, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.8868 - val_loss: 1.5975 - lr: 7.9361e-05\n",
      "Epoch 49/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8818\n",
      "Epoch 49: val_loss did not improve from 1.59754\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.8820 - val_loss: 1.6004 - lr: 7.9361e-05\n",
      "Epoch 50/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8858\n",
      "Epoch 50: val_loss improved from 1.59754 to 1.58029, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.8858 - val_loss: 1.5803 - lr: 7.8568e-05\n",
      "Epoch 51/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8797\n",
      "Epoch 51: val_loss did not improve from 1.58029\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.8797 - val_loss: 1.5848 - lr: 7.8568e-05\n",
      "Epoch 52/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8848\n",
      "Epoch 52: val_loss improved from 1.58029 to 1.56186, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.8848 - val_loss: 1.5619 - lr: 7.7782e-05\n",
      "Epoch 53/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8737\n",
      "Epoch 53: val_loss did not improve from 1.56186\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 3s 50ms/step - loss: 0.8737 - val_loss: 1.5694 - lr: 7.7782e-05\n",
      "Epoch 54/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8704\n",
      "Epoch 54: val_loss did not improve from 1.56186\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.8704 - val_loss: 1.5662 - lr: 7.7004e-05\n",
      "Epoch 55/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8656\n",
      "Epoch 55: val_loss did not improve from 1.56186\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.8720 - val_loss: 1.5869 - lr: 7.6234e-05\n",
      "Epoch 56/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8777\n",
      "Epoch 56: val_loss improved from 1.56186 to 1.55340, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.8759 - val_loss: 1.5534 - lr: 7.5472e-05\n",
      "Epoch 57/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8636\n",
      "Epoch 57: val_loss did not improve from 1.55340\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8633 - val_loss: 1.5843 - lr: 7.5472e-05\n",
      "Epoch 58/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8741\n",
      "Epoch 58: val_loss did not improve from 1.55340\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.8711 - val_loss: 1.5665 - lr: 7.4717e-05\n",
      "Epoch 59/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8692\n",
      "Epoch 59: val_loss did not improve from 1.55340\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8680 - val_loss: 1.5718 - lr: 7.3970e-05\n",
      "Epoch 60/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8738\n",
      "Epoch 60: val_loss did not improve from 1.55340\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.8738 - val_loss: 1.5726 - lr: 7.3230e-05\n",
      "Epoch 61/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8477\n",
      "Epoch 61: val_loss did not improve from 1.55340\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.8477 - val_loss: 1.5934 - lr: 7.2498e-05\n",
      "Epoch 62/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8587\n",
      "Epoch 62: val_loss did not improve from 1.55340\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8580 - val_loss: 1.6084 - lr: 7.1773e-05\n",
      "Epoch 63/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8737\n",
      "Epoch 63: val_loss did not improve from 1.55340\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.8724 - val_loss: 1.5745 - lr: 7.1055e-05\n",
      "Epoch 64/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8613\n",
      "Epoch 64: val_loss did not improve from 1.55340\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8613 - val_loss: 1.5614 - lr: 7.0345e-05\n",
      "Epoch 65/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8559\n",
      "Epoch 65: val_loss improved from 1.55340 to 1.53845, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8563 - val_loss: 1.5384 - lr: 6.9641e-05\n",
      "Epoch 66/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8487\n",
      "Epoch 66: val_loss did not improve from 1.53845\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.8468 - val_loss: 1.5400 - lr: 6.9641e-05\n",
      "Epoch 67/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8531\n",
      "Epoch 67: val_loss improved from 1.53845 to 1.53535, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8531 - val_loss: 1.5353 - lr: 6.8945e-05\n",
      "Epoch 68/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8529\n",
      "Epoch 68: val_loss did not improve from 1.53535\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8529 - val_loss: 1.5429 - lr: 6.8945e-05\n",
      "Epoch 69/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8547\n",
      "Epoch 69: val_loss improved from 1.53535 to 1.51993, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8561 - val_loss: 1.5199 - lr: 6.8255e-05\n",
      "Epoch 70/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8554\n",
      "Epoch 70: val_loss did not improve from 1.51993\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8539 - val_loss: 1.5759 - lr: 6.8255e-05\n",
      "Epoch 71/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8508\n",
      "Epoch 71: val_loss did not improve from 1.51993\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8496 - val_loss: 1.5298 - lr: 6.7573e-05\n",
      "Epoch 72/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8357\n",
      "Epoch 72: val_loss did not improve from 1.51993\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8357 - val_loss: 1.5404 - lr: 6.6897e-05\n",
      "Epoch 73/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8556\n",
      "Epoch 73: val_loss improved from 1.51993 to 1.51232, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.8556 - val_loss: 1.5123 - lr: 6.6228e-05\n",
      "Epoch 74/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8551\n",
      "Epoch 74: val_loss did not improve from 1.51232\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.8551 - val_loss: 1.5602 - lr: 6.6228e-05\n",
      "Epoch 75/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8492\n",
      "Epoch 75: val_loss did not improve from 1.51232\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8504 - val_loss: 1.5219 - lr: 6.5566e-05\n",
      "Epoch 76/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8624\n",
      "Epoch 76: val_loss did not improve from 1.51232\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.8624 - val_loss: 1.5290 - lr: 6.4910e-05\n",
      "Epoch 77/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8540\n",
      "Epoch 77: val_loss did not improve from 1.51232\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 3s 52ms/step - loss: 0.8540 - val_loss: 1.5141 - lr: 6.4261e-05\n",
      "Epoch 78/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8550\n",
      "Epoch 78: val_loss did not improve from 1.51232\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.8561 - val_loss: 1.5230 - lr: 6.3619e-05\n",
      "Epoch 79/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8571\n",
      "Epoch 79: val_loss improved from 1.51232 to 1.50248, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.14.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 3s 50ms/step - loss: 0.8573 - val_loss: 1.5025 - lr: 6.2982e-05\n",
      "Epoch 80/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8521\n",
      "Epoch 80: val_loss did not improve from 1.50248\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.8514 - val_loss: 1.5128 - lr: 6.2982e-05\n",
      "Epoch 81/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8478\n",
      "Epoch 81: val_loss did not improve from 1.50248\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.8497 - val_loss: 1.5081 - lr: 6.2353e-05\n",
      "Epoch 82/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8457\n",
      "Epoch 82: val_loss did not improve from 1.50248\n",
      "\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.8457 - val_loss: 1.5153 - lr: 6.1729e-05\n",
      "Epoch 83/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8472\n",
      "Epoch 83: val_loss did not improve from 1.50248\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.8459 - val_loss: 1.5070 - lr: 6.1112e-05\n",
      "Epoch 84/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8377\n",
      "Epoch 84: val_loss did not improve from 1.50248\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.8359 - val_loss: 1.5265 - lr: 6.0501e-05\n",
      "Epoch 85/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8425\n",
      "Epoch 85: val_loss did not improve from 1.50248\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.8426 - val_loss: 1.5175 - lr: 5.9896e-05\n",
      "Epoch 86/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8304\n",
      "Epoch 86: val_loss did not improve from 1.50248\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 5.8703650938696225e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.8300 - val_loss: 1.5092 - lr: 5.9297e-05\n",
      "Epoch 87/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8281\n",
      "Epoch 87: val_loss did not improve from 1.50248\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 5.811661550978897e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.8288 - val_loss: 1.5082 - lr: 5.8704e-05\n",
      "Epoch 88/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8346\n",
      "Epoch 88: val_loss did not improve from 1.50248\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 5.753545068728272e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.8361 - val_loss: 1.5155 - lr: 5.8117e-05\n",
      "Epoch 89/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8389\n",
      "Epoch 89: val_loss did not improve from 1.50248\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 5.6960095243994146e-05.\n",
      "66/66 [==============================] - 3s 52ms/step - loss: 0.8389 - val_loss: 1.5032 - lr: 5.7535e-05\n",
      "Epoch 90/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8398\n",
      "Epoch 90: val_loss did not improve from 1.50248\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 5.639049515593797e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.8399 - val_loss: 1.5074 - lr: 5.6960e-05\n",
      "Epoch 91/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8391\n",
      "Epoch 91: val_loss did not improve from 1.50248\n",
      "\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 5.5826589195930865e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.8391 - val_loss: 1.5222 - lr: 5.6390e-05\n",
      "Epoch 92/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8293\n",
      "Epoch 92: val_loss did not improve from 1.50248\n",
      "\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 5.526832333998754e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.8293 - val_loss: 1.5079 - lr: 5.5827e-05\n",
      "Epoch 93/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8291\n",
      "Epoch 93: val_loss did not improve from 1.50248\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 5.471563996252371e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.8283 - val_loss: 1.5191 - lr: 5.5268e-05\n",
      "Epoch 94/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8287\n",
      "Epoch 94: val_loss did not improve from 1.50248\n",
      "\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 5.416848503955407e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.8302 - val_loss: 1.5153 - lr: 5.4716e-05\n",
      "Epoch 95/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8463\n",
      "Epoch 95: val_loss did not improve from 1.50248\n",
      "\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 5.362680094549433e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.8449 - val_loss: 1.5071 - lr: 5.4168e-05\n",
      "Epoch 96/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8231\n",
      "Epoch 96: val_loss did not improve from 1.50248\n",
      "\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 5.309053365635918e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.8231 - val_loss: 1.5243 - lr: 5.3627e-05\n",
      "Epoch 97/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8316\n",
      "Epoch 97: val_loss improved from 1.50248 to 1.50110, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 3s 52ms/step - loss: 0.8313 - val_loss: 1.5011 - lr: 5.3091e-05\n",
      "Epoch 98/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8311\n",
      "Epoch 98: val_loss improved from 1.50110 to 1.49971, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.8296 - val_loss: 1.4997 - lr: 5.3091e-05\n",
      "Epoch 99/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8325\n",
      "Epoch 99: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 5.2559629148163366e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.8343 - val_loss: 1.5079 - lr: 5.3091e-05\n",
      "Epoch 100/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8260\n",
      "Epoch 100: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 5.2034033396921584e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.8258 - val_loss: 1.5146 - lr: 5.2560e-05\n",
      "Epoch 101/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8416\n",
      "Epoch 101: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 5.1513692378648555e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.8416 - val_loss: 1.5081 - lr: 5.2034e-05\n",
      "Epoch 102/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8322\n",
      "Epoch 102: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 5.099855567095801e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.8314 - val_loss: 1.5133 - lr: 5.1514e-05\n",
      "Epoch 103/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8292\n",
      "Epoch 103: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 5.048856924986467e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.8292 - val_loss: 1.5040 - lr: 5.0999e-05\n",
      "Epoch 104/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8281\n",
      "Epoch 104: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 4.998368269298226e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.8275 - val_loss: 1.5118 - lr: 5.0489e-05\n",
      "Epoch 105/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8312\n",
      "Epoch 105: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 4.9483845577924514e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.8325 - val_loss: 1.5082 - lr: 4.9984e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8219\n",
      "Epoch 106: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 4.898900748230517e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.8235 - val_loss: 1.5158 - lr: 4.9484e-05\n",
      "Epoch 107/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8216\n",
      "Epoch 107: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 4.849911798373796e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.8216 - val_loss: 1.5274 - lr: 4.8989e-05\n",
      "Epoch 108/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8290\n",
      "Epoch 108: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 4.801412665983662e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.8296 - val_loss: 1.5175 - lr: 4.8499e-05\n",
      "Epoch 109/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8303\n",
      "Epoch 109: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 4.75339866898139e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.8301 - val_loss: 1.5152 - lr: 4.8014e-05\n",
      "Epoch 110/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8277\n",
      "Epoch 110: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 4.705864765128354e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.8277 - val_loss: 1.5078 - lr: 4.7534e-05\n",
      "Epoch 111/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8296\n",
      "Epoch 111: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 4.658806272345828e-05.\n",
      "66/66 [==============================] - 3s 53ms/step - loss: 0.8288 - val_loss: 1.5204 - lr: 4.7059e-05\n",
      "Epoch 112/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8237\n",
      "Epoch 112: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 4.612218148395186e-05.\n",
      "66/66 [==============================] - 4s 58ms/step - loss: 0.8224 - val_loss: 1.5059 - lr: 4.6588e-05\n",
      "Epoch 113/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8233\n",
      "Epoch 113: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 4.566096071357606e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.8226 - val_loss: 1.5118 - lr: 4.6122e-05\n",
      "Epoch 114/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8150\n",
      "Epoch 114: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 4.52043499899446e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.8150 - val_loss: 1.5147 - lr: 4.5661e-05\n",
      "Epoch 115/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8259\n",
      "Epoch 115: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 4.475230609386927e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8270 - val_loss: 1.5261 - lr: 4.5204e-05\n",
      "Epoch 116/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8248\n",
      "Epoch 116: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 4.43047822045628e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.8237 - val_loss: 1.5271 - lr: 4.4752e-05\n",
      "Epoch 117/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8193\n",
      "Epoch 117: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 4.386173510283697e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.8193 - val_loss: 1.5238 - lr: 4.4305e-05\n",
      "Epoch 118/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8223\n",
      "Epoch 118: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 4.342311796790454e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.8238 - val_loss: 1.5205 - lr: 4.3862e-05\n",
      "Epoch 119/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8127\n",
      "Epoch 119: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 4.298888758057728e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.8096 - val_loss: 1.5221 - lr: 4.3423e-05\n",
      "Epoch 120/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8285\n",
      "Epoch 120: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 4.2558997120067946e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8288 - val_loss: 1.5292 - lr: 4.2989e-05\n",
      "Epoch 121/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8278\n",
      "Epoch 121: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 4.2133406968787315e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8269 - val_loss: 1.5316 - lr: 4.2559e-05\n",
      "Epoch 122/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8201\n",
      "Epoch 122: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 4.1712073907547166e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8194 - val_loss: 1.5316 - lr: 4.2133e-05\n",
      "Epoch 123/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8202\n",
      "Epoch 123: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 123: ReduceLROnPlateau reducing learning rate to 4.1294954717159274e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8184 - val_loss: 1.5263 - lr: 4.1712e-05\n",
      "Epoch 124/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8155\n",
      "Epoch 124: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 124: ReduceLROnPlateau reducing learning rate to 4.08820061784354e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8156 - val_loss: 1.5234 - lr: 4.1295e-05\n",
      "Epoch 125/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8309\n",
      "Epoch 125: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 4.0473185072187336e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8309 - val_loss: 1.5309 - lr: 4.0882e-05\n",
      "Epoch 126/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8126\n",
      "Epoch 126: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 4.0068451780825854e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8141 - val_loss: 1.5288 - lr: 4.0473e-05\n",
      "Epoch 127/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8059\n",
      "Epoch 127: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 127: ReduceLROnPlateau reducing learning rate to 3.966776668676175e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8044 - val_loss: 1.5267 - lr: 4.0068e-05\n",
      "Epoch 128/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8144\n",
      "Epoch 128: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 128: ReduceLROnPlateau reducing learning rate to 3.927109017240582e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8144 - val_loss: 1.5298 - lr: 3.9668e-05\n",
      "Epoch 129/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8117\n",
      "Epoch 129: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 129: ReduceLROnPlateau reducing learning rate to 3.887837901856983e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8117 - val_loss: 1.5282 - lr: 3.9271e-05\n",
      "Epoch 130/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8102\n",
      "Epoch 130: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 130: ReduceLROnPlateau reducing learning rate to 3.848959360766457e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8108 - val_loss: 1.5347 - lr: 3.8878e-05\n",
      "Epoch 131/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8065\n",
      "Epoch 131: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 131: ReduceLROnPlateau reducing learning rate to 3.810469792369986e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8054 - val_loss: 1.5218 - lr: 3.8490e-05\n",
      "Epoch 132/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8127\n",
      "Epoch 132: val_loss did not improve from 1.49971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 132: ReduceLROnPlateau reducing learning rate to 3.772365234908648e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8127 - val_loss: 1.5229 - lr: 3.8105e-05\n",
      "Epoch 133/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8106\n",
      "Epoch 133: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 133: ReduceLROnPlateau reducing learning rate to 3.734641726623522e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8123 - val_loss: 1.5162 - lr: 3.7724e-05\n",
      "Epoch 134/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8072\n",
      "Epoch 134: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 134: ReduceLROnPlateau reducing learning rate to 3.6972953057556876e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8132 - val_loss: 1.5333 - lr: 3.7346e-05\n",
      "Epoch 135/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8161\n",
      "Epoch 135: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 135: ReduceLROnPlateau reducing learning rate to 3.660322370706126e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8195 - val_loss: 1.5220 - lr: 3.6973e-05\n",
      "Epoch 136/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8094\n",
      "Epoch 136: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 3.623719319875818e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.8094 - val_loss: 1.5224 - lr: 3.6603e-05\n",
      "Epoch 137/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8107\n",
      "Epoch 137: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 137: ReduceLROnPlateau reducing learning rate to 3.5874821915058416e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8106 - val_loss: 1.5158 - lr: 3.6237e-05\n",
      "Epoch 138/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8151\n",
      "Epoch 138: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 138: ReduceLROnPlateau reducing learning rate to 3.551607383997179e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8151 - val_loss: 1.5329 - lr: 3.5875e-05\n",
      "Epoch 139/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8017\n",
      "Epoch 139: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 139: ReduceLROnPlateau reducing learning rate to 3.516091295750812e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8011 - val_loss: 1.5216 - lr: 3.5516e-05\n",
      "Epoch 140/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8092\n",
      "Epoch 140: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 140: ReduceLROnPlateau reducing learning rate to 3.480930325167719e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8092 - val_loss: 1.5195 - lr: 3.5161e-05\n",
      "Epoch 141/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8048\n",
      "Epoch 141: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 141: ReduceLROnPlateau reducing learning rate to 3.446120870648883e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8025 - val_loss: 1.5286 - lr: 3.4809e-05\n",
      "Epoch 142/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8129\n",
      "Epoch 142: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 142: ReduceLROnPlateau reducing learning rate to 3.4116596907551866e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8144 - val_loss: 1.5262 - lr: 3.4461e-05\n",
      "Epoch 143/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8173\n",
      "Epoch 143: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 143: ReduceLROnPlateau reducing learning rate to 3.37754318388761e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8148 - val_loss: 1.5208 - lr: 3.4117e-05\n",
      "Epoch 144/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8020\n",
      "Epoch 144: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 144: ReduceLROnPlateau reducing learning rate to 3.3437677484471354e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8034 - val_loss: 1.5465 - lr: 3.3775e-05\n",
      "Epoch 145/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8201\n",
      "Epoch 145: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 145: ReduceLROnPlateau reducing learning rate to 3.310330142994644e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8191 - val_loss: 1.5342 - lr: 3.3438e-05\n",
      "Epoch 146/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8043\n",
      "Epoch 146: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 146: ReduceLROnPlateau reducing learning rate to 3.277226765931118e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.8043 - val_loss: 1.5335 - lr: 3.3103e-05\n",
      "Epoch 147/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8097\n",
      "Epoch 147: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 147: ReduceLROnPlateau reducing learning rate to 3.24445437581744e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.8121 - val_loss: 1.5251 - lr: 3.2772e-05\n",
      "Epoch 148/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8155\n",
      "Epoch 148: val_loss did not improve from 1.49971\n",
      "\n",
      "Epoch 148: ReduceLROnPlateau reducing learning rate to 3.212009731214493e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.8155 - val_loss: 1.5481 - lr: 3.2445e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_11 (InputLayer)          [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_30 (LSTM)                 (None, 45, 24)       3744        ['input_11[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 45, 24)       0           ['lstm_30[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_31 (LSTM)                 (None, 45, 16)       2624        ['dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 45, 16)       0           ['lstm_31[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_32 (LSTM)                 (None, 32)           6272        ['dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 40)           1320        ['lstm_32[0][0]']                \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 5)            205         ['dense_20[0][0]']               \n",
      "                                                                                                  \n",
      " tf.unstack_10 (TFOpLambda)     [(None,),            0           ['dense_21[0][0]']               \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_50 (TFOpLambda)  (None, 1)           0           ['tf.unstack_10[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_20 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_50[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_54 (TFOpLambda)  (None, 1)           0           ['tf.unstack_10[0][4]']          \n",
      "                                                                                                  \n",
      " tf.math.multiply_30 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_20[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_21 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_54[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_31 (TFOpLambd  (None, 1)           0           ['tf.math.multiply_30[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_51 (TFOpLambda)  (None, 1)           0           ['tf.unstack_10[0][1]']          \n",
      "                                                                                                  \n",
      " tf.expand_dims_53 (TFOpLambda)  (None, 1)           0           ['tf.unstack_10[0][3]']          \n",
      "                                                                                                  \n",
      " tf.math.multiply_32 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_21[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_20 (TFOpL  (None, 1)           0           ['tf.math.multiply_31[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.softplus_20 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_51[0][0]']      \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_52 (TFOpLambda)  (None, 1)           0           ['tf.unstack_10[0][2]']          \n",
      "                                                                                                  \n",
      " tf.math.softplus_21 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_53[0][0]']      \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_21 (TFOpL  (None, 1)           0           ['tf.math.multiply_32[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack_10 (TFOpLambda)       (None, 5, 1)         0           ['tf.__operators__.add_20[0][0]',\n",
      "                                                                  'tf.math.softplus_20[0][0]',    \n",
      "                                                                  'tf.expand_dims_52[0][0]',      \n",
      "                                                                  'tf.math.softplus_21[0][0]',    \n",
      "                                                                  'tf.__operators__.add_21[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _1_CCMP_t+1_0.15\n",
      "Epoch 1/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 3.4012\n",
      "Epoch 1: val_loss improved from inf to 3.99037, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 10s 77ms/step - loss: 3.4105 - val_loss: 3.9904 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 2.5744\n",
      "Epoch 2: val_loss improved from 3.99037 to 3.41504, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 2.5744 - val_loss: 3.4150 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.7904\n",
      "Epoch 3: val_loss improved from 3.41504 to 3.12599, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 1.7904 - val_loss: 3.1260 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/66 [============================>.] - ETA: 0s - loss: 1.5650\n",
      "Epoch 4: val_loss improved from 3.12599 to 3.08796, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 1.5646 - val_loss: 3.0880 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.4506\n",
      "Epoch 5: val_loss improved from 3.08796 to 3.07832, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 1.4506 - val_loss: 3.0783 - lr: 1.0000e-04\n",
      "Epoch 6/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.3593\n",
      "Epoch 6: val_loss improved from 3.07832 to 3.02663, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 3s 52ms/step - loss: 1.3574 - val_loss: 3.0266 - lr: 1.0000e-04\n",
      "Epoch 7/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.3027\n",
      "Epoch 7: val_loss did not improve from 3.02663\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 1.3037 - val_loss: 3.3586 - lr: 1.0000e-04\n",
      "Epoch 8/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.2495\n",
      "Epoch 8: val_loss did not improve from 3.02663\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 3s 53ms/step - loss: 1.2486 - val_loss: 3.5608 - lr: 9.9000e-05\n",
      "Epoch 9/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.2071\n",
      "Epoch 9: val_loss did not improve from 3.02663\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 3s 52ms/step - loss: 1.2064 - val_loss: 3.6442 - lr: 9.8010e-05\n",
      "Epoch 10/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1840\n",
      "Epoch 10: val_loss did not improve from 3.02663\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 1.1857 - val_loss: 3.9030 - lr: 9.7030e-05\n",
      "Epoch 11/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1628\n",
      "Epoch 11: val_loss did not improve from 3.02663\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 1.1628 - val_loss: 3.6673 - lr: 9.6060e-05\n",
      "Epoch 12/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1421\n",
      "Epoch 12: val_loss did not improve from 3.02663\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 1.1427 - val_loss: 4.0385 - lr: 9.5099e-05\n",
      "Epoch 13/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1283\n",
      "Epoch 13: val_loss did not improve from 3.02663\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 1.1283 - val_loss: 4.4370 - lr: 9.4148e-05\n",
      "Epoch 14/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1221\n",
      "Epoch 14: val_loss did not improve from 3.02663\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 1.1223 - val_loss: 3.9059 - lr: 9.3207e-05\n",
      "Epoch 15/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1039\n",
      "Epoch 15: val_loss did not improve from 3.02663\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 1.1027 - val_loss: 3.7217 - lr: 9.2274e-05\n",
      "Epoch 16/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0932\n",
      "Epoch 16: val_loss did not improve from 3.02663\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 1.0932 - val_loss: 3.6670 - lr: 9.1352e-05\n",
      "Epoch 17/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0743\n",
      "Epoch 17: val_loss did not improve from 3.02663\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 1.0739 - val_loss: 3.8652 - lr: 9.0438e-05\n",
      "Epoch 18/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0769\n",
      "Epoch 18: val_loss did not improve from 3.02663\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 1.0761 - val_loss: 3.5879 - lr: 8.9534e-05\n",
      "Epoch 19/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0692\n",
      "Epoch 19: val_loss did not improve from 3.02663\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 1.0683 - val_loss: 3.5602 - lr: 8.8638e-05\n",
      "Epoch 20/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0502\n",
      "Epoch 20: val_loss did not improve from 3.02663\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 1.0502 - val_loss: 3.5920 - lr: 8.7752e-05\n",
      "Epoch 21/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0369\n",
      "Epoch 21: val_loss did not improve from 3.02663\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 1.0361 - val_loss: 3.4066 - lr: 8.6875e-05\n",
      "Epoch 22/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0223\n",
      "Epoch 22: val_loss did not improve from 3.02663\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 1.0266 - val_loss: 3.2396 - lr: 8.6006e-05\n",
      "Epoch 23/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0119\n",
      "Epoch 23: val_loss did not improve from 3.02663\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 1.0119 - val_loss: 3.3184 - lr: 8.5146e-05\n",
      "Epoch 24/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0031\n",
      "Epoch 24: val_loss did not improve from 3.02663\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 1.0051 - val_loss: 3.2835 - lr: 8.4294e-05\n",
      "Epoch 25/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0050\n",
      "Epoch 25: val_loss did not improve from 3.02663\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 1.0054 - val_loss: 3.2264 - lr: 8.3451e-05\n",
      "Epoch 26/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9969\n",
      "Epoch 26: val_loss did not improve from 3.02663\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 3s 53ms/step - loss: 0.9963 - val_loss: 3.0829 - lr: 8.2617e-05\n",
      "Epoch 27/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0010\n",
      "Epoch 27: val_loss improved from 3.02663 to 2.81490, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 1.0019 - val_loss: 2.8149 - lr: 8.1791e-05\n",
      "Epoch 28/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9934\n",
      "Epoch 28: val_loss did not improve from 2.81490\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.9914 - val_loss: 2.9011 - lr: 8.1791e-05\n",
      "Epoch 29/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9688\n",
      "Epoch 29: val_loss did not improve from 2.81490\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.9724 - val_loss: 3.0148 - lr: 8.0973e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9551\n",
      "Epoch 30: val_loss did not improve from 2.81490\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.9559 - val_loss: 2.8896 - lr: 8.0163e-05\n",
      "Epoch 31/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9624\n",
      "Epoch 31: val_loss improved from 2.81490 to 2.74053, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.9617 - val_loss: 2.7405 - lr: 7.9361e-05\n",
      "Epoch 32/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9541\n",
      "Epoch 32: val_loss improved from 2.74053 to 2.69164, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.9534 - val_loss: 2.6916 - lr: 7.9361e-05\n",
      "Epoch 33/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9712\n",
      "Epoch 33: val_loss did not improve from 2.69164\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.9715 - val_loss: 2.7727 - lr: 7.9361e-05\n",
      "Epoch 34/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9511\n",
      "Epoch 34: val_loss did not improve from 2.69164\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.9504 - val_loss: 2.8817 - lr: 7.8568e-05\n",
      "Epoch 35/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9241\n",
      "Epoch 35: val_loss did not improve from 2.69164\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.9244 - val_loss: 2.7244 - lr: 7.7782e-05\n",
      "Epoch 36/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9571\n",
      "Epoch 36: val_loss did not improve from 2.69164\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.9561 - val_loss: 2.7581 - lr: 7.7004e-05\n",
      "Epoch 37/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9487\n",
      "Epoch 37: val_loss improved from 2.69164 to 2.51378, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.9487 - val_loss: 2.5138 - lr: 7.6234e-05\n",
      "Epoch 38/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9330\n",
      "Epoch 38: val_loss did not improve from 2.51378\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.9330 - val_loss: 2.8291 - lr: 7.6234e-05\n",
      "Epoch 39/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9272\n",
      "Epoch 39: val_loss did not improve from 2.51378\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.9272 - val_loss: 2.6477 - lr: 7.5472e-05\n",
      "Epoch 40/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9408\n",
      "Epoch 40: val_loss did not improve from 2.51378\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.9408 - val_loss: 2.5832 - lr: 7.4717e-05\n",
      "Epoch 41/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9211\n",
      "Epoch 41: val_loss improved from 2.51378 to 2.49738, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.9207 - val_loss: 2.4974 - lr: 7.3970e-05\n",
      "Epoch 42/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9223\n",
      "Epoch 42: val_loss did not improve from 2.49738\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.9210 - val_loss: 2.6036 - lr: 7.3970e-05\n",
      "Epoch 43/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9174\n",
      "Epoch 43: val_loss improved from 2.49738 to 2.40421, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.9174 - val_loss: 2.4042 - lr: 7.3230e-05\n",
      "Epoch 44/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9259\n",
      "Epoch 44: val_loss did not improve from 2.40421\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.9267 - val_loss: 2.5560 - lr: 7.3230e-05\n",
      "Epoch 45/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9082\n",
      "Epoch 45: val_loss did not improve from 2.40421\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.9082 - val_loss: 2.4120 - lr: 7.2498e-05\n",
      "Epoch 46/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9111\n",
      "Epoch 46: val_loss did not improve from 2.40421\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.9111 - val_loss: 2.5430 - lr: 7.1773e-05\n",
      "Epoch 47/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9088\n",
      "Epoch 47: val_loss improved from 2.40421 to 2.26339, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.9086 - val_loss: 2.2634 - lr: 7.1055e-05\n",
      "Epoch 48/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9310\n",
      "Epoch 48: val_loss did not improve from 2.26339\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.9295 - val_loss: 2.3138 - lr: 7.1055e-05\n",
      "Epoch 49/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9279\n",
      "Epoch 49: val_loss did not improve from 2.26339\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.9300 - val_loss: 2.3611 - lr: 7.0345e-05\n",
      "Epoch 50/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9086\n",
      "Epoch 50: val_loss did not improve from 2.26339\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.9088 - val_loss: 2.3977 - lr: 6.9641e-05\n",
      "Epoch 51/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9074\n",
      "Epoch 51: val_loss did not improve from 2.26339\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.9079 - val_loss: 2.4207 - lr: 6.8945e-05\n",
      "Epoch 52/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9011\n",
      "Epoch 52: val_loss did not improve from 2.26339\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.9075 - val_loss: 2.3133 - lr: 6.8255e-05\n",
      "Epoch 53/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9040\n",
      "Epoch 53: val_loss did not improve from 2.26339\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.9030 - val_loss: 2.2951 - lr: 6.7573e-05\n",
      "Epoch 54/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9011\n",
      "Epoch 54: val_loss improved from 2.26339 to 2.22941, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.9002 - val_loss: 2.2294 - lr: 6.6897e-05\n",
      "Epoch 55/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9043\n",
      "Epoch 55: val_loss did not improve from 2.22941\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 3s 42ms/step - loss: 0.9043 - val_loss: 2.3351 - lr: 6.6897e-05\n",
      "Epoch 56/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9054\n",
      "Epoch 56: val_loss improved from 2.22941 to 2.17616, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.9026 - val_loss: 2.1762 - lr: 6.6228e-05\n",
      "Epoch 57/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8916\n",
      "Epoch 57: val_loss improved from 2.17616 to 2.17609, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.15.hdf5\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8925 - val_loss: 2.1761 - lr: 6.6228e-05\n",
      "Epoch 58/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8915\n",
      "Epoch 58: val_loss improved from 2.17609 to 2.17396, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8915 - val_loss: 2.1740 - lr: 6.5566e-05\n",
      "Epoch 59/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8893\n",
      "Epoch 59: val_loss did not improve from 2.17396\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.8893 - val_loss: 2.3004 - lr: 6.5566e-05\n",
      "Epoch 60/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8926\n",
      "Epoch 60: val_loss did not improve from 2.17396\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8924 - val_loss: 2.2949 - lr: 6.4910e-05\n",
      "Epoch 61/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8882\n",
      "Epoch 61: val_loss did not improve from 2.17396\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8884 - val_loss: 2.2786 - lr: 6.4261e-05\n",
      "Epoch 62/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8872\n",
      "Epoch 62: val_loss did not improve from 2.17396\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8852 - val_loss: 2.2623 - lr: 6.3619e-05\n",
      "Epoch 63/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8913\n",
      "Epoch 63: val_loss did not improve from 2.17396\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8913 - val_loss: 2.2438 - lr: 6.2982e-05\n",
      "Epoch 64/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8825\n",
      "Epoch 64: val_loss improved from 2.17396 to 2.11333, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8831 - val_loss: 2.1133 - lr: 6.2353e-05\n",
      "Epoch 65/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8832\n",
      "Epoch 65: val_loss did not improve from 2.11333\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8878 - val_loss: 2.1983 - lr: 6.2353e-05\n",
      "Epoch 66/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8805\n",
      "Epoch 66: val_loss improved from 2.11333 to 2.10297, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8779 - val_loss: 2.1030 - lr: 6.1729e-05\n",
      "Epoch 67/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8851\n",
      "Epoch 67: val_loss did not improve from 2.10297\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8878 - val_loss: 2.2193 - lr: 6.1729e-05\n",
      "Epoch 68/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8836\n",
      "Epoch 68: val_loss did not improve from 2.10297\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8836 - val_loss: 2.1704 - lr: 6.1112e-05\n",
      "Epoch 69/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8727\n",
      "Epoch 69: val_loss did not improve from 2.10297\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8727 - val_loss: 2.1677 - lr: 6.0501e-05\n",
      "Epoch 70/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8851\n",
      "Epoch 70: val_loss improved from 2.10297 to 2.09515, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8857 - val_loss: 2.0951 - lr: 5.9896e-05\n",
      "Epoch 71/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8814\n",
      "Epoch 71: val_loss did not improve from 2.09515\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8826 - val_loss: 2.1746 - lr: 5.9896e-05\n",
      "Epoch 72/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8891\n",
      "Epoch 72: val_loss did not improve from 2.09515\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 5.8703650938696225e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8872 - val_loss: 2.1709 - lr: 5.9297e-05\n",
      "Epoch 73/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8791\n",
      "Epoch 73: val_loss did not improve from 2.09515\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 5.811661550978897e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8791 - val_loss: 2.2016 - lr: 5.8704e-05\n",
      "Epoch 74/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8709\n",
      "Epoch 74: val_loss did not improve from 2.09515\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 5.753545068728272e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.8709 - val_loss: 2.1247 - lr: 5.8117e-05\n",
      "Epoch 75/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8781\n",
      "Epoch 75: val_loss did not improve from 2.09515\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 5.6960095243994146e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.8772 - val_loss: 2.1282 - lr: 5.7535e-05\n",
      "Epoch 76/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8871\n",
      "Epoch 76: val_loss improved from 2.09515 to 2.08998, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.8871 - val_loss: 2.0900 - lr: 5.6960e-05\n",
      "Epoch 77/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8848\n",
      "Epoch 77: val_loss did not improve from 2.08998\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 5.639049515593797e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8848 - val_loss: 2.1620 - lr: 5.6960e-05\n",
      "Epoch 78/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8717\n",
      "Epoch 78: val_loss did not improve from 2.08998\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 5.5826589195930865e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8709 - val_loss: 2.1664 - lr: 5.6390e-05\n",
      "Epoch 79/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8692\n",
      "Epoch 79: val_loss improved from 2.08998 to 2.07779, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8668 - val_loss: 2.0778 - lr: 5.5827e-05\n",
      "Epoch 80/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8724\n",
      "Epoch 80: val_loss improved from 2.07779 to 2.01220, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.15.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8773 - val_loss: 2.0122 - lr: 5.5827e-05\n",
      "Epoch 81/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8732\n",
      "Epoch 81: val_loss did not improve from 2.01220\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 5.526832333998754e-05.\n",
      "66/66 [==============================] - 4s 59ms/step - loss: 0.8732 - val_loss: 2.0476 - lr: 5.5827e-05\n",
      "Epoch 82/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8713\n",
      "Epoch 82: val_loss did not improve from 2.01220\n",
      "\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 5.471563996252371e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8713 - val_loss: 2.0550 - lr: 5.5268e-05\n",
      "Epoch 83/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8734\n",
      "Epoch 83: val_loss did not improve from 2.01220\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 5.416848503955407e-05.\n",
      "66/66 [==============================] - 5s 82ms/step - loss: 0.8734 - val_loss: 2.0946 - lr: 5.4716e-05\n",
      "Epoch 84/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8832\n",
      "Epoch 84: val_loss did not improve from 2.01220\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 5.362680094549433e-05.\n",
      "66/66 [==============================] - 4s 67ms/step - loss: 0.8830 - val_loss: 2.0366 - lr: 5.4168e-05\n",
      "Epoch 85/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8696\n",
      "Epoch 85: val_loss did not improve from 2.01220\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 5.309053365635918e-05.\n",
      "66/66 [==============================] - 4s 61ms/step - loss: 0.8689 - val_loss: 2.1016 - lr: 5.3627e-05\n",
      "Epoch 86/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8629\n",
      "Epoch 86: val_loss did not improve from 2.01220\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 5.2559629148163366e-05.\n",
      "66/66 [==============================] - 4s 65ms/step - loss: 0.8613 - val_loss: 2.0275 - lr: 5.3091e-05\n",
      "Epoch 87/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8713\n",
      "Epoch 87: val_loss improved from 2.01220 to 1.98795, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 4s 60ms/step - loss: 0.8699 - val_loss: 1.9879 - lr: 5.2560e-05\n",
      "Epoch 88/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8815\n",
      "Epoch 88: val_loss improved from 1.98795 to 1.97867, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 4s 59ms/step - loss: 0.8794 - val_loss: 1.9787 - lr: 5.2560e-05\n",
      "Epoch 89/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8557\n",
      "Epoch 89: val_loss did not improve from 1.97867\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 5.2034033396921584e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.8557 - val_loss: 2.0641 - lr: 5.2560e-05\n",
      "Epoch 90/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8588\n",
      "Epoch 90: val_loss did not improve from 1.97867\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 5.1513692378648555e-05.\n",
      "66/66 [==============================] - 3s 38ms/step - loss: 0.8595 - val_loss: 2.1357 - lr: 5.2034e-05\n",
      "Epoch 91/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8716\n",
      "Epoch 91: val_loss did not improve from 1.97867\n",
      "\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 5.099855567095801e-05.\n",
      "66/66 [==============================] - 3s 38ms/step - loss: 0.8688 - val_loss: 2.0646 - lr: 5.1514e-05\n",
      "Epoch 92/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8805\n",
      "Epoch 92: val_loss improved from 1.97867 to 1.95195, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.8805 - val_loss: 1.9520 - lr: 5.0999e-05\n",
      "Epoch 93/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8698\n",
      "Epoch 93: val_loss did not improve from 1.95195\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 5.048856924986467e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.8721 - val_loss: 2.0322 - lr: 5.0999e-05\n",
      "Epoch 94/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8769\n",
      "Epoch 94: val_loss did not improve from 1.95195\n",
      "\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 4.998368269298226e-05.\n",
      "66/66 [==============================] - 4s 57ms/step - loss: 0.8769 - val_loss: 2.0541 - lr: 5.0489e-05\n",
      "Epoch 95/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8677\n",
      "Epoch 95: val_loss did not improve from 1.95195\n",
      "\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 4.9483845577924514e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.8677 - val_loss: 2.0692 - lr: 4.9984e-05\n",
      "Epoch 96/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8618\n",
      "Epoch 96: val_loss did not improve from 1.95195\n",
      "\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 4.898900748230517e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.8605 - val_loss: 2.0488 - lr: 4.9484e-05\n",
      "Epoch 97/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8660\n",
      "Epoch 97: val_loss improved from 1.95195 to 1.94599, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.8660 - val_loss: 1.9460 - lr: 4.8989e-05\n",
      "Epoch 98/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8591\n",
      "Epoch 98: val_loss did not improve from 1.94599\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 4.849911798373796e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8615 - val_loss: 2.0933 - lr: 4.8989e-05\n",
      "Epoch 99/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8563\n",
      "Epoch 99: val_loss did not improve from 1.94599\n",
      "\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 4.801412665983662e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.8564 - val_loss: 1.9825 - lr: 4.8499e-05\n",
      "Epoch 100/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8646\n",
      "Epoch 100: val_loss did not improve from 1.94599\n",
      "\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 4.75339866898139e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8646 - val_loss: 2.0268 - lr: 4.8014e-05\n",
      "Epoch 101/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8599\n",
      "Epoch 101: val_loss did not improve from 1.94599\n",
      "\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 4.705864765128354e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8625 - val_loss: 1.9750 - lr: 4.7534e-05\n",
      "Epoch 102/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8643\n",
      "Epoch 102: val_loss did not improve from 1.94599\n",
      "\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 4.658806272345828e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.8642 - val_loss: 2.0113 - lr: 4.7059e-05\n",
      "Epoch 103/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8656\n",
      "Epoch 103: val_loss did not improve from 1.94599\n",
      "\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 4.612218148395186e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.8656 - val_loss: 2.0502 - lr: 4.6588e-05\n",
      "Epoch 104/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8628\n",
      "Epoch 104: val_loss did not improve from 1.94599\n",
      "\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 4.566096071357606e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.8633 - val_loss: 1.9933 - lr: 4.6122e-05\n",
      "Epoch 105/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8653\n",
      "Epoch 105: val_loss did not improve from 1.94599\n",
      "\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 4.52043499899446e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.8653 - val_loss: 2.0598 - lr: 4.5661e-05\n",
      "Epoch 106/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8637\n",
      "Epoch 106: val_loss did not improve from 1.94599\n",
      "\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 4.475230609386927e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 3s 49ms/step - loss: 0.8651 - val_loss: 1.9882 - lr: 4.5204e-05\n",
      "Epoch 107/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8597\n",
      "Epoch 107: val_loss did not improve from 1.94599\n",
      "\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 4.43047822045628e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.8613 - val_loss: 2.0721 - lr: 4.4752e-05\n",
      "Epoch 108/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8643\n",
      "Epoch 108: val_loss did not improve from 1.94599\n",
      "\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 4.386173510283697e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.8636 - val_loss: 2.0378 - lr: 4.4305e-05\n",
      "Epoch 109/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8552\n",
      "Epoch 109: val_loss did not improve from 1.94599\n",
      "\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 4.342311796790454e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.8546 - val_loss: 1.9880 - lr: 4.3862e-05\n",
      "Epoch 110/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8519\n",
      "Epoch 110: val_loss did not improve from 1.94599\n",
      "\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 4.298888758057728e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.8519 - val_loss: 2.0601 - lr: 4.3423e-05\n",
      "Epoch 111/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8658\n",
      "Epoch 111: val_loss did not improve from 1.94599\n",
      "\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 4.2558997120067946e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.8658 - val_loss: 1.9556 - lr: 4.2989e-05\n",
      "Epoch 112/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8466\n",
      "Epoch 112: val_loss did not improve from 1.94599\n",
      "\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 4.2133406968787315e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.8466 - val_loss: 2.0021 - lr: 4.2559e-05\n",
      "Epoch 113/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8556\n",
      "Epoch 113: val_loss did not improve from 1.94599\n",
      "\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 4.1712073907547166e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8556 - val_loss: 2.0522 - lr: 4.2133e-05\n",
      "Epoch 114/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8582\n",
      "Epoch 114: val_loss did not improve from 1.94599\n",
      "\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 4.1294954717159274e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8596 - val_loss: 1.9600 - lr: 4.1712e-05\n",
      "Epoch 115/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8563\n",
      "Epoch 115: val_loss did not improve from 1.94599\n",
      "\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 4.08820061784354e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.8563 - val_loss: 1.9528 - lr: 4.1295e-05\n",
      "Epoch 116/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8567\n",
      "Epoch 116: val_loss did not improve from 1.94599\n",
      "\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 4.0473185072187336e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8567 - val_loss: 1.9549 - lr: 4.0882e-05\n",
      "Epoch 117/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8573\n",
      "Epoch 117: val_loss did not improve from 1.94599\n",
      "\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 4.0068451780825854e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8570 - val_loss: 2.0329 - lr: 4.0473e-05\n",
      "Epoch 118/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8510\n",
      "Epoch 118: val_loss improved from 1.94599 to 1.92630, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.8538 - val_loss: 1.9263 - lr: 4.0068e-05\n",
      "Epoch 119/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8655\n",
      "Epoch 119: val_loss did not improve from 1.92630\n",
      "\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 3.966776668676175e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.8655 - val_loss: 1.9314 - lr: 4.0068e-05\n",
      "Epoch 120/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8564\n",
      "Epoch 120: val_loss did not improve from 1.92630\n",
      "\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 3.927109017240582e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.8600 - val_loss: 2.0172 - lr: 3.9668e-05\n",
      "Epoch 121/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8482\n",
      "Epoch 121: val_loss did not improve from 1.92630\n",
      "\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 3.887837901856983e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.8494 - val_loss: 2.0184 - lr: 3.9271e-05\n",
      "Epoch 122/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8509\n",
      "Epoch 122: val_loss did not improve from 1.92630\n",
      "\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 3.848959360766457e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.8509 - val_loss: 2.1093 - lr: 3.8878e-05\n",
      "Epoch 123/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8539\n",
      "Epoch 123: val_loss did not improve from 1.92630\n",
      "\n",
      "Epoch 123: ReduceLROnPlateau reducing learning rate to 3.810469792369986e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8542 - val_loss: 2.0395 - lr: 3.8490e-05\n",
      "Epoch 124/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8507\n",
      "Epoch 124: val_loss did not improve from 1.92630\n",
      "\n",
      "Epoch 124: ReduceLROnPlateau reducing learning rate to 3.772365234908648e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.8507 - val_loss: 2.0622 - lr: 3.8105e-05\n",
      "Epoch 125/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8525\n",
      "Epoch 125: val_loss did not improve from 1.92630\n",
      "\n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 3.734641726623522e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.8541 - val_loss: 1.9414 - lr: 3.7724e-05\n",
      "Epoch 126/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8457\n",
      "Epoch 126: val_loss did not improve from 1.92630\n",
      "\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 3.6972953057556876e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8457 - val_loss: 1.9970 - lr: 3.7346e-05\n",
      "Epoch 127/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8422\n",
      "Epoch 127: val_loss did not improve from 1.92630\n",
      "\n",
      "Epoch 127: ReduceLROnPlateau reducing learning rate to 3.660322370706126e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8432 - val_loss: 2.0207 - lr: 3.6973e-05\n",
      "Epoch 128/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8442\n",
      "Epoch 128: val_loss did not improve from 1.92630\n",
      "\n",
      "Epoch 128: ReduceLROnPlateau reducing learning rate to 3.623719319875818e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8434 - val_loss: 1.9890 - lr: 3.6603e-05\n",
      "Epoch 129/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8525\n",
      "Epoch 129: val_loss did not improve from 1.92630\n",
      "\n",
      "Epoch 129: ReduceLROnPlateau reducing learning rate to 3.5874821915058416e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8521 - val_loss: 2.0197 - lr: 3.6237e-05\n",
      "Epoch 130/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8464\n",
      "Epoch 130: val_loss improved from 1.92630 to 1.91982, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8486 - val_loss: 1.9198 - lr: 3.5875e-05\n",
      "Epoch 131/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8666\n",
      "Epoch 131: val_loss did not improve from 1.91982\n",
      "\n",
      "Epoch 131: ReduceLROnPlateau reducing learning rate to 3.551607383997179e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8638 - val_loss: 1.9879 - lr: 3.5875e-05\n",
      "Epoch 132/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8421\n",
      "Epoch 132: val_loss did not improve from 1.91982\n",
      "\n",
      "Epoch 132: ReduceLROnPlateau reducing learning rate to 3.516091295750812e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8392 - val_loss: 1.9659 - lr: 3.5516e-05\n",
      "Epoch 133/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8542\n",
      "Epoch 133: val_loss did not improve from 1.91982\n",
      "\n",
      "Epoch 133: ReduceLROnPlateau reducing learning rate to 3.480930325167719e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.8530 - val_loss: 1.9657 - lr: 3.5161e-05\n",
      "Epoch 134/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8537\n",
      "Epoch 134: val_loss did not improve from 1.91982\n",
      "\n",
      "Epoch 134: ReduceLROnPlateau reducing learning rate to 3.446120870648883e-05.\n",
      "66/66 [==============================] - 4s 58ms/step - loss: 0.8528 - val_loss: 2.0135 - lr: 3.4809e-05\n",
      "Epoch 135/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8508\n",
      "Epoch 135: val_loss improved from 1.91982 to 1.90953, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 4s 56ms/step - loss: 0.8516 - val_loss: 1.9095 - lr: 3.4461e-05\n",
      "Epoch 136/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8435\n",
      "Epoch 136: val_loss did not improve from 1.90953\n",
      "\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 3.4116596907551866e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.8435 - val_loss: 2.0484 - lr: 3.4461e-05\n",
      "Epoch 137/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8466\n",
      "Epoch 137: val_loss did not improve from 1.90953\n",
      "\n",
      "Epoch 137: ReduceLROnPlateau reducing learning rate to 3.37754318388761e-05.\n",
      "66/66 [==============================] - 4s 58ms/step - loss: 0.8447 - val_loss: 1.9587 - lr: 3.4117e-05\n",
      "Epoch 138/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8406\n",
      "Epoch 138: val_loss did not improve from 1.90953\n",
      "\n",
      "Epoch 138: ReduceLROnPlateau reducing learning rate to 3.3437677484471354e-05.\n",
      "66/66 [==============================] - 3s 52ms/step - loss: 0.8397 - val_loss: 2.0276 - lr: 3.3775e-05\n",
      "Epoch 139/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8448\n",
      "Epoch 139: val_loss did not improve from 1.90953\n",
      "\n",
      "Epoch 139: ReduceLROnPlateau reducing learning rate to 3.310330142994644e-05.\n",
      "66/66 [==============================] - 4s 60ms/step - loss: 0.8448 - val_loss: 2.0154 - lr: 3.3438e-05\n",
      "Epoch 140/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8523\n",
      "Epoch 140: val_loss did not improve from 1.90953\n",
      "\n",
      "Epoch 140: ReduceLROnPlateau reducing learning rate to 3.277226765931118e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.8523 - val_loss: 1.9706 - lr: 3.3103e-05\n",
      "Epoch 141/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8574\n",
      "Epoch 141: val_loss did not improve from 1.90953\n",
      "\n",
      "Epoch 141: ReduceLROnPlateau reducing learning rate to 3.24445437581744e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.8574 - val_loss: 2.0105 - lr: 3.2772e-05\n",
      "Epoch 142/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8381\n",
      "Epoch 142: val_loss did not improve from 1.90953\n",
      "\n",
      "Epoch 142: ReduceLROnPlateau reducing learning rate to 3.212009731214493e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.8381 - val_loss: 2.0526 - lr: 3.2445e-05\n",
      "Epoch 143/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8368\n",
      "Epoch 143: val_loss did not improve from 1.90953\n",
      "\n",
      "Epoch 143: ReduceLROnPlateau reducing learning rate to 3.17988959068316e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.8360 - val_loss: 2.0117 - lr: 3.2120e-05\n",
      "Epoch 144/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8481\n",
      "Epoch 144: val_loss did not improve from 1.90953\n",
      "\n",
      "Epoch 144: ReduceLROnPlateau reducing learning rate to 3.1480907127843236e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.8481 - val_loss: 2.0231 - lr: 3.1799e-05\n",
      "Epoch 145/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8427\n",
      "Epoch 145: val_loss did not improve from 1.90953\n",
      "\n",
      "Epoch 145: ReduceLROnPlateau reducing learning rate to 3.116609856078867e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.8427 - val_loss: 1.9611 - lr: 3.1481e-05\n",
      "Epoch 146/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8383\n",
      "Epoch 146: val_loss did not improve from 1.90953\n",
      "\n",
      "Epoch 146: ReduceLROnPlateau reducing learning rate to 3.085443779127672e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.8376 - val_loss: 2.0581 - lr: 3.1166e-05\n",
      "Epoch 147/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8418\n",
      "Epoch 147: val_loss did not improve from 1.90953\n",
      "\n",
      "Epoch 147: ReduceLROnPlateau reducing learning rate to 3.054589240491623e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.8418 - val_loss: 2.0512 - lr: 3.0854e-05\n",
      "Epoch 148/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8461\n",
      "Epoch 148: val_loss did not improve from 1.90953\n",
      "\n",
      "Epoch 148: ReduceLROnPlateau reducing learning rate to 3.024043358891504e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.8461 - val_loss: 1.9935 - lr: 3.0546e-05\n",
      "Epoch 149/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8478\n",
      "Epoch 149: val_loss did not improve from 1.90953\n",
      "\n",
      "Epoch 149: ReduceLROnPlateau reducing learning rate to 2.9938028928881975e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.8478 - val_loss: 1.9746 - lr: 3.0240e-05\n",
      "Epoch 150/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8484\n",
      "Epoch 150: val_loss did not improve from 1.90953\n",
      "\n",
      "Epoch 150: ReduceLROnPlateau reducing learning rate to 2.963864781122538e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.8484 - val_loss: 1.9771 - lr: 2.9938e-05\n",
      "Epoch 151/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8342\n",
      "Epoch 151: val_loss did not improve from 1.90953\n",
      "\n",
      "Epoch 151: ReduceLROnPlateau reducing learning rate to 2.9342261423153105e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.8356 - val_loss: 2.0095 - lr: 2.9639e-05\n",
      "Epoch 152/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8360\n",
      "Epoch 152: val_loss did not improve from 1.90953\n",
      "\n",
      "Epoch 152: ReduceLROnPlateau reducing learning rate to 2.9048839151073478e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.8360 - val_loss: 2.0128 - lr: 2.9342e-05\n",
      "Epoch 153/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8402\n",
      "Epoch 153: val_loss did not improve from 1.90953\n",
      "\n",
      "Epoch 153: ReduceLROnPlateau reducing learning rate to 2.875835038139485e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.8381 - val_loss: 1.9721 - lr: 2.9049e-05\n",
      "Epoch 154/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8408\n",
      "Epoch 154: val_loss did not improve from 1.90953\n",
      "\n",
      "Epoch 154: ReduceLROnPlateau reducing learning rate to 2.8470766301325058e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.8412 - val_loss: 1.9524 - lr: 2.8758e-05\n",
      "Epoch 155/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8453\n",
      "Epoch 155: val_loss did not improve from 1.90953\n",
      "\n",
      "Epoch 155: ReduceLROnPlateau reducing learning rate to 2.8186058098071954e-05.\n",
      "66/66 [==============================] - 3s 52ms/step - loss: 0.8437 - val_loss: 1.9345 - lr: 2.8471e-05\n",
      "Epoch 156/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8457\n",
      "Epoch 156: val_loss improved from 1.90953 to 1.90791, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 4s 55ms/step - loss: 0.8445 - val_loss: 1.9079 - lr: 2.8186e-05\n",
      "Epoch 157/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8518\n",
      "Epoch 157: val_loss did not improve from 1.90791\n",
      "\n",
      "Epoch 157: ReduceLROnPlateau reducing learning rate to 2.7904196958843385e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.8518 - val_loss: 2.0049 - lr: 2.8186e-05\n",
      "Epoch 158/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8297\n",
      "Epoch 158: val_loss did not improve from 1.90791\n",
      "\n",
      "Epoch 158: ReduceLROnPlateau reducing learning rate to 2.7625155871646713e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 3s 39ms/step - loss: 0.8295 - val_loss: 2.0206 - lr: 2.7904e-05\n",
      "Epoch 159/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8483\n",
      "Epoch 159: val_loss did not improve from 1.90791\n",
      "\n",
      "Epoch 159: ReduceLROnPlateau reducing learning rate to 2.734890422289027e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.8463 - val_loss: 2.0365 - lr: 2.7625e-05\n",
      "Epoch 160/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8333\n",
      "Epoch 160: val_loss did not improve from 1.90791\n",
      "\n",
      "Epoch 160: ReduceLROnPlateau reducing learning rate to 2.7075415000581416e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.8347 - val_loss: 1.9658 - lr: 2.7349e-05\n",
      "Epoch 161/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8332\n",
      "Epoch 161: val_loss did not improve from 1.90791\n",
      "\n",
      "Epoch 161: ReduceLROnPlateau reducing learning rate to 2.6804661192727508e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.8323 - val_loss: 1.9708 - lr: 2.7075e-05\n",
      "Epoch 162/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8376\n",
      "Epoch 162: val_loss did not improve from 1.90791\n",
      "\n",
      "Epoch 162: ReduceLROnPlateau reducing learning rate to 2.6536613986536395e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.8364 - val_loss: 2.0081 - lr: 2.6805e-05\n",
      "Epoch 163/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8354\n",
      "Epoch 163: val_loss did not improve from 1.90791\n",
      "\n",
      "Epoch 163: ReduceLROnPlateau reducing learning rate to 2.6271248170814942e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.8337 - val_loss: 1.9780 - lr: 2.6537e-05\n",
      "Epoch 164/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8374\n",
      "Epoch 164: val_loss did not improve from 1.90791\n",
      "\n",
      "Epoch 164: ReduceLROnPlateau reducing learning rate to 2.6008534932770998e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.8369 - val_loss: 1.9500 - lr: 2.6271e-05\n",
      "Epoch 165/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8344\n",
      "Epoch 165: val_loss did not improve from 1.90791\n",
      "\n",
      "Epoch 165: ReduceLROnPlateau reducing learning rate to 2.5748449061211432e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.8396 - val_loss: 1.9391 - lr: 2.6009e-05\n",
      "Epoch 166/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8281\n",
      "Epoch 166: val_loss did not improve from 1.90791\n",
      "\n",
      "Epoch 166: ReduceLROnPlateau reducing learning rate to 2.5490965344943107e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.8274 - val_loss: 1.9148 - lr: 2.5748e-05\n",
      "Epoch 167/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8291\n",
      "Epoch 167: val_loss did not improve from 1.90791\n",
      "\n",
      "Epoch 167: ReduceLROnPlateau reducing learning rate to 2.523605497117387e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.8320 - val_loss: 1.9563 - lr: 2.5491e-05\n",
      "Epoch 168/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8438\n",
      "Epoch 168: val_loss did not improve from 1.90791\n",
      "\n",
      "Epoch 168: ReduceLROnPlateau reducing learning rate to 2.4983694529510104e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.8436 - val_loss: 1.9457 - lr: 2.5236e-05\n",
      "Epoch 169/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8330\n",
      "Epoch 169: val_loss did not improve from 1.90791\n",
      "\n",
      "Epoch 169: ReduceLROnPlateau reducing learning rate to 2.4733857007959158e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8321 - val_loss: 1.9351 - lr: 2.4984e-05\n",
      "Epoch 170/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8370\n",
      "Epoch 170: val_loss did not improve from 1.90791\n",
      "\n",
      "Epoch 170: ReduceLROnPlateau reducing learning rate to 2.4486518996127415e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.8365 - val_loss: 2.0060 - lr: 2.4734e-05\n",
      "Epoch 171/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8413\n",
      "Epoch 171: val_loss did not improve from 1.90791\n",
      "\n",
      "Epoch 171: ReduceLROnPlateau reducing learning rate to 2.424165348202223e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.8387 - val_loss: 1.9900 - lr: 2.4487e-05\n",
      "Epoch 172/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8311\n",
      "Epoch 172: val_loss did not improve from 1.90791\n",
      "\n",
      "Epoch 172: ReduceLROnPlateau reducing learning rate to 2.3999237055249977e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8311 - val_loss: 1.9489 - lr: 2.4242e-05\n",
      "Epoch 173/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8398\n",
      "Epoch 173: val_loss did not improve from 1.90791\n",
      "\n",
      "Epoch 173: ReduceLROnPlateau reducing learning rate to 2.375924450461753e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.8376 - val_loss: 1.9405 - lr: 2.3999e-05\n",
      "Epoch 174/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8469\n",
      "Epoch 174: val_loss did not improve from 1.90791\n",
      "\n",
      "Epoch 174: ReduceLROnPlateau reducing learning rate to 2.3521652419731254e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.8483 - val_loss: 1.9428 - lr: 2.3759e-05\n",
      "Epoch 175/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8378\n",
      "Epoch 175: val_loss did not improve from 1.90791\n",
      "\n",
      "Epoch 175: ReduceLROnPlateau reducing learning rate to 2.3286435589398024e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.8382 - val_loss: 1.9584 - lr: 2.3522e-05\n",
      "Epoch 176/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8498\n",
      "Epoch 176: val_loss did not improve from 1.90791\n",
      "\n",
      "Epoch 176: ReduceLROnPlateau reducing learning rate to 2.3053570603224217e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8498 - val_loss: 1.9851 - lr: 2.3286e-05\n",
      "Epoch 177/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8326\n",
      "Epoch 177: val_loss did not improve from 1.90791\n",
      "\n",
      "Epoch 177: ReduceLROnPlateau reducing learning rate to 2.2823034050816205e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.8312 - val_loss: 1.9760 - lr: 2.3054e-05\n",
      "Epoch 178/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8324\n",
      "Epoch 178: val_loss did not improve from 1.90791\n",
      "\n",
      "Epoch 178: ReduceLROnPlateau reducing learning rate to 2.2594804322579874e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.8314 - val_loss: 1.9375 - lr: 2.2823e-05\n",
      "Epoch 179/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8398\n",
      "Epoch 179: val_loss did not improve from 1.90791\n",
      "\n",
      "Epoch 179: ReduceLROnPlateau reducing learning rate to 2.2368856207322096e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.8378 - val_loss: 1.9461 - lr: 2.2595e-05\n",
      "Epoch 180/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8327\n",
      "Epoch 180: val_loss did not improve from 1.90791\n",
      "\n",
      "Epoch 180: ReduceLROnPlateau reducing learning rate to 2.2145168095448753e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.8327 - val_loss: 1.9590 - lr: 2.2369e-05\n",
      "Epoch 181/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8511\n",
      "Epoch 181: val_loss improved from 1.90791 to 1.89385, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8495 - val_loss: 1.8938 - lr: 2.2145e-05\n",
      "Epoch 182/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8255\n",
      "Epoch 182: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 182: ReduceLROnPlateau reducing learning rate to 2.1923716576566222e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.8244 - val_loss: 1.9594 - lr: 2.2145e-05\n",
      "Epoch 183/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8417\n",
      "Epoch 183: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 183: ReduceLROnPlateau reducing learning rate to 2.1704480041080387e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.8416 - val_loss: 1.9421 - lr: 2.1924e-05\n",
      "Epoch 184/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8361\n",
      "Epoch 184: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 184: ReduceLROnPlateau reducing learning rate to 2.1487435078597626e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8361 - val_loss: 1.9452 - lr: 2.1704e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 185/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8319\n",
      "Epoch 185: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 185: ReduceLROnPlateau reducing learning rate to 2.1272560079523828e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8319 - val_loss: 1.9573 - lr: 2.1487e-05\n",
      "Epoch 186/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8278\n",
      "Epoch 186: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 186: ReduceLROnPlateau reducing learning rate to 2.1059835235064383e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.8269 - val_loss: 1.9256 - lr: 2.1273e-05\n",
      "Epoch 187/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8355\n",
      "Epoch 187: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 187: ReduceLROnPlateau reducing learning rate to 2.084923713482567e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.8338 - val_loss: 1.9298 - lr: 2.1060e-05\n",
      "Epoch 188/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8360\n",
      "Epoch 188: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 188: ReduceLROnPlateau reducing learning rate to 2.0640744169213575e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.8358 - val_loss: 1.9374 - lr: 2.0849e-05\n",
      "Epoch 189/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8264\n",
      "Epoch 189: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 189: ReduceLROnPlateau reducing learning rate to 2.0434336529433495e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.8279 - val_loss: 1.9884 - lr: 2.0641e-05\n",
      "Epoch 190/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8386\n",
      "Epoch 190: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 190: ReduceLROnPlateau reducing learning rate to 2.0229992605891313e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8383 - val_loss: 1.9440 - lr: 2.0434e-05\n",
      "Epoch 191/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8301\n",
      "Epoch 191: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 191: ReduceLROnPlateau reducing learning rate to 2.0027692589792422e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.8319 - val_loss: 1.9558 - lr: 2.0230e-05\n",
      "Epoch 192/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8306\n",
      "Epoch 192: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 192: ReduceLROnPlateau reducing learning rate to 1.9827414871542714e-05.\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.8317 - val_loss: 1.9545 - lr: 2.0028e-05\n",
      "Epoch 193/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8324\n",
      "Epoch 193: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 193: ReduceLROnPlateau reducing learning rate to 1.9629141443147093e-05.\n",
      "66/66 [==============================] - 4s 57ms/step - loss: 0.8321 - val_loss: 1.9666 - lr: 1.9827e-05\n",
      "Epoch 194/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8308\n",
      "Epoch 194: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 194: ReduceLROnPlateau reducing learning rate to 1.943285069501144e-05.\n",
      "66/66 [==============================] - 3s 53ms/step - loss: 0.8311 - val_loss: 1.9629 - lr: 1.9629e-05\n",
      "Epoch 195/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8363\n",
      "Epoch 195: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 195: ReduceLROnPlateau reducing learning rate to 1.9238522818341152e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.8363 - val_loss: 1.9705 - lr: 1.9433e-05\n",
      "Epoch 196/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8379\n",
      "Epoch 196: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 196: ReduceLROnPlateau reducing learning rate to 1.9046138004341628e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.8388 - val_loss: 1.9783 - lr: 1.9239e-05\n",
      "Epoch 197/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8284\n",
      "Epoch 197: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 197: ReduceLROnPlateau reducing learning rate to 1.885567644421826e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.8285 - val_loss: 2.0000 - lr: 1.9046e-05\n",
      "Epoch 198/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8286\n",
      "Epoch 198: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 198: ReduceLROnPlateau reducing learning rate to 1.8667120129975956e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8286 - val_loss: 1.9651 - lr: 1.8856e-05\n",
      "Epoch 199/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8304\n",
      "Epoch 199: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 199: ReduceLROnPlateau reducing learning rate to 1.8480449252820108e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8286 - val_loss: 1.9632 - lr: 1.8667e-05\n",
      "Epoch 200/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8247\n",
      "Epoch 200: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 200: ReduceLROnPlateau reducing learning rate to 1.8295644003956113e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.8244 - val_loss: 1.9621 - lr: 1.8480e-05\n",
      "Epoch 201/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8333\n",
      "Epoch 201: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 201: ReduceLROnPlateau reducing learning rate to 1.8112688176188387e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.8347 - val_loss: 1.9819 - lr: 1.8296e-05\n",
      "Epoch 202/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8367\n",
      "Epoch 202: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 202: ReduceLROnPlateau reducing learning rate to 1.793156196072232e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.8338 - val_loss: 1.9845 - lr: 1.8113e-05\n",
      "Epoch 203/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8276\n",
      "Epoch 203: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 203: ReduceLROnPlateau reducing learning rate to 1.775224554876331e-05.\n",
      "66/66 [==============================] - 4s 55ms/step - loss: 0.8279 - val_loss: 1.9569 - lr: 1.7932e-05\n",
      "Epoch 204/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8312\n",
      "Epoch 204: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 204: ReduceLROnPlateau reducing learning rate to 1.7574722733115777e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.8292 - val_loss: 1.9658 - lr: 1.7752e-05\n",
      "Epoch 205/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8422\n",
      "Epoch 205: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 205: ReduceLROnPlateau reducing learning rate to 1.739897550578462e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.8436 - val_loss: 1.9328 - lr: 1.7575e-05\n",
      "Epoch 206/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8260\n",
      "Epoch 206: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 206: ReduceLROnPlateau reducing learning rate to 1.7224985858774742e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8260 - val_loss: 1.9586 - lr: 1.7399e-05\n",
      "Epoch 207/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8343\n",
      "Epoch 207: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 207: ReduceLROnPlateau reducing learning rate to 1.7052735784091056e-05.\n",
      "66/66 [==============================] - 3s 52ms/step - loss: 0.8356 - val_loss: 1.9592 - lr: 1.7225e-05\n",
      "Epoch 208/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8264\n",
      "Epoch 208: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 208: ReduceLROnPlateau reducing learning rate to 1.6882209074537968e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.8247 - val_loss: 1.9499 - lr: 1.7053e-05\n",
      "Epoch 209/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8316\n",
      "Epoch 209: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 209: ReduceLROnPlateau reducing learning rate to 1.6713387722120388e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.8321 - val_loss: 1.9637 - lr: 1.6882e-05\n",
      "Epoch 210/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8263\n",
      "Epoch 210: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 210: ReduceLROnPlateau reducing learning rate to 1.6546253718843217e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8294 - val_loss: 1.9500 - lr: 1.6713e-05\n",
      "Epoch 211/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8270\n",
      "Epoch 211: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 211: ReduceLROnPlateau reducing learning rate to 1.6380790857510874e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8270 - val_loss: 1.9521 - lr: 1.6546e-05\n",
      "Epoch 212/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8222\n",
      "Epoch 212: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 212: ReduceLROnPlateau reducing learning rate to 1.621698293092777e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8230 - val_loss: 1.9738 - lr: 1.6381e-05\n",
      "Epoch 213/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8403\n",
      "Epoch 213: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 213: ReduceLROnPlateau reducing learning rate to 1.605481373189832e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8384 - val_loss: 1.9597 - lr: 1.6217e-05\n",
      "Epoch 214/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8266\n",
      "Epoch 214: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 214: ReduceLROnPlateau reducing learning rate to 1.589426525242743e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8259 - val_loss: 1.9768 - lr: 1.6055e-05\n",
      "Epoch 215/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8349\n",
      "Epoch 215: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 215: ReduceLROnPlateau reducing learning rate to 1.5735323086119023e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8349 - val_loss: 1.9422 - lr: 1.5894e-05\n",
      "Epoch 216/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8268\n",
      "Epoch 216: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 216: ReduceLROnPlateau reducing learning rate to 1.5577969224978006e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8244 - val_loss: 1.9920 - lr: 1.5735e-05\n",
      "Epoch 217/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8311\n",
      "Epoch 217: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 217: ReduceLROnPlateau reducing learning rate to 1.5422189262608297e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8322 - val_loss: 1.9724 - lr: 1.5578e-05\n",
      "Epoch 218/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8338\n",
      "Epoch 218: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 218: ReduceLROnPlateau reducing learning rate to 1.5267966991814318e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8323 - val_loss: 1.9365 - lr: 1.5422e-05\n",
      "Epoch 219/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8387\n",
      "Epoch 219: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 219: ReduceLROnPlateau reducing learning rate to 1.511528800619999e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8387 - val_loss: 1.9315 - lr: 1.5268e-05\n",
      "Epoch 220/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8319\n",
      "Epoch 220: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 220: ReduceLROnPlateau reducing learning rate to 1.496413519816997e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8310 - val_loss: 1.9581 - lr: 1.5115e-05\n",
      "Epoch 221/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8381\n",
      "Epoch 221: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 221: ReduceLROnPlateau reducing learning rate to 1.4814494161328184e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8358 - val_loss: 1.9544 - lr: 1.4964e-05\n",
      "Epoch 222/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8289\n",
      "Epoch 222: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 222: ReduceLROnPlateau reducing learning rate to 1.4666349588878802e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8271 - val_loss: 1.9563 - lr: 1.4814e-05\n",
      "Epoch 223/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8305\n",
      "Epoch 223: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 223: ReduceLROnPlateau reducing learning rate to 1.4519686174025991e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8291 - val_loss: 1.9611 - lr: 1.4666e-05\n",
      "Epoch 224/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8354\n",
      "Epoch 224: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 224: ReduceLROnPlateau reducing learning rate to 1.4374489510373678e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.8356 - val_loss: 1.9385 - lr: 1.4520e-05\n",
      "Epoch 225/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8242\n",
      "Epoch 225: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 225: ReduceLROnPlateau reducing learning rate to 1.423074429112603e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8242 - val_loss: 1.9533 - lr: 1.4374e-05\n",
      "Epoch 226/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8365\n",
      "Epoch 226: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 226: ReduceLROnPlateau reducing learning rate to 1.4088437010286726e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.8345 - val_loss: 1.9312 - lr: 1.4231e-05\n",
      "Epoch 227/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8296\n",
      "Epoch 227: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 227: ReduceLROnPlateau reducing learning rate to 1.3947552361059934e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.8292 - val_loss: 1.9609 - lr: 1.4088e-05\n",
      "Epoch 228/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8281\n",
      "Epoch 228: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 228: ReduceLROnPlateau reducing learning rate to 1.3808076837449334e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.8258 - val_loss: 1.9616 - lr: 1.3948e-05\n",
      "Epoch 229/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8324\n",
      "Epoch 229: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 229: ReduceLROnPlateau reducing learning rate to 1.3669996033058851e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.8377 - val_loss: 1.9616 - lr: 1.3808e-05\n",
      "Epoch 230/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8291\n",
      "Epoch 230: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 230: ReduceLROnPlateau reducing learning rate to 1.3533296441892163e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.8283 - val_loss: 1.9509 - lr: 1.3670e-05\n",
      "Epoch 231/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8284\n",
      "Epoch 231: val_loss did not improve from 1.89385\n",
      "\n",
      "Epoch 231: ReduceLROnPlateau reducing learning rate to 1.3397963657553191e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.8261 - val_loss: 1.9580 - lr: 1.3533e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD7CAYAAAB+B7/XAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABIhklEQVR4nO3dd3gc1bn48e/MbFWvtiz3ety7aaaHkpgWEkouhIQQSPlB6oUbEgKhXEIgIR2S0AnchBASCL1X021sMBiPe1XvZfvM/P6YlSzJki3JsuRdvZ/n4UGamZ1992j97tl3zpyjOY6DEEKI9KEPdQBCCCEGliR2IYRIM5LYhRAizUhiF0KINCOJXQgh0oxniJ/fDywBygFriGMRQohUYQCjgPeBaNedQ53YlwBvDHEMQgiRqo4ClnfdONSJvRygvr4V2+77ePrCwixqa1sGPKhUI+2wm7SFS9rBla7toOsa+fmZkMyhXQ11YrcAbNvpV2Jve6yQduhI2sIl7eBK83botoTd68SulPoVUGSa5oVdtv8MuAioT2660zTN2/oZpBBCiP3Uq8SulPoM8FXgqW52Lwa+ZJrm2wMZmBBCiP7ZZ2JXShUANwI/B+Z1c8hi4CdKqfHA68DlpmlGBjRKIcSAcByH+vpqYrEIkNYlCgCqqnRs2x7qMPpJw+cLkJ9fjKZpfXpkb3rsfwGuAsZ23aGUygJWAVcAG4H7gKuTx/daYWFWXw7vpLg4u9+PTSfSDrtJW7i6a4eqqio8Hp3i4nFomtzGcjBzHJu6uhogQnHxiD49dq+JXSl1MbDDNM2XlFIXdt1vmmYLsKzD8bcC99DHxF5b29KvCxzFxdlUVzf3+XHpRtphN2kLV0/tUF1dS0HBSCx32MKgxzXYPB6dRCJ1X2dmZi7V1ZVoWrDTdl3X9toh3tdH9rnASUqp1cD1wOlKqd+07VRKjVNKXdTheA2I9zF2IcQgsW0LwxjqwXCitwzDg233/d7Nvf6FTdM8se3nZI/9WNM0f9DhkDBwi1LqFWArcCnwaJ+jGAKxtS+T2LKSjFOuGOpQhBhUfa3XiqHT379Vvz66lVJPA9eYprlCKfVN4AnAh3sH1K39imSQWZWbsCo3DHUYQgxbt956M2vWfEgiEWfnzh1MmDAJgLPP/hKnnHJ6r85x4YXncd99f+tx/+uvv8batZ9w8cXf2q9Yb7zxWhYsWMSyZaft13kGS68Tu2ma9+FeHMU0zWUdtv8L+NdAB3bAxUKQiOE4tlxEEmII/Pd//wiA8vIyvvOdb+41QfdkX485+uhjOOKIo/oVXyobtsU2JxZyf0jEwBsY2mCEEJ2cddZpzJw5mw0bTG6//S4efvjvrFz5Pk1NTRQVFXH99TdRUFDIkUcuZvnyFdx991+oqalmx47tVFZWcOqpZ/DVr36dJ598nJUrV3DVVddy1lmncfLJy3jvvbcJhyP89KfXMX36DDZv3siNN16HZVnMmzefd955i3/847EeY3vqqcd56KEH0TQNpWbwgx/8Dz6fj5tuuo7NmzcBcOaZZ3P66Wfy/PPP8re//RVd1yktLeXqq2/A7/cf8PYbvok96iZ2Jx5Bk8QuhqE315Sz/KNupxrZb0fOHcXSOaP26xyHHXYE119/Ezt37mD79q38+c/3oOs6N9xwDc899wz/9V9f7nT8xo0buP32u2hpaeaccz7PF75wzh7nzM3N5c47/8ojjzzEAw/cw403/pL//d9rueSSb3H44Ufyj3/8H5bV88XKTZs28te/3sMdd9xHbm4et956M/feeydHHHEkTU1N3Hvv36ipqeZPf/oDp59+Jnfe+SfuuONe8vMLuO2237F9+1amTlX71S69MWxrEJ167EKIg87MmbMBGDNmLJdd9gOeeOIx/vCH3/DJJ2sIh0N7HL9w4WK8Xi/5+QXk5OTQ2rrn5F+HHnoEAJMmTaGpqYmmpkYqKso5/PAjATjllDP2GtPq1StZuvQocnPzADj99DNZufI9Jk2azPbt2/jhDy/j5Zdf5NJLvwfA0qVH8e1vf53bb/8dxxxz/KAkdZAeO05cbpIVw9PSOfvfqz6Q2koW69Z9yrXXXsWXvnQexx33GQxDx3H2vO/F5/O1/6xp2j6PcRwHXTe6Pa4ne95v42BZFrm5eTzwwMO8//67vP32m1x00Zd54IGH+f73L2fjxjN4++3l3HDD1Vx00Tc4+eRl3Z57IA3LHrtj2xAPuz/H95ijXghxEFm9eiULFizi858/i7Fjx/HWW8sHbJqArKwsRo8ew9tvvwnACy88u9chhgsWLGL58tdpamoE4PHHH2PBgsUsX/4aN9xwDUcccSTf//7lBINBqqoq+dKXziQvL48LLvgan/3sKaxfbw5I3PsyPHvsyaQOQEISuxAHs8985iR+8pMr+MpXzgVAqRmUl5cN2Pl/+tPruOmm67nzztuZPHnqXi9uTpkylQsu+BqXXfYNEokESs3giit+jM/n59VXX+aCC87B5/Nx8snLmDx5Cl//+jf5/vcvxe/3k5+fz1VXXTtgce+N1pevIQfABGDLYE8pYDdV0/qQe2NS4MTL8E5c3OdzHEzkNvrdpC1cPbVDRcU2SkrGD0FEQ6M3Uwrce++dnHbamRQVFfHaay/z/PPPcOONvxykCPetu79ZhykFJuLeHNrJsOyxt184BZBSjBDD2siRJfzgB/8Pj8dDdnYOV1559VCHtN+GfWJ3pBQjxLC2bNlpKXNHaW8Nz4un0dbdv8ioGCFEmhmWiZ1ohx67lGKEEGlmWCb23aUYTUoxQoi0M4wTu4YWyJJSjBAi7QzPxB4NgS8IvqCUYoQQaWd4JvZYCM2fgebxyw1KQgyRb3/767z44nOdtoXDYZYt+wwNDQ3dPubGG6/l6aefoKammssv/263xxx55N7vSykr28VNN10PwLp1a/nFL27oe/Bd3H33X7j77r/s93kGyvBM7NEQmi8DvH7psQsxRE455XSef/7ZTttee+1lFi5cTF5e3l4fW1RUzK9+9ft+PW9FRTm7du0EYPr0mWkxbr2rYTmOnVgysRsenFh438cLkYbi698kbr5+QM7tVUfjnbZ0r8ccf/yJ3Hbb72hqaiQnJxeA5557mnPOOY9Vq1Zyxx23E41GaG5u4bvf/QFHHXVs+2PbFud45JEnKC8v4/rrryYcDjNr1uz2Y6qrq7j55htoamqmpqaaZctO4+KLv8Xvfvcrysp2ceutN3PccZ/hnnvu4I9/vIPt27dxyy030tzcRCAQ5Pvfv5wZM2Zx443XkpmZhWl+Sk1NNRdeePFeV3h68803uPPOP+E4NqWlo7niip9QUFDIH//4W95//110XeOoo47loou+wYoV73H77b9H0zSys7O59tqf7/NDrTeGZ489FkLzZ0opRoghlJGRwVFHHcPLL78IQE1NNdu3b+OQQw7jX//6B1deeTX33PN/XHnlT7nzzj/1eJ7f/OYWli07jfvu+xtz5sxr3/7CC89x4omf5Y477uOvf/0HDz/8dxoaGvje9y5HqRntKzi1ueGGqzn77C9x//0P8Z3v/JCf/vRHxGLutN5VVZXcfvtd/OIXv+a2237XYyz19XX88pc/56abfsX99z/EnDnz+PWvb6Giopx33nmL++//O3/60z1s3bqFaDTK/fffzRVX/Ji7736AJUsOZf36dfvTpO163WNXSv0KKDJN88Iu2+cDdwE5wOvAt0zTTAxIdAeIe/E0AxxLSjFi2PJOW7rPXvWBtmzZadx115/5/Oe/yPPPP8PJJy/DMAyuvvoG3nrrDV555cXk/Os9f7NetWol1157IwAnnfS59pr5eeddwIcfruRvf3uALVs2kUjEiUS6P08oFGLnzp0cc8zxAMyePYecnBy2b98GwCGHHIqmaUyaNLl9ZsfurF37CTNmzGLUqFIATj/9CzzwwH0UFRXj9/v59rcv4ogjjuLb3/4Ofr+fI488mp/85AqOOuoYjjrqGJYsOazvjdiNXvXYlVKfAb7aw+4HgctM05wGaMAlAxLZAdR+8dQbkB67EENo/vyF1NbWUFlZwXPPPdNe4rj00kv49NNPUGo6X/nKRfuYM11rn0RQ0zR03QDgD3/4DQ8//BAlJaP46le/Tm5uXo/ncZw9JwpzHNpXU/L5/O3n35uu53Ecd752j8fDHXfcx8UXf5vGxka+9a2vsX37Ns4993z+8Ie/MGbMWG6//ffcf//dez1/b+0zsSulCoAbgZ93s288EDRN853kpvuAswcksgPEsS2IR9wau8cnC20IMcQ++9lT+Otf7yEnJ4fRo8fQ1NTIjh3b+PrXv8Vhhy3ljTde2+v864sXH8Jzzz0NuBdfYzG3s7Zixbucf/5XOP74E9i+fRvV1VXYto1hePZY/i4zM4vS0tG89trLAHz88Rrq6mqZNGlyn17LzJmzWbt2Tfu0wo8//m8WLlzE+vXruOyybzBv3gIuu+z7TJgwie3bt3HJJV8lFGrlnHPO45xzzhvUUsxfgKuAsd3sKwU6LppYDowZgLj2yW6po2nXuzD60L49MHmxVPNnAA4kYjiOjaYNy8sNQgy5ZctO46yzTuPHP74GgJycXE499QwuuOAcPB4PCxcuIRKJ9FiO+eEP/4cbbriGxx9/lOnTZ5CRkQnAl798IddddzU+n48RI0qYPn0mZWW7mDZN0dLSzA03XN1pKbxrrrmBX/7y59x991/wen3ceOMteL3ePr2WgoJCrrjiKn7yk8uJxxOUlJRw5ZXXUFRUxOzZc/nKV84lEAgwZ848DjvsCAKBADfeeB2GYZCRkcGPfvTTfrZiZ3udj10pdTEw0zTNHyqlLgSO7VhjV0otBX5hmuZRyd+nAk+Ypjm9l88/AdjSn8Ab33uS2hfuZcLlD6D7M3r9uHh9BTtuv5Ti076DFWqi7qX7mXDFg+i+YH/CECKlfPLJWkpLh8987OmgrGwbs2bN7Gl3v+ZjPxcYpZRaDRQAWUqp35im+YPk/p1Ax0UTS4A+L23Sn4U2YmH3q1R1RS16Rs+rindlVVcD0BLVsaPuc1aX16Bn5PXp+Q8msrjEbtIWrp7awbbtfS48kU56s9DGwc627T3+lh0W2ujWXusPpmmeaJrmbNM05wPXAI93SOqYprkNiCR77gAXAM/0L/y+0TzJ5av6OKqlfQKwtoun/TiHEEIczPpVWFZKPa2Uartv93zgN0qpdUAW0L/bwfrK6yb2vs7O2DYXu+bP6Pc5hEhlQ7wcpuiD/v6tej2O3TTN+3BHvWCa5rIO2z8EDunXs+8HzeNzf+hlbzux82MS2z9EL3Cv7Wq+DDSP22OXkTFiuNB1A8tK4PH07aKgGBqWlWgfvtkXqTsUxNPW24716vDE5veJf/wCTsStVWm+DLRkXd1pqTsgIQpxsAkGs2hubuh23LY4uDiOTXNzPcFgz7X0nqTsXDFaH8sodsi9W8yuLwdNA28APbvI3dZc4/4/0kzs3X/iP+K83fV3IdJIVlYu9fXVVFbuBNK/JKPr+l7HwB/cNHy+AFlZuX1+ZOom9raLp71M7E44mdgbysCX4d5B5vWjBXNwmqsAsHZ+TNx8Hc+Uw/CM7nF4kRApS9M0CgpGDHUYg2a4jpJK4VKMW2Pv7VwvTnuPfZd712mSll2M3eQOgbSba91jw00DGakQQgyqlE3su3vs+66xO47T3mMnEUvederSc4qxm93E7rS4JRlJ7EKIVJayib1PQxWjrWDvvompY49dzy7GaanDsRPttXZJ7EKIVJa6iV33gKb3arijHe48zWbXxI5ju8m9RUoxQojUl7KJXdM0NF+gVz32tvo6ySk3NX/m7vPkFANgN1W119htSexCiBSWsokdQPf4eldjT/bY9bzktDb+Lj12wKraDFYsebwkdiFE6krpxN7XHrteOD75uA6jYjILQDewdn3ibvAFcSKS2IUQqSulE7vu9feuxh5qBMODnj8a6JLYdR2jdAZWuQmAUTxReuxCiJSW0old8wZ6NaWAE25EC+aiZ+a7j+syf7tvzkntPxvFk9zFN2T+GCFEikrpxK57fb0rxYSb0DJy0doSu69zYjfGzEHPKwVvED13ZPtjhBAiFaXslALg9thpbtjncU6oET27CKNkGr5DzsLoMl2ApmkEjrkIu7ESLZDtPibcBDnD59ZrIUT6SOkeu+bz48T3Xopx4hHshnK03JFohgf//FN3T/nbgTFyCt5pS9GCOYAMeRRCpK6UTuy6N7DPScASu9aCncAzdm6vzqkFO/TYhRAiBaV0Ytd6UWO3tn8E3gBGybTenTPZY3fCjVi1Owg//3ucWPerowshxMEopWvsujcAeynFOI5DYsdHeEbPQjN691I1w4teMIb4mheIm8txmquxqrfINL5CiJTRq2ynlLoeOAt3Zv67TdP8dZf9PwMuAuqTm+40TfO2gQy0O5rXD46FYyW6Tdx2YzlOax3GojP6dN7giZcReux/cZrbpvOtHpB4hRBiMOwzsSuljgGOB+YCXmCtUuop0zTNDoctBr5kmubbBybM7um+5CpHiSh0k9idxkoAjIKxfTtvbgkZZ1yFHWog/NSvcJoksQshUsc+a+ymab4GHGeaZgIYgfth0NrlsMXAT5RSHyml/qiUGpR15dpGt/R0k1LbpF5aVmGfz63njcJTOgMtu1B67EKIlNKri6emacaVUtcBa4GXgF1t+5RSWcAq4ApgIZAHXD3gkXZDa+ux9zCtgN1SC4anfaRLf+gdVlgSQohU0OuLp6Zp/kwpdTPwBHAJcEdyewuwrO04pdStwD3AVb09d2Fh31fhBmitcxfbyMs28Bfvmbwr442QW8yIEX1fDLZNdfEoWte/R3E35z+YHOzxDSZpC5e0g2s4tkNvauzTgYBpmqtN0wwppf6NW29v2z8OOME0zXuSmzQg3pcgamtbsO2+r5ie6XV77HXV9XiMPResDddWogXz92sx26gvDzvURFVZtXun60FouC7Y2x1pC5e0gytd20HXtb12iHtTipkE3KmU8iulfMAZwPIO+8PALUqpiUopDbgUeHQ/Yu413bf3dU+dllr0ftTXOz1Hcr52qbMLIVJFby6ePg08hVtHXwm8ZZrmQ0qpp5VSi03TrAa+iVuiMXF77LcewJjbtS1o7XRTY3esOE6oAS2raL+eoz2xS51dCJEielVjN03zWuDaLtuWdfj5X8C/BjKwfdm4s5GVH2xwi/vdTLHrtLpD6vWsgv16nral82TIoxAiVaTslAKbyxp5/dMGAJxoyx777eYaALTs/euxa/4s8Aawm6r26zxCCDFYUjaxezw6YceHo+k44T0vjjgt7hj2/a2xa5qGnjsSu6lyv84jhBCDJWUTu9fQAQ38Wd2uUWq31AG0L66xP/SckdiNktiFEKkhdRO7xw3d9md332MPN4E/E83w7vdz6bkjcVpqcKzEfp9LCCEOtJRN7B4jmdh9WdiRbhJ7tNWtjw8APbcEHIfEtlWEnv4Vdmv9vh8khBBDJGUTe1uP3fJmdbsohhNtQQtkDshzta2DGn3/EaydHxN57W4cp+83VAkhxGBIg8Se2X0pJtIysD12krNF+jOxdn5MYv3yfTxKCCGGRsom9rZSTNyTCfEwjtV5FgO3FDMwPXYtkAXJc/mXfBG9cDyxj56TXrsQ4qCUsom9rcce87gJt2uv3Ym0uAl5gOg5bjnGM34BvtknYNfvxCpfN2DnF0KIgZLyiT1uZADgdLiA6tgJiIcHrBQD4Bk7B8/ExeiZ+XgmHwr+TOJrXx6w8wshxEBJ2TVPvclSTFRPJvYOF1CdaAhgwC6eAvgXn9n+s+bx4Rk/H2vHmgE7vxBCDJSU77FH9G567BF3ioGB7LF3ZRSMwQk3tT+XEEIcLFI2sXuSiT2sBYGuPfZkYh/AGntXev5oAKz6Xfs4UgghBlfKJva2UkzE8YFudOqxE3GXZD2QPfa2xG5LYhdCHGRSN7G3XTy1HbRAdg899oGrsXelZRaAN4hdJ4ldCHFwSdnEbugamgaJhI2WVUiiYkP7WPbBqLFrmoaeX4pdv/OAPYcQQvRHyiZ2TdPwGjpxy8a/8Aycxgpiq54E3JuT0Aw4wGuUGvmjsevLDuhzCCFEX6XscEcAr9cgkbDxjJuLZ/JhxD74D4kda9ACme5/mnZAn1/PH41jvo4dbkIP5hzQ5xJCiN5K6cTu87g9doDAMRcRHzmZ6Lv/BCuGnl96wJ+/bXUmp7UeJLELIQ4SvUrsSqnrgbMAB7jbNM1fd9k/H7gLyAFeB75lmuYBn7zc69GJJ9zErnl8+GafiN1YSfyTFw9ofb1NWy/dCTce8OcSQoje2meNXSl1DHA8MBdYDHxHKaW6HPYgcJlpmtMADbhkoAPtjtdjkEj22Nv45n4WNH3AJgDbGy0jF9g9ht5xbJmrXQgx5PaZ2E3TfA04LtkDH4Hby29t26+UGg8ETdN8J7npPuDsgQ91Tx177G307CICR38N75yTDvjza8keux1yE3ti/Zu0/v0K7FDDAX9uIYToSa9KMaZpxpVS1wGXA/8EOg7eLgXKO/xeDozpSxCFhf0rm/i8OpquU1yc3XlH8bJ+na/vsmn1+glqYQqLs6l6exPYCbJilWSOHztIMbj2aINhTNrCJe3gGo7t0OuLp6Zp/kwpdTPwBG6p5Y7kLh239t5GA2z6oLa2Bdvu+9zmXo9BKByjunrPhTYGTSCH1toa7OpmQjtMAOo3ryOU17VadeAUF2cPbRscRKQtXNIOrnRtB13X9toh7k2NfXry4iimaYaAf+PW29vsBEZ1+L0EGJTB3d2VYgabFszBCTfiRFuxG9wvLnbN9iGNSQgxvPXmBqVJwJ1KKb9SygecAbSvC2ea5jYgopRamtx0AfDMgEfaDZ/HGPLErmfk4oSasKq3AKAFc7Fqtg5pTEKI4a03F0+fBp4CVgErgbdM03xIKfW0Umpx8rDzgd8opdYBWcDvD1TAHXm9u8exD5W2HrtVtdmNSR2F01Ir0/kKIYZMby+eXgtc22Xbsg4/fwgcMpCB9cbBUYrJxYm0YFVuRM8twSidDqufxKrdjmf0zCGNTQgxPKXsXDGQLMUcBD12cLB2foJROgO9aDwAdm3nOnvkzQeJrnh0CCIUQgw3KZ7YdRJD3mNPTiXgWBhj56AHstEC2Z0mB3Nsm/j65SS2rByiKIUQw0lKJ3aP5yCosSfvPkUz8JTOAEDPL8Vq2J3Y7fqdEI9gN1Xg2EMbrxAi/aV0Yvd5DRKJvo9/H0ht88UYJVPRfO4yfXpyOl/HcWOzKje6B1sJnJaa9sfG1y+n5e9XSLIXQgyo1E7sHh3bcbCGMDFqGfng8eOZsLB9m543CmKh9snBrMpN7fvshor2n62abTjN1TitdYMXsBAi7aV0Ym9fHm8I6+ya10/ml27GO+uE9m2710N1yzFW1SaMkVPdbQ27Z19oGxJpN1cPVrhCiGEgxRO7AUDCGuJyTEYemr67Kdvmgrfry7DDTTiNFRjj54E/E7uxHDvSjOM47Wuz2k1VQxK3ECI9pXRi93mHvsfeHS2YC74gdkMZ1o41AHhKZ6LnjSKx9QNaH/gu1vbV7T12p0l67EKIgZPSKyjtLsVYQxxJZ5qmYRSMxSo3ccJNaMFc9OIJ6LmjSCQvpNoNFTgRd3IiKcUIIQZSSvfY20ox8SEuxXTHO/1o7PpdJLasxDNuHpqmYxRPAMMHuoEdathdY5dSjBBiAKV4YnfDH+qblLrjmXI4Ws4IwMEYPx8A78zjyPryb9CyCnGaayAeAaQUI4QYWCmd2H1tPfaDMLFruoH/0HPQC8bgGT3L3ZZcsk/PyMOqd9cq0bKLcaItOLHQUIYrhEgjqV1jb7t4OsR3n/bEO3Ex3omL99iuZeThVG4AwCieSKK5GrupGiM5z4wQQuyPlO6xHwzj2PtDy8iD5F2pxojJANh1O4cwIiFEOknpxH4wl2L2RsvIa//ZGD0DLbOA+KZ3en6AEEL0QUon9vaLpwdpKaYnetvEYYAWyMY7bSnWzo+xW+txYmEir92DHW4awgiFEKksLRJ7yvXYM/N3/xzIwjttKTgO8Q1vkti5hrj5evuNTUII0Vcpndh93rYpBVIssbf12L0BNMOLnluCXjyJxJYPsCqSNzC11OI4No4VH8JIhRCpqFejYpRSPwPOSf76lGma/9PN/ouA+uSmO03TvG3AouyBL0V77Hqyxq4Fstq3ecbNJbbyPzixVgCclhpiq58mvu41Mr90C3b1FrTMfPQOvX0hhOjOPnvsSqkTgJOABcB8YJFS6swuhy0GvmSa5vzkfwc8qYO70AYcvMMde+TLAMOLFshu3+QZOwdwcBorAbCba7Eq1ien9a0n9PQviX3w+BAFLIRIJb3psZcD/22aZgxAKfUpMK7LMYuBnyilxgOvA5ebphkZ0Ei70T67Y4r12DVNQ8vIQ/Nntm/Tiyai+bPcGR99QeyWWrDdOXAS21dDLIzdWt/DGYUQYrd99thN0/zENM13AJRSU3FLMk+37VdKZQGrgCuAhUAecPWBCLYrQ9cwdC31euyAf9Hn8c0+sf13TdcxxswGwDNhEU5zjTvtAJDY6A6FdCIyUkYIsW+9vvNUKTULeAq4wjTNDW3bTdNsAZZ1OO5W4B7gqt6eu7Awa98H9cDn1fF4PRQXZ+/74INJ8Wf32BQ74b+ITF+AE49Ru355+3arYj0Aeqylx9eZcq//AJK2cEk7uIZjO/T24ulS4F/A903TfKjLvnHACaZp3pPcpAF9GspRW9uCbfd9hsbi4mwMXaepJUJ1dXOfH3/wyYXRh5LYtqp9ixbIbp/eN9HS2O3rLC7OTpPXv/+kLVzSDq50bQdd1/baIe7NxdOxwGPAeV2TelIYuEUpNVEppQGXAo/2L9y+83r0lBsVsy9aVlHyBw1PcmZIABJRnHh0SGISQqSO3vTYLwcCwK+VUm3b/gycDlxjmuYKpdQ3gScAH7AcuPUAxNotr6Gn3MXTfdGzCwF35ke9aAKYb6DlluA0VrgLd3iL24+16nZQs2I5ztzPo/mCQxSxEOJgss/Ebprm94DvdbPrzx2O+RduqWbQeT16Sl483RvNlwG+IHreKIziiQB4xi8g/tEz7gXUHDexW9VbCT39S4i2EsifjHfyIUMZthDiIJHSd56CO5Y93UoxAP5DzsY352SMEZPIPPdmvJOWAOAk55Bx4hHCL96G5g2geXxYySX3hBAi5RO710jPxO6beTye0TMB0HNHogVzANonB4u+9whOcw2B476Bv3SKJHYhRLvUT+wePeXmiumPtsTuhJuxGyqIr30J78zj8IxSBMYo7NptOInYEEcphDgYpHxi96Rpj70rzeMDbwAn3Eh01ROge/EtPAMA/2gFtoVVsxUAx07Q8o8fETffGMKIhRBDJeUT+3DpsYPba7cqN5HY+Dbemce1z+seGD0NgIS5HMe2sWt34DRWkihb1+nxjpXAibYOetxCiMGV0mueQnqOY++JFszBrtwIhhffvM+1bzcyc/HOPJ742pdx4hGMkqkA2I3lnR4fefUuEpveQS8ch3/pBXiSxwkh0kvKJ3aPkZpzxfSHHsjGBrwzjm2f+rdN4MivoHkDxD58Gjs5x4zdUIHjOGiaBoBVvg69cCxOLEz4iZ/jm38qvkVnoOkp/zYQQnSQBqUYI+1uUOqJllkAhgffvGXd7vfO+gygYVdvBk2DWKh9OgI73IQTasA7dSmZX7wez9Qjia16gtB/bsRuqBjEVyGEONBSP7Eb6XeDUk98C08n44yf9rjYhp5ViFE6HQBj7FwA7Aa3HGPXbHOPKRyH5gsSPPbrBE64FLupitbHrseJhQbhFQghBkPKJ3aPRxs2NXY9IxejaMJej/FOPwYA34xjAbAb3d64VesmdqNw91T63klLCB53CcRCWMnED+5F1sjyv0pPXogUlfLFVa+hk7CcTrXk4cwz+VAyR0xyJxIzPB167NvRsgo7LccHuHPRAHbtDhKOA5oOukF87cvYLXVkfPb7ezyHHWok+s5D+Jd8AT27eI/9QoihlfqJPbk8XsKy21dUGs40TUPLGQGAnlOCVb6e+Ob3sKo2YRSN3+N4PSPPHUZZu53YR8+ieQN4Z7i9fmv7aqyare3fEuzWepxwE9G3/4ZVbqJnF+Ff8sVBe21CiN5J/cRu7F7QWhJ7Z3rxBBLrlxN58Xb395nHdX9c4Th3DvhoKw6Q2LYaLZiDYyUIPXYDevFEMk69ktCTN+MkSztaMIfE1g86Jfb4pneJb3gLPZhL4JiLDvjrE0J0L/UTe/uC1n1fqCPdBY66EGf+KTh2AicW7rE+bxSOw9r5cfvvVtmneCYswjf/FGJrXyaxfjnxT17AaazAO/N4PBMWYteXEX37b9iNFei5JTjRViKv3AG6BysRxbfwdPTsoj7H7FgJcGz3TlshRL+kwcXTth67NcSRHHw0w+NO/VswFk/JtB6TpV441v1/Xil43Tnd9eKJGCMmETjqq+ANEn3fXTvFt+A0PGNm45mwAIDwK3cQefNBEls/ANsisPTLACR2fdKvmCOv3kXoyV/067FCCFfKJ/a2UkxCeuz9pidHyhjj5mGMcqcnMEZMAkAzvO4qTlYMvXhi+1BLPbsYz/gFOM01xD95kcg7D6FlFeKZthQtM7/TN4CuHMfBqtu15/ZYmMSWFdhVm7Hqywb4VQ4su7WeyPIHcBKyopU4+KR+YvfsrrGL/tHzSvEf9iV8c07CM34BeAMYxRPa97ct4OEZv6DT44Inf4/ML/8OY/RMiLbimXQImqZjjJ5NYtdaHHv338SxEoSe/Q3x9ctJbHqX0CNXkUgu0t3G7fUn3J83vbvXmB0rTviFP3YapjmY4uYbxNe+RGLLyiF5fiH2RhK7QNM0fHM/i56Zj3f6MWSd/2t3FackY+xcfIecjbebi6+aphE46kKMkVPxTT8aAM+YWRBtde+ATYqteQ5r+4dEVzxKfN1rwJ7JO775PbSsQoxRisTm93Cc3d/CnESM2NpXsJurAbDrdpHYsoL4xrd79RqdRJTI23/HbqntZavsnbVrrRvzxndwHAcn+YEkxMGgVxdPlVI/A85J/vqUaZr/02X/fOAuIAd4HfiWaZqD8k73GLuHO4r9p2kadEjqAJpu4J9/So+P0XNGkHHGVe2/e8bNBcNDfOPb6LklxM3XiX3wH7SsQpyWWqyWWtAMEltW4hxxPpqm48TCWDs/xjvrBPTckUSX/xWnsQItbxRW9RbCL96O01xNLJBN8OTvYTdWAmBXbd4jHjvUgFW5Ce/ERYSf/z163ij0wvHE1zyHXb2F4KlX4jRXEXryZoKf/SFG4ViceJTYJy/im30Cmse/1zZy4lF3YROPD2vnJ4T+fS1aMJuMZZf3pal3x9tYAYYXPauw2/3xje+g54xoL4+B+40h9vGLeMbOcef7Mbz9em6RnvbZY1dKnQCcBCwA5gOLlFJndjnsQeAy0zSnARpwyQDH2SPpsR98NF8GnvELSWx8l9AztxJ992H0/DFknHolWnLyMt+iM3BCDdiVmwBIlK0F28Izfj7GKHfRdKtqE4myTwk98QtwbALHfQM8PiLLH8Cud2v0VvUWHCvevrKU49hEXvoTkRf+QHzz+yS2fkBs7StumUfTsSrWE1/zLPENb+G01rvDPIHYR88Se++fJDav6PRa4ltWYjdVddpmVawHO+HOh+9Y2LXbsMo+xbHi/Wqv8PN/JPLqXQDYLbWdSlhW9RYiL/+F8HO/w4mFd8dlvoHdWEFs9ZPEP32tX8+b2P5h8jqBLNCSbnrTYy8H/ts0zRiAUupToP2+dKXUeCBomuY7yU33AdcBfxrYULsnif3g5J221C2nVLcQOP5beKccBoD/sHOxa3fgm30isVWPE1v7MsGSqVg71ri1/ZFTQdfBG8Cq2oy15jm0zDwyTvsxekYedlMVsQ/+g+ULuE9kxQm/cBvWrk/IPPcXNO94G6vcBCDy2t3uMbEQiU3v4pl8CE48QnT1U+2lJqtyI060ldiaZwH3A8Y7bSngTpwWefGPeKYc4U69AERXPEpiywrQPW7vXvdghxqIf/QMdt0u9MJxRN/9Bzg2/kWfJ/L6vfjmLevU2wawI83YFRsxxszCbtgFjTqJig2EH/85eu5I/EddiDFqGpE3HwRfECfcROipW3BCjQRP+H9YVZvwzTmZRNmn7mpasz7Tfue13VxNS9WHJOyMbqdmdiItRN7+G4kNbwFglE7fvaZuLIzmCw7Ie0AMnX0mdtM028etKaWm4pZklnY4pBQ3+bcpB8YMVID7IqWYg5MxZjZadjHGyCntSR3AO+VwmHI4AL65nyO26gkS048msWMNntEz0Qz3LWkUTySxbRVOaz3+Q89tn6bYKJkGjuPe+TpiMnbVJqztqwGIvvMPQrs+xiidgZ5dTNx8HaN0Blbtdvfi7rh56LklhB673l1wxJeBVbmB2JrnIRZGLxiLtevT9ukprG2r3edKridrN1W7JaXsYnzzPofm8eObezJ2UxXxj57Bqt5C7OMXSGx4E8CNv7kGLZiLMWJSp2kvom8+SGLTuwRP/h44DjgW0eX3g67j2BaRl/+Mb+Hp2FWbCBx7MVblJuLm64Dm9u5tC2PUdPT8UiKv3oW1ay2eMbOw6nYQevQGWq0YGF4yz/l5p2kfHCtB66PX4bTU4VtwGvF1r5HY9C7eSUuIrn6S2IpHCX72h+51kr0IPfNrnEgLgaO/hpEcLjvQHMcGBzR974UFO9xE7P1/4515XLd3Vw8Uu7kGzZ+ZEh98vb5BSSk1C3gKuMI0zQ0ddulAx7GGGtCnLFtYmLXvg3owckQ2AIEMH8XF2f0+T6o7GF+7/e3foRkeNL37O4LtE/+LnVveJfL873FiYfKOOouc5OuomzCDhrc+BaB4wVJ8he52O3cuW5/WwbHJUYtoDtVhhZoJjp9JePN7aB4fpZ+/DCceZeeGtyg87BTCmz6g+aNXGTnvMIzMXMo/mk94y0cUHHUWdS/9ldjqp8hQh5IxaT41z/yFPKMFX2EpFa+sAcBpqqQgw6Z1+3pagdHnX4OvsLT9dThFWWwLZGGbr5Co3k7eEV8gVr2d0IYV6IFMtJoNBKs/oua5uxhxxvfxZOfTvOk998Ebdy9faNftJGPaEnIPPY3yB64huvwB/KOnMeqIk8FxcOIXUfvi/TSvfhE0nZGzF4DhYcf7jxB99S9kHHkWTSufxQhkUHz6lVQ+cjPOB49QfNbuS2KRnetoaa6m+PTvkj3nGGqMBM2rXyI7Vk7zysfAcYi+fDuFF/6i02vsKF5fQfOOj0DTCT9+I2O/+Vs8ubs/POxYhMp/3wqOQ+4hp5AxeUG359mXqv/8jljNLkZ/7aZu30N2IkZ484fUvXQ/8bpynMp1jLn4V+i+II6VoP6Nh9m1+UP0QCYjzvgeRmZur57XcRxi5ZvwlUwkXleO1VKPr3gc2+/9KUZmLiVnX4lvxLh9n2gI9fbi6VLgX8D3TdN8qMvuncCoDr+XAH0ahFxb24Jt930cenFxNs2Nbt2xrj5EdXVzn8+RDoqLsw/i1773+q3vhO+5pZXabYQLZxBNvo54lvulT8sdSaOdAx1en140Hrt6CxF/EZ75p+EBtFEKdqyj4LjzaUhkgpZJ1gW/I+zPxM4cR3DMYupCOoSa0Q//KsHpu4gm59TBTsCsZYS9bnmn8s0n8U49gtDmD9GLJmDXbKVy7WoS5gq07GIarCy0Lu2tFY4jvmst+DNJTD8ZXUFw2vFYlRuJrfg3Na/9EzvURMVDN6L5M8DrB8cmvGkVGB6MEVOwytdhj11MS3Acxrh5WNs/wjjkPGpqdi9naE04HFa/iF40ntomC7AInPIjwi/8gdrn7wGPj+DJ3ydj0jy8808j9P4j7PjPn/Efei6a4SG69gMAQnlTiVQ3kyhdgLPiGcru+zGaP4vg535I+NnfsOvvN5L5+avR/JluEzVVg22h55UQXfUKAMFTf0T46Vspe/pugidcCrijj8Iv3Ia1cw1aRh7hf/ycjDN/hlE0HqtqM04sjGfMLBLbP0TLzMcoHIfj2Fi7PkUvHIueXLTdqi8j9PEbgEP5m8+iF4zBiUdI7FiDtX01WiDb/SYWj6AFsvEfdi7Rdx5m+x2Xo+UU44SbsGu2ERg3i/D2tex48Dq0QDZGyTR885eRWP8WdrgJo2A0ePzEN7yFd+IijNIZRJY/QGLDm+h5o9zX7VgYY+a4F82jYXY9cA0ZZ/0vekYudlMVVuVGtMx8nGgIu34nes4IvFMOx4m0gD8Dp7Xe/VaYiIJto+eVYIybh91QjmfMbLTk+64vdF3ba4d4n4ldKTUWeAw41zTNl7vuN01zm1IqopRaaprmm8AFwDN9jrSfPFJjT2lGwWiCJ/y/PbePmARoe4ydBzBGTsWu3oKeN9r9h5mUdcEfyC0tav+Qa0tKejAHffTM9uP0zHz0zHy3NJJdhFE0AaNoPI7jYIxSxNc8R3zNcwD4l3yR8HO/xSpbR2KXW3/vbhZRo2gC1q61+GYc1z6qxjN6JiR7mnbdDrxzPwd2AifUiGfCAuLmG1i71qLnleKZejh2c7V7MxgQPP6b2A3lne4nANCLJ+GZsAijQ6lEzxtFxhevx2mpQ8vIbX9+37zP4YSbiH/8PHb1VgIn/D+3hFUwBj2QnWzLKfjmnwKajmfyYRgFowmceBnhp24h9J//xbfwdDzj5hN6/EacUAPGmNnYTVXoI6fgGaXcaSdWPkr4pT+jZxcR3/QuTnM1/qMuxDtpCa0P/5jIq3eiZRZg7fgIdIOML1xP+LnfAjqeSUuw63Zg1+9CL5pAxuk/Ad0gtupxd6RQXgmRN+4DJ/nvW9MwRs/CiYXxTj4MTzIZa4YHvEESm993E6ptETj2YkqXfo6y914l8uIf3es2Oz92/+tyDwW6QWL98vZfvdOPIbFrLZ5x87Bqt2Ht+AjPpEPwLTyD0L9/RnT5/fgWnE746V/hRFv2eD/EzTewytZhjJ2L01yD3ViefD9qOObr8O7D7t/5tB/jSQ4WGEi96bFfDgSAXyvVHsCfgdOBa0zTXAGcD9yplMoBPgB+P+CR9qDjJGAifegZeQRPuaLbmql3+jHunbB5JZ22a969D1PsStM0Mj5/TXuPSdM0gqf+CLt6K3ZzDXpWAfqIyehFE4h/+jJYCYwxs7s9lzFuHtrm95OrWHXYXjwRDA9YCXwzj0fP6VCyaKhwE3v+aHzTj8GXnEsf3JFFxojJ3cYcPOk7e27XPe2zeu7eZhA44jyMEZOIvH4vocduwIm24FVHdzifjv+Qszs9zjNKETzpu0Tf/SeRl/+Cnj/GXX1r9olu4gw14Jt9IpD88Ig0Ed/4DsQi6AWlBE69Ek9ywRf/EV8m8tLtaJEWvLM+Q/yTl4i8dDs4Dsa42Vhla9FyRrgfEKufIvTotdihRoi24p1zMt4phxF9++94phzuDlvNKkDv8jrb+GYc274OQUfeiYvwXPAH8PgI/ecGrIr1+BZ/Ad+ck7F2rcUON+KdfCiJze9jR5owiiZ2usZgVWwg8uaD+BZ9HiO/FP+SLxB992ESWz9AC+YQPO3HYFtovgz0nGKi7z1C/NNXMMbOxdr+IWgawWWXux/0gFW3A6tiI8aoaRj5o/eIdyBoHW8CGQITgC37U4opr2jkG798lTOPnsRpR0wY6PhSwsFdihlcB6It4uvfJL5+OcaIyfgWfb79Am9vhZ75NVhxMk79UaftiZ0fE376V/gOOQf//O6XO+yvru1g1Wx1h43GIwROuLR9FMzeOI5N9PV7iZtv4Jm2lOCxl+BYCazKDRgl0zrVvZ1kj1rT9rzQaTdVu2sB6Dqt/7oau3YHevEkMs+8ptNx0VVPkNj6AXr+GDzj5uKZsLDH6zO91bUd7NZ6rOrNeMYv7Pf6DY7juB8ItdvwjF+Anjdqz2MiLWiBLBI7PwYr3u03z/3RoRQzEdjadX/Kz+5o6BoaDJt1T8Xg805b2j4Esj+CJ17a7XajZBre6Uf3KsnuL6NoAsHP/oD4Jy/i6eFbR1eapuM/+msYY+e2P0YzPHhKZ3R7bE86fkvxTj2SaO3fu21P/4LT8C84rVex9Zdbhlu0X+fQNM3t0e9l5FDbgja9beuBlvKJXdM0vJ7hs+6pSD093cmqeXwEjh68ees9o1Sf67mapg/oB493+tE4iQjeaUcO2DnFnlI+sYM7ll167EIc/DRfEP/CM4Y6jLSX8pOAAdJjF0KIDtInsUuPXQghgDRJ7B5DlykFhBAiKS0Su8+rE4nJ0nhCCAFpkthzM/00tsjUo0IIAWmS2POz/dS3yNqTQggBaZTYm1tjUmcXQgjSKLE7QIP02oUQIn0SO0BDs9TZhRAiPRJ7lpvY65ojQxyJEEIMvbRI7HntPXYpxQghRFok9syAB59Hp04SuxBCpEdi1zSNvGy/XDwVQgjSJLEDFGT7qZceuxBCpE9iz5PELoQQQBol9vwsN7H3Z4k9IYRIJ71aaCO5SPVbwKmmaW7tsu9nwEVAfXLTnaZp3jaQQfZGaVEmlu1QXhdidFHmYD+9EEIcNPaZ2JVShwJ3AtN6OGQx8CXTNN8eyMD6alJpDgCbyxolsQshhrXelGIuAS4FynrYvxj4iVLqI6XUH5VSgQGLrg9GFmQQ9HvYUj6wK9QLIUSq2WeP3TTNiwGU2nMRXKVUFrAKuALYCNwHXA1c1ZcgCguz+nJ4J8XF2bvjGZfPjuqWTtuGi+H4mnsibeGSdnANx3bYr8WsTdNsAZa1/a6UuhW4hz4m9traln5d9Cwuzqa6encPfXRRBs++W8OusgZ8XqPP50tVXdthOJO2cEk7uNK1HXRd22uHeL9GxSilximlLuqwSQPi+3PO/TFpVA6W7bCtMv3+kEII0Vv7O9wxDNyilJqolNJwa/GP7n9Y/TN5dC4A63c0DFUIQggx5PqV2JVSTyulFpumWQ18E3gCMHF77LcOYHx9kpPpY3RxJuu21e/7YCGESFO9rrGbpjmhw8/LOvz8L+BfAxtW/80Yl8/rH5YRT9h4PWlz/5UQQvRa2mW+GRPyiSVsNpc1DnUoQggxJNIusauxeWgarN0q5RghxPCUdok9I+Bl6uhc3llbIfPGCCGGpbRL7AAnLB5LdUOEVRtqhjoUIYQYdGmZ2BdMK6IoN8AL728f6lCEEGLQpWViN3SdExaPZf3ORraUNw11OEIIMajSMrEDHDV3FEG/wfPv7xjqUIQQYlClbWIP+j0cPa+U9z+torYxMtThCCHEoEnbxA5wwqKx6Do8+sbmoQ5FCCEGTVon9sLcACctGcdbH1ewuUxq7UKI4SGtEzvAKYePJzfLxx///RHbZdZHIcQwkPaJPej38N/nzkfXNW76vw/4eHPtUIckhBAHVNondoAxxVlcdcFiinOD/PafH/HGhz2t8ieEEKlvWCR2gPxsPz/+8kJmjM/j3mfW8X/PrycUSQx1WEIIMeD2a2m8VBP0e/je2fP4x8sbeXnlTt5YU8a0MXmce/wURhf3f91VIYQ4mAybHnsbj6Fz/onTuObCJRw1t5Ttlc387wMref79HbRGhmxVPyGEGDDDqsfe0fiSbMaXZLPssPH85fFPeOilDTz51lYuPnUmxXkBCnMCw2pBbCFE+hi2ib1NfrafK89fyNaKJu568lN++88PAfB5dQ6bWcK5x08h6B/2zSSESCG9ylhKqRzgLeBU0zS3dtk3H7gLyAFeB75lmmbKXZWcUJLDVRcs4v11VRi6xoadjbzxURnrttdzwcmKWRMKhjpEIYTolX3W2JVShwLLgWk9HPIgcJlpmtNwF7O+ZODCG1xt88ssnTOKCz83nf/5rwXYtsOtD63mpgdXsmJdFY4ji3cIIQ5uvbl4eglwKbDH4G+l1HggaJrmO8lN9wFnD1h0Q0yNy+fGSw7l3OOn0Nga4/bHPuaWv63itdW7WLWhmu2VzTiOQygSl4QvhDho7LMUY5rmxQBKqe52lwLlHX4vB8YMSGQHCa/H4ORDxnHi4rG8tnoXT769jfufNdv3ZwW9tITjjCrM4LQjJnDozJFomjaEEQshhrv9vSqoAx27qhpg9/UkhYX9H0NeXJzd78f21Tkn53D2SdMpq2klHEmwYUc9n26tY1RhJu98UsEdT6xl/a4mvv3FuWQEvAAkLJs3Vu9iwbQR5GX7D1hsg9kOBztpC5e0g2s4tsP+JvadwKgOv5fQTclmX2prW/q18HRxcTbV1YM/sZcP8AUMFk8tYvHUIgCOn1/KU29v5bHlW/h4Uw0Bn4HXY+A4DlsrminKDfCDc+ZRkBPg8Te3UJKfwSJV3P4BsD+Gqh0ORtIWLmkHV7q2g65re+0Q71diN01zm1IqopRaaprmm8AFwDP7c85Upesapy2diBqXzz9e3kBmwC3R1DVFOPvYyTz33nZu+dsqJo/O5YP11QA88tomzjpmMuNGZvPCih2MLMjglMPHo++jlFPTECYSsygpzMBjDLt7zIQQ+9CvxK6Uehq4xjTNFcD5wJ3JIZEfAL8fwPhSzrSxeVz91SV7bJ87pYib/+8DPlhfzalHTGDe5EL+74X13PvMOgAMXcOyHT7cWENupo/8bD+by5qIxi2++tnpFOYEMAyNZ9/d3r7c34SSbL571lyKimQ6BCHEbtoQj+aYAGxJtVJMf22vbGbN5lo+d+h4dF3Dth02lzWxs7qFeVOKWGFW8eZH5ViOQ01jhBF5QSKxBNUNnZf2O3Z+KaOLs/jnKxuJJWw0zZ0qIT/Lz8iCDHKzfNQ0hNE0jdHFmSydPYqivABBvwfLsmkJJ8jN9KHrGo7jYNlO2vT8U+09caBIO7jStR06lGImAlu77pfEfpALRRIsX1OO36tj2Q55WX4WTisGYEdVC6s31uDzeWhsilDbFKGyLkRDS5SivCAasK2ymYTltq3G7ivdHkNnZEGQcDRBQ3OMaWNzCUctsjK8TCjJ5sONtWQFPcyaWEBpUSavry7D5zUIRxNEYhbzphSyZPoIHOD9T6tYub4av9dg3uRCTlg8Bo+hs72yBdtxGJkfJOD30ByKs2JdFbGExXELRhPwuV8Yq+pDbClvZvH0Ygzd/YAJRxOsWFdFUV6QwtwATa0xJpRk7/MDaDi8J3pD2sGVru0giX0Y2Fs7NIVifLy5lpZQnNZIAsPQyAp6qW4IU1Ebwus1KMj28+m2erIzvFTWhahtijJtTC6xhM3WCve8+dl+vB6dgNdA17X27W2mjMnFsmy2lDeTGfCQleGjsi7UY8y5mT4WqWJ2VrWwfmcjAIfMGMEx80r5cFMtb64pp7XLtMqZAQ+LVDHZGT5qGyOUFmUyoSSb+uYoWyubyfB7MLwGtfUhLNshK+AlErOIWzZ+r0FxXoBIzGJEXpBFagQJ2+bZd7czdkQWh88qoaYxTFlNK15DJyfTh+2AbTtkZ3jZWd3CB+trKC3MYOG0YorygoA76snQNTRNw3acHq+PbK1o4tNt9Zy4eGyvvx3F4hb1zVFG5Af7PIRW/m240rUdJLEPAwPZDrbt0BqJk53hA2BnVQvldSHmTynC69mdkGoaw6xaX4PH0Jg1sYAR+RkArN1ax7trK6lvjrJgahE5mT6qGyJEYgkyAl7U2Dzils1Tb23lk611FOYGOXJOCQnL4T/LtwDu9Yb5U4s4aclYGlpitEbiZAa8rFpfzaqNNSQSNjmZPuqbo+3xBP0G0ZiN36cT8HkwdI2WcJyg34PXoxOOJmgOxdE0aHvL68lkDDAiP0hVfXivbeP3GkTjFpoGJQUZNLXGaI0kCPoNdE0jErOYPbGA8SXZBHwe6pojfLixBq/HoLy2FceB2ZMKaA7FKatpxWPo+Lw608flM6owA03TGFWQQVVDmDWbatm4qxHLdlikijn3uCmU1YZ455MKivICTBqVS3aml7LqVnbVtNIcihGJWWiaxjHzSzlkTik7yxppjcSpb45S1xylJRynoSXKxp2NHD6rhJkT8vlgfTVLZoxkdFEmjuMQS7gfgrG4RW1TBNuB0UWZ7e+FHdUtLJhahKHrNLS47V+cF6Q5FOP1D8uorA8zc3w+K9dXM7ook9OXTkRP/i28Hp2Nuxp5acVOvnjsZII+g101rYwbkUVuVuehwLbtuOVKx6GhOUp+tn+PDzfLtqlvjmLbTvv7r6uu/zbKalrZUdXCkhkj9jlIIRJLYOh6p/d9G8dxqG2MkJ3pw+819vqh3lE8YeM4zn5PMCiJfRhI1XawbBtd09r/wa7f0UDcshk/MpusYPfDQDv+wwhF4myrbCHoNxg/0h2rPGJETo9tEY1ZeL06m8uaMLfX0xpJcPisEt77tJINOxpYMK2YiaNySFg2LeF4e0+8qTVGwG+wWI2goTnKq6vLKK9tJS/LT06mj+ZQzL1Ooet8uKmG2sYIDu4H1OyJ7hxDIwsyyMn08cirmyjMCbB4ejGW5X6IrtlcR0u485TRo4szmTe5CF2Hp97e1v5hlBnwEI5a7R9IAD6P+w0j4DOSyTvWY5v7vDoj8jLYWd3S6UMuN9NHLGETjibweXVi8d23o0wdk4vjwMZd7jcrv88gHrfbY8jP9tPQEsVx3H3RmNX+/xH5QQxdo7w21OmDNCfDSyxhE4lZ7edo+xtu2NlAayTBqMIMbNuhsj5MToaXnEw/uo77fnGgvLaVWMKNc3JpDiPygziA19DJzfKz0qyirjlKZsDDxFE5OA58uLEGy3aYVJpDbVOE7KCXqWPz2FLWRGbQS1FugKLcANG4zXPvbScj4GFiSQ6byhqZO6mQSNyirKaVUDRBY0sMn0cnO8NHQ0uUWRMLyAp62VHVQlVDmCVqBDurW2hsjTG6OJPCnAAr1lURt2zmTymipCCDE5eMJbMfQ54lsQ8D0g67HQxtkbBsYnEbr2fP3t7OqhZG5Ac79dhsx8G2HeIJm/LaEMV5gfZvTOBedN9S3oTf5364WLbDtopmWsNxSoszKc4LtvcWE5bNu2sr0TwGiViCDL+H/Gw/BTkBsjO8GLqGAzz6+mYaW2IsO3w8q9ZXU1EXci/AZ/tpDsXJCnooyg3S2BrjtQ/LyAp6mDOpkKmjc3lvXRXZGV6K84JEYxbrdzZSWpjB4ukjGJEXZFNZE5NG5bByfRXvfepOqjdxVA6xhI3X0JgzuZDf/fMjRuYHOeWICZTXhtha0cSW8mYc22H6+HxyMn1s3NmA7cD8KUXsqm4hFE3gOG57OQ6MLAgyuiiTcNTinU8qCMfc0l0kZtEcijOpNIe5U4spq2pma3kzmgYzxuczqiiTZ97ZxoSSHGoaI1TUtTJldC7RuEV1Q6T9Q3bB1CJaIwmq6kNMLs1lzZZagn4PU0bn4vMYTByVTWVdmOZwjOygjw831eA47reH3Ewf76+roqQgg3Ejs9hV3UpFXYgZ4/PJzfKxZnMdkZjFD86Zx5TRuX1+j0liHwakHXaTtnAd7O3Q8drEgRCNW+51lX20g5P8kND13XFEYglCkQQFOYE9YtZ1rVcll7bjD9Rr3Fdil4nGhRCD7kAPr/X3soataRpd827A52kfsdVRX2MeyiHE6TF4WQghRDtJ7EIIkWYksQshRJqRxC6EEGlGErsQQqQZSexCCJFmhnq4owGdx5D21f48Np1IO+wmbeGSdnClYzt0eE3djusc6huUjgTeGMoAhBAihR0FLO+6cagTux9YgrsItjWUgQghRAoxcJclfR+Idt051IldCCHEAJOLp0IIkWYksQshRJqRxC6EEGlGErsQQqQZSexCCJFmJLELIUSakcQuhBBpZqinFOg3pdR5wE8BL/Bb0zRvG+KQBo1S6hVgBNC2AvI3gWzg10AQ+Idpmj8dovAOOKVUDvAWcKppmluVUifQzWtXSs0H7gJygNeBb5mmmRiaqAdeN+1wL+7d3K3JQ64zTfPRdG4HpdTPgHOSvz5lmub/DNf3Q0cp2WNXSo0GbsR9E88HvqGUmjmkQQ0SpZQGTAPmmaY53zTN+cBHwD3AGcAMYIlS6nNDF+WBo5Q6FPcW6mnJ34P0/NofBC4zTXMaoAGXDH7EB0bXdkhaDBzd9r4wTfPR5Pa0bIdkAj8JWICbBxYppf6LYfh+6ColEztwAvCyaZp1pmm2Ao8AZw1xTINFJf//vFLqQ6XUZcAhwAbTNLckeyAPAmcPWYQH1iXApUBZ8vduX7tSajwQNE3zneRx95FebdKpHZRSGcA44B6l1EdKqeuUUnqat0M58N+macZM04wDn+J+0A3H90MnqVqKKcX9o7Ypx/0HPhzkAy8B38EtQ70K3Mye7TFm0CMbBKZpXgygVNvnW7fvhTF72Z4WummHEuBl4P8BjcCTwNeBj0nTdjBN85O2n5VSU3FLMn9gGL4fukrVxK4DHSe50QB7iGIZVKZpvg283fa7Uupu4Ho6z/A2bNqDnt8Lw+o9YprmZuDMtt+VUn8AvgKsJc3bQSk1C3gKuAJI0Lk8NSzfD6laitmJO7NZmxJ2fzVPa0qpI5VSn+mwSQO2Mkzbg57fC8PqPaKUmqOU+mKHTRruxfW0bgel1FLcb7BXmqZ5P/J+AFI3sb8IfEYpVZysLX4ReHaIYxosecAvlVIBpVQ28FXgJ4BSSk1RShnAecAzQxjjYHqXbl67aZrbgEjyHz7ABaR3m2jAb5VS+UopL/AN4NF0bgel1FjgMeA80zQfSm6W9wMpmthN09wFXAW8AqwG/maa5ntDGtQgMU3zSdyvnauAlcA9yfLMhcC/cL96r8O9oJz2TNOM0PNrPx/4jVJqHZAF/H4oYhwMpml+BNwEvInbDqtN0/x7cne6tsPlQAD4tVJqtVJqNe574UKG+ftB5mMXQog0k5I9diGEED2TxC6EEGlGErsQQqQZSexCCJFmJLELIUSakcQuhBBpRhK7EEKkGUnsQgiRZv4/sg0/n71McUkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_12 (InputLayer)          [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_33 (LSTM)                 (None, 45, 24)       3744        ['input_12[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 45, 24)       0           ['lstm_33[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_34 (LSTM)                 (None, 45, 16)       2624        ['dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 45, 16)       0           ['lstm_34[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_35 (LSTM)                 (None, 32)           6272        ['dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " dense_22 (Dense)               (None, 40)           1320        ['lstm_35[0][0]']                \n",
      "                                                                                                  \n",
      " dense_23 (Dense)               (None, 5)            205         ['dense_22[0][0]']               \n",
      "                                                                                                  \n",
      " tf.unstack_11 (TFOpLambda)     [(None,),            0           ['dense_23[0][0]']               \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_55 (TFOpLambda)  (None, 1)           0           ['tf.unstack_11[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_22 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_55[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_59 (TFOpLambda)  (None, 1)           0           ['tf.unstack_11[0][4]']          \n",
      "                                                                                                  \n",
      " tf.math.multiply_33 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_22[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_23 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_59[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_34 (TFOpLambd  (None, 1)           0           ['tf.math.multiply_33[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_56 (TFOpLambda)  (None, 1)           0           ['tf.unstack_11[0][1]']          \n",
      "                                                                                                  \n",
      " tf.expand_dims_58 (TFOpLambda)  (None, 1)           0           ['tf.unstack_11[0][3]']          \n",
      "                                                                                                  \n",
      " tf.math.multiply_35 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_23[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_22 (TFOpL  (None, 1)           0           ['tf.math.multiply_34[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.softplus_22 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_56[0][0]']      \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_57 (TFOpLambda)  (None, 1)           0           ['tf.unstack_11[0][2]']          \n",
      "                                                                                                  \n",
      " tf.math.softplus_23 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_58[0][0]']      \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_23 (TFOpL  (None, 1)           0           ['tf.math.multiply_35[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack_11 (TFOpLambda)       (None, 5, 1)         0           ['tf.__operators__.add_22[0][0]',\n",
      "                                                                  'tf.math.softplus_22[0][0]',    \n",
      "                                                                  'tf.expand_dims_57[0][0]',      \n",
      "                                                                  'tf.math.softplus_23[0][0]',    \n",
      "                                                                  'tf.__operators__.add_23[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _1_CCMP_t+1_0.16\n",
      "Epoch 1/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 3.4947\n",
      "Epoch 1: val_loss improved from inf to 4.24211, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 9s 60ms/step - loss: 3.4991 - val_loss: 4.2421 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 3.0717\n",
      "Epoch 2: val_loss improved from 4.24211 to 3.21643, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 3.0724 - val_loss: 3.2164 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.9611\n",
      "Epoch 3: val_loss improved from 3.21643 to 2.87904, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 1.9585 - val_loss: 2.8790 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/66 [============================>.] - ETA: 0s - loss: 1.5933\n",
      "Epoch 4: val_loss improved from 2.87904 to 2.86094, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 4s 57ms/step - loss: 1.5916 - val_loss: 2.8609 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.4532\n",
      "Epoch 5: val_loss improved from 2.86094 to 2.54748, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 1.4532 - val_loss: 2.5475 - lr: 1.0000e-04\n",
      "Epoch 6/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.3628\n",
      "Epoch 6: val_loss did not improve from 2.54748\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 1.3608 - val_loss: 2.6116 - lr: 1.0000e-04\n",
      "Epoch 7/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.2965\n",
      "Epoch 7: val_loss did not improve from 2.54748\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 1.2951 - val_loss: 2.7007 - lr: 9.9000e-05\n",
      "Epoch 8/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.2516\n",
      "Epoch 8: val_loss did not improve from 2.54748\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 1.2503 - val_loss: 2.9709 - lr: 9.8010e-05\n",
      "Epoch 9/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.2135\n",
      "Epoch 9: val_loss improved from 2.54748 to 2.40719, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 1.2170 - val_loss: 2.4072 - lr: 9.7030e-05\n",
      "Epoch 10/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1933\n",
      "Epoch 10: val_loss did not improve from 2.40719\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 1.1933 - val_loss: 2.7390 - lr: 9.7030e-05\n",
      "Epoch 11/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1967\n",
      "Epoch 11: val_loss did not improve from 2.40719\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 1.1957 - val_loss: 2.5134 - lr: 9.6060e-05\n",
      "Epoch 12/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1582\n",
      "Epoch 12: val_loss improved from 2.40719 to 2.35122, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 1.1582 - val_loss: 2.3512 - lr: 9.5099e-05\n",
      "Epoch 13/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1404\n",
      "Epoch 13: val_loss did not improve from 2.35122\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 1.1414 - val_loss: 2.3575 - lr: 9.5099e-05\n",
      "Epoch 14/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1299\n",
      "Epoch 14: val_loss improved from 2.35122 to 2.22851, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 1.1294 - val_loss: 2.2285 - lr: 9.4148e-05\n",
      "Epoch 15/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1362\n",
      "Epoch 15: val_loss improved from 2.22851 to 2.16220, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 1.1354 - val_loss: 2.1622 - lr: 9.4148e-05\n",
      "Epoch 16/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0975\n",
      "Epoch 16: val_loss improved from 2.16220 to 2.13142, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 1.0973 - val_loss: 2.1314 - lr: 9.4148e-05\n",
      "Epoch 17/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0877\n",
      "Epoch 17: val_loss improved from 2.13142 to 2.06926, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 1.0919 - val_loss: 2.0693 - lr: 9.4148e-05\n",
      "Epoch 18/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0801\n",
      "Epoch 18: val_loss did not improve from 2.06926\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 1.0844 - val_loss: 2.1535 - lr: 9.4148e-05\n",
      "Epoch 19/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0876\n",
      "Epoch 19: val_loss did not improve from 2.06926\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 1.0855 - val_loss: 2.1040 - lr: 9.3207e-05\n",
      "Epoch 20/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0750\n",
      "Epoch 20: val_loss did not improve from 2.06926\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 1.0742 - val_loss: 2.1286 - lr: 9.2274e-05\n",
      "Epoch 21/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0692\n",
      "Epoch 21: val_loss did not improve from 2.06926\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 1.0731 - val_loss: 2.1873 - lr: 9.1352e-05\n",
      "Epoch 22/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0481\n",
      "Epoch 22: val_loss improved from 2.06926 to 2.00514, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 1.0481 - val_loss: 2.0051 - lr: 9.0438e-05\n",
      "Epoch 23/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0292\n",
      "Epoch 23: val_loss did not improve from 2.00514\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 1.0298 - val_loss: 2.0408 - lr: 9.0438e-05\n",
      "Epoch 24/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0414\n",
      "Epoch 24: val_loss improved from 2.00514 to 1.90162, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 1.0414 - val_loss: 1.9016 - lr: 8.9534e-05\n",
      "Epoch 25/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0350\n",
      "Epoch 25: val_loss did not improve from 1.90162\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 1.0345 - val_loss: 2.0127 - lr: 8.9534e-05\n",
      "Epoch 26/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0261\n",
      "Epoch 26: val_loss did not improve from 1.90162\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 1.0261 - val_loss: 1.9980 - lr: 8.8638e-05\n",
      "Epoch 27/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0310\n",
      "Epoch 27: val_loss did not improve from 1.90162\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 1.0301 - val_loss: 1.9688 - lr: 8.7752e-05\n",
      "Epoch 28/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0146\n",
      "Epoch 28: val_loss did not improve from 1.90162\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 1.0167 - val_loss: 1.9352 - lr: 8.6875e-05\n",
      "Epoch 29/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0167\n",
      "Epoch 29: val_loss improved from 1.90162 to 1.89175, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 1.0159 - val_loss: 1.8917 - lr: 8.6006e-05\n",
      "Epoch 30/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0009\n",
      "Epoch 30: val_loss improved from 1.89175 to 1.85581, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 1.0009 - val_loss: 1.8558 - lr: 8.6006e-05\n",
      "Epoch 31/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9960\n",
      "Epoch 31: val_loss did not improve from 1.85581\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.9949 - val_loss: 1.9251 - lr: 8.6006e-05\n",
      "Epoch 32/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0081\n",
      "Epoch 32: val_loss did not improve from 1.85581\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 1.0072 - val_loss: 1.8714 - lr: 8.5146e-05\n",
      "Epoch 33/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9993\n",
      "Epoch 33: val_loss improved from 1.85581 to 1.82858, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.9993 - val_loss: 1.8286 - lr: 8.4294e-05\n",
      "Epoch 34/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9898\n",
      "Epoch 34: val_loss did not improve from 1.82858\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.9901 - val_loss: 1.9036 - lr: 8.4294e-05\n",
      "Epoch 35/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9874\n",
      "Epoch 35: val_loss improved from 1.82858 to 1.80622, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.9882 - val_loss: 1.8062 - lr: 8.3451e-05\n",
      "Epoch 36/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9761\n",
      "Epoch 36: val_loss did not improve from 1.80622\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.9767 - val_loss: 1.8168 - lr: 8.3451e-05\n",
      "Epoch 37/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9741\n",
      "Epoch 37: val_loss did not improve from 1.80622\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.9744 - val_loss: 1.8783 - lr: 8.2617e-05\n",
      "Epoch 38/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9874\n",
      "Epoch 38: val_loss did not improve from 1.80622\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.9874 - val_loss: 1.8860 - lr: 8.1791e-05\n",
      "Epoch 39/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9953\n",
      "Epoch 39: val_loss did not improve from 1.80622\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.9940 - val_loss: 1.8512 - lr: 8.0973e-05\n",
      "Epoch 40/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9720\n",
      "Epoch 40: val_loss did not improve from 1.80622\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.9720 - val_loss: 1.8138 - lr: 8.0163e-05\n",
      "Epoch 41/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9698\n",
      "Epoch 41: val_loss did not improve from 1.80622\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.9693 - val_loss: 1.8517 - lr: 7.9361e-05\n",
      "Epoch 42/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9667\n",
      "Epoch 42: val_loss improved from 1.80622 to 1.77519, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.9684 - val_loss: 1.7752 - lr: 7.8568e-05\n",
      "Epoch 43/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9743\n",
      "Epoch 43: val_loss did not improve from 1.77519\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.9707 - val_loss: 1.8194 - lr: 7.8568e-05\n",
      "Epoch 44/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9708\n",
      "Epoch 44: val_loss improved from 1.77519 to 1.75214, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.9701 - val_loss: 1.7521 - lr: 7.7782e-05\n",
      "Epoch 45/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9750\n",
      "Epoch 45: val_loss improved from 1.75214 to 1.74817, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.9750 - val_loss: 1.7482 - lr: 7.7782e-05\n",
      "Epoch 46/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9637\n",
      "Epoch 46: val_loss did not improve from 1.74817\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.9632 - val_loss: 1.7650 - lr: 7.7782e-05\n",
      "Epoch 47/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9675\n",
      "Epoch 47: val_loss did not improve from 1.74817\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.9652 - val_loss: 1.8058 - lr: 7.7004e-05\n",
      "Epoch 48/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9644\n",
      "Epoch 48: val_loss did not improve from 1.74817\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.9672 - val_loss: 1.8290 - lr: 7.6234e-05\n",
      "Epoch 49/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9422\n",
      "Epoch 49: val_loss did not improve from 1.74817\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.9422 - val_loss: 1.7913 - lr: 7.5472e-05\n",
      "Epoch 50/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9445\n",
      "Epoch 50: val_loss improved from 1.74817 to 1.73296, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.9434 - val_loss: 1.7330 - lr: 7.4717e-05\n",
      "Epoch 51/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9484\n",
      "Epoch 51: val_loss did not improve from 1.73296\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 4s 61ms/step - loss: 0.9471 - val_loss: 1.7799 - lr: 7.4717e-05\n",
      "Epoch 52/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9496\n",
      "Epoch 52: val_loss did not improve from 1.73296\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 4s 59ms/step - loss: 0.9496 - val_loss: 1.7596 - lr: 7.3970e-05\n",
      "Epoch 53/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9428\n",
      "Epoch 53: val_loss did not improve from 1.73296\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 4s 57ms/step - loss: 0.9419 - val_loss: 1.7421 - lr: 7.3230e-05\n",
      "Epoch 54/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9494\n",
      "Epoch 54: val_loss did not improve from 1.73296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.9493 - val_loss: 1.7371 - lr: 7.2498e-05\n",
      "Epoch 55/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9510\n",
      "Epoch 55: val_loss improved from 1.73296 to 1.73097, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.9492 - val_loss: 1.7310 - lr: 7.1773e-05\n",
      "Epoch 56/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9471\n",
      "Epoch 56: val_loss improved from 1.73097 to 1.72846, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.9492 - val_loss: 1.7285 - lr: 7.1773e-05\n",
      "Epoch 57/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9333\n",
      "Epoch 57: val_loss improved from 1.72846 to 1.72148, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.9357 - val_loss: 1.7215 - lr: 7.1773e-05\n",
      "Epoch 58/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9369\n",
      "Epoch 58: val_loss improved from 1.72148 to 1.70311, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.9369 - val_loss: 1.7031 - lr: 7.1773e-05\n",
      "Epoch 59/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9421\n",
      "Epoch 59: val_loss improved from 1.70311 to 1.69550, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.9418 - val_loss: 1.6955 - lr: 7.1773e-05\n",
      "Epoch 60/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9432\n",
      "Epoch 60: val_loss did not improve from 1.69550\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9432 - val_loss: 1.7043 - lr: 7.1773e-05\n",
      "Epoch 61/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9416\n",
      "Epoch 61: val_loss did not improve from 1.69550\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9417 - val_loss: 1.7648 - lr: 7.1055e-05\n",
      "Epoch 62/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9362\n",
      "Epoch 62: val_loss did not improve from 1.69550\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.9362 - val_loss: 1.7498 - lr: 7.0345e-05\n",
      "Epoch 63/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9331\n",
      "Epoch 63: val_loss did not improve from 1.69550\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9345 - val_loss: 1.6976 - lr: 6.9641e-05\n",
      "Epoch 64/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9388\n",
      "Epoch 64: val_loss did not improve from 1.69550\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.9376 - val_loss: 1.7246 - lr: 6.8945e-05\n",
      "Epoch 65/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9355\n",
      "Epoch 65: val_loss did not improve from 1.69550\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9360 - val_loss: 1.6978 - lr: 6.8255e-05\n",
      "Epoch 66/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9223\n",
      "Epoch 66: val_loss improved from 1.69550 to 1.69293, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.9223 - val_loss: 1.6929 - lr: 6.7573e-05\n",
      "Epoch 67/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9211\n",
      "Epoch 67: val_loss did not improve from 1.69293\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9211 - val_loss: 1.7192 - lr: 6.7573e-05\n",
      "Epoch 68/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9280\n",
      "Epoch 68: val_loss did not improve from 1.69293\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9280 - val_loss: 1.7189 - lr: 6.6897e-05\n",
      "Epoch 69/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9208\n",
      "Epoch 69: val_loss did not improve from 1.69293\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9208 - val_loss: 1.6973 - lr: 6.6228e-05\n",
      "Epoch 70/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9279\n",
      "Epoch 70: val_loss did not improve from 1.69293\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9279 - val_loss: 1.7012 - lr: 6.5566e-05\n",
      "Epoch 71/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9220\n",
      "Epoch 71: val_loss did not improve from 1.69293\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9263 - val_loss: 1.7418 - lr: 6.4910e-05\n",
      "Epoch 72/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9173\n",
      "Epoch 72: val_loss did not improve from 1.69293\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9159 - val_loss: 1.7084 - lr: 6.4261e-05\n",
      "Epoch 73/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9315\n",
      "Epoch 73: val_loss did not improve from 1.69293\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9308 - val_loss: 1.7259 - lr: 6.3619e-05\n",
      "Epoch 74/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9339\n",
      "Epoch 74: val_loss did not improve from 1.69293\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9319 - val_loss: 1.7074 - lr: 6.2982e-05\n",
      "Epoch 75/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9224\n",
      "Epoch 75: val_loss did not improve from 1.69293\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9242 - val_loss: 1.7340 - lr: 6.2353e-05\n",
      "Epoch 76/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9103\n",
      "Epoch 76: val_loss did not improve from 1.69293\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9103 - val_loss: 1.7048 - lr: 6.1729e-05\n",
      "Epoch 77/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9223\n",
      "Epoch 77: val_loss improved from 1.69293 to 1.69192, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.9223 - val_loss: 1.6919 - lr: 6.1112e-05\n",
      "Epoch 78/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9184\n",
      "Epoch 78: val_loss improved from 1.69192 to 1.67977, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9196 - val_loss: 1.6798 - lr: 6.1112e-05\n",
      "Epoch 79/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9180\n",
      "Epoch 79: val_loss did not improve from 1.67977\n",
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9180 - val_loss: 1.6995 - lr: 6.1112e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9252\n",
      "Epoch 80: val_loss did not improve from 1.67977\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9252 - val_loss: 1.7138 - lr: 6.0501e-05\n",
      "Epoch 81/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9163\n",
      "Epoch 81: val_loss improved from 1.67977 to 1.67496, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.9163 - val_loss: 1.6750 - lr: 5.9896e-05\n",
      "Epoch 82/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9213\n",
      "Epoch 82: val_loss did not improve from 1.67496\n",
      "\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9213 - val_loss: 1.7275 - lr: 5.9896e-05\n",
      "Epoch 83/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9027\n",
      "Epoch 83: val_loss did not improve from 1.67496\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 5.8703650938696225e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9023 - val_loss: 1.6811 - lr: 5.9297e-05\n",
      "Epoch 84/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9067\n",
      "Epoch 84: val_loss improved from 1.67496 to 1.66110, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9067 - val_loss: 1.6611 - lr: 5.8704e-05\n",
      "Epoch 85/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9101\n",
      "Epoch 85: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 5.811661550978897e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9110 - val_loss: 1.6979 - lr: 5.8704e-05\n",
      "Epoch 86/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9114\n",
      "Epoch 86: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 5.753545068728272e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9123 - val_loss: 1.6986 - lr: 5.8117e-05\n",
      "Epoch 87/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9080\n",
      "Epoch 87: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 5.6960095243994146e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9080 - val_loss: 1.7188 - lr: 5.7535e-05\n",
      "Epoch 88/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9114\n",
      "Epoch 88: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 5.639049515593797e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9111 - val_loss: 1.6708 - lr: 5.6960e-05\n",
      "Epoch 89/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9185\n",
      "Epoch 89: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 5.5826589195930865e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9187 - val_loss: 1.6814 - lr: 5.6390e-05\n",
      "Epoch 90/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9029\n",
      "Epoch 90: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 5.526832333998754e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.9029 - val_loss: 1.6813 - lr: 5.5827e-05\n",
      "Epoch 91/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9058\n",
      "Epoch 91: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 5.471563996252371e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9058 - val_loss: 1.6944 - lr: 5.5268e-05\n",
      "Epoch 92/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8939\n",
      "Epoch 92: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 5.416848503955407e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8939 - val_loss: 1.6864 - lr: 5.4716e-05\n",
      "Epoch 93/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9056\n",
      "Epoch 93: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 5.362680094549433e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9038 - val_loss: 1.7125 - lr: 5.4168e-05\n",
      "Epoch 94/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9022\n",
      "Epoch 94: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 5.309053365635918e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9001 - val_loss: 1.7118 - lr: 5.3627e-05\n",
      "Epoch 95/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8977\n",
      "Epoch 95: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 5.2559629148163366e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.8964 - val_loss: 1.7149 - lr: 5.3091e-05\n",
      "Epoch 96/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8968\n",
      "Epoch 96: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 5.2034033396921584e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.8938 - val_loss: 1.6963 - lr: 5.2560e-05\n",
      "Epoch 97/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8999\n",
      "Epoch 97: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 5.1513692378648555e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8994 - val_loss: 1.7731 - lr: 5.2034e-05\n",
      "Epoch 98/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9024\n",
      "Epoch 98: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 5.099855567095801e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9024 - val_loss: 1.6820 - lr: 5.1514e-05\n",
      "Epoch 99/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8988\n",
      "Epoch 99: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 5.048856924986467e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.8977 - val_loss: 1.6858 - lr: 5.0999e-05\n",
      "Epoch 100/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8898\n",
      "Epoch 100: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 4.998368269298226e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8912 - val_loss: 1.6639 - lr: 5.0489e-05\n",
      "Epoch 101/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8925\n",
      "Epoch 101: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 4.9483845577924514e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8917 - val_loss: 1.6966 - lr: 4.9984e-05\n",
      "Epoch 102/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9020\n",
      "Epoch 102: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 4.898900748230517e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9056 - val_loss: 1.7080 - lr: 4.9484e-05\n",
      "Epoch 103/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8904\n",
      "Epoch 103: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 4.849911798373796e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.8903 - val_loss: 1.7029 - lr: 4.8989e-05\n",
      "Epoch 104/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8941\n",
      "Epoch 104: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 4.801412665983662e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.8937 - val_loss: 1.7331 - lr: 4.8499e-05\n",
      "Epoch 105/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9005\n",
      "Epoch 105: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 4.75339866898139e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9005 - val_loss: 1.6760 - lr: 4.8014e-05\n",
      "Epoch 106/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8957\n",
      "Epoch 106: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 4.705864765128354e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.8962 - val_loss: 1.6953 - lr: 4.7534e-05\n",
      "Epoch 107/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8921\n",
      "Epoch 107: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 4.658806272345828e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.8912 - val_loss: 1.7018 - lr: 4.7059e-05\n",
      "Epoch 108/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8845\n",
      "Epoch 108: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 4.612218148395186e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.8845 - val_loss: 1.6993 - lr: 4.6588e-05\n",
      "Epoch 109/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8864\n",
      "Epoch 109: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 4.566096071357606e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.8864 - val_loss: 1.7109 - lr: 4.6122e-05\n",
      "Epoch 110/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8996\n",
      "Epoch 110: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 4.52043499899446e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9001 - val_loss: 1.7284 - lr: 4.5661e-05\n",
      "Epoch 111/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8873\n",
      "Epoch 111: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 4.475230609386927e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.8919 - val_loss: 1.6829 - lr: 4.5204e-05\n",
      "Epoch 112/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8943\n",
      "Epoch 112: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 4.43047822045628e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.8943 - val_loss: 1.7039 - lr: 4.4752e-05\n",
      "Epoch 113/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9010\n",
      "Epoch 113: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 4.386173510283697e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9008 - val_loss: 1.7393 - lr: 4.4305e-05\n",
      "Epoch 114/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8923\n",
      "Epoch 114: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 4.342311796790454e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.8925 - val_loss: 1.6955 - lr: 4.3862e-05\n",
      "Epoch 115/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8863\n",
      "Epoch 115: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 4.298888758057728e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.8853 - val_loss: 1.7181 - lr: 4.3423e-05\n",
      "Epoch 116/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8866\n",
      "Epoch 116: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 4.2558997120067946e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.8876 - val_loss: 1.6844 - lr: 4.2989e-05\n",
      "Epoch 117/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8829\n",
      "Epoch 117: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 4.2133406968787315e-05.\n",
      "66/66 [==============================] - 3s 53ms/step - loss: 0.8811 - val_loss: 1.6827 - lr: 4.2559e-05\n",
      "Epoch 118/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8927\n",
      "Epoch 118: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 4.1712073907547166e-05.\n",
      "66/66 [==============================] - 3s 38ms/step - loss: 0.8910 - val_loss: 1.7145 - lr: 4.2133e-05\n",
      "Epoch 119/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8954\n",
      "Epoch 119: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 4.1294954717159274e-05.\n",
      "66/66 [==============================] - 3s 38ms/step - loss: 0.8925 - val_loss: 1.7176 - lr: 4.1712e-05\n",
      "Epoch 120/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8904\n",
      "Epoch 120: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 4.08820061784354e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.8917 - val_loss: 1.6910 - lr: 4.1295e-05\n",
      "Epoch 121/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8754\n",
      "Epoch 121: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 4.0473185072187336e-05.\n",
      "66/66 [==============================] - 3s 39ms/step - loss: 0.8773 - val_loss: 1.6954 - lr: 4.0882e-05\n",
      "Epoch 122/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8878\n",
      "Epoch 122: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 4.0068451780825854e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.8878 - val_loss: 1.6905 - lr: 4.0473e-05\n",
      "Epoch 123/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8856\n",
      "Epoch 123: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 123: ReduceLROnPlateau reducing learning rate to 3.966776668676175e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.8856 - val_loss: 1.6932 - lr: 4.0068e-05\n",
      "Epoch 124/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8784\n",
      "Epoch 124: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 124: ReduceLROnPlateau reducing learning rate to 3.927109017240582e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.8769 - val_loss: 1.7600 - lr: 3.9668e-05\n",
      "Epoch 125/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8914\n",
      "Epoch 125: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 3.887837901856983e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.8908 - val_loss: 1.6928 - lr: 3.9271e-05\n",
      "Epoch 126/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8838\n",
      "Epoch 126: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 3.848959360766457e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.8825 - val_loss: 1.7111 - lr: 3.8878e-05\n",
      "Epoch 127/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8779\n",
      "Epoch 127: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 127: ReduceLROnPlateau reducing learning rate to 3.810469792369986e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.8779 - val_loss: 1.7085 - lr: 3.8490e-05\n",
      "Epoch 128/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8847\n",
      "Epoch 128: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 128: ReduceLROnPlateau reducing learning rate to 3.772365234908648e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.8848 - val_loss: 1.7206 - lr: 3.8105e-05\n",
      "Epoch 129/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8974\n",
      "Epoch 129: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 129: ReduceLROnPlateau reducing learning rate to 3.734641726623522e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.8987 - val_loss: 1.6914 - lr: 3.7724e-05\n",
      "Epoch 130/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8804\n",
      "Epoch 130: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 130: ReduceLROnPlateau reducing learning rate to 3.6972953057556876e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.8804 - val_loss: 1.7112 - lr: 3.7346e-05\n",
      "Epoch 131/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8986\n",
      "Epoch 131: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 131: ReduceLROnPlateau reducing learning rate to 3.660322370706126e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.8986 - val_loss: 1.7033 - lr: 3.6973e-05\n",
      "Epoch 132/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8954\n",
      "Epoch 132: val_loss did not improve from 1.66110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 132: ReduceLROnPlateau reducing learning rate to 3.623719319875818e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.8943 - val_loss: 1.7080 - lr: 3.6603e-05\n",
      "Epoch 133/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8895\n",
      "Epoch 133: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 133: ReduceLROnPlateau reducing learning rate to 3.5874821915058416e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.8878 - val_loss: 1.7249 - lr: 3.6237e-05\n",
      "Epoch 134/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8909\n",
      "Epoch 134: val_loss did not improve from 1.66110\n",
      "\n",
      "Epoch 134: ReduceLROnPlateau reducing learning rate to 3.551607383997179e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.8909 - val_loss: 1.7358 - lr: 3.5875e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_13 (InputLayer)          [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_36 (LSTM)                 (None, 45, 24)       3744        ['input_13[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 45, 24)       0           ['lstm_36[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_37 (LSTM)                 (None, 45, 16)       2624        ['dropout_24[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_25 (Dropout)           (None, 45, 16)       0           ['lstm_37[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_38 (LSTM)                 (None, 32)           6272        ['dropout_25[0][0]']             \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 40)           1320        ['lstm_38[0][0]']                \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 5)            205         ['dense_24[0][0]']               \n",
      "                                                                                                  \n",
      " tf.unstack_12 (TFOpLambda)     [(None,),            0           ['dense_25[0][0]']               \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_60 (TFOpLambda)  (None, 1)           0           ['tf.unstack_12[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_24 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_60[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_64 (TFOpLambda)  (None, 1)           0           ['tf.unstack_12[0][4]']          \n",
      "                                                                                                  \n",
      " tf.math.multiply_36 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_24[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_25 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_64[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_37 (TFOpLambd  (None, 1)           0           ['tf.math.multiply_36[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_61 (TFOpLambda)  (None, 1)           0           ['tf.unstack_12[0][1]']          \n",
      "                                                                                                  \n",
      " tf.expand_dims_63 (TFOpLambda)  (None, 1)           0           ['tf.unstack_12[0][3]']          \n",
      "                                                                                                  \n",
      " tf.math.multiply_38 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_25[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_24 (TFOpL  (None, 1)           0           ['tf.math.multiply_37[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.softplus_24 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_61[0][0]']      \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_62 (TFOpLambda)  (None, 1)           0           ['tf.unstack_12[0][2]']          \n",
      "                                                                                                  \n",
      " tf.math.softplus_25 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_63[0][0]']      \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_25 (TFOpL  (None, 1)           0           ['tf.math.multiply_38[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack_12 (TFOpLambda)       (None, 5, 1)         0           ['tf.__operators__.add_24[0][0]',\n",
      "                                                                  'tf.math.softplus_24[0][0]',    \n",
      "                                                                  'tf.expand_dims_62[0][0]',      \n",
      "                                                                  'tf.math.softplus_25[0][0]',    \n",
      "                                                                  'tf.__operators__.add_25[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _1_CCMP_t+1_0.17\n",
      "Epoch 1/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 3.4755\n",
      "Epoch 1: val_loss improved from inf to 4.16225, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.17.hdf5\n",
      "66/66 [==============================] - 8s 58ms/step - loss: 3.4744 - val_loss: 4.1622 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 2.8632\n",
      "Epoch 2: val_loss improved from 4.16225 to 3.90260, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.17.hdf5\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 2.8632 - val_loss: 3.9026 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 2.0667\n",
      "Epoch 3: val_loss improved from 3.90260 to 3.53449, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.17.hdf5\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 2.0635 - val_loss: 3.5345 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/66 [============================>.] - ETA: 0s - loss: 1.6751\n",
      "Epoch 4: val_loss improved from 3.53449 to 3.44888, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.17.hdf5\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 1.6744 - val_loss: 3.4489 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.4673\n",
      "Epoch 5: val_loss improved from 3.44888 to 3.02395, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.17.hdf5\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 1.4666 - val_loss: 3.0240 - lr: 1.0000e-04\n",
      "Epoch 6/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.3538\n",
      "Epoch 6: val_loss improved from 3.02395 to 2.84867, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.17.hdf5\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 1.3538 - val_loss: 2.8487 - lr: 1.0000e-04\n",
      "Epoch 7/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.2947\n",
      "Epoch 7: val_loss did not improve from 2.84867\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 1.2923 - val_loss: 3.0361 - lr: 1.0000e-04\n",
      "Epoch 8/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.2438\n",
      "Epoch 8: val_loss did not improve from 2.84867\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 1.2434 - val_loss: 3.0866 - lr: 9.9000e-05\n",
      "Epoch 9/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.2073\n",
      "Epoch 9: val_loss improved from 2.84867 to 2.78807, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.17.hdf5\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 1.2082 - val_loss: 2.7881 - lr: 9.8010e-05\n",
      "Epoch 10/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1877\n",
      "Epoch 10: val_loss did not improve from 2.78807\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 1.1878 - val_loss: 3.1514 - lr: 9.8010e-05\n",
      "Epoch 11/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1461\n",
      "Epoch 11: val_loss did not improve from 2.78807\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 1.1459 - val_loss: 2.8684 - lr: 9.7030e-05\n",
      "Epoch 12/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1442\n",
      "Epoch 12: val_loss did not improve from 2.78807\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 1.1411 - val_loss: 3.3835 - lr: 9.6060e-05\n",
      "Epoch 13/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1211\n",
      "Epoch 13: val_loss did not improve from 2.78807\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 1.1197 - val_loss: 3.4406 - lr: 9.5099e-05\n",
      "Epoch 14/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1162\n",
      "Epoch 14: val_loss did not improve from 2.78807\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 1.1162 - val_loss: 2.9009 - lr: 9.4148e-05\n",
      "Epoch 15/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0972\n",
      "Epoch 15: val_loss did not improve from 2.78807\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 1.0972 - val_loss: 3.0964 - lr: 9.3207e-05\n",
      "Epoch 16/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0841\n",
      "Epoch 16: val_loss did not improve from 2.78807\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 1.0882 - val_loss: 2.8836 - lr: 9.2274e-05\n",
      "Epoch 17/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0885\n",
      "Epoch 17: val_loss improved from 2.78807 to 2.64691, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.17.hdf5\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 1.0901 - val_loss: 2.6469 - lr: 9.1352e-05\n",
      "Epoch 18/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0672\n",
      "Epoch 18: val_loss did not improve from 2.64691\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 1.0666 - val_loss: 2.7835 - lr: 9.1352e-05\n",
      "Epoch 19/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0702\n",
      "Epoch 19: val_loss did not improve from 2.64691\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 1.0702 - val_loss: 3.0450 - lr: 9.0438e-05\n",
      "Epoch 20/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0564\n",
      "Epoch 20: val_loss did not improve from 2.64691\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 1.0583 - val_loss: 2.9387 - lr: 8.9534e-05\n",
      "Epoch 21/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0739\n",
      "Epoch 21: val_loss did not improve from 2.64691\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 1.0730 - val_loss: 2.8757 - lr: 8.8638e-05\n",
      "Epoch 22/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0512\n",
      "Epoch 22: val_loss did not improve from 2.64691\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 4s 55ms/step - loss: 1.0536 - val_loss: 2.7559 - lr: 8.7752e-05\n",
      "Epoch 23/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0517\n",
      "Epoch 23: val_loss did not improve from 2.64691\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 1.0517 - val_loss: 3.0502 - lr: 8.6875e-05\n",
      "Epoch 24/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0404\n",
      "Epoch 24: val_loss did not improve from 2.64691\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 1.0392 - val_loss: 3.0790 - lr: 8.6006e-05\n",
      "Epoch 25/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0546\n",
      "Epoch 25: val_loss did not improve from 2.64691\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 1.0555 - val_loss: 2.8314 - lr: 8.5146e-05\n",
      "Epoch 26/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0388\n",
      "Epoch 26: val_loss did not improve from 2.64691\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 1.0393 - val_loss: 2.7490 - lr: 8.4294e-05\n",
      "Epoch 27/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0428\n",
      "Epoch 27: val_loss did not improve from 2.64691\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 1.0428 - val_loss: 2.6536 - lr: 8.3451e-05\n",
      "Epoch 28/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0384\n",
      "Epoch 28: val_loss did not improve from 2.64691\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 1.0384 - val_loss: 2.8467 - lr: 8.2617e-05\n",
      "Epoch 29/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0227\n",
      "Epoch 29: val_loss did not improve from 2.64691\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 1.0218 - val_loss: 2.7167 - lr: 8.1791e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0345\n",
      "Epoch 30: val_loss did not improve from 2.64691\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 1.0334 - val_loss: 2.8381 - lr: 8.0973e-05\n",
      "Epoch 31/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0310\n",
      "Epoch 31: val_loss did not improve from 2.64691\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 1.0325 - val_loss: 2.7953 - lr: 8.0163e-05\n",
      "Epoch 32/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0209\n",
      "Epoch 32: val_loss improved from 2.64691 to 2.51824, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.17.hdf5\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 1.0209 - val_loss: 2.5182 - lr: 7.9361e-05\n",
      "Epoch 33/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0203\n",
      "Epoch 33: val_loss improved from 2.51824 to 2.51407, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.17.hdf5\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 1.0203 - val_loss: 2.5141 - lr: 7.9361e-05\n",
      "Epoch 34/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0125\n",
      "Epoch 34: val_loss did not improve from 2.51407\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 1.0132 - val_loss: 2.6543 - lr: 7.9361e-05\n",
      "Epoch 35/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0289\n",
      "Epoch 35: val_loss did not improve from 2.51407\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 1.0289 - val_loss: 2.6505 - lr: 7.8568e-05\n",
      "Epoch 36/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0249\n",
      "Epoch 36: val_loss did not improve from 2.51407\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 1.0249 - val_loss: 2.7398 - lr: 7.7782e-05\n",
      "Epoch 37/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0114\n",
      "Epoch 37: val_loss improved from 2.51407 to 2.44347, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.17.hdf5\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 1.0093 - val_loss: 2.4435 - lr: 7.7004e-05\n",
      "Epoch 38/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0072\n",
      "Epoch 38: val_loss improved from 2.44347 to 2.41241, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.17.hdf5\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 1.0095 - val_loss: 2.4124 - lr: 7.7004e-05\n",
      "Epoch 39/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0097\n",
      "Epoch 39: val_loss did not improve from 2.41241\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 1.0091 - val_loss: 2.6333 - lr: 7.7004e-05\n",
      "Epoch 40/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0236\n",
      "Epoch 40: val_loss improved from 2.41241 to 2.39917, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.17.hdf5\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 1.0244 - val_loss: 2.3992 - lr: 7.6234e-05\n",
      "Epoch 41/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0125\n",
      "Epoch 41: val_loss improved from 2.39917 to 2.39447, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.17.hdf5\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 1.0126 - val_loss: 2.3945 - lr: 7.6234e-05\n",
      "Epoch 42/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0038\n",
      "Epoch 42: val_loss did not improve from 2.39447\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 1.0038 - val_loss: 2.4860 - lr: 7.6234e-05\n",
      "Epoch 43/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0064\n",
      "Epoch 43: val_loss did not improve from 2.39447\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 1.0064 - val_loss: 2.4143 - lr: 7.5472e-05\n",
      "Epoch 44/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9933\n",
      "Epoch 44: val_loss did not improve from 2.39447\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9933 - val_loss: 2.4357 - lr: 7.4717e-05\n",
      "Epoch 45/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9937\n",
      "Epoch 45: val_loss did not improve from 2.39447\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9926 - val_loss: 2.4940 - lr: 7.3970e-05\n",
      "Epoch 46/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0098\n",
      "Epoch 46: val_loss improved from 2.39447 to 2.36978, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.17.hdf5\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 1.0098 - val_loss: 2.3698 - lr: 7.3230e-05\n",
      "Epoch 47/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9953\n",
      "Epoch 47: val_loss did not improve from 2.36978\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9937 - val_loss: 2.4446 - lr: 7.3230e-05\n",
      "Epoch 48/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9956\n",
      "Epoch 48: val_loss did not improve from 2.36978\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9956 - val_loss: 2.4142 - lr: 7.2498e-05\n",
      "Epoch 49/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9891\n",
      "Epoch 49: val_loss did not improve from 2.36978\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9893 - val_loss: 2.4620 - lr: 7.1773e-05\n",
      "Epoch 50/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0024\n",
      "Epoch 50: val_loss did not improve from 2.36978\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 1.0003 - val_loss: 2.4250 - lr: 7.1055e-05\n",
      "Epoch 51/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9875\n",
      "Epoch 51: val_loss did not improve from 2.36978\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9875 - val_loss: 2.3812 - lr: 7.0345e-05\n",
      "Epoch 52/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9826\n",
      "Epoch 52: val_loss improved from 2.36978 to 2.26345, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.17.hdf5\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9815 - val_loss: 2.2634 - lr: 6.9641e-05\n",
      "Epoch 53/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9817\n",
      "Epoch 53: val_loss did not improve from 2.26345\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9814 - val_loss: 2.4350 - lr: 6.9641e-05\n",
      "Epoch 54/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9766\n",
      "Epoch 54: val_loss did not improve from 2.26345\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9766 - val_loss: 2.4785 - lr: 6.8945e-05\n",
      "Epoch 55/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9889\n",
      "Epoch 55: val_loss did not improve from 2.26345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9889 - val_loss: 2.3280 - lr: 6.8255e-05\n",
      "Epoch 56/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9906\n",
      "Epoch 56: val_loss did not improve from 2.26345\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9890 - val_loss: 2.2800 - lr: 6.7573e-05\n",
      "Epoch 57/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9697\n",
      "Epoch 57: val_loss did not improve from 2.26345\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9697 - val_loss: 2.2836 - lr: 6.6897e-05\n",
      "Epoch 58/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9841\n",
      "Epoch 58: val_loss did not improve from 2.26345\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9853 - val_loss: 2.3160 - lr: 6.6228e-05\n",
      "Epoch 59/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9768\n",
      "Epoch 59: val_loss improved from 2.26345 to 2.15789, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.17.hdf5\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9771 - val_loss: 2.1579 - lr: 6.5566e-05\n",
      "Epoch 60/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9866\n",
      "Epoch 60: val_loss did not improve from 2.15789\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.9837 - val_loss: 2.2560 - lr: 6.5566e-05\n",
      "Epoch 61/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9735\n",
      "Epoch 61: val_loss did not improve from 2.15789\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9735 - val_loss: 2.3064 - lr: 6.4910e-05\n",
      "Epoch 62/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9744\n",
      "Epoch 62: val_loss did not improve from 2.15789\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9733 - val_loss: 2.4265 - lr: 6.4261e-05\n",
      "Epoch 63/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9638\n",
      "Epoch 63: val_loss did not improve from 2.15789\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9638 - val_loss: 2.4580 - lr: 6.3619e-05\n",
      "Epoch 64/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9791\n",
      "Epoch 64: val_loss did not improve from 2.15789\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9784 - val_loss: 2.2729 - lr: 6.2982e-05\n",
      "Epoch 65/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9716\n",
      "Epoch 65: val_loss improved from 2.15789 to 2.15142, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.17.hdf5\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9722 - val_loss: 2.1514 - lr: 6.2353e-05\n",
      "Epoch 66/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9780\n",
      "Epoch 66: val_loss did not improve from 2.15142\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9780 - val_loss: 2.3306 - lr: 6.2353e-05\n",
      "Epoch 67/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9706\n",
      "Epoch 67: val_loss did not improve from 2.15142\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9736 - val_loss: 2.2185 - lr: 6.1729e-05\n",
      "Epoch 68/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9670\n",
      "Epoch 68: val_loss did not improve from 2.15142\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9675 - val_loss: 2.1725 - lr: 6.1112e-05\n",
      "Epoch 69/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9556\n",
      "Epoch 69: val_loss did not improve from 2.15142\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9556 - val_loss: 2.3331 - lr: 6.0501e-05\n",
      "Epoch 70/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9596\n",
      "Epoch 70: val_loss did not improve from 2.15142\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9598 - val_loss: 2.2818 - lr: 5.9896e-05\n",
      "Epoch 71/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9726\n",
      "Epoch 71: val_loss did not improve from 2.15142\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 5.8703650938696225e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9726 - val_loss: 2.3210 - lr: 5.9297e-05\n",
      "Epoch 72/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9671\n",
      "Epoch 72: val_loss did not improve from 2.15142\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 5.811661550978897e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9707 - val_loss: 2.2002 - lr: 5.8704e-05\n",
      "Epoch 73/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9589\n",
      "Epoch 73: val_loss did not improve from 2.15142\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 5.753545068728272e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.9582 - val_loss: 2.2392 - lr: 5.8117e-05\n",
      "Epoch 74/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9621\n",
      "Epoch 74: val_loss did not improve from 2.15142\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 5.6960095243994146e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9614 - val_loss: 2.2256 - lr: 5.7535e-05\n",
      "Epoch 75/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9621\n",
      "Epoch 75: val_loss improved from 2.15142 to 2.12159, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.17.hdf5\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9624 - val_loss: 2.1216 - lr: 5.6960e-05\n",
      "Epoch 76/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9547\n",
      "Epoch 76: val_loss did not improve from 2.12159\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 5.639049515593797e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9552 - val_loss: 2.1545 - lr: 5.6960e-05\n",
      "Epoch 77/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9615\n",
      "Epoch 77: val_loss did not improve from 2.12159\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 5.5826589195930865e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9612 - val_loss: 2.2906 - lr: 5.6390e-05\n",
      "Epoch 78/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9634\n",
      "Epoch 78: val_loss did not improve from 2.12159\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 5.526832333998754e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9612 - val_loss: 2.3988 - lr: 5.5827e-05\n",
      "Epoch 79/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9689\n",
      "Epoch 79: val_loss did not improve from 2.12159\n",
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 5.471563996252371e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9676 - val_loss: 2.2970 - lr: 5.5268e-05\n",
      "Epoch 80/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9561\n",
      "Epoch 80: val_loss did not improve from 2.12159\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 5.416848503955407e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9565 - val_loss: 2.1834 - lr: 5.4716e-05\n",
      "Epoch 81/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9582\n",
      "Epoch 81: val_loss did not improve from 2.12159\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 5.362680094549433e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9582 - val_loss: 2.2505 - lr: 5.4168e-05\n",
      "Epoch 82/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9574\n",
      "Epoch 82: val_loss improved from 2.12159 to 2.09968, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.17.hdf5\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9574 - val_loss: 2.0997 - lr: 5.3627e-05\n",
      "Epoch 83/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9516\n",
      "Epoch 83: val_loss did not improve from 2.09968\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 5.309053365635918e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.9518 - val_loss: 2.2429 - lr: 5.3627e-05\n",
      "Epoch 84/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9741\n",
      "Epoch 84: val_loss did not improve from 2.09968\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 5.2559629148163366e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9728 - val_loss: 2.1771 - lr: 5.3091e-05\n",
      "Epoch 85/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9522\n",
      "Epoch 85: val_loss did not improve from 2.09968\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 5.2034033396921584e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9522 - val_loss: 2.1703 - lr: 5.2560e-05\n",
      "Epoch 86/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9514\n",
      "Epoch 86: val_loss did not improve from 2.09968\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 5.1513692378648555e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9514 - val_loss: 2.1465 - lr: 5.2034e-05\n",
      "Epoch 87/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9467\n",
      "Epoch 87: val_loss did not improve from 2.09968\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 5.099855567095801e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9476 - val_loss: 2.1194 - lr: 5.1514e-05\n",
      "Epoch 88/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9600\n",
      "Epoch 88: val_loss did not improve from 2.09968\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 5.048856924986467e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9600 - val_loss: 2.1858 - lr: 5.0999e-05\n",
      "Epoch 89/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9433\n",
      "Epoch 89: val_loss did not improve from 2.09968\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 4.998368269298226e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9441 - val_loss: 2.2321 - lr: 5.0489e-05\n",
      "Epoch 90/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9461\n",
      "Epoch 90: val_loss did not improve from 2.09968\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 4.9483845577924514e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9463 - val_loss: 2.2042 - lr: 4.9984e-05\n",
      "Epoch 91/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9587\n",
      "Epoch 91: val_loss improved from 2.09968 to 2.08011, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.17.hdf5\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9571 - val_loss: 2.0801 - lr: 4.9484e-05\n",
      "Epoch 92/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9589\n",
      "Epoch 92: val_loss did not improve from 2.08011\n",
      "\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 4.898900748230517e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9568 - val_loss: 2.0943 - lr: 4.9484e-05\n",
      "Epoch 93/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9631\n",
      "Epoch 93: val_loss did not improve from 2.08011\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 4.849911798373796e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9631 - val_loss: 2.1547 - lr: 4.8989e-05\n",
      "Epoch 94/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9523\n",
      "Epoch 94: val_loss did not improve from 2.08011\n",
      "\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 4.801412665983662e-05.\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 0.9510 - val_loss: 2.2209 - lr: 4.8499e-05\n",
      "Epoch 95/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9362\n",
      "Epoch 95: val_loss did not improve from 2.08011\n",
      "\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 4.75339866898139e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9391 - val_loss: 2.2139 - lr: 4.8014e-05\n",
      "Epoch 96/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9479\n",
      "Epoch 96: val_loss did not improve from 2.08011\n",
      "\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 4.705864765128354e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9472 - val_loss: 2.1416 - lr: 4.7534e-05\n",
      "Epoch 97/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9386\n",
      "Epoch 97: val_loss did not improve from 2.08011\n",
      "\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 4.658806272345828e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9386 - val_loss: 2.1108 - lr: 4.7059e-05\n",
      "Epoch 98/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9573\n",
      "Epoch 98: val_loss improved from 2.08011 to 2.03856, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.17.hdf5\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9566 - val_loss: 2.0386 - lr: 4.6588e-05\n",
      "Epoch 99/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9509\n",
      "Epoch 99: val_loss did not improve from 2.03856\n",
      "\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 4.612218148395186e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9502 - val_loss: 2.1047 - lr: 4.6588e-05\n",
      "Epoch 100/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9449\n",
      "Epoch 100: val_loss did not improve from 2.03856\n",
      "\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 4.566096071357606e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9433 - val_loss: 2.1279 - lr: 4.6122e-05\n",
      "Epoch 101/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9456\n",
      "Epoch 101: val_loss did not improve from 2.03856\n",
      "\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 4.52043499899446e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9456 - val_loss: 2.1545 - lr: 4.5661e-05\n",
      "Epoch 102/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9375\n",
      "Epoch 102: val_loss did not improve from 2.03856\n",
      "\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 4.475230609386927e-05.\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.9393 - val_loss: 2.1216 - lr: 4.5204e-05\n",
      "Epoch 103/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9430\n",
      "Epoch 103: val_loss improved from 2.03856 to 2.02780, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.17.hdf5\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.9430 - val_loss: 2.0278 - lr: 4.4752e-05\n",
      "Epoch 104/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9435\n",
      "Epoch 104: val_loss did not improve from 2.02780\n",
      "\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 4.43047822045628e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9449 - val_loss: 2.1033 - lr: 4.4752e-05\n",
      "Epoch 105/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9399\n",
      "Epoch 105: val_loss did not improve from 2.02780\n",
      "\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 4.386173510283697e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.9399 - val_loss: 2.1609 - lr: 4.4305e-05\n",
      "Epoch 106/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9448\n",
      "Epoch 106: val_loss did not improve from 2.02780\n",
      "\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 4.342311796790454e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.9448 - val_loss: 2.1054 - lr: 4.3862e-05\n",
      "Epoch 107/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9505\n",
      "Epoch 107: val_loss did not improve from 2.02780\n",
      "\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 4.298888758057728e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 3s 49ms/step - loss: 0.9485 - val_loss: 2.0867 - lr: 4.3423e-05\n",
      "Epoch 108/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9338\n",
      "Epoch 108: val_loss did not improve from 2.02780\n",
      "\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 4.2558997120067946e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.9338 - val_loss: 2.1143 - lr: 4.2989e-05\n",
      "Epoch 109/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9363\n",
      "Epoch 109: val_loss did not improve from 2.02780\n",
      "\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 4.2133406968787315e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.9348 - val_loss: 2.1594 - lr: 4.2559e-05\n",
      "Epoch 110/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9464\n",
      "Epoch 110: val_loss did not improve from 2.02780\n",
      "\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 4.1712073907547166e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.9448 - val_loss: 2.1709 - lr: 4.2133e-05\n",
      "Epoch 111/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9318\n",
      "Epoch 111: val_loss did not improve from 2.02780\n",
      "\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 4.1294954717159274e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.9313 - val_loss: 2.0797 - lr: 4.1712e-05\n",
      "Epoch 112/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9360\n",
      "Epoch 112: val_loss did not improve from 2.02780\n",
      "\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 4.08820061784354e-05.\n",
      "66/66 [==============================] - 4s 53ms/step - loss: 0.9360 - val_loss: 2.0492 - lr: 4.1295e-05\n",
      "Epoch 113/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9368\n",
      "Epoch 113: val_loss did not improve from 2.02780\n",
      "\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 4.0473185072187336e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9375 - val_loss: 2.1286 - lr: 4.0882e-05\n",
      "Epoch 114/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9402\n",
      "Epoch 114: val_loss did not improve from 2.02780\n",
      "\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 4.0068451780825854e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.9402 - val_loss: 2.0600 - lr: 4.0473e-05\n",
      "Epoch 115/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9348\n",
      "Epoch 115: val_loss did not improve from 2.02780\n",
      "\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 3.966776668676175e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.9348 - val_loss: 2.0577 - lr: 4.0068e-05\n",
      "Epoch 116/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9310\n",
      "Epoch 116: val_loss did not improve from 2.02780\n",
      "\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 3.927109017240582e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9310 - val_loss: 2.1469 - lr: 3.9668e-05\n",
      "Epoch 117/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9312\n",
      "Epoch 117: val_loss did not improve from 2.02780\n",
      "\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 3.887837901856983e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9312 - val_loss: 2.1529 - lr: 3.9271e-05\n",
      "Epoch 118/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9393\n",
      "Epoch 118: val_loss improved from 2.02780 to 2.01236, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.17.hdf5\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.9393 - val_loss: 2.0124 - lr: 3.8878e-05\n",
      "Epoch 119/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9361\n",
      "Epoch 119: val_loss did not improve from 2.01236\n",
      "\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 3.848959360766457e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9361 - val_loss: 2.0735 - lr: 3.8878e-05\n",
      "Epoch 120/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9253\n",
      "Epoch 120: val_loss did not improve from 2.01236\n",
      "\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 3.810469792369986e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9253 - val_loss: 2.0168 - lr: 3.8490e-05\n",
      "Epoch 121/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9367\n",
      "Epoch 121: val_loss did not improve from 2.01236\n",
      "\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 3.772365234908648e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9359 - val_loss: 2.0703 - lr: 3.8105e-05\n",
      "Epoch 122/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9251\n",
      "Epoch 122: val_loss did not improve from 2.01236\n",
      "\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 3.734641726623522e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.9251 - val_loss: 2.0836 - lr: 3.7724e-05\n",
      "Epoch 123/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9381\n",
      "Epoch 123: val_loss improved from 2.01236 to 1.96507, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.17.hdf5\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.9387 - val_loss: 1.9651 - lr: 3.7346e-05\n",
      "Epoch 124/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9443\n",
      "Epoch 124: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 124: ReduceLROnPlateau reducing learning rate to 3.6972953057556876e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.9445 - val_loss: 1.9758 - lr: 3.7346e-05\n",
      "Epoch 125/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9375\n",
      "Epoch 125: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 3.660322370706126e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9375 - val_loss: 2.1425 - lr: 3.6973e-05\n",
      "Epoch 126/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9239\n",
      "Epoch 126: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 3.623719319875818e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9252 - val_loss: 2.0911 - lr: 3.6603e-05\n",
      "Epoch 127/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9244\n",
      "Epoch 127: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 127: ReduceLROnPlateau reducing learning rate to 3.5874821915058416e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9263 - val_loss: 2.0643 - lr: 3.6237e-05\n",
      "Epoch 128/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9377\n",
      "Epoch 128: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 128: ReduceLROnPlateau reducing learning rate to 3.551607383997179e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9377 - val_loss: 2.0957 - lr: 3.5875e-05\n",
      "Epoch 129/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9297\n",
      "Epoch 129: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 129: ReduceLROnPlateau reducing learning rate to 3.516091295750812e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9297 - val_loss: 2.0702 - lr: 3.5516e-05\n",
      "Epoch 130/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9321\n",
      "Epoch 130: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 130: ReduceLROnPlateau reducing learning rate to 3.480930325167719e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9320 - val_loss: 2.0469 - lr: 3.5161e-05\n",
      "Epoch 131/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9269\n",
      "Epoch 131: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 131: ReduceLROnPlateau reducing learning rate to 3.446120870648883e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9269 - val_loss: 2.1497 - lr: 3.4809e-05\n",
      "Epoch 132/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9377\n",
      "Epoch 132: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 132: ReduceLROnPlateau reducing learning rate to 3.4116596907551866e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9377 - val_loss: 1.9734 - lr: 3.4461e-05\n",
      "Epoch 133/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9284\n",
      "Epoch 133: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 133: ReduceLROnPlateau reducing learning rate to 3.37754318388761e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9284 - val_loss: 2.1321 - lr: 3.4117e-05\n",
      "Epoch 134/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9406\n",
      "Epoch 134: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 134: ReduceLROnPlateau reducing learning rate to 3.3437677484471354e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.9388 - val_loss: 2.0924 - lr: 3.3775e-05\n",
      "Epoch 135/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9224\n",
      "Epoch 135: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 135: ReduceLROnPlateau reducing learning rate to 3.310330142994644e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9208 - val_loss: 2.1651 - lr: 3.3438e-05\n",
      "Epoch 136/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9283\n",
      "Epoch 136: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 3.277226765931118e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9278 - val_loss: 2.1142 - lr: 3.3103e-05\n",
      "Epoch 137/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9242\n",
      "Epoch 137: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 137: ReduceLROnPlateau reducing learning rate to 3.24445437581744e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9231 - val_loss: 2.0566 - lr: 3.2772e-05\n",
      "Epoch 138/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9268\n",
      "Epoch 138: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 138: ReduceLROnPlateau reducing learning rate to 3.212009731214493e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9278 - val_loss: 2.1022 - lr: 3.2445e-05\n",
      "Epoch 139/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9340\n",
      "Epoch 139: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 139: ReduceLROnPlateau reducing learning rate to 3.17988959068316e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9340 - val_loss: 2.0478 - lr: 3.2120e-05\n",
      "Epoch 140/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9257\n",
      "Epoch 140: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 140: ReduceLROnPlateau reducing learning rate to 3.1480907127843236e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9257 - val_loss: 2.0692 - lr: 3.1799e-05\n",
      "Epoch 141/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9217\n",
      "Epoch 141: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 141: ReduceLROnPlateau reducing learning rate to 3.116609856078867e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9222 - val_loss: 2.0037 - lr: 3.1481e-05\n",
      "Epoch 142/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9344\n",
      "Epoch 142: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 142: ReduceLROnPlateau reducing learning rate to 3.085443779127672e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9351 - val_loss: 2.0352 - lr: 3.1166e-05\n",
      "Epoch 143/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9176\n",
      "Epoch 143: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 143: ReduceLROnPlateau reducing learning rate to 3.054589240491623e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.9190 - val_loss: 2.0058 - lr: 3.0854e-05\n",
      "Epoch 144/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9120\n",
      "Epoch 144: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 144: ReduceLROnPlateau reducing learning rate to 3.024043358891504e-05.\n",
      "66/66 [==============================] - 4s 56ms/step - loss: 0.9136 - val_loss: 2.1164 - lr: 3.0546e-05\n",
      "Epoch 145/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9199\n",
      "Epoch 145: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 145: ReduceLROnPlateau reducing learning rate to 2.9938028928881975e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9199 - val_loss: 2.0632 - lr: 3.0240e-05\n",
      "Epoch 146/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9203\n",
      "Epoch 146: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 146: ReduceLROnPlateau reducing learning rate to 2.963864781122538e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.9183 - val_loss: 2.1130 - lr: 2.9938e-05\n",
      "Epoch 147/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9302\n",
      "Epoch 147: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 147: ReduceLROnPlateau reducing learning rate to 2.9342261423153105e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.9323 - val_loss: 2.0678 - lr: 2.9639e-05\n",
      "Epoch 148/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9219\n",
      "Epoch 148: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 148: ReduceLROnPlateau reducing learning rate to 2.9048839151073478e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.9217 - val_loss: 2.0790 - lr: 2.9342e-05\n",
      "Epoch 149/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9235\n",
      "Epoch 149: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 149: ReduceLROnPlateau reducing learning rate to 2.875835038139485e-05.\n",
      "66/66 [==============================] - 3s 41ms/step - loss: 0.9235 - val_loss: 2.0361 - lr: 2.9049e-05\n",
      "Epoch 150/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9311\n",
      "Epoch 150: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 150: ReduceLROnPlateau reducing learning rate to 2.8470766301325058e-05.\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 0.9311 - val_loss: 2.0829 - lr: 2.8758e-05\n",
      "Epoch 151/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9267\n",
      "Epoch 151: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 151: ReduceLROnPlateau reducing learning rate to 2.8186058098071954e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.9267 - val_loss: 2.1043 - lr: 2.8471e-05\n",
      "Epoch 152/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9269\n",
      "Epoch 152: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 152: ReduceLROnPlateau reducing learning rate to 2.7904196958843385e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.9260 - val_loss: 2.0331 - lr: 2.8186e-05\n",
      "Epoch 153/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9195\n",
      "Epoch 153: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 153: ReduceLROnPlateau reducing learning rate to 2.7625155871646713e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.9219 - val_loss: 2.0559 - lr: 2.7904e-05\n",
      "Epoch 154/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9195\n",
      "Epoch 154: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 154: ReduceLROnPlateau reducing learning rate to 2.734890422289027e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.9197 - val_loss: 2.1023 - lr: 2.7625e-05\n",
      "Epoch 155/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9218\n",
      "Epoch 155: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 155: ReduceLROnPlateau reducing learning rate to 2.7075415000581416e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.9220 - val_loss: 2.0443 - lr: 2.7349e-05\n",
      "Epoch 156/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9239\n",
      "Epoch 156: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 156: ReduceLROnPlateau reducing learning rate to 2.6804661192727508e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.9239 - val_loss: 2.1507 - lr: 2.7075e-05\n",
      "Epoch 157/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9273\n",
      "Epoch 157: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 157: ReduceLROnPlateau reducing learning rate to 2.6536613986536395e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.9261 - val_loss: 2.0985 - lr: 2.6805e-05\n",
      "Epoch 158/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9214\n",
      "Epoch 158: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 158: ReduceLROnPlateau reducing learning rate to 2.6271248170814942e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.9209 - val_loss: 2.1475 - lr: 2.6537e-05\n",
      "Epoch 159/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9151\n",
      "Epoch 159: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 159: ReduceLROnPlateau reducing learning rate to 2.6008534932770998e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.9151 - val_loss: 2.1188 - lr: 2.6271e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9255\n",
      "Epoch 160: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 160: ReduceLROnPlateau reducing learning rate to 2.5748449061211432e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.9237 - val_loss: 2.1040 - lr: 2.6009e-05\n",
      "Epoch 161/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9189\n",
      "Epoch 161: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 161: ReduceLROnPlateau reducing learning rate to 2.5490965344943107e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.9172 - val_loss: 2.0323 - lr: 2.5748e-05\n",
      "Epoch 162/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9133\n",
      "Epoch 162: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 162: ReduceLROnPlateau reducing learning rate to 2.523605497117387e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.9111 - val_loss: 2.0481 - lr: 2.5491e-05\n",
      "Epoch 163/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9266\n",
      "Epoch 163: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 163: ReduceLROnPlateau reducing learning rate to 2.4983694529510104e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.9254 - val_loss: 2.1119 - lr: 2.5236e-05\n",
      "Epoch 164/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9151\n",
      "Epoch 164: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 164: ReduceLROnPlateau reducing learning rate to 2.4733857007959158e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.9151 - val_loss: 2.1330 - lr: 2.4984e-05\n",
      "Epoch 165/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9270\n",
      "Epoch 165: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 165: ReduceLROnPlateau reducing learning rate to 2.4486518996127415e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.9258 - val_loss: 2.1312 - lr: 2.4734e-05\n",
      "Epoch 166/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9202\n",
      "Epoch 166: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 166: ReduceLROnPlateau reducing learning rate to 2.424165348202223e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 0.9200 - val_loss: 2.0891 - lr: 2.4487e-05\n",
      "Epoch 167/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9290\n",
      "Epoch 167: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 167: ReduceLROnPlateau reducing learning rate to 2.3999237055249977e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.9290 - val_loss: 2.0760 - lr: 2.4242e-05\n",
      "Epoch 168/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9108\n",
      "Epoch 168: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 168: ReduceLROnPlateau reducing learning rate to 2.375924450461753e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.9115 - val_loss: 2.1256 - lr: 2.3999e-05\n",
      "Epoch 169/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9136\n",
      "Epoch 169: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 169: ReduceLROnPlateau reducing learning rate to 2.3521652419731254e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.9139 - val_loss: 2.0263 - lr: 2.3759e-05\n",
      "Epoch 170/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9234\n",
      "Epoch 170: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 170: ReduceLROnPlateau reducing learning rate to 2.3286435589398024e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.9227 - val_loss: 2.0976 - lr: 2.3522e-05\n",
      "Epoch 171/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9046\n",
      "Epoch 171: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 171: ReduceLROnPlateau reducing learning rate to 2.3053570603224217e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.9047 - val_loss: 2.0679 - lr: 2.3286e-05\n",
      "Epoch 172/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9208\n",
      "Epoch 172: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 172: ReduceLROnPlateau reducing learning rate to 2.2823034050816205e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.9188 - val_loss: 2.1257 - lr: 2.3054e-05\n",
      "Epoch 173/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9145\n",
      "Epoch 173: val_loss did not improve from 1.96507\n",
      "\n",
      "Epoch 173: ReduceLROnPlateau reducing learning rate to 2.2594804322579874e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 0.9145 - val_loss: 2.0629 - lr: 2.2823e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_14 (InputLayer)          [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_39 (LSTM)                 (None, 45, 24)       3744        ['input_14[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_26 (Dropout)           (None, 45, 24)       0           ['lstm_39[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_40 (LSTM)                 (None, 45, 16)       2624        ['dropout_26[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_27 (Dropout)           (None, 45, 16)       0           ['lstm_40[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_41 (LSTM)                 (None, 32)           6272        ['dropout_27[0][0]']             \n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 40)           1320        ['lstm_41[0][0]']                \n",
      "                                                                                                  \n",
      " dense_27 (Dense)               (None, 5)            205         ['dense_26[0][0]']               \n",
      "                                                                                                  \n",
      " tf.unstack_13 (TFOpLambda)     [(None,),            0           ['dense_27[0][0]']               \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_65 (TFOpLambda)  (None, 1)           0           ['tf.unstack_13[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_26 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_65[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_69 (TFOpLambda)  (None, 1)           0           ['tf.unstack_13[0][4]']          \n",
      "                                                                                                  \n",
      " tf.math.multiply_39 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_26[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_27 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_69[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_40 (TFOpLambd  (None, 1)           0           ['tf.math.multiply_39[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_66 (TFOpLambda)  (None, 1)           0           ['tf.unstack_13[0][1]']          \n",
      "                                                                                                  \n",
      " tf.expand_dims_68 (TFOpLambda)  (None, 1)           0           ['tf.unstack_13[0][3]']          \n",
      "                                                                                                  \n",
      " tf.math.multiply_41 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_27[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_26 (TFOpL  (None, 1)           0           ['tf.math.multiply_40[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.softplus_26 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_66[0][0]']      \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_67 (TFOpLambda)  (None, 1)           0           ['tf.unstack_13[0][2]']          \n",
      "                                                                                                  \n",
      " tf.math.softplus_27 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_68[0][0]']      \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_27 (TFOpL  (None, 1)           0           ['tf.math.multiply_41[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack_13 (TFOpLambda)       (None, 5, 1)         0           ['tf.__operators__.add_26[0][0]',\n",
      "                                                                  'tf.math.softplus_26[0][0]',    \n",
      "                                                                  'tf.expand_dims_67[0][0]',      \n",
      "                                                                  'tf.math.softplus_27[0][0]',    \n",
      "                                                                  'tf.__operators__.add_27[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _1_CCMP_t+1_0.18\n",
      "Epoch 1/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 3.5037\n",
      "Epoch 1: val_loss improved from inf to 4.17161, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 8s 59ms/step - loss: 3.5018 - val_loss: 4.1716 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 3.0903\n",
      "Epoch 2: val_loss improved from 4.17161 to 3.37863, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 3.0882 - val_loss: 3.3786 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 2.0926\n",
      "Epoch 3: val_loss improved from 3.37863 to 3.37410, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 2.0911 - val_loss: 3.3741 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/66 [============================>.] - ETA: 0s - loss: 1.5889\n",
      "Epoch 4: val_loss improved from 3.37410 to 3.13737, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 1.5902 - val_loss: 3.1374 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.4333\n",
      "Epoch 5: val_loss improved from 3.13737 to 2.92948, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 1.4333 - val_loss: 2.9295 - lr: 1.0000e-04\n",
      "Epoch 6/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.3814\n",
      "Epoch 6: val_loss improved from 2.92948 to 2.83906, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 1.3800 - val_loss: 2.8391 - lr: 1.0000e-04\n",
      "Epoch 7/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.3391\n",
      "Epoch 7: val_loss did not improve from 2.83906\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 1.3378 - val_loss: 2.9659 - lr: 1.0000e-04\n",
      "Epoch 8/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.3091\n",
      "Epoch 8: val_loss improved from 2.83906 to 2.64055, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 1.3114 - val_loss: 2.6406 - lr: 9.9000e-05\n",
      "Epoch 9/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.2773\n",
      "Epoch 9: val_loss improved from 2.64055 to 2.61003, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 1.2756 - val_loss: 2.6100 - lr: 9.9000e-05\n",
      "Epoch 10/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.2623\n",
      "Epoch 10: val_loss improved from 2.61003 to 2.38272, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 1.2605 - val_loss: 2.3827 - lr: 9.9000e-05\n",
      "Epoch 11/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.2407\n",
      "Epoch 11: val_loss did not improve from 2.38272\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 1.2394 - val_loss: 2.3899 - lr: 9.9000e-05\n",
      "Epoch 12/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.2088\n",
      "Epoch 12: val_loss improved from 2.38272 to 2.36922, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 1.2090 - val_loss: 2.3692 - lr: 9.8010e-05\n",
      "Epoch 13/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.2123\n",
      "Epoch 13: val_loss improved from 2.36922 to 2.27138, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 1.2123 - val_loss: 2.2714 - lr: 9.8010e-05\n",
      "Epoch 14/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1884\n",
      "Epoch 14: val_loss did not improve from 2.27138\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 1.1878 - val_loss: 2.2910 - lr: 9.8010e-05\n",
      "Epoch 15/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1955\n",
      "Epoch 15: val_loss improved from 2.27138 to 2.09920, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 3s 45ms/step - loss: 1.1980 - val_loss: 2.0992 - lr: 9.7030e-05\n",
      "Epoch 16/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1726\n",
      "Epoch 16: val_loss improved from 2.09920 to 2.07468, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 1.1712 - val_loss: 2.0747 - lr: 9.7030e-05\n",
      "Epoch 17/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1648\n",
      "Epoch 17: val_loss improved from 2.07468 to 2.04776, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 1.1634 - val_loss: 2.0478 - lr: 9.7030e-05\n",
      "Epoch 18/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1606\n",
      "Epoch 18: val_loss did not improve from 2.04776\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 1.1606 - val_loss: 2.0535 - lr: 9.7030e-05\n",
      "Epoch 19/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1368\n",
      "Epoch 19: val_loss improved from 2.04776 to 2.02308, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 1.1370 - val_loss: 2.0231 - lr: 9.6060e-05\n",
      "Epoch 20/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1545\n",
      "Epoch 20: val_loss improved from 2.02308 to 2.01832, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 3s 44ms/step - loss: 1.1536 - val_loss: 2.0183 - lr: 9.6060e-05\n",
      "Epoch 21/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1514\n",
      "Epoch 21: val_loss did not improve from 2.01832\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 1.1516 - val_loss: 2.0290 - lr: 9.6060e-05\n",
      "Epoch 22/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1162\n",
      "Epoch 22: val_loss did not improve from 2.01832\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 3s 42ms/step - loss: 1.1162 - val_loss: 2.0728 - lr: 9.5099e-05\n",
      "Epoch 23/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1241\n",
      "Epoch 23: val_loss improved from 2.01832 to 1.99277, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 1.1223 - val_loss: 1.9928 - lr: 9.4148e-05\n",
      "Epoch 24/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1031\n",
      "Epoch 24: val_loss improved from 1.99277 to 1.97887, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 1.1032 - val_loss: 1.9789 - lr: 9.4148e-05\n",
      "Epoch 25/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1011\n",
      "Epoch 25: val_loss did not improve from 1.97887\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 3s 43ms/step - loss: 1.0997 - val_loss: 2.0182 - lr: 9.4148e-05\n",
      "Epoch 26/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0958\n",
      "Epoch 26: val_loss improved from 1.97887 to 1.92091, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 3s 52ms/step - loss: 1.0935 - val_loss: 1.9209 - lr: 9.3207e-05\n",
      "Epoch 27/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0919\n",
      "Epoch 27: val_loss did not improve from 1.92091\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 4s 59ms/step - loss: 1.0909 - val_loss: 1.9445 - lr: 9.3207e-05\n",
      "Epoch 28/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0851\n",
      "Epoch 28: val_loss did not improve from 1.92091\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 4s 55ms/step - loss: 1.0858 - val_loss: 1.9472 - lr: 9.2274e-05\n",
      "Epoch 29/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0938\n",
      "Epoch 29: val_loss did not improve from 1.92091\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 3s 52ms/step - loss: 1.0938 - val_loss: 1.9878 - lr: 9.1352e-05\n",
      "Epoch 30/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0833\n",
      "Epoch 30: val_loss did not improve from 1.92091\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 1.0818 - val_loss: 1.9785 - lr: 9.0438e-05\n",
      "Epoch 31/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0647\n",
      "Epoch 31: val_loss improved from 1.92091 to 1.90350, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 1.0647 - val_loss: 1.9035 - lr: 8.9534e-05\n",
      "Epoch 32/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0691\n",
      "Epoch 32: val_loss improved from 1.90350 to 1.87433, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 1.0718 - val_loss: 1.8743 - lr: 8.9534e-05\n",
      "Epoch 33/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0579\n",
      "Epoch 33: val_loss improved from 1.87433 to 1.86126, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 1.0579 - val_loss: 1.8613 - lr: 8.9534e-05\n",
      "Epoch 34/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0566\n",
      "Epoch 34: val_loss did not improve from 1.86126\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 1.0566 - val_loss: 1.8681 - lr: 8.9534e-05\n",
      "Epoch 35/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0718\n",
      "Epoch 35: val_loss did not improve from 1.86126\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 1.0701 - val_loss: 1.8907 - lr: 8.8638e-05\n",
      "Epoch 36/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0462\n",
      "Epoch 36: val_loss improved from 1.86126 to 1.84489, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 1.0462 - val_loss: 1.8449 - lr: 8.7752e-05\n",
      "Epoch 37/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0574\n",
      "Epoch 37: val_loss improved from 1.84489 to 1.82213, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 1.0621 - val_loss: 1.8221 - lr: 8.7752e-05\n",
      "Epoch 38/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0462\n",
      "Epoch 38: val_loss did not improve from 1.82213\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 1.0466 - val_loss: 1.8754 - lr: 8.7752e-05\n",
      "Epoch 39/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0449\n",
      "Epoch 39: val_loss did not improve from 1.82213\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 1.0460 - val_loss: 1.8434 - lr: 8.6875e-05\n",
      "Epoch 40/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0307\n",
      "Epoch 40: val_loss did not improve from 1.82213\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 1.0368 - val_loss: 1.8460 - lr: 8.6006e-05\n",
      "Epoch 41/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0404\n",
      "Epoch 41: val_loss did not improve from 1.82213\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 1.0391 - val_loss: 1.8397 - lr: 8.5146e-05\n",
      "Epoch 42/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0278\n",
      "Epoch 42: val_loss improved from 1.82213 to 1.81793, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 1.0252 - val_loss: 1.8179 - lr: 8.4294e-05\n",
      "Epoch 43/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0284\n",
      "Epoch 43: val_loss improved from 1.81793 to 1.79474, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 1.0284 - val_loss: 1.7947 - lr: 8.4294e-05\n",
      "Epoch 44/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0290\n",
      "Epoch 44: val_loss did not improve from 1.79474\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 1.0290 - val_loss: 1.8135 - lr: 8.4294e-05\n",
      "Epoch 45/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0281\n",
      "Epoch 45: val_loss did not improve from 1.79474\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 1.0281 - val_loss: 1.8153 - lr: 8.3451e-05\n",
      "Epoch 46/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0179\n",
      "Epoch 46: val_loss improved from 1.79474 to 1.77997, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 3s 52ms/step - loss: 1.0179 - val_loss: 1.7800 - lr: 8.2617e-05\n",
      "Epoch 47/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0149\n",
      "Epoch 47: val_loss improved from 1.77997 to 1.77066, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 1.0149 - val_loss: 1.7707 - lr: 8.2617e-05\n",
      "Epoch 48/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0181\n",
      "Epoch 48: val_loss did not improve from 1.77066\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 1.0181 - val_loss: 1.8388 - lr: 8.2617e-05\n",
      "Epoch 49/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0155\n",
      "Epoch 49: val_loss did not improve from 1.77066\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 1.0155 - val_loss: 1.8151 - lr: 8.1791e-05\n",
      "Epoch 50/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0194\n",
      "Epoch 50: val_loss did not improve from 1.77066\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 1.0199 - val_loss: 1.8462 - lr: 8.0973e-05\n",
      "Epoch 51/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0181\n",
      "Epoch 51: val_loss did not improve from 1.77066\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 1.0175 - val_loss: 1.7998 - lr: 8.0163e-05\n",
      "Epoch 52/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0111\n",
      "Epoch 52: val_loss did not improve from 1.77066\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 1.0113 - val_loss: 1.8126 - lr: 7.9361e-05\n",
      "Epoch 53/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - ETA: 0s - loss: 1.0151\n",
      "Epoch 53: val_loss did not improve from 1.77066\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 1.0151 - val_loss: 1.8106 - lr: 7.8568e-05\n",
      "Epoch 54/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0089\n",
      "Epoch 54: val_loss did not improve from 1.77066\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 3s 52ms/step - loss: 1.0089 - val_loss: 1.7768 - lr: 7.7782e-05\n",
      "Epoch 55/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0099\n",
      "Epoch 55: val_loss did not improve from 1.77066\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 1.0099 - val_loss: 1.7779 - lr: 7.7004e-05\n",
      "Epoch 56/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0029\n",
      "Epoch 56: val_loss did not improve from 1.77066\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 1.0029 - val_loss: 1.8095 - lr: 7.6234e-05\n",
      "Epoch 57/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0138\n",
      "Epoch 57: val_loss improved from 1.77066 to 1.74530, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 1.0152 - val_loss: 1.7453 - lr: 7.5472e-05\n",
      "Epoch 58/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0070\n",
      "Epoch 58: val_loss did not improve from 1.74530\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 1.0070 - val_loss: 1.7635 - lr: 7.5472e-05\n",
      "Epoch 59/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0084\n",
      "Epoch 59: val_loss did not improve from 1.74530\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 1.0084 - val_loss: 1.7943 - lr: 7.4717e-05\n",
      "Epoch 60/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0132\n",
      "Epoch 60: val_loss did not improve from 1.74530\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 1.0097 - val_loss: 1.8116 - lr: 7.3970e-05\n",
      "Epoch 61/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9953\n",
      "Epoch 61: val_loss did not improve from 1.74530\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9936 - val_loss: 1.7896 - lr: 7.3230e-05\n",
      "Epoch 62/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9980\n",
      "Epoch 62: val_loss did not improve from 1.74530\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9970 - val_loss: 1.7955 - lr: 7.2498e-05\n",
      "Epoch 63/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0000\n",
      "Epoch 63: val_loss did not improve from 1.74530\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 1.0000 - val_loss: 1.7751 - lr: 7.1773e-05\n",
      "Epoch 64/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9986\n",
      "Epoch 64: val_loss did not improve from 1.74530\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9972 - val_loss: 1.7544 - lr: 7.1055e-05\n",
      "Epoch 65/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0016\n",
      "Epoch 65: val_loss did not improve from 1.74530\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 1.0017 - val_loss: 1.8321 - lr: 7.0345e-05\n",
      "Epoch 66/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9929\n",
      "Epoch 66: val_loss did not improve from 1.74530\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9919 - val_loss: 1.7852 - lr: 6.9641e-05\n",
      "Epoch 67/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9929\n",
      "Epoch 67: val_loss did not improve from 1.74530\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9929 - val_loss: 1.8277 - lr: 6.8945e-05\n",
      "Epoch 68/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9929\n",
      "Epoch 68: val_loss did not improve from 1.74530\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9921 - val_loss: 1.8025 - lr: 6.8255e-05\n",
      "Epoch 69/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9978\n",
      "Epoch 69: val_loss did not improve from 1.74530\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9961 - val_loss: 1.7602 - lr: 6.7573e-05\n",
      "Epoch 70/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0029\n",
      "Epoch 70: val_loss did not improve from 1.74530\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 1.0041 - val_loss: 1.7740 - lr: 6.6897e-05\n",
      "Epoch 71/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9949\n",
      "Epoch 71: val_loss did not improve from 1.74530\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9949 - val_loss: 1.8135 - lr: 6.6228e-05\n",
      "Epoch 72/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9929\n",
      "Epoch 72: val_loss improved from 1.74530 to 1.73746, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.9921 - val_loss: 1.7375 - lr: 6.5566e-05\n",
      "Epoch 73/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9760\n",
      "Epoch 73: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9738 - val_loss: 1.7727 - lr: 6.5566e-05\n",
      "Epoch 74/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9808\n",
      "Epoch 74: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9814 - val_loss: 1.8159 - lr: 6.4910e-05\n",
      "Epoch 75/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9845\n",
      "Epoch 75: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9831 - val_loss: 1.7874 - lr: 6.4261e-05\n",
      "Epoch 76/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9755\n",
      "Epoch 76: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9754 - val_loss: 1.7391 - lr: 6.3619e-05\n",
      "Epoch 77/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9837\n",
      "Epoch 77: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9815 - val_loss: 1.7631 - lr: 6.2982e-05\n",
      "Epoch 78/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9735\n",
      "Epoch 78: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9735 - val_loss: 1.7947 - lr: 6.2353e-05\n",
      "Epoch 79/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9793\n",
      "Epoch 79: val_loss did not improve from 1.73746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9793 - val_loss: 1.7752 - lr: 6.1729e-05\n",
      "Epoch 80/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9813\n",
      "Epoch 80: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9813 - val_loss: 1.7542 - lr: 6.1112e-05\n",
      "Epoch 81/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9813\n",
      "Epoch 81: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9812 - val_loss: 1.7489 - lr: 6.0501e-05\n",
      "Epoch 82/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9793\n",
      "Epoch 82: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9792 - val_loss: 1.7641 - lr: 5.9896e-05\n",
      "Epoch 83/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9714\n",
      "Epoch 83: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 5.8703650938696225e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9714 - val_loss: 1.7710 - lr: 5.9297e-05\n",
      "Epoch 84/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9666\n",
      "Epoch 84: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 5.811661550978897e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9666 - val_loss: 1.7710 - lr: 5.8704e-05\n",
      "Epoch 85/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9739\n",
      "Epoch 85: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 5.753545068728272e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9739 - val_loss: 1.8064 - lr: 5.8117e-05\n",
      "Epoch 86/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9725\n",
      "Epoch 86: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 5.6960095243994146e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9725 - val_loss: 1.7802 - lr: 5.7535e-05\n",
      "Epoch 87/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9736\n",
      "Epoch 87: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 5.639049515593797e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9736 - val_loss: 1.7604 - lr: 5.6960e-05\n",
      "Epoch 88/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9680\n",
      "Epoch 88: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 5.5826589195930865e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9672 - val_loss: 1.7988 - lr: 5.6390e-05\n",
      "Epoch 89/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9682\n",
      "Epoch 89: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 5.526832333998754e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9670 - val_loss: 1.7737 - lr: 5.5827e-05\n",
      "Epoch 90/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9661\n",
      "Epoch 90: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 5.471563996252371e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9667 - val_loss: 1.7845 - lr: 5.5268e-05\n",
      "Epoch 91/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9682\n",
      "Epoch 91: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 5.416848503955407e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9682 - val_loss: 1.7901 - lr: 5.4716e-05\n",
      "Epoch 92/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9597\n",
      "Epoch 92: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 5.362680094549433e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.9622 - val_loss: 1.7619 - lr: 5.4168e-05\n",
      "Epoch 93/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9705\n",
      "Epoch 93: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 5.309053365635918e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9693 - val_loss: 1.7995 - lr: 5.3627e-05\n",
      "Epoch 94/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9554\n",
      "Epoch 94: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 5.2559629148163366e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9597 - val_loss: 1.7565 - lr: 5.3091e-05\n",
      "Epoch 95/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9638\n",
      "Epoch 95: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 5.2034033396921584e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9636 - val_loss: 1.8304 - lr: 5.2560e-05\n",
      "Epoch 96/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9744\n",
      "Epoch 96: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 5.1513692378648555e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9744 - val_loss: 1.7705 - lr: 5.2034e-05\n",
      "Epoch 97/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9628\n",
      "Epoch 97: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 5.099855567095801e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9632 - val_loss: 1.8289 - lr: 5.1514e-05\n",
      "Epoch 98/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9535\n",
      "Epoch 98: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 5.048856924986467e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9532 - val_loss: 1.7707 - lr: 5.0999e-05\n",
      "Epoch 99/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9752\n",
      "Epoch 99: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 4.998368269298226e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9752 - val_loss: 1.7811 - lr: 5.0489e-05\n",
      "Epoch 100/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9548\n",
      "Epoch 100: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 4.9483845577924514e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9548 - val_loss: 1.8358 - lr: 4.9984e-05\n",
      "Epoch 101/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9698\n",
      "Epoch 101: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 4.898900748230517e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9686 - val_loss: 1.7848 - lr: 4.9484e-05\n",
      "Epoch 102/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9624\n",
      "Epoch 102: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 4.849911798373796e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9624 - val_loss: 1.8335 - lr: 4.8989e-05\n",
      "Epoch 103/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9550\n",
      "Epoch 103: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 4.801412665983662e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9537 - val_loss: 1.8816 - lr: 4.8499e-05\n",
      "Epoch 104/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9635\n",
      "Epoch 104: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 4.75339866898139e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9623 - val_loss: 1.8136 - lr: 4.8014e-05\n",
      "Epoch 105/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9616\n",
      "Epoch 105: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 4.705864765128354e-05.\n",
      "66/66 [==============================] - 3s 46ms/step - loss: 0.9616 - val_loss: 1.8185 - lr: 4.7534e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9565\n",
      "Epoch 106: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 4.658806272345828e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.9566 - val_loss: 1.8022 - lr: 4.7059e-05\n",
      "Epoch 107/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9563\n",
      "Epoch 107: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 4.612218148395186e-05.\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.9535 - val_loss: 1.7863 - lr: 4.6588e-05\n",
      "Epoch 108/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9632\n",
      "Epoch 108: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 4.566096071357606e-05.\n",
      "66/66 [==============================] - 4s 54ms/step - loss: 0.9632 - val_loss: 1.7936 - lr: 4.6122e-05\n",
      "Epoch 109/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9591\n",
      "Epoch 109: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 4.52043499899446e-05.\n",
      "66/66 [==============================] - 3s 47ms/step - loss: 0.9591 - val_loss: 1.8397 - lr: 4.5661e-05\n",
      "Epoch 110/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9495\n",
      "Epoch 110: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 4.475230609386927e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.9503 - val_loss: 1.8927 - lr: 4.5204e-05\n",
      "Epoch 111/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9638\n",
      "Epoch 111: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 4.43047822045628e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.9638 - val_loss: 1.8417 - lr: 4.4752e-05\n",
      "Epoch 112/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9532\n",
      "Epoch 112: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 4.386173510283697e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.9532 - val_loss: 1.8542 - lr: 4.4305e-05\n",
      "Epoch 113/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9579\n",
      "Epoch 113: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 4.342311796790454e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.9575 - val_loss: 1.8365 - lr: 4.3862e-05\n",
      "Epoch 114/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9511\n",
      "Epoch 114: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 4.298888758057728e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.9511 - val_loss: 1.8743 - lr: 4.3423e-05\n",
      "Epoch 115/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9541\n",
      "Epoch 115: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 4.2558997120067946e-05.\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.9548 - val_loss: 1.8730 - lr: 4.2989e-05\n",
      "Epoch 116/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9459\n",
      "Epoch 116: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 4.2133406968787315e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9440 - val_loss: 1.8112 - lr: 4.2559e-05\n",
      "Epoch 117/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9537\n",
      "Epoch 117: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 4.1712073907547166e-05.\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.9537 - val_loss: 1.8774 - lr: 4.2133e-05\n",
      "Epoch 118/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9525\n",
      "Epoch 118: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 4.1294954717159274e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9519 - val_loss: 1.8817 - lr: 4.1712e-05\n",
      "Epoch 119/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9420\n",
      "Epoch 119: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 4.08820061784354e-05.\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.9406 - val_loss: 1.8535 - lr: 4.1295e-05\n",
      "Epoch 120/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9541\n",
      "Epoch 120: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 4.0473185072187336e-05.\n",
      "66/66 [==============================] - 4s 59ms/step - loss: 0.9541 - val_loss: 1.8732 - lr: 4.0882e-05\n",
      "Epoch 121/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9577\n",
      "Epoch 121: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 4.0068451780825854e-05.\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.9560 - val_loss: 1.8449 - lr: 4.0473e-05\n",
      "Epoch 122/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9454\n",
      "Epoch 122: val_loss did not improve from 1.73746\n",
      "\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 3.966776668676175e-05.\n",
      "66/66 [==============================] - 3s 40ms/step - loss: 0.9457 - val_loss: 1.8735 - lr: 4.0068e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_15 (InputLayer)          [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_42 (LSTM)                 (None, 45, 24)       3744        ['input_15[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_28 (Dropout)           (None, 45, 24)       0           ['lstm_42[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_43 (LSTM)                 (None, 45, 16)       2624        ['dropout_28[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_29 (Dropout)           (None, 45, 16)       0           ['lstm_43[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_44 (LSTM)                 (None, 32)           6272        ['dropout_29[0][0]']             \n",
      "                                                                                                  \n",
      " dense_28 (Dense)               (None, 40)           1320        ['lstm_44[0][0]']                \n",
      "                                                                                                  \n",
      " dense_29 (Dense)               (None, 5)            205         ['dense_28[0][0]']               \n",
      "                                                                                                  \n",
      " tf.unstack_14 (TFOpLambda)     [(None,),            0           ['dense_29[0][0]']               \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_70 (TFOpLambda)  (None, 1)           0           ['tf.unstack_14[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_28 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_70[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_74 (TFOpLambda)  (None, 1)           0           ['tf.unstack_14[0][4]']          \n",
      "                                                                                                  \n",
      " tf.math.multiply_42 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_28[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_29 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_74[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_43 (TFOpLambd  (None, 1)           0           ['tf.math.multiply_42[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_71 (TFOpLambda)  (None, 1)           0           ['tf.unstack_14[0][1]']          \n",
      "                                                                                                  \n",
      " tf.expand_dims_73 (TFOpLambda)  (None, 1)           0           ['tf.unstack_14[0][3]']          \n",
      "                                                                                                  \n",
      " tf.math.multiply_44 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_29[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_28 (TFOpL  (None, 1)           0           ['tf.math.multiply_43[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.softplus_28 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_71[0][0]']      \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_72 (TFOpLambda)  (None, 1)           0           ['tf.unstack_14[0][2]']          \n",
      "                                                                                                  \n",
      " tf.math.softplus_29 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_73[0][0]']      \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_29 (TFOpL  (None, 1)           0           ['tf.math.multiply_44[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack_14 (TFOpLambda)       (None, 5, 1)         0           ['tf.__operators__.add_28[0][0]',\n",
      "                                                                  'tf.math.softplus_28[0][0]',    \n",
      "                                                                  'tf.expand_dims_72[0][0]',      \n",
      "                                                                  'tf.math.softplus_29[0][0]',    \n",
      "                                                                  'tf.__operators__.add_29[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _1_CCMP_t+1_0.19\n",
      "Epoch 1/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 3.4777\n",
      "Epoch 1: val_loss improved from inf to 3.99463, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 10s 79ms/step - loss: 3.4813 - val_loss: 3.9946 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 2.9127\n",
      "Epoch 2: val_loss improved from 3.99463 to 3.29913, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 4s 68ms/step - loss: 2.9111 - val_loss: 3.2991 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 2.0310\n",
      "Epoch 3: val_loss improved from 3.29913 to 3.01345, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 4s 65ms/step - loss: 2.0315 - val_loss: 3.0134 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/66 [============================>.] - ETA: 0s - loss: 1.6727\n",
      "Epoch 4: val_loss improved from 3.01345 to 2.77717, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 5s 68ms/step - loss: 1.6753 - val_loss: 2.7772 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.5426\n",
      "Epoch 5: val_loss improved from 2.77717 to 2.73104, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 4s 66ms/step - loss: 1.5416 - val_loss: 2.7310 - lr: 1.0000e-04\n",
      "Epoch 6/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.4530\n",
      "Epoch 6: val_loss improved from 2.73104 to 2.62522, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 1.4606 - val_loss: 2.6252 - lr: 1.0000e-04\n",
      "Epoch 7/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.3882\n",
      "Epoch 7: val_loss improved from 2.62522 to 2.37631, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 4s 66ms/step - loss: 1.3887 - val_loss: 2.3763 - lr: 1.0000e-04\n",
      "Epoch 8/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.3374\n",
      "Epoch 8: val_loss did not improve from 2.37631\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 4s 66ms/step - loss: 1.3374 - val_loss: 2.4051 - lr: 1.0000e-04\n",
      "Epoch 9/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.3082\n",
      "Epoch 9: val_loss improved from 2.37631 to 2.35415, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 4s 66ms/step - loss: 1.3069 - val_loss: 2.3541 - lr: 9.9000e-05\n",
      "Epoch 10/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.2813\n",
      "Epoch 10: val_loss did not improve from 2.35415\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 4s 66ms/step - loss: 1.2807 - val_loss: 2.4074 - lr: 9.9000e-05\n",
      "Epoch 11/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.2452\n",
      "Epoch 11: val_loss did not improve from 2.35415\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 4s 66ms/step - loss: 1.2459 - val_loss: 2.3920 - lr: 9.8010e-05\n",
      "Epoch 12/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.2272\n",
      "Epoch 12: val_loss improved from 2.35415 to 2.31170, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 4s 67ms/step - loss: 1.2269 - val_loss: 2.3117 - lr: 9.7030e-05\n",
      "Epoch 13/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.2182\n",
      "Epoch 13: val_loss improved from 2.31170 to 2.30120, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 4s 67ms/step - loss: 1.2157 - val_loss: 2.3012 - lr: 9.7030e-05\n",
      "Epoch 14/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1944\n",
      "Epoch 14: val_loss did not improve from 2.30120\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 5s 69ms/step - loss: 1.1913 - val_loss: 2.4973 - lr: 9.7030e-05\n",
      "Epoch 15/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1831\n",
      "Epoch 15: val_loss improved from 2.30120 to 2.12006, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 4s 66ms/step - loss: 1.1813 - val_loss: 2.1201 - lr: 9.6060e-05\n",
      "Epoch 16/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1727\n",
      "Epoch 16: val_loss did not improve from 2.12006\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 4s 67ms/step - loss: 1.1717 - val_loss: 2.1999 - lr: 9.6060e-05\n",
      "Epoch 17/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1540\n",
      "Epoch 17: val_loss did not improve from 2.12006\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 4s 66ms/step - loss: 1.1559 - val_loss: 2.1833 - lr: 9.5099e-05\n",
      "Epoch 18/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1506\n",
      "Epoch 18: val_loss did not improve from 2.12006\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 4s 66ms/step - loss: 1.1533 - val_loss: 2.1976 - lr: 9.4148e-05\n",
      "Epoch 19/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1434\n",
      "Epoch 19: val_loss did not improve from 2.12006\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 4s 66ms/step - loss: 1.1438 - val_loss: 2.1477 - lr: 9.3207e-05\n",
      "Epoch 20/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1356\n",
      "Epoch 20: val_loss improved from 2.12006 to 2.10087, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 4s 67ms/step - loss: 1.1368 - val_loss: 2.1009 - lr: 9.2274e-05\n",
      "Epoch 21/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1258\n",
      "Epoch 21: val_loss did not improve from 2.10087\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 4s 66ms/step - loss: 1.1260 - val_loss: 2.1254 - lr: 9.2274e-05\n",
      "Epoch 22/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1193\n",
      "Epoch 22: val_loss improved from 2.10087 to 2.07174, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 4s 66ms/step - loss: 1.1189 - val_loss: 2.0717 - lr: 9.1352e-05\n",
      "Epoch 23/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1218\n",
      "Epoch 23: val_loss did not improve from 2.07174\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 4s 66ms/step - loss: 1.1220 - val_loss: 2.1136 - lr: 9.1352e-05\n",
      "Epoch 24/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1202\n",
      "Epoch 24: val_loss improved from 2.07174 to 2.05814, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 4s 67ms/step - loss: 1.1182 - val_loss: 2.0581 - lr: 9.0438e-05\n",
      "Epoch 25/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1066\n",
      "Epoch 25: val_loss did not improve from 2.05814\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 4s 66ms/step - loss: 1.1080 - val_loss: 2.0672 - lr: 9.0438e-05\n",
      "Epoch 26/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1091\n",
      "Epoch 26: val_loss did not improve from 2.05814\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 4s 66ms/step - loss: 1.1085 - val_loss: 2.1174 - lr: 8.9534e-05\n",
      "Epoch 27/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1136\n",
      "Epoch 27: val_loss did not improve from 2.05814\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 4s 66ms/step - loss: 1.1119 - val_loss: 2.1123 - lr: 8.8638e-05\n",
      "Epoch 28/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0947\n",
      "Epoch 28: val_loss improved from 2.05814 to 2.04582, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 4s 67ms/step - loss: 1.0937 - val_loss: 2.0458 - lr: 8.7752e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0872\n",
      "Epoch 29: val_loss improved from 2.04582 to 1.99269, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 4s 67ms/step - loss: 1.0880 - val_loss: 1.9927 - lr: 8.7752e-05\n",
      "Epoch 30/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1065\n",
      "Epoch 30: val_loss did not improve from 1.99269\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 4s 66ms/step - loss: 1.1061 - val_loss: 2.0730 - lr: 8.7752e-05\n",
      "Epoch 31/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0937\n",
      "Epoch 31: val_loss did not improve from 1.99269\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 4s 68ms/step - loss: 1.0927 - val_loss: 2.0007 - lr: 8.6875e-05\n",
      "Epoch 32/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0814\n",
      "Epoch 32: val_loss did not improve from 1.99269\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 4s 66ms/step - loss: 1.0858 - val_loss: 2.0564 - lr: 8.6006e-05\n",
      "Epoch 33/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0904\n",
      "Epoch 33: val_loss did not improve from 1.99269\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 4s 67ms/step - loss: 1.0904 - val_loss: 2.0421 - lr: 8.5146e-05\n",
      "Epoch 34/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0750\n",
      "Epoch 34: val_loss did not improve from 1.99269\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 4s 67ms/step - loss: 1.0755 - val_loss: 1.9986 - lr: 8.4294e-05\n",
      "Epoch 35/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0728\n",
      "Epoch 35: val_loss did not improve from 1.99269\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 4s 66ms/step - loss: 1.0737 - val_loss: 2.0583 - lr: 8.3451e-05\n",
      "Epoch 36/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0693\n",
      "Epoch 36: val_loss did not improve from 1.99269\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 4s 66ms/step - loss: 1.0681 - val_loss: 2.0073 - lr: 8.2617e-05\n",
      "Epoch 37/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0645\n",
      "Epoch 37: val_loss did not improve from 1.99269\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 1.0645 - val_loss: 2.0207 - lr: 8.1791e-05\n",
      "Epoch 38/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0665\n",
      "Epoch 38: val_loss did not improve from 1.99269\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 1.0665 - val_loss: 2.0065 - lr: 8.0973e-05\n",
      "Epoch 39/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0676\n",
      "Epoch 39: val_loss improved from 1.99269 to 1.95365, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 6s 84ms/step - loss: 1.0676 - val_loss: 1.9537 - lr: 8.0163e-05\n",
      "Epoch 40/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0550\n",
      "Epoch 40: val_loss improved from 1.95365 to 1.94914, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 5s 82ms/step - loss: 1.0529 - val_loss: 1.9491 - lr: 8.0163e-05\n",
      "Epoch 41/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0600\n",
      "Epoch 41: val_loss did not improve from 1.94914\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 1.0600 - val_loss: 1.9841 - lr: 8.0163e-05\n",
      "Epoch 42/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0613\n",
      "Epoch 42: val_loss did not improve from 1.94914\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 1.0619 - val_loss: 1.9572 - lr: 7.9361e-05\n",
      "Epoch 43/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0667\n",
      "Epoch 43: val_loss did not improve from 1.94914\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 1.0646 - val_loss: 1.9537 - lr: 7.8568e-05\n",
      "Epoch 44/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0543\n",
      "Epoch 44: val_loss did not improve from 1.94914\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 1.0548 - val_loss: 1.9859 - lr: 7.7782e-05\n",
      "Epoch 45/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0521\n",
      "Epoch 45: val_loss did not improve from 1.94914\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 1.0521 - val_loss: 1.9606 - lr: 7.7004e-05\n",
      "Epoch 46/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0580\n",
      "Epoch 46: val_loss did not improve from 1.94914\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 1.0577 - val_loss: 1.9677 - lr: 7.6234e-05\n",
      "Epoch 47/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0513\n",
      "Epoch 47: val_loss did not improve from 1.94914\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 1.0497 - val_loss: 1.9938 - lr: 7.5472e-05\n",
      "Epoch 48/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0631\n",
      "Epoch 48: val_loss did not improve from 1.94914\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 1.0631 - val_loss: 1.9816 - lr: 7.4717e-05\n",
      "Epoch 49/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0315\n",
      "Epoch 49: val_loss improved from 1.94914 to 1.93789, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 1.0301 - val_loss: 1.9379 - lr: 7.3970e-05\n",
      "Epoch 50/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0488\n",
      "Epoch 50: val_loss did not improve from 1.93789\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 1.0498 - val_loss: 1.9503 - lr: 7.3970e-05\n",
      "Epoch 51/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0502\n",
      "Epoch 51: val_loss improved from 1.93789 to 1.92637, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 1.0497 - val_loss: 1.9264 - lr: 7.3230e-05\n",
      "Epoch 52/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0480\n",
      "Epoch 52: val_loss did not improve from 1.92637\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 1.0476 - val_loss: 1.9660 - lr: 7.3230e-05\n",
      "Epoch 53/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0405\n",
      "Epoch 53: val_loss did not improve from 1.92637\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 1.0389 - val_loss: 1.9737 - lr: 7.2498e-05\n",
      "Epoch 54/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0338\n",
      "Epoch 54: val_loss did not improve from 1.92637\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 1.0345 - val_loss: 1.9917 - lr: 7.1773e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0346\n",
      "Epoch 55: val_loss did not improve from 1.92637\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 1.0363 - val_loss: 1.9402 - lr: 7.1055e-05\n",
      "Epoch 56/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0396\n",
      "Epoch 56: val_loss did not improve from 1.92637\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 1.0419 - val_loss: 1.9430 - lr: 7.0345e-05\n",
      "Epoch 57/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0231\n",
      "Epoch 57: val_loss did not improve from 1.92637\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 1.0216 - val_loss: 1.9309 - lr: 6.9641e-05\n",
      "Epoch 58/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0277\n",
      "Epoch 58: val_loss did not improve from 1.92637\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 1.0317 - val_loss: 1.9471 - lr: 6.8945e-05\n",
      "Epoch 59/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0325\n",
      "Epoch 59: val_loss did not improve from 1.92637\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 1.0349 - val_loss: 1.9444 - lr: 6.8255e-05\n",
      "Epoch 60/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0240\n",
      "Epoch 60: val_loss did not improve from 1.92637\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 1.0306 - val_loss: 1.9381 - lr: 6.7573e-05\n",
      "Epoch 61/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0266\n",
      "Epoch 61: val_loss did not improve from 1.92637\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 1.0293 - val_loss: 1.9488 - lr: 6.6897e-05\n",
      "Epoch 62/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0325\n",
      "Epoch 62: val_loss did not improve from 1.92637\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 1.0301 - val_loss: 1.9355 - lr: 6.6228e-05\n",
      "Epoch 63/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0252\n",
      "Epoch 63: val_loss did not improve from 1.92637\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 1.0263 - val_loss: 1.9345 - lr: 6.5566e-05\n",
      "Epoch 64/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0202\n",
      "Epoch 64: val_loss did not improve from 1.92637\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 5s 75ms/step - loss: 1.0214 - val_loss: 1.9461 - lr: 6.4910e-05\n",
      "Epoch 65/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0369\n",
      "Epoch 65: val_loss did not improve from 1.92637\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 5s 75ms/step - loss: 1.0356 - val_loss: 1.9310 - lr: 6.4261e-05\n",
      "Epoch 66/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0295\n",
      "Epoch 66: val_loss did not improve from 1.92637\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 5s 75ms/step - loss: 1.0289 - val_loss: 1.9519 - lr: 6.3619e-05\n",
      "Epoch 67/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0175\n",
      "Epoch 67: val_loss did not improve from 1.92637\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 1.0175 - val_loss: 1.9407 - lr: 6.2982e-05\n",
      "Epoch 68/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0251\n",
      "Epoch 68: val_loss did not improve from 1.92637\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 5s 75ms/step - loss: 1.0249 - val_loss: 1.9485 - lr: 6.2353e-05\n",
      "Epoch 69/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0295\n",
      "Epoch 69: val_loss did not improve from 1.92637\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 5s 75ms/step - loss: 1.0352 - val_loss: 1.9332 - lr: 6.1729e-05\n",
      "Epoch 70/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0162\n",
      "Epoch 70: val_loss did not improve from 1.92637\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 1.0153 - val_loss: 1.9590 - lr: 6.1112e-05\n",
      "Epoch 71/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0234\n",
      "Epoch 71: val_loss did not improve from 1.92637\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 5s 75ms/step - loss: 1.0233 - val_loss: 1.9459 - lr: 6.0501e-05\n",
      "Epoch 72/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0188\n",
      "Epoch 72: val_loss did not improve from 1.92637\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 5s 75ms/step - loss: 1.0203 - val_loss: 1.9386 - lr: 5.9896e-05\n",
      "Epoch 73/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0199\n",
      "Epoch 73: val_loss did not improve from 1.92637\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 5.8703650938696225e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 1.0195 - val_loss: 1.9422 - lr: 5.9297e-05\n",
      "Epoch 74/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0187\n",
      "Epoch 74: val_loss did not improve from 1.92637\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 5.811661550978897e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 1.0182 - val_loss: 1.9405 - lr: 5.8704e-05\n",
      "Epoch 75/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0248\n",
      "Epoch 75: val_loss did not improve from 1.92637\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 5.753545068728272e-05.\n",
      "66/66 [==============================] - 5s 75ms/step - loss: 1.0229 - val_loss: 1.9580 - lr: 5.8117e-05\n",
      "Epoch 76/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0090\n",
      "Epoch 76: val_loss did not improve from 1.92637\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 5.6960095243994146e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 1.0076 - val_loss: 1.9408 - lr: 5.7535e-05\n",
      "Epoch 77/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0106\n",
      "Epoch 77: val_loss did not improve from 1.92637\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 5.639049515593797e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 1.0111 - val_loss: 1.9303 - lr: 5.6960e-05\n",
      "Epoch 78/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0126\n",
      "Epoch 78: val_loss did not improve from 1.92637\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 5.5826589195930865e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 1.0112 - val_loss: 1.9378 - lr: 5.6390e-05\n",
      "Epoch 79/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0065\n",
      "Epoch 79: val_loss did not improve from 1.92637\n",
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 5.526832333998754e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 1.0078 - val_loss: 1.9415 - lr: 5.5827e-05\n",
      "Epoch 80/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0052\n",
      "Epoch 80: val_loss did not improve from 1.92637\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 5.471563996252371e-05.\n",
      "66/66 [==============================] - 5s 75ms/step - loss: 1.0051 - val_loss: 1.9376 - lr: 5.5268e-05\n",
      "Epoch 81/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0179\n",
      "Epoch 81: val_loss did not improve from 1.92637\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 5.416848503955407e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 5s 74ms/step - loss: 1.0163 - val_loss: 1.9497 - lr: 5.4716e-05\n",
      "Epoch 82/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0155\n",
      "Epoch 82: val_loss did not improve from 1.92637\n",
      "\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 5.362680094549433e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 1.0169 - val_loss: 1.9587 - lr: 5.4168e-05\n",
      "Epoch 83/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0125\n",
      "Epoch 83: val_loss did not improve from 1.92637\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 5.309053365635918e-05.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 1.0123 - val_loss: 1.9392 - lr: 5.3627e-05\n",
      "Epoch 84/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0134\n",
      "Epoch 84: val_loss did not improve from 1.92637\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 5.2559629148163366e-05.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 1.0160 - val_loss: 1.9384 - lr: 5.3091e-05\n",
      "Epoch 85/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0115\n",
      "Epoch 85: val_loss did not improve from 1.92637\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 5.2034033396921584e-05.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 1.0110 - val_loss: 1.9545 - lr: 5.2560e-05\n",
      "Epoch 86/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0099\n",
      "Epoch 86: val_loss did not improve from 1.92637\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 5.1513692378648555e-05.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 1.0105 - val_loss: 1.9289 - lr: 5.2034e-05\n",
      "Epoch 87/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0078\n",
      "Epoch 87: val_loss improved from 1.92637 to 1.92439, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_1_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 1.0075 - val_loss: 1.9244 - lr: 5.1514e-05\n",
      "Epoch 88/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0061\n",
      "Epoch 88: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 5.099855567095801e-05.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 1.0056 - val_loss: 1.9475 - lr: 5.1514e-05\n",
      "Epoch 89/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0044\n",
      "Epoch 89: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 5.048856924986467e-05.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 1.0062 - val_loss: 1.9517 - lr: 5.0999e-05\n",
      "Epoch 90/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0092\n",
      "Epoch 90: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 4.998368269298226e-05.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 1.0069 - val_loss: 1.9446 - lr: 5.0489e-05\n",
      "Epoch 91/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0056\n",
      "Epoch 91: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 4.9483845577924514e-05.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 1.0020 - val_loss: 1.9400 - lr: 4.9984e-05\n",
      "Epoch 92/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0080\n",
      "Epoch 92: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 4.898900748230517e-05.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 1.0106 - val_loss: 1.9503 - lr: 4.9484e-05\n",
      "Epoch 93/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0124\n",
      "Epoch 93: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 4.849911798373796e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 1.0144 - val_loss: 1.9313 - lr: 4.8989e-05\n",
      "Epoch 94/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0066\n",
      "Epoch 94: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 4.801412665983662e-05.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 1.0071 - val_loss: 1.9526 - lr: 4.8499e-05\n",
      "Epoch 95/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9967\n",
      "Epoch 95: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 4.75339866898139e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.9967 - val_loss: 1.9588 - lr: 4.8014e-05\n",
      "Epoch 96/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0102\n",
      "Epoch 96: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 4.705864765128354e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 1.0103 - val_loss: 1.9428 - lr: 4.7534e-05\n",
      "Epoch 97/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0054\n",
      "Epoch 97: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 4.658806272345828e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 1.0054 - val_loss: 1.9789 - lr: 4.7059e-05\n",
      "Epoch 98/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0077\n",
      "Epoch 98: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 4.612218148395186e-05.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 1.0066 - val_loss: 1.9635 - lr: 4.6588e-05\n",
      "Epoch 99/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9960\n",
      "Epoch 99: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 4.566096071357606e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.9955 - val_loss: 1.9475 - lr: 4.6122e-05\n",
      "Epoch 100/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0004\n",
      "Epoch 100: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 4.52043499899446e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 1.0007 - val_loss: 1.9650 - lr: 4.5661e-05\n",
      "Epoch 101/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0093\n",
      "Epoch 101: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 4.475230609386927e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 1.0092 - val_loss: 1.9332 - lr: 4.5204e-05\n",
      "Epoch 102/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0066\n",
      "Epoch 102: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 4.43047822045628e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 1.0072 - val_loss: 1.9459 - lr: 4.4752e-05\n",
      "Epoch 103/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9894\n",
      "Epoch 103: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 4.386173510283697e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.9873 - val_loss: 1.9419 - lr: 4.4305e-05\n",
      "Epoch 104/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0025\n",
      "Epoch 104: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 4.342311796790454e-05.\n",
      "66/66 [==============================] - 5s 75ms/step - loss: 1.0011 - val_loss: 1.9539 - lr: 4.3862e-05\n",
      "Epoch 105/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9964\n",
      "Epoch 105: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 4.298888758057728e-05.\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.9957 - val_loss: 1.9485 - lr: 4.3423e-05\n",
      "Epoch 106/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0029\n",
      "Epoch 106: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 4.2558997120067946e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 1.0026 - val_loss: 1.9368 - lr: 4.2989e-05\n",
      "Epoch 107/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9986\n",
      "Epoch 107: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 4.2133406968787315e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.9977 - val_loss: 1.9542 - lr: 4.2559e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9971\n",
      "Epoch 108: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 4.1712073907547166e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.9977 - val_loss: 1.9651 - lr: 4.2133e-05\n",
      "Epoch 109/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9825\n",
      "Epoch 109: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 4.1294954717159274e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.9846 - val_loss: 1.9750 - lr: 4.1712e-05\n",
      "Epoch 110/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0139\n",
      "Epoch 110: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 4.08820061784354e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 1.0116 - val_loss: 1.9612 - lr: 4.1295e-05\n",
      "Epoch 111/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9966\n",
      "Epoch 111: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 4.0473185072187336e-05.\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.9950 - val_loss: 1.9708 - lr: 4.0882e-05\n",
      "Epoch 112/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0024\n",
      "Epoch 112: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 4.0068451780825854e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 1.0014 - val_loss: 1.9480 - lr: 4.0473e-05\n",
      "Epoch 113/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0027\n",
      "Epoch 113: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 3.966776668676175e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 1.0005 - val_loss: 1.9633 - lr: 4.0068e-05\n",
      "Epoch 114/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9865\n",
      "Epoch 114: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 3.927109017240582e-05.\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.9919 - val_loss: 1.9680 - lr: 3.9668e-05\n",
      "Epoch 115/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9928\n",
      "Epoch 115: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 3.887837901856983e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.9928 - val_loss: 1.9845 - lr: 3.9271e-05\n",
      "Epoch 116/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9931\n",
      "Epoch 116: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 3.848959360766457e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.9922 - val_loss: 1.9715 - lr: 3.8878e-05\n",
      "Epoch 117/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9827\n",
      "Epoch 117: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 3.810469792369986e-05.\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.9858 - val_loss: 1.9815 - lr: 3.8490e-05\n",
      "Epoch 118/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9876\n",
      "Epoch 118: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 3.772365234908648e-05.\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.9882 - val_loss: 1.9702 - lr: 3.8105e-05\n",
      "Epoch 119/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9884\n",
      "Epoch 119: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 3.734641726623522e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.9878 - val_loss: 1.9772 - lr: 3.7724e-05\n",
      "Epoch 120/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9927\n",
      "Epoch 120: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 3.6972953057556876e-05.\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.9930 - val_loss: 1.9810 - lr: 3.7346e-05\n",
      "Epoch 121/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9942\n",
      "Epoch 121: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 3.660322370706126e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.9930 - val_loss: 1.9804 - lr: 3.6973e-05\n",
      "Epoch 122/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9923\n",
      "Epoch 122: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 3.623719319875818e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.9908 - val_loss: 1.9924 - lr: 3.6603e-05\n",
      "Epoch 123/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0003\n",
      "Epoch 123: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 123: ReduceLROnPlateau reducing learning rate to 3.5874821915058416e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.9993 - val_loss: 1.9826 - lr: 3.6237e-05\n",
      "Epoch 124/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9928\n",
      "Epoch 124: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 124: ReduceLROnPlateau reducing learning rate to 3.551607383997179e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.9919 - val_loss: 1.9891 - lr: 3.5875e-05\n",
      "Epoch 125/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9883\n",
      "Epoch 125: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 3.516091295750812e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.9875 - val_loss: 1.9723 - lr: 3.5516e-05\n",
      "Epoch 126/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9928\n",
      "Epoch 126: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 3.480930325167719e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.9915 - val_loss: 1.9806 - lr: 3.5161e-05\n",
      "Epoch 127/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9851\n",
      "Epoch 127: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 127: ReduceLROnPlateau reducing learning rate to 3.446120870648883e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.9876 - val_loss: 1.9789 - lr: 3.4809e-05\n",
      "Epoch 128/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9902\n",
      "Epoch 128: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 128: ReduceLROnPlateau reducing learning rate to 3.4116596907551866e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.9914 - val_loss: 1.9800 - lr: 3.4461e-05\n",
      "Epoch 129/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9853\n",
      "Epoch 129: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 129: ReduceLROnPlateau reducing learning rate to 3.37754318388761e-05.\n",
      "66/66 [==============================] - 5s 75ms/step - loss: 0.9869 - val_loss: 1.9769 - lr: 3.4117e-05\n",
      "Epoch 130/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9975\n",
      "Epoch 130: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 130: ReduceLROnPlateau reducing learning rate to 3.3437677484471354e-05.\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.9981 - val_loss: 1.9875 - lr: 3.3775e-05\n",
      "Epoch 131/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9852\n",
      "Epoch 131: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 131: ReduceLROnPlateau reducing learning rate to 3.310330142994644e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.9864 - val_loss: 1.9832 - lr: 3.3438e-05\n",
      "Epoch 132/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0015\n",
      "Epoch 132: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 132: ReduceLROnPlateau reducing learning rate to 3.277226765931118e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.9999 - val_loss: 1.9794 - lr: 3.3103e-05\n",
      "Epoch 133/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9878\n",
      "Epoch 133: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 133: ReduceLROnPlateau reducing learning rate to 3.24445437581744e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.9868 - val_loss: 1.9681 - lr: 3.2772e-05\n",
      "Epoch 134/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9868\n",
      "Epoch 134: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 134: ReduceLROnPlateau reducing learning rate to 3.212009731214493e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.9910 - val_loss: 1.9701 - lr: 3.2445e-05\n",
      "Epoch 135/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0020\n",
      "Epoch 135: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 135: ReduceLROnPlateau reducing learning rate to 3.17988959068316e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 1.0000 - val_loss: 1.9749 - lr: 3.2120e-05\n",
      "Epoch 136/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9944\n",
      "Epoch 136: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 3.1480907127843236e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.9935 - val_loss: 1.9881 - lr: 3.1799e-05\n",
      "Epoch 137/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9880\n",
      "Epoch 137: val_loss did not improve from 1.92439\n",
      "\n",
      "Epoch 137: ReduceLROnPlateau reducing learning rate to 3.116609856078867e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.9889 - val_loss: 1.9758 - lr: 3.1481e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD7CAYAAABgzo9kAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA+UUlEQVR4nO3dd3gc1b3/8ffM9lXvsuXejm1csTEG23QImBZyaZdAQmghF5KQBBISSig/QkJCekiC6eQSSCAFLsVAqAZMtbGx8bHBRbIlW8Xq2j7z+2MlWZYlWZIlrXb9fT0PD9qZ2dmPVuvvnjlz5oxh2zZCCCFSj5noAEIIIQaHFHghhEhRUuCFECJFSYEXQogUJQVeCCFSlDPBr+8BDgMqgFiCswghRLJwACOA94FQdxslusAfBryZ4AxCCJGslgAruluZ6AJfAVBb24xl9X08fl5eOjU1TQMeajBJ5qGRbJmTLS9I5qHSVWbTNMjJSYPWGtqdRBf4GIBl2f0q8G3PTTaSeWgkW+ZkywuSeaj0kLnHrm05ySqEEClKCrwQQqSoRHfRCCGGkG3b1NZWEQ4HgQPrqqisNLEsa2CCDZHkymzgdnvJy0vr9x56XeCVUr8A8rXWF3daPge4D8gE3gCu1FpH+51ICDFomprqMQyDoqJRGMaBHcA7nSbRaLIUy7hkymzbFnV11VRXV2MYvn7to1d/YaXU8cBXu1n9F+BqrfUUwAAu71cSIcSgCwSayMjIPuDiLgafYZhkZORQW1vb733s96+slMoF7gB+0sW6sYBPa72yddFDwDn9TiOEGFSWFcPhkJ7ZZOFwOIlG+38NaG/+0n8GbgBGd7FuJHuPw6wARvU1RF5eel+fQsumD9n+r8coueRnGEn2gS0oyEh0hD6TzINvKPJWVpq4XI4B25/TmXxHAsmYub+fjR4ro1LqMqBMa/0fpdTFXWxisveZGgPocwdXTU1Tn8emhrdvI1y5lcoduzB9mX19yYQpKMigqqox0TH6RDIPvqHKa1nWgPVBH2h/9t13/4y1az8mGo2wfXsZ48ZNAOCcc87n1FPP6NU+Lr74Ah566LFu169Y8TobNnzKZZddeUCZ77jjFubOncfSpaf3+bkDofNnwzSNXjWM99f0PQ8YoZRaDeQC6UqpX2mtv9O6fjvx+RDaFAPlvcx8QAyXN/5DJAT9O/8ghEig733vBwBUVJTzzW9+vcdC3Z39PWfx4qNZvPjofuVLBT0WeK31iW0/t7bgj+lQ3NFab1NKBZVSi7TWbwEXAc8PVti9tBZ4OxIckpcTQgyds88+nenTZ7Bpk+aee+7jb3/7Kx9++D4NDQ3k5+dz2213kpubx+LF81mx4gPuv//PVFdXUVZWyq5dOznttDP56lcv5bnnnmHVqg+54YZbOPvs0znllFNZufJtAoEgN954K1OnTmPz5s+4445bicVizJ49h5Ur3+aJJ/7VbbZnn32axx//C4ZhoNQ0vvOd7+N2u7nzzlvZvPlzAM466xzOOOMsXnzxBR577BFM02TkyJHcdNPteDyeIXoX+zkOXin1HHCz1voD4MvAMqVUJvAR8NsBzNetPS14KfBC9MdbaytYsabHqUx6ZBjQ3S2dF88awaKZI7pe2UsLFx7JbbfdyfbtZZSWbuVPf3oA0zS5/fabWb78ef77vy/ca/vPPtvEPffcR1NTI+ee+0W+9KVz99lnVlYWy5Y9wpNPPs6jjz7AHXf8nP/3/27h8suv5IgjFvPEE/9LLNb9Sc3PP/+MRx55gHvvfYisrGzuvvtnPPjgMo48cjENDQ08+OBjVFdX8cc//o4zzjiLZcv+yL33PkhOTi5/+MNvKC3dyuTJ6oDel77odYHXWj9EfJQMWuulHZZ/DCwY6GD7JS14IVLa9OkzABg1ajRXX/0dnnnmX5SWbmPdurWUlOw7luPQQ+fjcrnIycklMzOT5uZ9JxVbuPBIACZMmMTrr79KQ0M9O3dWcMQRiwE49dQz+fvfH+820+rVH7Jo0RKysrIBOOOMs7jzzlu58MKvUlq6je9+92oWLlzEVVd9G4BFi5bwjW9cylFHHcPRRx83pMUdkvhKVkMKvBAHZNHMA2tlD/ZFQ21dGRs2fMott9zA+edfwLHHHo/DYWJ3cejgdrvbfzYMo5tt9nSP2LaNaTq63K47+w4GsYnFYmRlZfPoo3/j/fff5Z133uKSSy7k0Uf/xjXXXMtnn53JO++s4Pbbb+KSS67gC19Y2uW+B0PyjRdqJV00QhwcVq/+kLlz5/HFL57N6NFjePvtFQM23UB6ejolJaN45523AHjppRcwDKPb7efOnceKFW/Q0FAPwNNP/4u5c+ezYsXr3H77zRx55GKuueZafD4flZW7OP/8s8jOzuaii77GySefysaNekBy91bStuBxxb+J7Ui3NzMRQqSA448/iR/96Dq+8pXzAFBqGhUVAzdY78Ybb+XOO29j2bJ7mDhxco8nQSdNmsxFF32Nq6++gmg0ilLTuO66H+J2e3jttVe46KJzcbvdfOELS5k4cRKXXvp1rrnmKjweDzk5Odxwwy0Dlrs3jL4cngyCccCW/oyDt6Nhmh64AvdhZ+OZe9qghBsMyTY+GyTzUBiqvDt3bqO4eOyA7CuZ5nVp01XmBx9cxumnn0V+fj6vv/4KL774PHfc8fMEJdxXZWUZhYV7X2faYRz8eGBrd89N3ha8wwWGKV00QogDUlRUzHe+8z84nU4yMjK5/vqbEh1pwCRtgTcMA9Pjk5OsQogDsnTp6Qm7QnWwJe1JVoifaJUCL4QQXUvqAm96fNJFI4QQ3UjuAu+WLhohhOhOUhd4w+2NTzYmhBBiH0ld4E239MELIUR3krzASxeNEMnqG9+4lJdfXr7XskAgwNKlx1NXV9flc+644xaee+4ZqquruPbab3W5zeLF83t83fLyHdx5520AbNiwnp/+9Pa+h+/k/vv/zP33//mA9zPQkrrAx7topMALkYxOPfUMXnzxhb2Wvf76Kxx66Hyys7N7fG5+fgG/+EX/Jq7dubOCHTu2AzB16vSUGvfeWdKOg4e2LhrpgxeiPyIb3yKi3+j387ub0AvApY7CNWVRj88/7rgT+cMffkNDQz2ZmVkALF/+HOeeewGrVn3IvffeQygUpLGxiW996zssWXJM+3PbbhLy5JPPUFFRzm233UQgEOCQQ2a0b1NVVcmdd95OU1Mj1dVVLF16Olde+T/85je/oLx8B3ff/TOOPfZ4HnjgXn7/+3spLd3GXXfdQWNjA16vj2uuuZZp0w7hjjtuIS0tHa0/pbq6iosvvqzHO0699dabLFv2R2zbYuTIEq677kfk5ubx+9//mvfffxfTNFiy5BguueQKPvjgPe6557cYhkFGRga33PKT/X659UVSt+BNtw9iYWyr/zelFUIkht/vZ8mSo3nllZcBqK6uorR0GwsWLOSpp57g+utv4oEH/pfrr7+RZcv+2O1+fvWru1i69HQeeugxZs6c3b78pZeWc+KJX+Deex/ikUee4G9/+yt1dbV8+9vXotS09jtKtbn99ps455zzefjhx/nmN7/LjTf+gHA4DEBl5S7uuec+fvrTX/KHP/ym2yy1tbv5+c9/wp13/oKHH36cmTNn88tf3sXOnRWsXPk2Dz/8V/74xwfYunULoVCIhx++n+uu+yH33/8ohx12OBs3bjiQt3QfSd2CN9ytM0pGQ+D2JzaMEEnGNWXRflvZPRmIuWiWLj2d++77E1/84n/x4ovP84UvLMXhcHDTTbfz9ttv8uqrL7Nu3VoCgUC3+1i16kNuueUOAE466ZT2PvULLriIjz76gMcee5QtWz4nGo10u5+Wlha2b9/O0UcfB8CMGTPJzMyktHQbAAsWHI5hGEyYMLF9JsmurF+/jmnTDmHEiJEAnHHGl3j00YfIzy/A4/HwjW9cwpFHLuEb3/gmHo+HxYuP4kc/uo4lS45myZKjOeywhX1/E3uQ/C14wA5LP7wQyWjOnEOpqalm166dLF/+fHvXx1VXXc6nn65Dqal85SuX7GfOdqN9skLDMDBNBwC/+92v+PvfH6e4eARf/eqlZGVld3sHKtve94vKtmm/u1PbPPI9TSXc1X5sOz5fvNPp5N57H+Kyy75BfX09V175NUpLt3HeeV/md7/7M6NGjeaee37Lww/f3+P++yo1CnxUCrwQyerkk0/lkUceIDMzk5KSUTQ01FNWto1LL72ShQsX8eabr/c4//v8+QtYvvw5IH6SNhyOn5f74IN3ueCCizjuuBMoLd1GVVUllhXD4XDuc1u+tLR0Ro4s4fXXXwHgk0/Wsnt3DRMmTOzT7zJ9+gzWr1/bPp3x00//g0MPncfGjRu4+uormD17LldffQ3jxk2gtHQbl1/+VVpamjn33As499wLpIumo/YuGmnBC5G0li49nbPPPp0f/vBmADIzszjttDO56KJzcTqdHHroYQSDwW67V7773e9z++038/TT/2Tq1Gn4/WkAXHjhxdx++814PB4KC4uZOnU65eXlTJw4maamRm6//SZOPfXM9v3cfPPt/PznP+H++/+My+XmjjvuwuVy9el3yc3N47rrbuBHP7qWSCRKcXEx119/M/n5+cyYMYuvfOU8vF4vM2fOZuHCI/F6vdxxx604HA78fj8/+MGN/XwXu5a088EDpLeUUvGXm/Gd9gOcI6cNeLjBkGzzlINkHgoyH/zQSMbMBzIffJJ30UgLXgghupPUBd6QPnghhOhWUhf4tha8jKIRovcS3C0r+uBA/1ZJXuDjLXiZrkCI3jFNB7FYNNExRC/FYlGcTke/n9+rUTRKqduAswEbuF9r/ctO638MXALUti5aprX+Q79T9ZLROjZVJhwTond8vnQaG+vIzs7DMJK6fZfybNuisbGWnJycfu9jvwVeKXU0cBwwC3AB65VSz2qtdYfN5gPna63f6XeSfjAME5weKfBC9FJ6eha1tVXs2rWdeHut/0zT7HF8+nCUXJkN3G4v+fn51NQ092sP+y3wWuvXlVLHaq2jSqmS1ud0frX5wI+UUmOBN4BrtdZDUnUNl9z0Q4jeMgyD3NzCAdlXsg1FheTMbJr9P9LqVReN1jqilLoVuBb4O7CjbZ1SKh1YBVwHfAY8BNwE3NDbEK3jOfskHIlRXtWEw+vD7YhSUJDR530kSjJlbSOZB1+y5QXJPFT6m7lPFzoppfzAM8ATWut7u9lmLvCA1npuL3Y5jn5e6PTGx+U89tJGfjHuFcz0XPwnX9On5ydKMrYgJPPgS7a8IJmHSleZB+xCJ6XUVKXUHACtdQvwD+L98W3rxyilLunwFAOI9D5+/1i2TThqEXO4ZRSNEEJ0oTddNBOAW5VSi4mflTkTeKDD+gBwl1LqVeLfJFcB/xzgnPvwe+LRY6YHZ7RlsF9OCCGSzn5b8Frr54Bnifezfwi8rbV+XCn1nFJqvta6Cvg68a4bTbwFf/cgZgbA117g3RDufq5oIYQ4WPX2JOstwC2dli3t8PNTwFMDGWx/2gp8xHBhR2UUjRBCdJa0Vzq0ddGEcclUBUII0YWkLfBtLfiQ7YJoUObXEEKITpK2wPs7Fnjbhlg4wYmEEGJ4SdoC73aZOEyDgBUv9LZczSqEEHtJ2gJvGAZ+r4tArHWmNRkLL4QQe0naAg+Q5nPS0lrgbRkqKYQQe0nyAu+iKdpa4GWopBBC7CW5C7zXRUOkdSh/qH/TaQohRKpK6gLv9zqpjsRv22c17U5wGiGEGF6SvMC7qA67wXRiN9UkOo4QQgwrSV3g030uWoIxjPRcLCnwQgixl6Qu8H6vi2A4hpGeJwVeCCE6SeoCn+ZrvcjJlytdNEII0UlyF3ivC4CIJxu7uQ7biiY4kRBCDB9JXeD9vniBD7mzARu7qTaheYQQYjhJ6gKf5o130QRcmQBYTdWJjCOEEMNKchf41hZ8sxkv8LaMhRdCiHbJXeBb++AbjTRAWvBCCNFRUhd4f2uBb4mYGL4sGUkjhBAdJHWBbxsm2RKKxsfCN0qBF0KINkld4F1OBy6nSSAYxczIkxa8EEJ0kNQFHuL3Zm1vwTftlnuzCiFEq5Qo8IFQFDM9D2Jh7GBjoiMJIcSw4OzNRkqp24CzARu4X2v9y07r5wD3AZnAG8CVWushuazU36EFD8S7aXyZQ/HSQggxrO23Ba+UOho4DpgFzAe+qZRSnTb7C3C11noKYACXD3TQ7vg9jj0teJBJx4QQotV+C7zW+nXg2NYWeSHxVn/77ZOUUmMBn9Z6Zeuih4BzBj5q1/bqogFsGUkjhBBAL/vgtdYRpdStwHrgP8CODqtHAhUdHlcAowYs4X74vU5aglHwpIHLh9VYOVQvLYQQw1qv+uABtNY/Vkr9DHiGeBfMva2rTOJ9820MwOpLiLy89L5svpfcbD+BcCWFhZmE80bgCOymoCCj3/sbCsM9X1ck8+BLtrwgmYdKfzPvt8ArpaYCXq31aq11i1LqH8T749tsB0Z0eFwMlPclRE1NE5bV9+GNBQUZYFmEIzEqdtZj+fOJVG+lqmr4jqQpKMgY1vm6IpkHX7LlBck8VLrKbJpGrxrGvemimQAsU0p5lFJu4ExgRdtKrfU2IKiUWtS66CLg+V5mP2B+T+uMkqEoZlYRdmO1zAsvhBD07iTrc8CzwCrgQ+BtrfXjSqnnlFLzWzf7MvArpdQGIB347WAF7szn2TNdgZlVDLaF3SCTjgkhRK/64LXWtwC3dFq2tMPPHwMLBjJYb3VuwQNYDTsxs4sTEUcIIYaNlLiSFaAlGMVoK/D1uxIZSQghhoWkL/BejwOAUCSG4UkHt18KvBBCkAIF3u2MF/hwxMIwDMysIinwQghBKhR4V/xXCEdiAJiZRVgNUuCFECIFCnxrCz4av7bKzCrCbqrBjkUSGUsIIRIu6Qu8p72LprUFn1UEto3VUJXIWEIIkXBJX+BdrV00oY4FHrClH14IcZBL+gJvGgYup0mkrYsmc89YeCGEOJglfYEHcDtNwpF4gTe86eBJk5E0QoiDXmoUeJeDUDTW/tjMKia2c5PMSSOEOKilRoF3mu0nWQHcs76AVbuD8Pv/SGAqIYRIrNQo8C5HexcNgGvCAlxTjyH88XNEy9YmMJkQQiROihR4k3CHLhoAz5EXYOaMIvjaMuyYdNUIIQ4+qVHgnXu34AEMpxvPgrOxAw3EytcnKJkQQiROShR4j8uxVx98G0fJdHB5iW75MAGphBAisVKiwLtdJqHovreBNZxunGNmE936EbbVp9vECiFE0kuNAu/sugUP4Bw/HzvYSGznxiFOJYQQiZUaBd5ldl/gR88Eh4voVummEUIcXFKkwDvaZ5PszHB5cY6eSXTLh9i2dNMIIQ4eqVHgW+eisWy7y/XOcfOwm3dj1ZQNcTIhhEiclCjwntY54SPdtOLNgnEAWHXlQxVJCCESLiUKvMu5912dOjMzCgCwGiqHLJMQQiRaShT49rs6Rbrph3e6MdJypMALIQ4qKVLgW1vw0a5b8ABmZiG23OVJCHEQcfZmI6XUj4FzWx8+q7X+fhfrLwFqWxct01r/YcBS7see2/Z1P0rGyCgktl0mHhNCHDz2W+CVUicAJwFzARt4QSl1ltb6nx02mw+cr7V+Z3Bi9qytiybUTR88gJlZQLSlDjsawnB6hiqaEEIkTG9a8BXA97TWYQCl1KfAmE7bzAd+pJQaC7wBXKu1Dg5o0h70tosGwGqowpE7akhyCSFEIu23D15rvU5rvRJAKTWZeFfNc23rlVLpwCrgOuBQIBu4aTDCdsfdiy6aPQW+/ydareZagiufwLa6/yIRQojhold98ABKqUOAZ4HrtNab2pZrrZuApR22uxt4ALiht/vOy0vv7ab7KCjIINR6fZPH56agIKPL7WLpE9gG+K16srvZZn/qPn+V5jXPU3T4ibgLxvYzMd1mHM4k8+BLtrwgmYdKfzP39iTrIuAp4Bqt9eOd1o0BTtBaP9C6yAAifQlRU9OEZXV9FWpPCgoyqKpqpLkx3htUs7uZqqrG7p/g9tNYXkakp216ECj7LP46OypwGrn92kdb5mQimQdfsuUFyTxUuspsmkavGsa9Ock6GvgXcJ7W+pUuNgkAdymlXgW2AlcB/+xiu0HTm5OsEO+mOaAumt07ALAD9f3ehxBCDJXetOCvBbzAL5VSbcv+BJwB3Ky1/kAp9XXgGcANrADuHoSs3XLv50rWNmZmIbHqrf16DduysGqlwAshksd+C7zW+tvAt7tY9acO2zxFvAsnIdqmKuhuLpo2ZmZhfFZJK4ZhOvr0GnZjJcTiPU9WS0P/ggohxBBKiStZDcPA7TR7HEUDYGQWgB3DbqohvOF1ottWt6+L1ZUTWP4bIhvf6vIm3bHd29t/tgNS4IUQw1+vR9EMd26Xg1AP4+Bhz1DJwH/+hFW1GRxu0v7rNozMQoKv3YdVuZnotlUY7z+J76Rv42idhRLa+t8NzJyR0kUjhEgKKdGCh57v6tSmfSx81WZcs04Gh5PA6/cR+eQlrMrNeI+9At8p34VYlNCHe58ntnaXYWQWYmTkSwteCJEUUqcF73Tsv4smLQfXISfgKJ6Ca+ICHLmjCb62jNCuz3CMnoVz0hEYhoFr+nGEP3oaq6Fyz5dC7Q4cuSUYnnSiVVuH4DcSQogDc1C14A3DxLvoQlwTFwDgnHwkznGHgsuLd/FXMAwDANe0Y8AwCa/7DwB2NIxVvwszdxSGLxM72Ci3/xNCDHup04Lv4b6s3TEMA+8JV2GHmjF9me3LzbQcnOPnEdFv4pn/Jaz6nWBbmDmj4v3vtoUdbMLo8BwhhBhuUqYF73HuvwXfFcN07FXc27hmnADhFsKrniG2Kz4zQ7wFnwXISBohxPCXUi34+uY+zZDQI0fRZBwjpxFe/X/xBaYTM6uwvbBLgRdCDHcpVeB7mi64rwzDwLf0OmKVnxPd+hGmLwPDdGL44619GSophBjuUqfA97OLpieGaeIsnoyzeHL7MrOti0auZhVCDHMp0wfvdu1/mOTAvJAfTKe04IUQw14KFXhzQLtoumMYBoYvE0sKvBBimEudAu90EI3Z/ZpXvq8MX6acZBVCDHupU+B7cV/WgWL4s9r74O1ICNvad3IyIYRItBQ6ybrnvqxe9+C+lunLJFq9DduK0vzkjRCL4J69FDOzkPD6V7Bqd+A//XrMjILBDSKEED1IvRb8AI+k6Yrhy8IONBLd/AF2YxWGJ53QO48RWP5rrOpt2KFmgq/dJ9MZCCESKmVa8J622/b1cbqC/jB8mWDHCK96GiOzEP/Zt2FVbsYKNOAcPYvIprcIvfEgkU9ewj3zC4OeRwghupI6Lfj2LpqhaMHHL3ayastxTz8ewzBxFE3CNe5QDIcTlzoKx5jZhN57klht+aDnEUKIrqROgR/KLhp//GInnG5cavG+6w0D71Ffw3B5Cb7yR+xoGAA72ESsWYZXCiGGRgoV+NYW/JB00cQLvGvyIgxPWpfbmP5svMdcilVTRui9vxPZ+iFNT/yA7fdfhx0OAPFpiCOfvdP+BdBRdPs6Ai/+luA7f41vI/35Qog+Spk+eLdz6FrwZnYx7sPO7rL13pFzzBxcM04i8smLRD55CTOnhFhtOaH3nsSz6EKCKx4muvEtXLNOxrvw/PbnWYEGgq/8KT78MraWSCyCJxLCPe2YQf7NhBCpJGUKfNtJ1qGYrsAwTDxzT+vVtp7Dz8Fuqol/Kcw7C/Pjf9DwwfMQCxPd+BZGRgGRtS/imrwIR95obNsmtOIR7HAL/i/dgplTQsu/7yD80dO4pizCcLgG95cTQqSMlOui2d+Nt4ea4XDhO+mbeBacg+FwknvMBRhpOUT0mzjHzcN/1s0YnjSCKx7GDjYRWbuc6JYPcM8/C0fu6PiXyfyzsJt3E/n09b32HdnyAcHX78e2B//qXSFE8kmhAt/WRTO8+6pNjw/vcV/HOWUR3mMvx/Rm4Fl4Htauz2h65GpCKx/HUTwF96xT2p/jKDkExwhFePX/7Tlha1uE3v07Ef0msbK18WXRMME3HiS8djl2qHmv141VbqblhV9htdQN2e8qhEisXnXRKKV+DJzb+vBZrfX3O62fA9wHZAJvAFdqrYf0+v22PvjIMGvBd8U5QuEcofY8nrwId/0uMB04SqbjKJyAYTra1xuGgXv+lwg8cyfhtcvxzD2dWNkn2A27wHAQXv1/OMfMIrz6WSIb4q380HtP4VlwNu6ZJ2FbFsE3H4yf8F3xCN4Tv9l+/1khROrabwteKXUCcBIwF5gDzFNKndVps78AV2utpwAGcPkA59wvp8PEMCAYHv4FvjPDMPAc9l945n0RZ/EUDHPf713nCIVz3DzCHz2NVb+T8LqXMXxZeBacTWznRiIbVxD++FmcExfi/9ItOEZOJfTOX4mWriGy8U2smjIcJYcQ3foR0c9XYlsW0R3rie3eMfS/sBBiSPSmi6YC+J7WOqy1jgCfAmPaViqlxgI+rfXK1kUPAecMdND9MQyDDL+bxpaBu23fcONZdCE4nARevodY2Rpc047BNf04DE86wdfuA9OF54jzceSPw3fiVZh5owi8+mfC7z2Jo2gyvlO+i1k4keCKR2n+67UEnr2LlidvoOWZO4luX5foX08IMcD2W+C11uvairdSajLxrprnOmwykviXQJsKYNRAhuytrDQ39U2hRLz0kDDTcvAcfh5WTSkYDlzTj8VweXDNPBEAz2FfwvRnA2A4PfhO/CbYNnawEc+RF2CYDnzHXIbhcGHmluA94X/i+2uqIbD8V1h1O/uUx4513QsXrdCEPvgnscrNfT4BbFsWkS0fEttd1qfnCdEbVihAZNPbhNe9jFW/E9u2sZpqiJZv6PJ6lB73FWjAqt/Vp+fZsSix3TuG7LqWXg+TVEodAjwLXKe13tRhlQl0/FdsAH1Kn5eX3pfN91JQkLHn51w/Dc3hvZYNRweSz84/lV2Vn+LMzCN/7Oj4shPOIzBxGr4Jc/bqu6cgg+CXf0x0dwXp02e1L+O7D+y1z+jhJ7L93m8TW/kIhRfeimHEv/ejDTXs/NudVBaNJ//kyzBdnvbnNHz4AtUvPkjWYaeQe+yXMRwubNui7u1/0vj642BbhD/6N678URSf8wNcuSN7/L2sUAstWz6m7s2/Ea4sxZGRy+grfo3p7fpCst4Y7p+DzoZL3pbPVxHcrsmceyLOzLwetz2gz3IsSrDsUwyXB8/Iye3nhexoBMPZ9XDgcFUZdW//g5bPV+GfOJf06YuJ1O0isHk1pjeNtKkLceWOJLRrC+HKbUTrq4g11YJtg2GyrXxTe0EOAYbLix0JAuBIzyH7iC/Gr0AvXUespR7D5cV0urFjEexYDNOXjsOXTnDHRkLbN9JW+hxp2TizCnBmF+KfdChpkw/D9KZhRyNEG6qJ1O4ksGUNjWtfw2ppwJGRR/qMJeQsOXevf1cD/T4bvWlhKaUWAU8B12itH++0bizwH631pNbHS4BbtdbH9eL1xwFbamqa+nWjjoKCDKqqGtsf3//setZvreXuqxb1eV9DpXPm4SKy4Q2CbzyAZ/FXcU8/FquljpZnfordvBuiEcy80XiP+lp8SuRPXyf83t8ws4qx6ndiFozHkTeG2K5NWLXlOCcejufw84huX0vo3b9h+nPwf/EmjC4+yLGqrQTfeACrpgywMTKLcE09ivD7T+Gashjv0Zf06/cZyPc5Vr0Nw5/VfnTUE6tpN4bTjeHtW6OloCCDyu07idXuwFEwvsvrHWzbHtCT47ZtYzfvxqrfhaNgHIbbT7RsDYHlvwErBqYD56QjcM86BUduCVb9LsJrnsdRNBnXlEX9eo/tUDPRsrVEt60iWrYGWq/qNnNKcBRNjLekG6pwzTgBz4KzIRoh/OlrxHZ9ht1YiVVbDk4PzlEziJavb3++kVUEwWbsUNOeFzOdGOm58b+bYYJt4R85juio+Zj+bKJla7BqyzFzSjB8mUTWvUysQsf358vESM+HaBA7Fo3/PUwTO9iMHWjAzCnBOe5QzPRcrObd2I01WE01WLU7sFvqwHCAaUDHo1zDgXPsHByjDiFa+jGxHevxnfJdnCOn9fiedfU+m6bR1jAeD2zt7rn7bcErpUYD/wLO01q/0nm91nqbUiqolFqktX4LuAh4fn/7HQzZ6R4amsNYto0po0T6xKmW4Ph8JaG3/kJk3cvY4QB2qBnf0uvI8tns/OevafnnrXu2n7gQ77GXEd22muAbD8aLROEEvDNPxqmWxEf+TD0aMz2PwHN3E3zzQVxTFhP9/F2M9Hzcc5Zit9QTeOFXYDpwz/sijqKJOEZOjZ9kDrcQXv0sjlGHxOfVt2KYraOL7HCAyMa3sENNGN50zLQ8zLzRGG4f0bI1xHZuomnKbOzcaRjO+M0BbMsitlNjN1ThHD8Pw5NGbOcmQh/+C8ObjnP8fAxvOrGdmyASxKWOwkjPJfT2/8ZHJhkmzjGzcY6fHx/llJ6H3VKHHWwCpxusGOG1y4l+9g6YLlyHHId71imYbfMWtbKj4XgBcLji00631BL5bCU7tq8mVPEZ2DZGZiGehefhHDsXMIjtWEd49bNYNWV4jr4E17hD9+zPtol+9g7RsrXYTTXY0TCO/LGYBeMxs4ow03KJVW0mWroGMy0H9+yl4PYSXrOcyMfP7ymILh+uyUcQ0Sswc0rwHn0pEf0mkQ1vEN24ArNgAlb1VrAtIp++hh2oxz7+XGKVm4nt+gzDnx1//0tXE938AZgOzOxiHAUTcE48HMObTnjVM0Q2vAFWFMObgWv8fBxj52IHG4msf5XIZ+/G//5Fk4h88hLRLR/G399YGDOnBDOrGOfEhbimH4vpzcCOhomVb8DMKsTMKsa2osTKN2C31GHmj8XMHrn3ES17F0v3ISfstc414TBi1dvA6cLMGtGvL1PbtrAqNxPdthpsC9w+TH82RmYhjpyS9i9+9/TjBvwLuyv7bcErpX4DXAJ83mHxn4AzgJu11h8opWYDy4gPk/wI+JrWujed4eMYwBb8Sx+U8deXN/Hrby0m0z/Id/3op+HaggewWuoIr3kBu34XdrgF9/wv4RyhKCjIYNfWUmIVOl6cXN54ATTjXTm2FQPD7PbDGvroacIf/CP+wOWFSBAzbwxYMazm3fjPvBFHTslez7GjYVr+cQtW3Z7ZOA1fZnwkUNka6DTOfy8OZ7zl5EmL79fhxNq9Y8+N0p0eHMWTiW3/BMOfDVYMO9jhb2I4wLZa79xVh2vWyRiGSWTjip5v1ehw45p+LHawMV7oMXAUT8ZROJFY7Q6sqi17P99wgB0f9eUZORm7eHr8COnjZ+MtVQDDiBd9fzaGNx1r93ZcM07ENWEBuP2E3n2CWNkaDH82ZlYRmA5iVVsh3LJXNMObES/mbj+mLwurrhzHmNk4x8zGTM8lsvFtolvex8wqxnf6DzHbZkwNNhJZ9wrRz9/FUTId9+xTCK18gujm93Bm5hNtqO70HrhwjpkNTg9WXTlW9bZ4oTMMwMSlFuOashizcGL756f9b96h4EXLPyX8/j8ws4txzTwZR+7en4/+Gs7//rpzIC34XnXRDKJxDGCBf39DJX/81yfcdskCRhX2v19/MKXKB6wvbNsisvYljPRcnGNmE9u+juAbD2CHWvAt/R7OkuldPs9qqSNWthbDm44dixD9/D2i2z/BWTId95zTMPPHYAebsBuridWUYgcacY46BLNgPOkt26h+/2Xs5lrsWATTn41z4gLM9DzC614hVrYG55RFeOZ9ERwuYjs3QjSCo2gidixKZP0rxMo/xT3n1HjBIn4UYNWVE6v8HLulPt4y82VgxyLx5446pL0bx6rbSWTTW0S3rsKq3YGZMwKzYEK8Ve3PjvfpNu2Of1lOXEDRxInt77FtxYh+thKrsSp+5JJZiHPSQgBCK58gsu7lPW+Sw43n8HNwHXJ8+7kT27axm6qx6iuxmqpx5JRgFk7Aqt1B6N2/YzdVx+dS6nAkAGA11WC4fRhuf89/T8uKX5DXsAN77GE4x8yO/x0CDfGjG7dvzz4DDUS3fIDdVINr2jEJv8tZqvz7OygL/MayOn76vx/x3fNmM2N8zyeHEiVVPmAHygo2YrfU48gdnAFXw+l9tq1ol9c2dNSXvFZDJVbdTqzm3ThHTou33BNgOL3HvZUqmQesDz6ZZKXHu2Xqm/o23EkMPdObAd7hMWpksO2vuPeVmVmImVk4oPsUqSll5qKB+Dh4gPpmKfBCCJFSBd7rduJxO6QFL4QQpFiBh9arWZtT92pWIYTorZQr8NlpbmnBCyEEKVjgM9M91EkfvBBCpF6Bz05z0yBdNEIIkXoFPivdTSAUIzQEN98WQojhLPUKfFp8QisZKimEONilXoFvv9hJummEEAe31CvwaXI1qxBCQCoW+HTpohFCCEjBAp/hc2EahlzsJIQ46KVcgTdNg4w0F3XSRSOEOMilXIEHyE7zSB+8EOKgl5IFvijXR1llIwme614IIRIqJQv81DE51DWF2VUbSHQUIYRImNQs8GNzANiwrTbBSYQQInFSssAX5fjIyfDwqRR4IcRBLCULvGEYTB2TjS6tlX54IcRBKyULPMT74RtaIpRXNyc6ihBCJETqFvi2fvjSusQGEUKIBEnZAl+Q7SM/yysnWoUQBy1nbzZSSmUCbwOnaa23dlr3Y+ASoK2SLtNa/2EgQ/bX1DE5rNpURTRm4XSk7HeZEEJ0ab8FXil1OLAMmNLNJvOB87XW7wxksIEwTxWwYm0FH22sYsG0okTHEUKIIdWbZu3lwFVAeTfr5wM/UkqtUUr9XinlHbB0B2jmhDwKsr288tGOREcRQoght98Cr7W+TGv9ZlfrlFLpwCrgOuBQIBu4aSADHgjTNDh27ig2ltVRVtmU6DhCCDGkjN6OE1dKbQWO6dwH32mbucADWuu5vXz9ccCWXm7bL40tYS6+dTnHzh/N1efMGcyXEkKIoTYe2Nrdyl6dZO2OUmoMcILW+oHWRQYQ6et+amqasKy+X5BUUJBBVVXjfrdbML2IVz8s47SFY0jzuvr8OgOpt5mHE8k8+JItL0jmodJVZtM0yMtL3+9zD3RoSQC4Syk1XillEO+r/+cB7nPAnTBvFOGIxWurpC9eCHHw6FeBV0o9p5Sar7WuAr4OPANo4i34uwcw34AYU5TBjAm5vPh+GaFILNFxhBBiSPS6i0ZrPa7Dz0s7/PwU8NTAxhp4py4cy88eW8WKNRUcP29UouMIIcSgO2iu/pkyOptJo7J44d1tRGNWouMIIcSgO2gKvGEYnLpwLDUNId5aW5HoOEIIMegOmgIPMGtiHlNGZfH3Vz+ntjGU6DhCCDGoDqoCbxgGX1s6jWjM4pEXNshc8UKIlHZQFXiAolw/Xzp6Ih9/XsNba3cmOo4QQgyag67AQ3xcvBqdzUPPb+DNj7ubYkcIIZLbQVngTdPgW2fPYtq4HB58fgP/XrEFS7prhBAp5qAs8AA+j5Nvnz2LI2cU8+8VW/j9U2tpDvZ5lgUhhBi2DtoCD+B0mFx66jT++4TJrN1cw20PvU9lbUuiYwkhxIA4qAs8xEfWnDh/ND/48qEEQjF+/tdVVNcFEh1LCCEO2EFf4NtMKsnie+fNIRCKcddfV1Fe3ZzoSEIIcUCkwHcwtjiD750/h0Aoys33v8ejL2q5IEoIkbQOaD74VDR+RCZ3XLGQp1ds4bVV5bz20Q4mjspiycwRLJ41AsMwEh1RCCF6RVrwXcj0u7nwJMVPrjicM5eMJxCK8uDzG/jV3z+mvjmc6HhCCNEr0oLvQWGOnzMWjef0I8fxykc7+Nurn3HjspXMmZzPzAl5zBifiz/Bd4gSQojuSIHvBcMwOH7eKKaOyeaZt7eyelM1b63diWkYTCzJZOEhxSyaUYzb5Uh0VCGEaCcFvg9KCtK58swZxCyLLeWNrNlczepNNTy6XPPvNzdz2NQiMtPd5KR7mDo2m/wsX6IjCyEOYlLg+8FhmkwalcWkUVmctWQCurSO598tZcXair1uCTgiz8/EkizGF2cwfVwuRbn+BKYWQhxspMAfIMMwmDo2h6ljcwCIRC0qa1tYt2U367fVsnpTNSvWxG8wMjI/jQWHFFOc7aUox09TIEJzMEKG301upoeCLB+mKaN0hBADQwr8AHM5TUoK0ikpSOekBWOwbZuq+iAff1bNqo1VPPvWFiLRrm8ZmOF3MWtiHiX56di2TSgSo64pTDga46hZI9u/RNo0NIepaQgyrjhDhm8KIfYhBX6QGYZBYbaPE+eP5sT5o8nOSWP1pxXU1AdJ97lI87pobAlTXR/k09YWf9s89Qbxom/ZsHLdLmZMyOWwqYWMzEvj48+refH9MsIRi8IcH4tnjmDK6GzGFmUQCEeprguSl+UlJ8OT2DdACJEwUuCHmMtpMq44k3HFmfusWzJ7JJZlEwzHcJgGTqeBwzQJR2K88tEOnn1nK59s3t2+/YJphUwfl8vbayv4xxub99mfaRjMn1rA4dOLSPO6SPO5KMz24XLK5Q9CHAykwA8zpmng9+79Z3G7HJx8+BhOPGwU1XVByqubKcj2MaowHYCjZo+kvinElp2NlFU24fc4ycvyoktreePjct77tHLP/g2DgmxvvHWf7sEwDcKRGMFwjHAkhtNhcuTMYuarQmIxm9LKRjbsaGBnZSPNwQgtwSiGYTBtbA5TRmfv9WXREoxgGAY+Tzz/jupmdGkti2aOwCNDSIUYckaC70s6DthSU9OEZfU9R0FBBlVVjQMeajANdeZgOMqOqmaC4RiNLWEqalqoqGlmd2OofZ4dt8uB1+XA4zKpbQpRVRfE73ESCEfp/PFwOgxsG2KWjdNhkp3uJs3roq45RH1TGMOA0YXpOB0mm8sbAJgyOptvnz0Lp8Pg1Y92sL11IjenaeDzOsn0u5lYksW44gycjn2PLizbJhK1+vQlkWyfjWTLC5J5qHSV2TQN8vLSAcYDW7t7bq9a8EqpTOBt4DSt9dZO6+YA9wGZwBvAlVrraK/Ti0HldTuZWJLV6+0t2+aTzTW8/2kleVlexo3IZMq4PEKBMH6vE7fTJByx2FBaiy6ro74pRGMgwqiCNEbkpxGOxNi0vZ6WYJRzj52E3+vkkRc0d/11FS3BCFV1QbLT3RiGQTRm0RKMEmv9cve4HBTl+ijI9uFymARCUeqaw1TUNBON2pwwfxRnLh5PzLJZt2U3u2pbqG8KYwN5mR5yM7x43A5cTpOMqmZqdreQ7nNSkO0jO92DaRrELIvVm6pZtamavEwv44ozUGOy97oiua4pxMefVbO9spkls0cwpihjoP8sQgyJ/RZ4pdThwDJgSjeb/AW4TGu9Uil1P3A58MeBiyiGkmkYzJqYz6yJ+e3LOrcgPG4HsyflM3tSfle72Eea18Wf/v0Jxbl+vnf+HA4Zl9u+zrZtGlsibCyrY2NZHTtrW9hR1Yxl2XjdDjLS3Bw9u4RAKMpL75fx5poKgh2OLNJ98cLcFOj5blwO0yAvy0u4dWRSmtdJSyi+H4dpcMj4XDL8LjaXN1BR09L+nFdWbWfJrJF43Q42ba8jFLHI9MfPZ7icJi6HictpYpoGdY3xo5/iPD8nHTaa8SMysSy79XXs1u4rBw5zz1GKbdtsr2rm/U3V1NYFwLbJzvBQkO0jErWoa4ofZRXl+MnL8sbPzTgMXM6+dXlZlk19c5jdDUEs22ZMYQYe9977aDuaT8SILNu2CffxKE3sX29a8JcDVwGPdl6hlBoL+LTWK1sXPQTcihR40cE8VcDdVy0izefcq7hBvJhkprmZP7WQ+VMLe9zPsYeWsPy9Ugpz/MyelMfYoj1dOoFQlPrmMOFIjHDUIi83jYb6AI2BMNV1QarqA9TUB4nFbI6cUczsSflEYhbbdjayelM172+oJBSJMXFkZvv6nAwP/16xhVc+3IHDYTBhRCa5GV4aWru6olGLSMwi0vr/7DQ3+Vle1nxezbvrd5GZ5qapJbLX/X4NIM3nIivNTWaam90NQXbV9v0GMy6nSZrXSZrPRbrXhdNpEotZxCy7vbszL8tLbqaXiupmNm6vIxDacxGeYcDognRmTsxj2tgcPt1Wy4q1FYQjMQqz/WSkuYhGLUzTYMb4PGZPyiMcsaioaeazHfXo0jrqm8N43Q7yMr2csXg808bmEIlarPm8GjAYme8nZtlsrWiktilEQZaXgmwfToeJYcQvBHQ5HTQFIvz535+gy+o5evZIjptXQnMgyvaqJmKWjctpUpDlZWJJ1oBOB1JTH6SqLsCowvT2hkJf1DWFMAyDrDQ3AC3BKFt2NjBxZCZe957Satk2ZbuaqG8OMaognZwMz5B9ifa6D14ptRU4pmMXjVLqCODnWuvFrY8nAc9prbtr7Xc2DtjSh7xCDLmmlnBr10/viktLMMJL75WytbyBnEwPWekeHKaBZds0NkeoawpR1xikrjGE3+viiJkjOHRqIX6PExuorguwa3cLbqeDvGwvlmVTXtVMTX2AaMwmGrNoDkRobAnTFIjQ0BwmGrVwOAycDrP9tSp3B9hV28KIPD8zJuYzsSSL/Gwftg0by2pZt7mG9Vt2Y1k2pgHzphVRlOOnvLqZxpYwbpeD5kCErRUNe/1+XreD6ePzGJGfRkswwrrNNVTWBpg9OZ8t5Q009HLG1TSfi6PmlPChrmR3fZDDphfx3rqd7V12nTkdJiUFaRiGgW3bWDatXxRpTCjJwut20NgSIRyJYRgGkWiM8upmdtY0E2299sTjdpDmc1HfFGbX7j235ywpSGNkQTqFOX7CkRhVtQEs2yY/24ff62R3Q5D6pjC5mfEBChu27mbDtloMA2ZMyCc/28vbaysIhWN43Q6OnDUSj9tB5e4WNpbW0diy5z3J8LsYmZ9OSWE6Xz11OrmZ3l69X9048D74HphAx7+GAXR9FU8P5CTr8CaZIdDHG3wdOa2QI6f1fETSUUGOvz1vusskvSh9z0qHwZSRGTCy7+cCLNvG7KK1OL4wjS/MG0VTIMKm7XWMLcrottBU1gVYv2U36T4Xxbl+ivP8OB1m+3scjsR44b1SXl21g8mjsjh6zkjSvC4qapoxMBg3IoPcDC/V9QGq64NYlk0kFj8X8tJ7pWT4XVz/5UOZMDKTLy0ez0ebqijI8jGqMA2300EkarGjuokN2+rY1XrPZMMwMIx411PpzgbeW7cTm3i3mtvlwLJtHIZBYY6PMYXp8Za/DYbDZHd9gFH5aRx3aAlFOX7KKhvZUtHIzupmPvm8Bo/LJDfTi2FA2a5GguEo2ekeMnwu9Lbd1NQHGZmfxllHTcCybN5dv4uNpbUcPr2QmRPyWfN5NW+tKcdpGuRlepk1IZfp43LJy/JSVtnE9qomKmsDbNy2my2lu4nt5xzPfk6y9uhAC/x2YESHx8VA+QHuUwgxQLoq7h2l+1zMnVzQ4zaF2T4K55Z0u97tcnDGovGcsWj8XsvHj9j7Wo+2K7zbLJhWxFfCUZwOs72rLS/Ly4nzR+/zGnlZ3r3OC3UWisSwbRuPy9Fj90dXxXLWxLxut+9K2/mUNmcsGgfsOXcxTxVw8SlTu8wxZXR2n17rQB3QFS9a621AUCm1qHXRRcDzB5xKCHFQ8LqdXQ6N7SuPy4HX7RySvu3OrxE/mth32XDQr3dWKfWcUmp+68MvA79SSm0A0oHfDlQ4IYQQ/dfrLhqt9bgOPy/t8PPHwIKBjSWEEOJAyaQkQgiRoqTACyFEipICL4QQKUoKvBBCpKhETxfsAA7oNnXJeIs7yTw0ki1zsuUFyTxUOmfu8LjHy6sTPV3wYuDNRAYQQogktgRY0d3KRBd4D3AYUAHE9rOtEEKIOAfxWQTeB0LdbZToAi+EEGKQyElWIYRIUVLghRAiRUmBF0KIFCUFXgghUpQUeCGESFFS4IUQIkVJgRdCiBSV6KkK+k0pdQFwI+ACfq21/kOCI+1DKfVj4NzWh89qrb+vlDoB+CXgA57QWt+YsIA9UEr9AsjXWl883DMrpU4HfgykAS9qrb+dBJkvBH7Y+vB5rfW1wzGzUioTeBs4TWu9tbuMSqk5wH1AJvAGcKXWOjpMMl8BfIv4/aM/AL6utQ4P58wdll8NnK21Pqb18Rz6kDkpW/BKqRLgDuJTHcwBrlBKTU9oqE5a/yGcBMwlnnGeUuq/gQeAM4FpwGFKqVMSFrIbSqnjga+2/uxjGGdWSk0A/gR8EZgFHNqabzhn9hO/89nRwGxgSeuX1LDKrJQ6nPhl8FNaH/f0WfgLcLXWegpgAJcPfeIuM08BrgOOJP75MIGrWjcflpk7LJ8OXN9p8z5lTsoCD5wAvKK13q21bgaeBM5OcKbOKoDvaa3DWusI8CnxP+AmrfWW1m/dvwDnJDJkZ0qpXOJfnj9pXbSA4Z35LOItye2t7/N5QAvDO7OD+L+9NOJHoC6ggeGX+XLixbC89XGXnwWl1FjAp7Ve2brdQyQue+fMIeB/tNYNWmsbWAuMGeaZUUp5gD8DN3dY1ufMydpFM5J4AW1TwTC7baDWel3bz0qpycS7an7HvrlHDXG0/fkzcAPQdmv7rt7r4ZR5EhBWSj0NjAH+D1jHMM6stW5USt0EbCD+ZfQ6w/B91lpfBqCUalvUXcZhk71zZq31NmBb67IC4GrgYoZx5lZ3Ej9a2tJhWZ8zJ2sL3iTen9bGAKwEZemRUuoQ4CXih4mbGca5lVKXAWVa6/90WDzc32sn8SO6S4EjgMOBCQzjzEqpWcAlwFji/2hjxI/uhm3mVt19Fob7Z6StW/c/wP1a69cYxpmVUicCY7TWD3Za1efMydqC3058msw2xXQ4vBkulFKLgKeAa7TWjyuljiY+A1yb4Zb7PGCEUmo1kAukEy9CHWf6HG6ZdwIva62rAJRS/yR+2DqcM38B+I/WuhJAKfUQcC3DOzPE/9119fntbvmwoJSaCiwHfqu1vrt18XDO/N/AIa3/DtOBYqXUE8D36WPmZC3wLwO3tB5yNQP/BVyR2Eh7U0qNBv4FnKe1fqV18bvxVWoS8UOvC4gfhg0LWusT235WSl0MHANcCWwarpmJd8k8rJTKBhqBU4ifk7l+GGf+GLhLKZVGvIvmdOKfjS8P48zQzedXa71NKRVUSi3SWr8FXAQ8n8igbZRSGcCLwA1a60fblg/nzFrrS9p+VkodA9yitT6v9XGfMidlF43WegfxfuJXgdXAY1rr9xIaal/XAl7gl0qp1a3fxhe3/vcUsJ54H+yTCcrXK1rrIMM4s9b6XeAu4qMQ1hPvb/0jwzvzi8BfgQ+BNcRPst7CMM4M+/0sfBn4lVJqA/FW528TkbELlwFFwPfa/h0qpW5rXTdcM/ekT5llPnghhEhRSdmCF0IIsX9S4IUQIkVJgRdCiBQlBV4IIVKUFHghhEhRUuCFECJFSYEXQogUJQVeCCFS1P8H7rA0XfwlLAsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_15\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_16 (InputLayer)          [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_45 (LSTM)                 (None, 45, 24)       3744        ['input_16[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_30 (Dropout)           (None, 45, 24)       0           ['lstm_45[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_46 (LSTM)                 (None, 45, 16)       2624        ['dropout_30[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_31 (Dropout)           (None, 45, 16)       0           ['lstm_46[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_47 (LSTM)                 (None, 32)           6272        ['dropout_31[0][0]']             \n",
      "                                                                                                  \n",
      " dense_30 (Dense)               (None, 40)           1320        ['lstm_47[0][0]']                \n",
      "                                                                                                  \n",
      " dense_31 (Dense)               (None, 5)            205         ['dense_30[0][0]']               \n",
      "                                                                                                  \n",
      " tf.unstack_15 (TFOpLambda)     [(None,),            0           ['dense_31[0][0]']               \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_75 (TFOpLambda)  (None, 1)           0           ['tf.unstack_15[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_30 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_75[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_79 (TFOpLambda)  (None, 1)           0           ['tf.unstack_15[0][4]']          \n",
      "                                                                                                  \n",
      " tf.math.multiply_45 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_30[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_31 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_79[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_46 (TFOpLambd  (None, 1)           0           ['tf.math.multiply_45[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_76 (TFOpLambda)  (None, 1)           0           ['tf.unstack_15[0][1]']          \n",
      "                                                                                                  \n",
      " tf.expand_dims_78 (TFOpLambda)  (None, 1)           0           ['tf.unstack_15[0][3]']          \n",
      "                                                                                                  \n",
      " tf.math.multiply_47 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_31[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_30 (TFOpL  (None, 1)           0           ['tf.math.multiply_46[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.softplus_30 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_76[0][0]']      \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_77 (TFOpLambda)  (None, 1)           0           ['tf.unstack_15[0][2]']          \n",
      "                                                                                                  \n",
      " tf.math.softplus_31 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_78[0][0]']      \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_31 (TFOpL  (None, 1)           0           ['tf.math.multiply_47[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack_15 (TFOpLambda)       (None, 5, 1)         0           ['tf.__operators__.add_30[0][0]',\n",
      "                                                                  'tf.math.softplus_30[0][0]',    \n",
      "                                                                  'tf.expand_dims_77[0][0]',      \n",
      "                                                                  'tf.math.softplus_31[0][0]',    \n",
      "                                                                  'tf.__operators__.add_31[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _2_CCMP_t+1_0.05\n",
      "Epoch 1/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 3.3801\n",
      "Epoch 1: val_loss improved from inf to 4.18522, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.05.hdf5\n",
      "66/66 [==============================] - 9s 72ms/step - loss: 3.3801 - val_loss: 4.1852 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 2.7752\n",
      "Epoch 2: val_loss improved from 4.18522 to 3.16271, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.05.hdf5\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 2.7738 - val_loss: 3.1627 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.5131\n",
      "Epoch 3: val_loss improved from 3.16271 to 2.50762, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.05.hdf5\n",
      "66/66 [==============================] - 4s 65ms/step - loss: 1.5245 - val_loss: 2.5076 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/66 [============================>.] - ETA: 0s - loss: 1.0443\n",
      "Epoch 4: val_loss improved from 2.50762 to 2.29758, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.05.hdf5\n",
      "66/66 [==============================] - 4s 66ms/step - loss: 1.0453 - val_loss: 2.2976 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8586\n",
      "Epoch 5: val_loss improved from 2.29758 to 2.19037, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.05.hdf5\n",
      "66/66 [==============================] - 4s 63ms/step - loss: 0.8605 - val_loss: 2.1904 - lr: 1.0000e-04\n",
      "Epoch 6/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7603\n",
      "Epoch 6: val_loss improved from 2.19037 to 2.10534, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.05.hdf5\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.7594 - val_loss: 2.1053 - lr: 1.0000e-04\n",
      "Epoch 7/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6873\n",
      "Epoch 7: val_loss did not improve from 2.10534\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.6854 - val_loss: 2.2582 - lr: 1.0000e-04\n",
      "Epoch 8/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6174\n",
      "Epoch 8: val_loss did not improve from 2.10534\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 4s 61ms/step - loss: 0.6172 - val_loss: 2.2219 - lr: 9.9000e-05\n",
      "Epoch 9/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5810\n",
      "Epoch 9: val_loss improved from 2.10534 to 2.05779, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.05.hdf5\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.5806 - val_loss: 2.0578 - lr: 9.8010e-05\n",
      "Epoch 10/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5441\n",
      "Epoch 10: val_loss did not improve from 2.05779\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 4s 61ms/step - loss: 0.5454 - val_loss: 2.1653 - lr: 9.8010e-05\n",
      "Epoch 11/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5061\n",
      "Epoch 11: val_loss improved from 2.05779 to 1.99240, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.05.hdf5\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.5063 - val_loss: 1.9924 - lr: 9.7030e-05\n",
      "Epoch 12/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4756\n",
      "Epoch 12: val_loss did not improve from 1.99240\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 4s 61ms/step - loss: 0.4801 - val_loss: 2.2124 - lr: 9.7030e-05\n",
      "Epoch 13/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4587\n",
      "Epoch 13: val_loss improved from 1.99240 to 1.93368, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.05.hdf5\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.4592 - val_loss: 1.9337 - lr: 9.6060e-05\n",
      "Epoch 14/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4421\n",
      "Epoch 14: val_loss improved from 1.93368 to 1.86531, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.05.hdf5\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.4427 - val_loss: 1.8653 - lr: 9.6060e-05\n",
      "Epoch 15/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4329\n",
      "Epoch 15: val_loss did not improve from 1.86531\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 4s 61ms/step - loss: 0.4328 - val_loss: 1.9650 - lr: 9.6060e-05\n",
      "Epoch 16/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4153\n",
      "Epoch 16: val_loss did not improve from 1.86531\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 4s 61ms/step - loss: 0.4148 - val_loss: 1.9661 - lr: 9.5099e-05\n",
      "Epoch 17/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4174\n",
      "Epoch 17: val_loss improved from 1.86531 to 1.80911, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.05.hdf5\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.4153 - val_loss: 1.8091 - lr: 9.4148e-05\n",
      "Epoch 18/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3914\n",
      "Epoch 18: val_loss did not improve from 1.80911\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.3905 - val_loss: 2.0184 - lr: 9.4148e-05\n",
      "Epoch 19/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3929\n",
      "Epoch 19: val_loss did not improve from 1.80911\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 4s 63ms/step - loss: 0.3925 - val_loss: 1.9654 - lr: 9.3207e-05\n",
      "Epoch 20/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3796\n",
      "Epoch 20: val_loss did not improve from 1.80911\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 4s 61ms/step - loss: 0.3781 - val_loss: 1.8704 - lr: 9.2274e-05\n",
      "Epoch 21/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3697\n",
      "Epoch 21: val_loss did not improve from 1.80911\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.3697 - val_loss: 1.9767 - lr: 9.1352e-05\n",
      "Epoch 22/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3827\n",
      "Epoch 22: val_loss did not improve from 1.80911\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.3829 - val_loss: 2.0532 - lr: 9.0438e-05\n",
      "Epoch 23/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3694\n",
      "Epoch 23: val_loss did not improve from 1.80911\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.3740 - val_loss: 2.0736 - lr: 8.9534e-05\n",
      "Epoch 24/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3663\n",
      "Epoch 24: val_loss did not improve from 1.80911\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 4s 61ms/step - loss: 0.3663 - val_loss: 1.8681 - lr: 8.8638e-05\n",
      "Epoch 25/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3652\n",
      "Epoch 25: val_loss improved from 1.80911 to 1.69500, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.05.hdf5\n",
      "66/66 [==============================] - 4s 63ms/step - loss: 0.3682 - val_loss: 1.6950 - lr: 8.7752e-05\n",
      "Epoch 26/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3578\n",
      "Epoch 26: val_loss did not improve from 1.69500\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.3617 - val_loss: 1.7351 - lr: 8.7752e-05\n",
      "Epoch 27/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3561\n",
      "Epoch 27: val_loss did not improve from 1.69500\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 4s 65ms/step - loss: 0.3571 - val_loss: 1.7233 - lr: 8.6875e-05\n",
      "Epoch 28/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3483\n",
      "Epoch 28: val_loss did not improve from 1.69500\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.3490 - val_loss: 1.8390 - lr: 8.6006e-05\n",
      "Epoch 29/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3495\n",
      "Epoch 29: val_loss did not improve from 1.69500\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.3479 - val_loss: 1.8815 - lr: 8.5146e-05\n",
      "Epoch 30/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3505\n",
      "Epoch 30: val_loss improved from 1.69500 to 1.67968, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.05.hdf5\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.3503 - val_loss: 1.6797 - lr: 8.4294e-05\n",
      "Epoch 31/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3416\n",
      "Epoch 31: val_loss improved from 1.67968 to 1.64809, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.05.hdf5\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.3405 - val_loss: 1.6481 - lr: 8.4294e-05\n",
      "Epoch 32/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3283\n",
      "Epoch 32: val_loss improved from 1.64809 to 1.59810, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.05.hdf5\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.3340 - val_loss: 1.5981 - lr: 8.4294e-05\n",
      "Epoch 33/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3298\n",
      "Epoch 33: val_loss did not improve from 1.59810\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.3281 - val_loss: 1.7055 - lr: 8.4294e-05\n",
      "Epoch 34/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3413\n",
      "Epoch 34: val_loss did not improve from 1.59810\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.3387 - val_loss: 1.7780 - lr: 8.3451e-05\n",
      "Epoch 35/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3336\n",
      "Epoch 35: val_loss did not improve from 1.59810\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.3325 - val_loss: 1.6365 - lr: 8.2617e-05\n",
      "Epoch 36/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3272\n",
      "Epoch 36: val_loss did not improve from 1.59810\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.3259 - val_loss: 1.6578 - lr: 8.1791e-05\n",
      "Epoch 37/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3279\n",
      "Epoch 37: val_loss improved from 1.59810 to 1.37905, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.05.hdf5\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.3345 - val_loss: 1.3791 - lr: 8.0973e-05\n",
      "Epoch 38/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3301\n",
      "Epoch 38: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.3285 - val_loss: 1.4705 - lr: 8.0973e-05\n",
      "Epoch 39/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3172\n",
      "Epoch 39: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 4s 61ms/step - loss: 0.3161 - val_loss: 1.5998 - lr: 8.0163e-05\n",
      "Epoch 40/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3148\n",
      "Epoch 40: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 4s 61ms/step - loss: 0.3123 - val_loss: 1.7181 - lr: 7.9361e-05\n",
      "Epoch 41/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3203\n",
      "Epoch 41: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 4s 61ms/step - loss: 0.3211 - val_loss: 1.6468 - lr: 7.8568e-05\n",
      "Epoch 42/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3191\n",
      "Epoch 42: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 4s 63ms/step - loss: 0.3198 - val_loss: 1.6578 - lr: 7.7782e-05\n",
      "Epoch 43/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3086\n",
      "Epoch 43: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.3133 - val_loss: 1.6104 - lr: 7.7004e-05\n",
      "Epoch 44/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3056\n",
      "Epoch 44: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 4s 61ms/step - loss: 0.3031 - val_loss: 1.5533 - lr: 7.6234e-05\n",
      "Epoch 45/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3052\n",
      "Epoch 45: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.3052 - val_loss: 1.5784 - lr: 7.5472e-05\n",
      "Epoch 46/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3124\n",
      "Epoch 46: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.3100 - val_loss: 1.4922 - lr: 7.4717e-05\n",
      "Epoch 47/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3033\n",
      "Epoch 47: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.3028 - val_loss: 1.5326 - lr: 7.3970e-05\n",
      "Epoch 48/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2974\n",
      "Epoch 48: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 4s 64ms/step - loss: 0.2992 - val_loss: 1.6311 - lr: 7.3230e-05\n",
      "Epoch 49/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2831\n",
      "Epoch 49: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.2818 - val_loss: 1.5110 - lr: 7.2498e-05\n",
      "Epoch 50/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2995\n",
      "Epoch 50: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.2979 - val_loss: 1.4469 - lr: 7.1773e-05\n",
      "Epoch 51/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2906\n",
      "Epoch 51: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.2898 - val_loss: 1.6183 - lr: 7.1055e-05\n",
      "Epoch 52/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2926\n",
      "Epoch 52: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.2930 - val_loss: 1.5903 - lr: 7.0345e-05\n",
      "Epoch 53/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3008\n",
      "Epoch 53: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.2982 - val_loss: 1.5655 - lr: 6.9641e-05\n",
      "Epoch 54/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2939\n",
      "Epoch 54: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.2939 - val_loss: 1.4974 - lr: 6.8945e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2785\n",
      "Epoch 55: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.2771 - val_loss: 1.6201 - lr: 6.8255e-05\n",
      "Epoch 56/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2726\n",
      "Epoch 56: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.2726 - val_loss: 1.6812 - lr: 6.7573e-05\n",
      "Epoch 57/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2911\n",
      "Epoch 57: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 4s 63ms/step - loss: 0.2917 - val_loss: 1.7161 - lr: 6.6897e-05\n",
      "Epoch 58/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2885\n",
      "Epoch 58: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.2881 - val_loss: 1.5796 - lr: 6.6228e-05\n",
      "Epoch 59/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2879\n",
      "Epoch 59: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.2870 - val_loss: 1.6074 - lr: 6.5566e-05\n",
      "Epoch 60/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2797\n",
      "Epoch 60: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 4s 61ms/step - loss: 0.2786 - val_loss: 1.6180 - lr: 6.4910e-05\n",
      "Epoch 61/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2859\n",
      "Epoch 61: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 4s 64ms/step - loss: 0.2854 - val_loss: 1.5788 - lr: 6.4261e-05\n",
      "Epoch 62/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2746\n",
      "Epoch 62: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.2744 - val_loss: 1.5172 - lr: 6.3619e-05\n",
      "Epoch 63/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2721\n",
      "Epoch 63: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.2722 - val_loss: 1.5525 - lr: 6.2982e-05\n",
      "Epoch 64/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2858\n",
      "Epoch 64: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.2877 - val_loss: 1.6375 - lr: 6.2353e-05\n",
      "Epoch 65/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2686\n",
      "Epoch 65: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.2690 - val_loss: 1.7487 - lr: 6.1729e-05\n",
      "Epoch 66/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2797\n",
      "Epoch 66: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 4s 61ms/step - loss: 0.2811 - val_loss: 1.7024 - lr: 6.1112e-05\n",
      "Epoch 67/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2709\n",
      "Epoch 67: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.2708 - val_loss: 1.5779 - lr: 6.0501e-05\n",
      "Epoch 68/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2685\n",
      "Epoch 68: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.2682 - val_loss: 1.6047 - lr: 5.9896e-05\n",
      "Epoch 69/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2629\n",
      "Epoch 69: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 5.8703650938696225e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.2629 - val_loss: 1.7340 - lr: 5.9297e-05\n",
      "Epoch 70/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2636\n",
      "Epoch 70: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 5.811661550978897e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.2641 - val_loss: 1.7975 - lr: 5.8704e-05\n",
      "Epoch 71/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2659\n",
      "Epoch 71: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 5.753545068728272e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.2665 - val_loss: 1.8100 - lr: 5.8117e-05\n",
      "Epoch 72/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2619\n",
      "Epoch 72: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 5.6960095243994146e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.2658 - val_loss: 1.6183 - lr: 5.7535e-05\n",
      "Epoch 73/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2665\n",
      "Epoch 73: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 5.639049515593797e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.2662 - val_loss: 1.6331 - lr: 5.6960e-05\n",
      "Epoch 74/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2718\n",
      "Epoch 74: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 5.5826589195930865e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.2707 - val_loss: 1.6815 - lr: 5.6390e-05\n",
      "Epoch 75/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2631\n",
      "Epoch 75: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 5.526832333998754e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.2629 - val_loss: 1.7376 - lr: 5.5827e-05\n",
      "Epoch 76/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2522\n",
      "Epoch 76: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 5.471563996252371e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.2551 - val_loss: 1.8316 - lr: 5.5268e-05\n",
      "Epoch 77/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2711\n",
      "Epoch 77: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 5.416848503955407e-05.\n",
      "66/66 [==============================] - 4s 65ms/step - loss: 0.2708 - val_loss: 1.7312 - lr: 5.4716e-05\n",
      "Epoch 78/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2574\n",
      "Epoch 78: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 5.362680094549433e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.2581 - val_loss: 1.7562 - lr: 5.4168e-05\n",
      "Epoch 79/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2646\n",
      "Epoch 79: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 5.309053365635918e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.2659 - val_loss: 1.7730 - lr: 5.3627e-05\n",
      "Epoch 80/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2452\n",
      "Epoch 80: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 5.2559629148163366e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.2484 - val_loss: 1.6906 - lr: 5.3091e-05\n",
      "Epoch 81/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2558\n",
      "Epoch 81: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 5.2034033396921584e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 4s 62ms/step - loss: 0.2542 - val_loss: 1.8695 - lr: 5.2560e-05\n",
      "Epoch 82/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2662\n",
      "Epoch 82: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 5.1513692378648555e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.2680 - val_loss: 1.9933 - lr: 5.2034e-05\n",
      "Epoch 83/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2557\n",
      "Epoch 83: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 5.099855567095801e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.2554 - val_loss: 1.7623 - lr: 5.1514e-05\n",
      "Epoch 84/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2564\n",
      "Epoch 84: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 5.048856924986467e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.2559 - val_loss: 1.8001 - lr: 5.0999e-05\n",
      "Epoch 85/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2499\n",
      "Epoch 85: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 4.998368269298226e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.2505 - val_loss: 1.9115 - lr: 5.0489e-05\n",
      "Epoch 86/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2518\n",
      "Epoch 86: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 4.9483845577924514e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.2526 - val_loss: 1.8966 - lr: 4.9984e-05\n",
      "Epoch 87/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2508\n",
      "Epoch 87: val_loss did not improve from 1.37905\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 4.898900748230517e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.2513 - val_loss: 1.9096 - lr: 4.9484e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_16\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_17 (InputLayer)          [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_48 (LSTM)                 (None, 45, 24)       3744        ['input_17[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_32 (Dropout)           (None, 45, 24)       0           ['lstm_48[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_49 (LSTM)                 (None, 45, 16)       2624        ['dropout_32[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_33 (Dropout)           (None, 45, 16)       0           ['lstm_49[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_50 (LSTM)                 (None, 32)           6272        ['dropout_33[0][0]']             \n",
      "                                                                                                  \n",
      " dense_32 (Dense)               (None, 40)           1320        ['lstm_50[0][0]']                \n",
      "                                                                                                  \n",
      " dense_33 (Dense)               (None, 5)            205         ['dense_32[0][0]']               \n",
      "                                                                                                  \n",
      " tf.unstack_16 (TFOpLambda)     [(None,),            0           ['dense_33[0][0]']               \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_80 (TFOpLambda)  (None, 1)           0           ['tf.unstack_16[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_32 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_80[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_84 (TFOpLambda)  (None, 1)           0           ['tf.unstack_16[0][4]']          \n",
      "                                                                                                  \n",
      " tf.math.multiply_48 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_32[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_33 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_84[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_49 (TFOpLambd  (None, 1)           0           ['tf.math.multiply_48[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_81 (TFOpLambda)  (None, 1)           0           ['tf.unstack_16[0][1]']          \n",
      "                                                                                                  \n",
      " tf.expand_dims_83 (TFOpLambda)  (None, 1)           0           ['tf.unstack_16[0][3]']          \n",
      "                                                                                                  \n",
      " tf.math.multiply_50 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_33[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_32 (TFOpL  (None, 1)           0           ['tf.math.multiply_49[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.softplus_32 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_81[0][0]']      \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_82 (TFOpLambda)  (None, 1)           0           ['tf.unstack_16[0][2]']          \n",
      "                                                                                                  \n",
      " tf.math.softplus_33 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_83[0][0]']      \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_33 (TFOpL  (None, 1)           0           ['tf.math.multiply_50[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack_16 (TFOpLambda)       (None, 5, 1)         0           ['tf.__operators__.add_32[0][0]',\n",
      "                                                                  'tf.math.softplus_32[0][0]',    \n",
      "                                                                  'tf.expand_dims_82[0][0]',      \n",
      "                                                                  'tf.math.softplus_33[0][0]',    \n",
      "                                                                  'tf.__operators__.add_33[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _2_CCMP_t+1_0.06\n",
      "Epoch 1/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 3.4045\n",
      "Epoch 1: val_loss improved from inf to 4.40045, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 10s 81ms/step - loss: 3.4065 - val_loss: 4.4004 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 2.9048\n",
      "Epoch 2: val_loss improved from 4.40045 to 3.66119, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 2.9023 - val_loss: 3.6612 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.7287\n",
      "Epoch 3: val_loss improved from 3.66119 to 2.85726, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 1.7287 - val_loss: 2.8573 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/66 [============================>.] - ETA: 0s - loss: 1.1159\n",
      "Epoch 4: val_loss did not improve from 2.85726\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 1.1155 - val_loss: 3.1413 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8619\n",
      "Epoch 5: val_loss did not improve from 2.85726\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 5s 81ms/step - loss: 0.8619 - val_loss: 3.2063 - lr: 9.9000e-05\n",
      "Epoch 6/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7661\n",
      "Epoch 6: val_loss did not improve from 2.85726\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.7661 - val_loss: 3.1175 - lr: 9.8010e-05\n",
      "Epoch 7/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7242\n",
      "Epoch 7: val_loss did not improve from 2.85726\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.7242 - val_loss: 3.4167 - lr: 9.7030e-05\n",
      "Epoch 8/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6884\n",
      "Epoch 8: val_loss improved from 2.85726 to 2.78020, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.6884 - val_loss: 2.7802 - lr: 9.6060e-05\n",
      "Epoch 9/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6724\n",
      "Epoch 9: val_loss improved from 2.78020 to 2.65932, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.6724 - val_loss: 2.6593 - lr: 9.6060e-05\n",
      "Epoch 10/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6515\n",
      "Epoch 10: val_loss did not improve from 2.65932\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.6496 - val_loss: 2.7237 - lr: 9.6060e-05\n",
      "Epoch 11/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6450\n",
      "Epoch 11: val_loss did not improve from 2.65932\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.6431 - val_loss: 2.7310 - lr: 9.5099e-05\n",
      "Epoch 12/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6254\n",
      "Epoch 12: val_loss did not improve from 2.65932\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6236 - val_loss: 2.8643 - lr: 9.4148e-05\n",
      "Epoch 13/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6176\n",
      "Epoch 13: val_loss did not improve from 2.65932\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6154 - val_loss: 2.6895 - lr: 9.3207e-05\n",
      "Epoch 14/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5997\n",
      "Epoch 14: val_loss did not improve from 2.65932\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.5997 - val_loss: 2.6956 - lr: 9.2274e-05\n",
      "Epoch 15/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5890\n",
      "Epoch 15: val_loss improved from 2.65932 to 2.39392, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.5890 - val_loss: 2.3939 - lr: 9.1352e-05\n",
      "Epoch 16/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5815\n",
      "Epoch 16: val_loss did not improve from 2.39392\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5809 - val_loss: 2.7289 - lr: 9.1352e-05\n",
      "Epoch 17/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5865\n",
      "Epoch 17: val_loss improved from 2.39392 to 2.09894, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5920 - val_loss: 2.0989 - lr: 9.0438e-05\n",
      "Epoch 18/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5906\n",
      "Epoch 18: val_loss did not improve from 2.09894\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5883 - val_loss: 2.1683 - lr: 9.0438e-05\n",
      "Epoch 19/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5539\n",
      "Epoch 19: val_loss did not improve from 2.09894\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5564 - val_loss: 2.3440 - lr: 8.9534e-05\n",
      "Epoch 20/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5581\n",
      "Epoch 20: val_loss did not improve from 2.09894\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5584 - val_loss: 2.1615 - lr: 8.8638e-05\n",
      "Epoch 21/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5413\n",
      "Epoch 21: val_loss did not improve from 2.09894\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.5401 - val_loss: 2.3591 - lr: 8.7752e-05\n",
      "Epoch 22/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5346\n",
      "Epoch 22: val_loss did not improve from 2.09894\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5366 - val_loss: 2.3329 - lr: 8.6875e-05\n",
      "Epoch 23/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5115\n",
      "Epoch 23: val_loss improved from 2.09894 to 2.03528, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5143 - val_loss: 2.0353 - lr: 8.6006e-05\n",
      "Epoch 24/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5104\n",
      "Epoch 24: val_loss did not improve from 2.03528\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5114 - val_loss: 2.4720 - lr: 8.6006e-05\n",
      "Epoch 25/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5150\n",
      "Epoch 25: val_loss did not improve from 2.03528\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5150 - val_loss: 2.0757 - lr: 8.5146e-05\n",
      "Epoch 26/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5006\n",
      "Epoch 26: val_loss did not improve from 2.03528\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5008 - val_loss: 2.3605 - lr: 8.4294e-05\n",
      "Epoch 27/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4947\n",
      "Epoch 27: val_loss did not improve from 2.03528\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4933 - val_loss: 2.1455 - lr: 8.3451e-05\n",
      "Epoch 28/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4862\n",
      "Epoch 28: val_loss improved from 2.03528 to 2.03527, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.06.hdf5\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4855 - val_loss: 2.0353 - lr: 8.2617e-05\n",
      "Epoch 29/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4687\n",
      "Epoch 29: val_loss did not improve from 2.03527\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 5s 76ms/step - loss: 0.4719 - val_loss: 2.1548 - lr: 8.1791e-05\n",
      "Epoch 30/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4740\n",
      "Epoch 30: val_loss did not improve from 2.03527\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.4727 - val_loss: 2.2077 - lr: 8.0973e-05\n",
      "Epoch 31/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4730\n",
      "Epoch 31: val_loss did not improve from 2.03527\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 0.4737 - val_loss: 2.1918 - lr: 8.0163e-05\n",
      "Epoch 32/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4574\n",
      "Epoch 32: val_loss improved from 2.03527 to 1.94176, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4598 - val_loss: 1.9418 - lr: 7.9361e-05\n",
      "Epoch 33/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4648\n",
      "Epoch 33: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 0.4644 - val_loss: 2.2332 - lr: 7.9361e-05\n",
      "Epoch 34/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4505\n",
      "Epoch 34: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 0.4483 - val_loss: 2.1694 - lr: 7.8568e-05\n",
      "Epoch 35/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4561\n",
      "Epoch 35: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 0.4538 - val_loss: 2.1493 - lr: 7.7782e-05\n",
      "Epoch 36/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4354\n",
      "Epoch 36: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 5s 75ms/step - loss: 0.4384 - val_loss: 2.0590 - lr: 7.7004e-05\n",
      "Epoch 37/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4499\n",
      "Epoch 37: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 5s 75ms/step - loss: 0.4497 - val_loss: 2.2633 - lr: 7.6234e-05\n",
      "Epoch 38/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4179\n",
      "Epoch 38: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.4183 - val_loss: 2.3518 - lr: 7.5472e-05\n",
      "Epoch 39/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4388\n",
      "Epoch 39: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 0.4366 - val_loss: 2.2250 - lr: 7.4717e-05\n",
      "Epoch 40/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4111\n",
      "Epoch 40: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 5s 75ms/step - loss: 0.4111 - val_loss: 2.2497 - lr: 7.3970e-05\n",
      "Epoch 41/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4234\n",
      "Epoch 41: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 5s 75ms/step - loss: 0.4242 - val_loss: 2.1629 - lr: 7.3230e-05\n",
      "Epoch 42/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4297\n",
      "Epoch 42: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 0.4327 - val_loss: 2.0463 - lr: 7.2498e-05\n",
      "Epoch 43/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4208\n",
      "Epoch 43: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 5s 75ms/step - loss: 0.4200 - val_loss: 2.2184 - lr: 7.1773e-05\n",
      "Epoch 44/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4121\n",
      "Epoch 44: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 5s 75ms/step - loss: 0.4130 - val_loss: 2.3884 - lr: 7.1055e-05\n",
      "Epoch 45/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4189\n",
      "Epoch 45: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 5s 75ms/step - loss: 0.4198 - val_loss: 2.4462 - lr: 7.0345e-05\n",
      "Epoch 46/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4071\n",
      "Epoch 46: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 5s 75ms/step - loss: 0.4077 - val_loss: 2.4059 - lr: 6.9641e-05\n",
      "Epoch 47/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4052\n",
      "Epoch 47: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 5s 75ms/step - loss: 0.4055 - val_loss: 2.6255 - lr: 6.8945e-05\n",
      "Epoch 48/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4104\n",
      "Epoch 48: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 5s 75ms/step - loss: 0.4092 - val_loss: 2.5008 - lr: 6.8255e-05\n",
      "Epoch 49/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4185\n",
      "Epoch 49: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 5s 75ms/step - loss: 0.4176 - val_loss: 2.5120 - lr: 6.7573e-05\n",
      "Epoch 50/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4015\n",
      "Epoch 50: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 5s 75ms/step - loss: 0.4019 - val_loss: 2.2530 - lr: 6.6897e-05\n",
      "Epoch 51/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3966\n",
      "Epoch 51: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.3965 - val_loss: 2.4704 - lr: 6.6228e-05\n",
      "Epoch 52/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3991\n",
      "Epoch 52: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 5s 75ms/step - loss: 0.3994 - val_loss: 2.3385 - lr: 6.5566e-05\n",
      "Epoch 53/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3922\n",
      "Epoch 53: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.3932 - val_loss: 2.5397 - lr: 6.4910e-05\n",
      "Epoch 54/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3896\n",
      "Epoch 54: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.3907 - val_loss: 2.3483 - lr: 6.4261e-05\n",
      "Epoch 55/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4051\n",
      "Epoch 55: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.4040 - val_loss: 2.3850 - lr: 6.3619e-05\n",
      "Epoch 56/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3920\n",
      "Epoch 56: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.3908 - val_loss: 2.3711 - lr: 6.2982e-05\n",
      "Epoch 57/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3866\n",
      "Epoch 57: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.3889 - val_loss: 2.4968 - lr: 6.2353e-05\n",
      "Epoch 58/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3974\n",
      "Epoch 58: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.3985 - val_loss: 2.3973 - lr: 6.1729e-05\n",
      "Epoch 59/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3885\n",
      "Epoch 59: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.3881 - val_loss: 2.4118 - lr: 6.1112e-05\n",
      "Epoch 60/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3849\n",
      "Epoch 60: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.3849 - val_loss: 2.5702 - lr: 6.0501e-05\n",
      "Epoch 61/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3870\n",
      "Epoch 61: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.3891 - val_loss: 2.4857 - lr: 5.9896e-05\n",
      "Epoch 62/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3985\n",
      "Epoch 62: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 5.8703650938696225e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.3982 - val_loss: 2.5269 - lr: 5.9297e-05\n",
      "Epoch 63/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3787\n",
      "Epoch 63: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 5.811661550978897e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.3762 - val_loss: 2.6716 - lr: 5.8704e-05\n",
      "Epoch 64/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3875\n",
      "Epoch 64: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 5.753545068728272e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.3857 - val_loss: 2.4871 - lr: 5.8117e-05\n",
      "Epoch 65/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3665\n",
      "Epoch 65: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 5.6960095243994146e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.3655 - val_loss: 2.5542 - lr: 5.7535e-05\n",
      "Epoch 66/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3721\n",
      "Epoch 66: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 5.639049515593797e-05.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 0.3724 - val_loss: 2.2776 - lr: 5.6960e-05\n",
      "Epoch 67/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3727\n",
      "Epoch 67: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 5.5826589195930865e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.3728 - val_loss: 2.5434 - lr: 5.6390e-05\n",
      "Epoch 68/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3791\n",
      "Epoch 68: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 5.526832333998754e-05.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 0.3786 - val_loss: 2.4421 - lr: 5.5827e-05\n",
      "Epoch 69/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3758\n",
      "Epoch 69: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 5.471563996252371e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.3755 - val_loss: 2.7074 - lr: 5.5268e-05\n",
      "Epoch 70/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3693\n",
      "Epoch 70: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 5.416848503955407e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.3682 - val_loss: 2.6134 - lr: 5.4716e-05\n",
      "Epoch 71/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3734\n",
      "Epoch 71: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 5.362680094549433e-05.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 0.3721 - val_loss: 2.4917 - lr: 5.4168e-05\n",
      "Epoch 72/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3684\n",
      "Epoch 72: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 5.309053365635918e-05.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 0.3675 - val_loss: 2.4964 - lr: 5.3627e-05\n",
      "Epoch 73/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3700\n",
      "Epoch 73: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 5.2559629148163366e-05.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 0.3712 - val_loss: 2.4573 - lr: 5.3091e-05\n",
      "Epoch 74/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3712\n",
      "Epoch 74: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 5.2034033396921584e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.3730 - val_loss: 2.3007 - lr: 5.2560e-05\n",
      "Epoch 75/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3637\n",
      "Epoch 75: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 5.1513692378648555e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.3635 - val_loss: 2.5190 - lr: 5.2034e-05\n",
      "Epoch 76/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3712\n",
      "Epoch 76: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 5.099855567095801e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.3715 - val_loss: 2.2566 - lr: 5.1514e-05\n",
      "Epoch 77/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3653\n",
      "Epoch 77: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 5.048856924986467e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.3635 - val_loss: 2.4633 - lr: 5.0999e-05\n",
      "Epoch 78/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3692\n",
      "Epoch 78: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 4.998368269298226e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.3693 - val_loss: 2.4965 - lr: 5.0489e-05\n",
      "Epoch 79/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3638\n",
      "Epoch 79: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 4.9483845577924514e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.3635 - val_loss: 2.4968 - lr: 4.9984e-05\n",
      "Epoch 80/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3625\n",
      "Epoch 80: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 4.898900748230517e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.3631 - val_loss: 2.5103 - lr: 4.9484e-05\n",
      "Epoch 81/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3585\n",
      "Epoch 81: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 4.849911798373796e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.3575 - val_loss: 2.1343 - lr: 4.8989e-05\n",
      "Epoch 82/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3605\n",
      "Epoch 82: val_loss did not improve from 1.94176\n",
      "\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 4.801412665983662e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "66/66 [==============================] - 5s 75ms/step - loss: 0.3575 - val_loss: 2.3856 - lr: 4.8499e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_17\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_18 (InputLayer)          [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_51 (LSTM)                 (None, 45, 24)       3744        ['input_18[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_34 (Dropout)           (None, 45, 24)       0           ['lstm_51[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_52 (LSTM)                 (None, 45, 16)       2624        ['dropout_34[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_35 (Dropout)           (None, 45, 16)       0           ['lstm_52[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_53 (LSTM)                 (None, 32)           6272        ['dropout_35[0][0]']             \n",
      "                                                                                                  \n",
      " dense_34 (Dense)               (None, 40)           1320        ['lstm_53[0][0]']                \n",
      "                                                                                                  \n",
      " dense_35 (Dense)               (None, 5)            205         ['dense_34[0][0]']               \n",
      "                                                                                                  \n",
      " tf.unstack_17 (TFOpLambda)     [(None,),            0           ['dense_35[0][0]']               \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_85 (TFOpLambda)  (None, 1)           0           ['tf.unstack_17[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_34 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_85[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_89 (TFOpLambda)  (None, 1)           0           ['tf.unstack_17[0][4]']          \n",
      "                                                                                                  \n",
      " tf.math.multiply_51 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_34[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_35 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_89[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_52 (TFOpLambd  (None, 1)           0           ['tf.math.multiply_51[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_86 (TFOpLambda)  (None, 1)           0           ['tf.unstack_17[0][1]']          \n",
      "                                                                                                  \n",
      " tf.expand_dims_88 (TFOpLambda)  (None, 1)           0           ['tf.unstack_17[0][3]']          \n",
      "                                                                                                  \n",
      " tf.math.multiply_53 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_35[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_34 (TFOpL  (None, 1)           0           ['tf.math.multiply_52[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.softplus_34 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_86[0][0]']      \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_87 (TFOpLambda)  (None, 1)           0           ['tf.unstack_17[0][2]']          \n",
      "                                                                                                  \n",
      " tf.math.softplus_35 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_88[0][0]']      \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_35 (TFOpL  (None, 1)           0           ['tf.math.multiply_53[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack_17 (TFOpLambda)       (None, 5, 1)         0           ['tf.__operators__.add_34[0][0]',\n",
      "                                                                  'tf.math.softplus_34[0][0]',    \n",
      "                                                                  'tf.expand_dims_87[0][0]',      \n",
      "                                                                  'tf.math.softplus_35[0][0]',    \n",
      "                                                                  'tf.__operators__.add_35[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _2_CCMP_t+1_0.07\n",
      "Epoch 1/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 3.3382\n",
      "Epoch 1: val_loss improved from inf to 4.21278, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 10s 87ms/step - loss: 3.3396 - val_loss: 4.2128 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 2.6825\n",
      "Epoch 2: val_loss improved from 4.21278 to 3.82312, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 2.6795 - val_loss: 3.8231 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.8451\n",
      "Epoch 3: val_loss did not improve from 3.82312\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 5s 81ms/step - loss: 1.8437 - val_loss: 4.2519 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.4022\n",
      "Epoch 4: val_loss did not improve from 3.82312\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 1.4004 - val_loss: 4.0541 - lr: 9.9000e-05\n",
      "Epoch 5/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1368\n",
      "Epoch 5: val_loss improved from 3.82312 to 3.67292, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 1.1383 - val_loss: 3.6729 - lr: 9.8010e-05\n",
      "Epoch 6/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9667\n",
      "Epoch 6: val_loss improved from 3.67292 to 3.63577, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.9670 - val_loss: 3.6358 - lr: 9.8010e-05\n",
      "Epoch 7/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8800\n",
      "Epoch 7: val_loss improved from 3.63577 to 3.43737, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.8796 - val_loss: 3.4374 - lr: 9.8010e-05\n",
      "Epoch 8/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8239\n",
      "Epoch 8: val_loss did not improve from 3.43737\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.8218 - val_loss: 3.4592 - lr: 9.8010e-05\n",
      "Epoch 9/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7817\n",
      "Epoch 9: val_loss improved from 3.43737 to 3.13350, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.7796 - val_loss: 3.1335 - lr: 9.7030e-05\n",
      "Epoch 10/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7371\n",
      "Epoch 10: val_loss did not improve from 3.13350\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.7378 - val_loss: 3.2513 - lr: 9.7030e-05\n",
      "Epoch 11/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7007\n",
      "Epoch 11: val_loss did not improve from 3.13350\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 0.7014 - val_loss: 3.3881 - lr: 9.6060e-05\n",
      "Epoch 12/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6849\n",
      "Epoch 12: val_loss did not improve from 3.13350\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6837 - val_loss: 3.3970 - lr: 9.5099e-05\n",
      "Epoch 13/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6590\n",
      "Epoch 13: val_loss improved from 3.13350 to 3.12876, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6597 - val_loss: 3.1288 - lr: 9.4148e-05\n",
      "Epoch 14/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6299\n",
      "Epoch 14: val_loss improved from 3.12876 to 3.01232, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6285 - val_loss: 3.0123 - lr: 9.4148e-05\n",
      "Epoch 15/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6089\n",
      "Epoch 15: val_loss did not improve from 3.01232\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6097 - val_loss: 3.1737 - lr: 9.4148e-05\n",
      "Epoch 16/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6046\n",
      "Epoch 16: val_loss did not improve from 3.01232\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6061 - val_loss: 3.0482 - lr: 9.3207e-05\n",
      "Epoch 17/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5902\n",
      "Epoch 17: val_loss improved from 3.01232 to 2.85756, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5903 - val_loss: 2.8576 - lr: 9.2274e-05\n",
      "Epoch 18/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5949\n",
      "Epoch 18: val_loss improved from 2.85756 to 2.80937, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5944 - val_loss: 2.8094 - lr: 9.2274e-05\n",
      "Epoch 19/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5911\n",
      "Epoch 19: val_loss improved from 2.80937 to 2.54799, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5911 - val_loss: 2.5480 - lr: 9.2274e-05\n",
      "Epoch 20/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5702\n",
      "Epoch 20: val_loss did not improve from 2.54799\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5705 - val_loss: 2.5994 - lr: 9.2274e-05\n",
      "Epoch 21/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5766\n",
      "Epoch 21: val_loss improved from 2.54799 to 2.44972, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5762 - val_loss: 2.4497 - lr: 9.1352e-05\n",
      "Epoch 22/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5544\n",
      "Epoch 22: val_loss improved from 2.44972 to 2.44623, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5547 - val_loss: 2.4462 - lr: 9.1352e-05\n",
      "Epoch 23/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5660\n",
      "Epoch 23: val_loss improved from 2.44623 to 2.38170, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5647 - val_loss: 2.3817 - lr: 9.1352e-05\n",
      "Epoch 24/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5404\n",
      "Epoch 24: val_loss did not improve from 2.38170\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5394 - val_loss: 2.3966 - lr: 9.1352e-05\n",
      "Epoch 25/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5560\n",
      "Epoch 25: val_loss did not improve from 2.38170\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5547 - val_loss: 2.4170 - lr: 9.0438e-05\n",
      "Epoch 26/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5504\n",
      "Epoch 26: val_loss improved from 2.38170 to 2.30087, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5504 - val_loss: 2.3009 - lr: 8.9534e-05\n",
      "Epoch 27/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5297\n",
      "Epoch 27: val_loss improved from 2.30087 to 2.28402, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 6s 84ms/step - loss: 0.5288 - val_loss: 2.2840 - lr: 8.9534e-05\n",
      "Epoch 28/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5370\n",
      "Epoch 28: val_loss improved from 2.28402 to 2.27472, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.07.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5370 - val_loss: 2.2747 - lr: 8.9534e-05\n",
      "Epoch 29/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5224\n",
      "Epoch 29: val_loss improved from 2.27472 to 2.09983, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5221 - val_loss: 2.0998 - lr: 8.9534e-05\n",
      "Epoch 30/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5325\n",
      "Epoch 30: val_loss improved from 2.09983 to 2.08066, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5322 - val_loss: 2.0807 - lr: 8.9534e-05\n",
      "Epoch 31/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5249\n",
      "Epoch 31: val_loss improved from 2.08066 to 1.92966, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.5237 - val_loss: 1.9297 - lr: 8.9534e-05\n",
      "Epoch 32/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5158\n",
      "Epoch 32: val_loss did not improve from 1.92966\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5144 - val_loss: 2.0817 - lr: 8.9534e-05\n",
      "Epoch 33/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5122\n",
      "Epoch 33: val_loss improved from 1.92966 to 1.82482, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5115 - val_loss: 1.8248 - lr: 8.8638e-05\n",
      "Epoch 34/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5038\n",
      "Epoch 34: val_loss did not improve from 1.82482\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5059 - val_loss: 2.0328 - lr: 8.8638e-05\n",
      "Epoch 35/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5065\n",
      "Epoch 35: val_loss did not improve from 1.82482\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5067 - val_loss: 1.8321 - lr: 8.7752e-05\n",
      "Epoch 36/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5058\n",
      "Epoch 36: val_loss did not improve from 1.82482\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5070 - val_loss: 2.0066 - lr: 8.6875e-05\n",
      "Epoch 37/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5005\n",
      "Epoch 37: val_loss did not improve from 1.82482\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5000 - val_loss: 1.8256 - lr: 8.6006e-05\n",
      "Epoch 38/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5065\n",
      "Epoch 38: val_loss improved from 1.82482 to 1.70090, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5051 - val_loss: 1.7009 - lr: 8.5146e-05\n",
      "Epoch 39/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5041\n",
      "Epoch 39: val_loss did not improve from 1.70090\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5034 - val_loss: 1.7660 - lr: 8.5146e-05\n",
      "Epoch 40/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4978\n",
      "Epoch 40: val_loss did not improve from 1.70090\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4962 - val_loss: 1.7763 - lr: 8.4294e-05\n",
      "Epoch 41/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4903\n",
      "Epoch 41: val_loss did not improve from 1.70090\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4898 - val_loss: 1.7316 - lr: 8.3451e-05\n",
      "Epoch 42/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4881\n",
      "Epoch 42: val_loss improved from 1.70090 to 1.69176, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4868 - val_loss: 1.6918 - lr: 8.2617e-05\n",
      "Epoch 43/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5035\n",
      "Epoch 43: val_loss did not improve from 1.69176\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5038 - val_loss: 1.7175 - lr: 8.2617e-05\n",
      "Epoch 44/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4831\n",
      "Epoch 44: val_loss did not improve from 1.69176\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 0.4859 - val_loss: 1.7210 - lr: 8.1791e-05\n",
      "Epoch 45/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4758\n",
      "Epoch 45: val_loss improved from 1.69176 to 1.62519, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4758 - val_loss: 1.6252 - lr: 8.0973e-05\n",
      "Epoch 46/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4793\n",
      "Epoch 46: val_loss did not improve from 1.62519\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4785 - val_loss: 1.7466 - lr: 8.0973e-05\n",
      "Epoch 47/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4792\n",
      "Epoch 47: val_loss improved from 1.62519 to 1.56680, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.4792 - val_loss: 1.5668 - lr: 8.0163e-05\n",
      "Epoch 48/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4775\n",
      "Epoch 48: val_loss did not improve from 1.56680\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4770 - val_loss: 1.8260 - lr: 8.0163e-05\n",
      "Epoch 49/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4633\n",
      "Epoch 49: val_loss did not improve from 1.56680\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4649 - val_loss: 1.6304 - lr: 7.9361e-05\n",
      "Epoch 50/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4690\n",
      "Epoch 50: val_loss did not improve from 1.56680\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.4690 - val_loss: 1.6549 - lr: 7.8568e-05\n",
      "Epoch 51/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4752\n",
      "Epoch 51: val_loss improved from 1.56680 to 1.54086, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.4732 - val_loss: 1.5409 - lr: 7.7782e-05\n",
      "Epoch 52/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4631\n",
      "Epoch 52: val_loss improved from 1.54086 to 1.53674, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4619 - val_loss: 1.5367 - lr: 7.7782e-05\n",
      "Epoch 53/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4613\n",
      "Epoch 53: val_loss improved from 1.53674 to 1.52774, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.07.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 5s 78ms/step - loss: 0.4619 - val_loss: 1.5277 - lr: 7.7782e-05\n",
      "Epoch 54/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4764\n",
      "Epoch 54: val_loss did not improve from 1.52774\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 0.4767 - val_loss: 1.5813 - lr: 7.7782e-05\n",
      "Epoch 55/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4700\n",
      "Epoch 55: val_loss did not improve from 1.52774\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.4717 - val_loss: 1.6579 - lr: 7.7004e-05\n",
      "Epoch 56/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4618\n",
      "Epoch 56: val_loss did not improve from 1.52774\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 0.4614 - val_loss: 1.6181 - lr: 7.6234e-05\n",
      "Epoch 57/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4653\n",
      "Epoch 57: val_loss improved from 1.52774 to 1.49552, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.4649 - val_loss: 1.4955 - lr: 7.5472e-05\n",
      "Epoch 58/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4565\n",
      "Epoch 58: val_loss did not improve from 1.49552\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4567 - val_loss: 1.5927 - lr: 7.5472e-05\n",
      "Epoch 59/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4643\n",
      "Epoch 59: val_loss improved from 1.49552 to 1.45427, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4665 - val_loss: 1.4543 - lr: 7.4717e-05\n",
      "Epoch 60/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4552\n",
      "Epoch 60: val_loss did not improve from 1.45427\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4550 - val_loss: 1.6428 - lr: 7.4717e-05\n",
      "Epoch 61/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4501\n",
      "Epoch 61: val_loss did not improve from 1.45427\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4499 - val_loss: 1.6890 - lr: 7.3970e-05\n",
      "Epoch 62/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4625\n",
      "Epoch 62: val_loss did not improve from 1.45427\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4613 - val_loss: 1.7153 - lr: 7.3230e-05\n",
      "Epoch 63/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4505\n",
      "Epoch 63: val_loss did not improve from 1.45427\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 0.4505 - val_loss: 1.4685 - lr: 7.2498e-05\n",
      "Epoch 64/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4519\n",
      "Epoch 64: val_loss did not improve from 1.45427\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 0.4539 - val_loss: 1.4724 - lr: 7.1773e-05\n",
      "Epoch 65/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4436\n",
      "Epoch 65: val_loss did not improve from 1.45427\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4440 - val_loss: 1.5564 - lr: 7.1055e-05\n",
      "Epoch 66/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4502\n",
      "Epoch 66: val_loss did not improve from 1.45427\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.4497 - val_loss: 1.5233 - lr: 7.0345e-05\n",
      "Epoch 67/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4460\n",
      "Epoch 67: val_loss did not improve from 1.45427\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4463 - val_loss: 1.5935 - lr: 6.9641e-05\n",
      "Epoch 68/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4434\n",
      "Epoch 68: val_loss did not improve from 1.45427\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 0.4432 - val_loss: 1.5833 - lr: 6.8945e-05\n",
      "Epoch 69/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4373\n",
      "Epoch 69: val_loss did not improve from 1.45427\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4392 - val_loss: 1.5380 - lr: 6.8255e-05\n",
      "Epoch 70/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4357\n",
      "Epoch 70: val_loss did not improve from 1.45427\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4370 - val_loss: 1.6511 - lr: 6.7573e-05\n",
      "Epoch 71/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4452\n",
      "Epoch 71: val_loss did not improve from 1.45427\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4464 - val_loss: 1.5175 - lr: 6.6897e-05\n",
      "Epoch 72/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4362\n",
      "Epoch 72: val_loss did not improve from 1.45427\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4370 - val_loss: 1.5570 - lr: 6.6228e-05\n",
      "Epoch 73/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4361\n",
      "Epoch 73: val_loss did not improve from 1.45427\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.4361 - val_loss: 1.5260 - lr: 6.5566e-05\n",
      "Epoch 74/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4340\n",
      "Epoch 74: val_loss did not improve from 1.45427\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 5s 81ms/step - loss: 0.4340 - val_loss: 1.4640 - lr: 6.4910e-05\n",
      "Epoch 75/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4411\n",
      "Epoch 75: val_loss did not improve from 1.45427\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4403 - val_loss: 1.5183 - lr: 6.4261e-05\n",
      "Epoch 76/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4328\n",
      "Epoch 76: val_loss did not improve from 1.45427\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4336 - val_loss: 1.5168 - lr: 6.3619e-05\n",
      "Epoch 77/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4380\n",
      "Epoch 77: val_loss did not improve from 1.45427\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4382 - val_loss: 1.4988 - lr: 6.2982e-05\n",
      "Epoch 78/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4387\n",
      "Epoch 78: val_loss did not improve from 1.45427\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 0.4402 - val_loss: 1.5455 - lr: 6.2353e-05\n",
      "Epoch 79/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4287\n",
      "Epoch 79: val_loss did not improve from 1.45427\n",
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4293 - val_loss: 1.5098 - lr: 6.1729e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4306\n",
      "Epoch 80: val_loss improved from 1.45427 to 1.41748, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4295 - val_loss: 1.4175 - lr: 6.1112e-05\n",
      "Epoch 81/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4272\n",
      "Epoch 81: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 0.4302 - val_loss: 1.5010 - lr: 6.1112e-05\n",
      "Epoch 82/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4207\n",
      "Epoch 82: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4186 - val_loss: 1.4258 - lr: 6.0501e-05\n",
      "Epoch 83/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4255\n",
      "Epoch 83: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 0.4289 - val_loss: 1.5371 - lr: 5.9896e-05\n",
      "Epoch 84/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4317\n",
      "Epoch 84: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 5.8703650938696225e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4318 - val_loss: 1.5159 - lr: 5.9297e-05\n",
      "Epoch 85/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4222\n",
      "Epoch 85: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 5.811661550978897e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4238 - val_loss: 1.4924 - lr: 5.8704e-05\n",
      "Epoch 86/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4247\n",
      "Epoch 86: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 5.753545068728272e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4222 - val_loss: 1.4677 - lr: 5.8117e-05\n",
      "Epoch 87/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4184\n",
      "Epoch 87: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 5.6960095243994146e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4180 - val_loss: 1.5210 - lr: 5.7535e-05\n",
      "Epoch 88/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4171\n",
      "Epoch 88: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 5.639049515593797e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.4159 - val_loss: 1.5703 - lr: 5.6960e-05\n",
      "Epoch 89/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4303\n",
      "Epoch 89: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 5.5826589195930865e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4304 - val_loss: 1.5554 - lr: 5.6390e-05\n",
      "Epoch 90/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4205\n",
      "Epoch 90: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 5.526832333998754e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4213 - val_loss: 1.5595 - lr: 5.5827e-05\n",
      "Epoch 91/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4156\n",
      "Epoch 91: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 5.471563996252371e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4152 - val_loss: 1.4955 - lr: 5.5268e-05\n",
      "Epoch 92/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4178\n",
      "Epoch 92: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 5.416848503955407e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4173 - val_loss: 1.5807 - lr: 5.4716e-05\n",
      "Epoch 93/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4178\n",
      "Epoch 93: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 5.362680094549433e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4184 - val_loss: 1.5726 - lr: 5.4168e-05\n",
      "Epoch 94/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4056\n",
      "Epoch 94: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 5.309053365635918e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4049 - val_loss: 1.6328 - lr: 5.3627e-05\n",
      "Epoch 95/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4263\n",
      "Epoch 95: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 5.2559629148163366e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4260 - val_loss: 1.5589 - lr: 5.3091e-05\n",
      "Epoch 96/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4159\n",
      "Epoch 96: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 5.2034033396921584e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4161 - val_loss: 1.6274 - lr: 5.2560e-05\n",
      "Epoch 97/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4270\n",
      "Epoch 97: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 5.1513692378648555e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.4258 - val_loss: 1.6266 - lr: 5.2034e-05\n",
      "Epoch 98/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4201\n",
      "Epoch 98: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 5.099855567095801e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.4196 - val_loss: 1.4578 - lr: 5.1514e-05\n",
      "Epoch 99/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4220\n",
      "Epoch 99: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 5.048856924986467e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.4219 - val_loss: 1.5816 - lr: 5.0999e-05\n",
      "Epoch 100/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4159\n",
      "Epoch 100: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 4.998368269298226e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.4178 - val_loss: 1.5293 - lr: 5.0489e-05\n",
      "Epoch 101/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4117\n",
      "Epoch 101: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 4.9483845577924514e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.4123 - val_loss: 1.5937 - lr: 4.9984e-05\n",
      "Epoch 102/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4255\n",
      "Epoch 102: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 4.898900748230517e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.4245 - val_loss: 1.5120 - lr: 4.9484e-05\n",
      "Epoch 103/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4164\n",
      "Epoch 103: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 4.849911798373796e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4172 - val_loss: 1.5874 - lr: 4.8989e-05\n",
      "Epoch 104/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4106\n",
      "Epoch 104: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 4.801412665983662e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.4106 - val_loss: 1.5137 - lr: 4.8499e-05\n",
      "Epoch 105/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4040\n",
      "Epoch 105: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 4.75339866898139e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.4071 - val_loss: 1.5888 - lr: 4.8014e-05\n",
      "Epoch 106/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4072\n",
      "Epoch 106: val_loss did not improve from 1.41748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 4.705864765128354e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.4060 - val_loss: 1.6593 - lr: 4.7534e-05\n",
      "Epoch 107/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4117\n",
      "Epoch 107: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 4.658806272345828e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4113 - val_loss: 1.5629 - lr: 4.7059e-05\n",
      "Epoch 108/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4105\n",
      "Epoch 108: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 4.612218148395186e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.4094 - val_loss: 1.5993 - lr: 4.6588e-05\n",
      "Epoch 109/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4194\n",
      "Epoch 109: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 4.566096071357606e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.4190 - val_loss: 1.6187 - lr: 4.6122e-05\n",
      "Epoch 110/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4115\n",
      "Epoch 110: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 4.52043499899446e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.4123 - val_loss: 1.5263 - lr: 4.5661e-05\n",
      "Epoch 111/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4134\n",
      "Epoch 111: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 4.475230609386927e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.4130 - val_loss: 1.5226 - lr: 4.5204e-05\n",
      "Epoch 112/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4043\n",
      "Epoch 112: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 4.43047822045628e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.4044 - val_loss: 1.6057 - lr: 4.4752e-05\n",
      "Epoch 113/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3999\n",
      "Epoch 113: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 4.386173510283697e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.4004 - val_loss: 1.5928 - lr: 4.4305e-05\n",
      "Epoch 114/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4047\n",
      "Epoch 114: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 4.342311796790454e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.4031 - val_loss: 1.5736 - lr: 4.3862e-05\n",
      "Epoch 115/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4054\n",
      "Epoch 115: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 4.298888758057728e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.4047 - val_loss: 1.5721 - lr: 4.3423e-05\n",
      "Epoch 116/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4135\n",
      "Epoch 116: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 4.2558997120067946e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.4115 - val_loss: 1.5780 - lr: 4.2989e-05\n",
      "Epoch 117/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4090\n",
      "Epoch 117: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 4.2133406968787315e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.4089 - val_loss: 1.6354 - lr: 4.2559e-05\n",
      "Epoch 118/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4019\n",
      "Epoch 118: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 4.1712073907547166e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.4009 - val_loss: 1.6159 - lr: 4.2133e-05\n",
      "Epoch 119/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4074\n",
      "Epoch 119: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 4.1294954717159274e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.4085 - val_loss: 1.6308 - lr: 4.1712e-05\n",
      "Epoch 120/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4073\n",
      "Epoch 120: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 4.08820061784354e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.4073 - val_loss: 1.5553 - lr: 4.1295e-05\n",
      "Epoch 121/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3945\n",
      "Epoch 121: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 4.0473185072187336e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.3949 - val_loss: 1.5406 - lr: 4.0882e-05\n",
      "Epoch 122/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4106\n",
      "Epoch 122: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 4.0068451780825854e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.4096 - val_loss: 1.6740 - lr: 4.0473e-05\n",
      "Epoch 123/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4145\n",
      "Epoch 123: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 123: ReduceLROnPlateau reducing learning rate to 3.966776668676175e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.4165 - val_loss: 1.6284 - lr: 4.0068e-05\n",
      "Epoch 124/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4034\n",
      "Epoch 124: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 124: ReduceLROnPlateau reducing learning rate to 3.927109017240582e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.4031 - val_loss: 1.5351 - lr: 3.9668e-05\n",
      "Epoch 125/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4005\n",
      "Epoch 125: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 3.887837901856983e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.4018 - val_loss: 1.5604 - lr: 3.9271e-05\n",
      "Epoch 126/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4068\n",
      "Epoch 126: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 3.848959360766457e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.4047 - val_loss: 1.5504 - lr: 3.8878e-05\n",
      "Epoch 127/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3982\n",
      "Epoch 127: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 127: ReduceLROnPlateau reducing learning rate to 3.810469792369986e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.3997 - val_loss: 1.5431 - lr: 3.8490e-05\n",
      "Epoch 128/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3995\n",
      "Epoch 128: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 128: ReduceLROnPlateau reducing learning rate to 3.772365234908648e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.3995 - val_loss: 1.5425 - lr: 3.8105e-05\n",
      "Epoch 129/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3965\n",
      "Epoch 129: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 129: ReduceLROnPlateau reducing learning rate to 3.734641726623522e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.3950 - val_loss: 1.6436 - lr: 3.7724e-05\n",
      "Epoch 130/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4046\n",
      "Epoch 130: val_loss did not improve from 1.41748\n",
      "\n",
      "Epoch 130: ReduceLROnPlateau reducing learning rate to 3.6972953057556876e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.4038 - val_loss: 1.5412 - lr: 3.7346e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_18\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_19 (InputLayer)          [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_54 (LSTM)                 (None, 45, 24)       3744        ['input_19[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_36 (Dropout)           (None, 45, 24)       0           ['lstm_54[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_55 (LSTM)                 (None, 45, 16)       2624        ['dropout_36[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 45, 16)       0           ['lstm_55[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_56 (LSTM)                 (None, 32)           6272        ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      " dense_36 (Dense)               (None, 40)           1320        ['lstm_56[0][0]']                \n",
      "                                                                                                  \n",
      " dense_37 (Dense)               (None, 5)            205         ['dense_36[0][0]']               \n",
      "                                                                                                  \n",
      " tf.unstack_18 (TFOpLambda)     [(None,),            0           ['dense_37[0][0]']               \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_90 (TFOpLambda)  (None, 1)           0           ['tf.unstack_18[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_36 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_90[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_94 (TFOpLambda)  (None, 1)           0           ['tf.unstack_18[0][4]']          \n",
      "                                                                                                  \n",
      " tf.math.multiply_54 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_36[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_37 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_94[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_55 (TFOpLambd  (None, 1)           0           ['tf.math.multiply_54[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_91 (TFOpLambda)  (None, 1)           0           ['tf.unstack_18[0][1]']          \n",
      "                                                                                                  \n",
      " tf.expand_dims_93 (TFOpLambda)  (None, 1)           0           ['tf.unstack_18[0][3]']          \n",
      "                                                                                                  \n",
      " tf.math.multiply_56 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_37[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_36 (TFOpL  (None, 1)           0           ['tf.math.multiply_55[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.softplus_36 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_91[0][0]']      \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_92 (TFOpLambda)  (None, 1)           0           ['tf.unstack_18[0][2]']          \n",
      "                                                                                                  \n",
      " tf.math.softplus_37 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_93[0][0]']      \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_37 (TFOpL  (None, 1)           0           ['tf.math.multiply_56[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack_18 (TFOpLambda)       (None, 5, 1)         0           ['tf.__operators__.add_36[0][0]',\n",
      "                                                                  'tf.math.softplus_36[0][0]',    \n",
      "                                                                  'tf.expand_dims_92[0][0]',      \n",
      "                                                                  'tf.math.softplus_37[0][0]',    \n",
      "                                                                  'tf.__operators__.add_37[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _2_CCMP_t+1_0.08\n",
      "Epoch 1/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 3.3752\n",
      "Epoch 1: val_loss improved from inf to 3.92748, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 10s 79ms/step - loss: 3.3744 - val_loss: 3.9275 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 2.9269\n",
      "Epoch 2: val_loss improved from 3.92748 to 3.17692, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 2.9269 - val_loss: 3.1769 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.9910\n",
      "Epoch 3: val_loss improved from 3.17692 to 2.87321, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 1.9910 - val_loss: 2.8732 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/66 [============================>.] - ETA: 0s - loss: 1.2357\n",
      "Epoch 4: val_loss did not improve from 2.87321\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 1.2355 - val_loss: 3.1526 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0329\n",
      "Epoch 5: val_loss did not improve from 2.87321\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 4s 61ms/step - loss: 1.0341 - val_loss: 3.1150 - lr: 9.9000e-05\n",
      "Epoch 6/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9339\n",
      "Epoch 6: val_loss did not improve from 2.87321\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.9321 - val_loss: 3.4627 - lr: 9.8010e-05\n",
      "Epoch 7/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8783\n",
      "Epoch 7: val_loss did not improve from 2.87321\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 4s 62ms/step - loss: 0.8787 - val_loss: 3.5289 - lr: 9.7030e-05\n",
      "Epoch 8/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8502\n",
      "Epoch 8: val_loss did not improve from 2.87321\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 4s 63ms/step - loss: 0.8516 - val_loss: 3.3811 - lr: 9.6060e-05\n",
      "Epoch 9/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8101\n",
      "Epoch 9: val_loss did not improve from 2.87321\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 4s 63ms/step - loss: 0.8074 - val_loss: 3.6457 - lr: 9.5099e-05\n",
      "Epoch 10/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7773\n",
      "Epoch 10: val_loss did not improve from 2.87321\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 4s 63ms/step - loss: 0.7785 - val_loss: 3.5622 - lr: 9.4148e-05\n",
      "Epoch 11/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7638\n",
      "Epoch 11: val_loss did not improve from 2.87321\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 4s 64ms/step - loss: 0.7654 - val_loss: 3.4948 - lr: 9.3207e-05\n",
      "Epoch 12/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7457\n",
      "Epoch 12: val_loss did not improve from 2.87321\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 4s 63ms/step - loss: 0.7473 - val_loss: 3.5681 - lr: 9.2274e-05\n",
      "Epoch 13/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7325\n",
      "Epoch 13: val_loss did not improve from 2.87321\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 4s 64ms/step - loss: 0.7327 - val_loss: 3.2754 - lr: 9.1352e-05\n",
      "Epoch 14/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7105\n",
      "Epoch 14: val_loss did not improve from 2.87321\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 4s 66ms/step - loss: 0.7120 - val_loss: 3.5333 - lr: 9.0438e-05\n",
      "Epoch 15/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7160\n",
      "Epoch 15: val_loss did not improve from 2.87321\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 4s 64ms/step - loss: 0.7154 - val_loss: 3.3151 - lr: 8.9534e-05\n",
      "Epoch 16/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6817\n",
      "Epoch 16: val_loss did not improve from 2.87321\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 4s 64ms/step - loss: 0.6947 - val_loss: 3.0139 - lr: 8.8638e-05\n",
      "Epoch 17/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6948\n",
      "Epoch 17: val_loss did not improve from 2.87321\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 4s 64ms/step - loss: 0.6954 - val_loss: 3.2906 - lr: 8.7752e-05\n",
      "Epoch 18/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6814\n",
      "Epoch 18: val_loss did not improve from 2.87321\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 4s 65ms/step - loss: 0.6799 - val_loss: 3.5051 - lr: 8.6875e-05\n",
      "Epoch 19/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6897\n",
      "Epoch 19: val_loss did not improve from 2.87321\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 4s 65ms/step - loss: 0.6877 - val_loss: 2.9809 - lr: 8.6006e-05\n",
      "Epoch 20/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6675\n",
      "Epoch 20: val_loss did not improve from 2.87321\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 4s 64ms/step - loss: 0.6672 - val_loss: 2.9665 - lr: 8.5146e-05\n",
      "Epoch 21/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6552\n",
      "Epoch 21: val_loss did not improve from 2.87321\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 4s 65ms/step - loss: 0.6540 - val_loss: 3.3384 - lr: 8.4294e-05\n",
      "Epoch 22/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6527\n",
      "Epoch 22: val_loss improved from 2.87321 to 2.78031, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 4s 65ms/step - loss: 0.6515 - val_loss: 2.7803 - lr: 8.3451e-05\n",
      "Epoch 23/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6569\n",
      "Epoch 23: val_loss did not improve from 2.78031\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 4s 64ms/step - loss: 0.6564 - val_loss: 2.8703 - lr: 8.3451e-05\n",
      "Epoch 24/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6441\n",
      "Epoch 24: val_loss did not improve from 2.78031\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 4s 64ms/step - loss: 0.6439 - val_loss: 2.8872 - lr: 8.2617e-05\n",
      "Epoch 25/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6489\n",
      "Epoch 25: val_loss improved from 2.78031 to 2.73131, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 4s 65ms/step - loss: 0.6501 - val_loss: 2.7313 - lr: 8.1791e-05\n",
      "Epoch 26/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6415\n",
      "Epoch 26: val_loss improved from 2.73131 to 2.62349, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 4s 65ms/step - loss: 0.6404 - val_loss: 2.6235 - lr: 8.1791e-05\n",
      "Epoch 27/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6417\n",
      "Epoch 27: val_loss improved from 2.62349 to 2.62251, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 4s 65ms/step - loss: 0.6419 - val_loss: 2.6225 - lr: 8.1791e-05\n",
      "Epoch 28/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6432\n",
      "Epoch 28: val_loss did not improve from 2.62251\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 4s 65ms/step - loss: 0.6420 - val_loss: 2.8185 - lr: 8.1791e-05\n",
      "Epoch 29/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6274\n",
      "Epoch 29: val_loss improved from 2.62251 to 2.52322, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 4s 66ms/step - loss: 0.6262 - val_loss: 2.5232 - lr: 8.0973e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6163\n",
      "Epoch 30: val_loss improved from 2.52322 to 2.36773, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 4s 65ms/step - loss: 0.6155 - val_loss: 2.3677 - lr: 8.0973e-05\n",
      "Epoch 31/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6240\n",
      "Epoch 31: val_loss did not improve from 2.36773\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 4s 64ms/step - loss: 0.6241 - val_loss: 2.4402 - lr: 8.0973e-05\n",
      "Epoch 32/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6137\n",
      "Epoch 32: val_loss did not improve from 2.36773\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 4s 65ms/step - loss: 0.6131 - val_loss: 2.6943 - lr: 8.0163e-05\n",
      "Epoch 33/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6222\n",
      "Epoch 33: val_loss did not improve from 2.36773\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 4s 64ms/step - loss: 0.6201 - val_loss: 2.4256 - lr: 7.9361e-05\n",
      "Epoch 34/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6168\n",
      "Epoch 34: val_loss did not improve from 2.36773\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 4s 64ms/step - loss: 0.6164 - val_loss: 2.3792 - lr: 7.8568e-05\n",
      "Epoch 35/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6010\n",
      "Epoch 35: val_loss improved from 2.36773 to 2.31226, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 4s 65ms/step - loss: 0.5993 - val_loss: 2.3123 - lr: 7.7782e-05\n",
      "Epoch 36/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6075\n",
      "Epoch 36: val_loss did not improve from 2.31226\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 4s 65ms/step - loss: 0.6054 - val_loss: 2.3909 - lr: 7.7782e-05\n",
      "Epoch 37/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6054\n",
      "Epoch 37: val_loss did not improve from 2.31226\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 4s 65ms/step - loss: 0.6056 - val_loss: 2.4732 - lr: 7.7004e-05\n",
      "Epoch 38/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6016\n",
      "Epoch 38: val_loss improved from 2.31226 to 2.28743, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 4s 66ms/step - loss: 0.6026 - val_loss: 2.2874 - lr: 7.6234e-05\n",
      "Epoch 39/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6066\n",
      "Epoch 39: val_loss improved from 2.28743 to 2.12861, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 0.6066 - val_loss: 2.1286 - lr: 7.6234e-05\n",
      "Epoch 40/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6016\n",
      "Epoch 40: val_loss did not improve from 2.12861\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.6016 - val_loss: 2.2053 - lr: 7.6234e-05\n",
      "Epoch 41/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5883\n",
      "Epoch 41: val_loss did not improve from 2.12861\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.5862 - val_loss: 2.2174 - lr: 7.5472e-05\n",
      "Epoch 42/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5970\n",
      "Epoch 42: val_loss did not improve from 2.12861\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.5972 - val_loss: 2.1920 - lr: 7.4717e-05\n",
      "Epoch 43/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5900\n",
      "Epoch 43: val_loss improved from 2.12861 to 2.10886, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 0.5909 - val_loss: 2.1089 - lr: 7.3970e-05\n",
      "Epoch 44/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5863\n",
      "Epoch 44: val_loss did not improve from 2.10886\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.5849 - val_loss: 2.1408 - lr: 7.3970e-05\n",
      "Epoch 45/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5846\n",
      "Epoch 45: val_loss improved from 2.10886 to 2.09542, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 0.5834 - val_loss: 2.0954 - lr: 7.3230e-05\n",
      "Epoch 46/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5796\n",
      "Epoch 46: val_loss did not improve from 2.09542\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 5s 75ms/step - loss: 0.5765 - val_loss: 2.1492 - lr: 7.3230e-05\n",
      "Epoch 47/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5888\n",
      "Epoch 47: val_loss did not improve from 2.09542\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 5s 75ms/step - loss: 0.5885 - val_loss: 2.2146 - lr: 7.2498e-05\n",
      "Epoch 48/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5895\n",
      "Epoch 48: val_loss improved from 2.09542 to 2.07926, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 5s 75ms/step - loss: 0.5899 - val_loss: 2.0793 - lr: 7.1773e-05\n",
      "Epoch 49/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5788\n",
      "Epoch 49: val_loss improved from 2.07926 to 2.03619, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 0.5774 - val_loss: 2.0362 - lr: 7.1773e-05\n",
      "Epoch 50/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5785\n",
      "Epoch 50: val_loss improved from 2.03619 to 1.95181, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 5s 75ms/step - loss: 0.5811 - val_loss: 1.9518 - lr: 7.1773e-05\n",
      "Epoch 51/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5700\n",
      "Epoch 51: val_loss did not improve from 1.95181\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.5690 - val_loss: 1.9937 - lr: 7.1773e-05\n",
      "Epoch 52/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5649\n",
      "Epoch 52: val_loss did not improve from 1.95181\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.5656 - val_loss: 1.9820 - lr: 7.1055e-05\n",
      "Epoch 53/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5774\n",
      "Epoch 53: val_loss did not improve from 1.95181\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 5s 75ms/step - loss: 0.5774 - val_loss: 2.0272 - lr: 7.0345e-05\n",
      "Epoch 54/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5718\n",
      "Epoch 54: val_loss did not improve from 1.95181\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 0.5719 - val_loss: 1.9537 - lr: 6.9641e-05\n",
      "Epoch 55/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5684\n",
      "Epoch 55: val_loss improved from 1.95181 to 1.84596, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.5702 - val_loss: 1.8460 - lr: 6.8945e-05\n",
      "Epoch 56/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5660\n",
      "Epoch 56: val_loss did not improve from 1.84596\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5657 - val_loss: 1.9734 - lr: 6.8945e-05\n",
      "Epoch 57/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5679\n",
      "Epoch 57: val_loss did not improve from 1.84596\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 5s 75ms/step - loss: 0.5679 - val_loss: 1.9794 - lr: 6.8255e-05\n",
      "Epoch 58/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5633\n",
      "Epoch 58: val_loss did not improve from 1.84596\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 5s 75ms/step - loss: 0.5633 - val_loss: 2.0546 - lr: 6.7573e-05\n",
      "Epoch 59/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5692\n",
      "Epoch 59: val_loss did not improve from 1.84596\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 0.5692 - val_loss: 1.9515 - lr: 6.6897e-05\n",
      "Epoch 60/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5557\n",
      "Epoch 60: val_loss did not improve from 1.84596\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 5s 75ms/step - loss: 0.5557 - val_loss: 1.9821 - lr: 6.6228e-05\n",
      "Epoch 61/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5655\n",
      "Epoch 61: val_loss did not improve from 1.84596\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 0.5650 - val_loss: 1.9630 - lr: 6.5566e-05\n",
      "Epoch 62/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5669\n",
      "Epoch 62: val_loss did not improve from 1.84596\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 0.5689 - val_loss: 1.8775 - lr: 6.4910e-05\n",
      "Epoch 63/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5541\n",
      "Epoch 63: val_loss did not improve from 1.84596\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 0.5573 - val_loss: 1.9882 - lr: 6.4261e-05\n",
      "Epoch 64/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5573\n",
      "Epoch 64: val_loss did not improve from 1.84596\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.5549 - val_loss: 1.8930 - lr: 6.3619e-05\n",
      "Epoch 65/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5563\n",
      "Epoch 65: val_loss improved from 1.84596 to 1.84277, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 0.5553 - val_loss: 1.8428 - lr: 6.2982e-05\n",
      "Epoch 66/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5491\n",
      "Epoch 66: val_loss improved from 1.84277 to 1.76057, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 0.5490 - val_loss: 1.7606 - lr: 6.2982e-05\n",
      "Epoch 67/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5528\n",
      "Epoch 67: val_loss did not improve from 1.76057\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.5548 - val_loss: 1.8153 - lr: 6.2982e-05\n",
      "Epoch 68/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5527\n",
      "Epoch 68: val_loss did not improve from 1.76057\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.5541 - val_loss: 1.8930 - lr: 6.2353e-05\n",
      "Epoch 69/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5496\n",
      "Epoch 69: val_loss did not improve from 1.76057\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 0.5485 - val_loss: 1.9828 - lr: 6.1729e-05\n",
      "Epoch 70/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5560\n",
      "Epoch 70: val_loss did not improve from 1.76057\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.5558 - val_loss: 1.9022 - lr: 6.1112e-05\n",
      "Epoch 71/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5495\n",
      "Epoch 71: val_loss did not improve from 1.76057\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.5488 - val_loss: 1.8570 - lr: 6.0501e-05\n",
      "Epoch 72/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5443\n",
      "Epoch 72: val_loss did not improve from 1.76057\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.5464 - val_loss: 1.8257 - lr: 5.9896e-05\n",
      "Epoch 73/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5411\n",
      "Epoch 73: val_loss did not improve from 1.76057\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 5.8703650938696225e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.5388 - val_loss: 1.8019 - lr: 5.9297e-05\n",
      "Epoch 74/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5484\n",
      "Epoch 74: val_loss did not improve from 1.76057\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 5.811661550978897e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.5477 - val_loss: 1.8973 - lr: 5.8704e-05\n",
      "Epoch 75/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5438\n",
      "Epoch 75: val_loss did not improve from 1.76057\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 5.753545068728272e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.5442 - val_loss: 1.9110 - lr: 5.8117e-05\n",
      "Epoch 76/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5442\n",
      "Epoch 76: val_loss did not improve from 1.76057\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 5.6960095243994146e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.5461 - val_loss: 1.9879 - lr: 5.7535e-05\n",
      "Epoch 77/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5547\n",
      "Epoch 77: val_loss improved from 1.76057 to 1.76032, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.5531 - val_loss: 1.7603 - lr: 5.6960e-05\n",
      "Epoch 78/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5422\n",
      "Epoch 78: val_loss did not improve from 1.76032\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 5.639049515593797e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.5431 - val_loss: 1.8706 - lr: 5.6960e-05\n",
      "Epoch 79/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5443\n",
      "Epoch 79: val_loss did not improve from 1.76032\n",
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 5.5826589195930865e-05.\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.5428 - val_loss: 1.8286 - lr: 5.6390e-05\n",
      "Epoch 80/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5361\n",
      "Epoch 80: val_loss did not improve from 1.76032\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 5.526832333998754e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.5355 - val_loss: 1.7946 - lr: 5.5827e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5327\n",
      "Epoch 81: val_loss did not improve from 1.76032\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 5.471563996252371e-05.\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.5345 - val_loss: 1.8911 - lr: 5.5268e-05\n",
      "Epoch 82/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5339\n",
      "Epoch 82: val_loss improved from 1.76032 to 1.72135, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.5318 - val_loss: 1.7214 - lr: 5.4716e-05\n",
      "Epoch 83/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5372\n",
      "Epoch 83: val_loss did not improve from 1.72135\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 5.416848503955407e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.5347 - val_loss: 1.8020 - lr: 5.4716e-05\n",
      "Epoch 84/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5347\n",
      "Epoch 84: val_loss did not improve from 1.72135\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 5.362680094549433e-05.\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.5343 - val_loss: 1.8213 - lr: 5.4168e-05\n",
      "Epoch 85/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5368\n",
      "Epoch 85: val_loss did not improve from 1.72135\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 5.309053365635918e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.5357 - val_loss: 1.7849 - lr: 5.3627e-05\n",
      "Epoch 86/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5323\n",
      "Epoch 86: val_loss did not improve from 1.72135\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 5.2559629148163366e-05.\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.5368 - val_loss: 1.7748 - lr: 5.3091e-05\n",
      "Epoch 87/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5328\n",
      "Epoch 87: val_loss did not improve from 1.72135\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 5.2034033396921584e-05.\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.5330 - val_loss: 1.7655 - lr: 5.2560e-05\n",
      "Epoch 88/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5273\n",
      "Epoch 88: val_loss did not improve from 1.72135\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 5.1513692378648555e-05.\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.5340 - val_loss: 1.8910 - lr: 5.2034e-05\n",
      "Epoch 89/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5446\n",
      "Epoch 89: val_loss did not improve from 1.72135\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 5.099855567095801e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.5452 - val_loss: 1.9245 - lr: 5.1514e-05\n",
      "Epoch 90/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5336\n",
      "Epoch 90: val_loss did not improve from 1.72135\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 5.048856924986467e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.5323 - val_loss: 1.8059 - lr: 5.0999e-05\n",
      "Epoch 91/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5233\n",
      "Epoch 91: val_loss did not improve from 1.72135\n",
      "\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 4.998368269298226e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.5244 - val_loss: 1.8201 - lr: 5.0489e-05\n",
      "Epoch 92/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5310\n",
      "Epoch 92: val_loss improved from 1.72135 to 1.68566, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 0.5310 - val_loss: 1.6857 - lr: 4.9984e-05\n",
      "Epoch 93/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5212\n",
      "Epoch 93: val_loss did not improve from 1.68566\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 4.9483845577924514e-05.\n",
      "66/66 [==============================] - 5s 69ms/step - loss: 0.5247 - val_loss: 1.8204 - lr: 4.9984e-05\n",
      "Epoch 94/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5306\n",
      "Epoch 94: val_loss did not improve from 1.68566\n",
      "\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 4.898900748230517e-05.\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.5305 - val_loss: 1.8145 - lr: 4.9484e-05\n",
      "Epoch 95/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5299\n",
      "Epoch 95: val_loss did not improve from 1.68566\n",
      "\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 4.849911798373796e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.5299 - val_loss: 1.7697 - lr: 4.8989e-05\n",
      "Epoch 96/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5282\n",
      "Epoch 96: val_loss did not improve from 1.68566\n",
      "\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 4.801412665983662e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.5278 - val_loss: 1.7877 - lr: 4.8499e-05\n",
      "Epoch 97/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5309\n",
      "Epoch 97: val_loss did not improve from 1.68566\n",
      "\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 4.75339866898139e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.5303 - val_loss: 1.8237 - lr: 4.8014e-05\n",
      "Epoch 98/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5268\n",
      "Epoch 98: val_loss did not improve from 1.68566\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 4.705864765128354e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.5266 - val_loss: 1.7580 - lr: 4.7534e-05\n",
      "Epoch 99/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5231\n",
      "Epoch 99: val_loss did not improve from 1.68566\n",
      "\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 4.658806272345828e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.5231 - val_loss: 1.7650 - lr: 4.7059e-05\n",
      "Epoch 100/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5219\n",
      "Epoch 100: val_loss did not improve from 1.68566\n",
      "\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 4.612218148395186e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.5201 - val_loss: 1.8290 - lr: 4.6588e-05\n",
      "Epoch 101/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5248\n",
      "Epoch 101: val_loss did not improve from 1.68566\n",
      "\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 4.566096071357606e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.5239 - val_loss: 1.8126 - lr: 4.6122e-05\n",
      "Epoch 102/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5215\n",
      "Epoch 102: val_loss did not improve from 1.68566\n",
      "\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 4.52043499899446e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.5218 - val_loss: 1.8751 - lr: 4.5661e-05\n",
      "Epoch 103/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5144\n",
      "Epoch 103: val_loss did not improve from 1.68566\n",
      "\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 4.475230609386927e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.5144 - val_loss: 1.8349 - lr: 4.5204e-05\n",
      "Epoch 104/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5170\n",
      "Epoch 104: val_loss did not improve from 1.68566\n",
      "\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 4.43047822045628e-05.\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.5168 - val_loss: 1.8246 - lr: 4.4752e-05\n",
      "Epoch 105/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5187\n",
      "Epoch 105: val_loss did not improve from 1.68566\n",
      "\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 4.386173510283697e-05.\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.5189 - val_loss: 1.7462 - lr: 4.4305e-05\n",
      "Epoch 106/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5170\n",
      "Epoch 106: val_loss did not improve from 1.68566\n",
      "\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 4.342311796790454e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.5180 - val_loss: 1.8578 - lr: 4.3862e-05\n",
      "Epoch 107/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5244\n",
      "Epoch 107: val_loss did not improve from 1.68566\n",
      "\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 4.298888758057728e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.5234 - val_loss: 1.8275 - lr: 4.3423e-05\n",
      "Epoch 108/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5151\n",
      "Epoch 108: val_loss did not improve from 1.68566\n",
      "\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 4.2558997120067946e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.5138 - val_loss: 1.9384 - lr: 4.2989e-05\n",
      "Epoch 109/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5232\n",
      "Epoch 109: val_loss did not improve from 1.68566\n",
      "\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 4.2133406968787315e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.5214 - val_loss: 1.8426 - lr: 4.2559e-05\n",
      "Epoch 110/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5207\n",
      "Epoch 110: val_loss did not improve from 1.68566\n",
      "\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 4.1712073907547166e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.5208 - val_loss: 1.7660 - lr: 4.2133e-05\n",
      "Epoch 111/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5167\n",
      "Epoch 111: val_loss did not improve from 1.68566\n",
      "\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 4.1294954717159274e-05.\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.5152 - val_loss: 1.8272 - lr: 4.1712e-05\n",
      "Epoch 112/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5182\n",
      "Epoch 112: val_loss did not improve from 1.68566\n",
      "\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 4.08820061784354e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.5197 - val_loss: 1.7503 - lr: 4.1295e-05\n",
      "Epoch 113/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5239\n",
      "Epoch 113: val_loss improved from 1.68566 to 1.67873, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.5240 - val_loss: 1.6787 - lr: 4.0882e-05\n",
      "Epoch 114/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5209\n",
      "Epoch 114: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 4.0473185072187336e-05.\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.5198 - val_loss: 1.8501 - lr: 4.0882e-05\n",
      "Epoch 115/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5114\n",
      "Epoch 115: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 4.0068451780825854e-05.\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.5122 - val_loss: 1.6825 - lr: 4.0473e-05\n",
      "Epoch 116/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5118\n",
      "Epoch 116: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 3.966776668676175e-05.\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.5119 - val_loss: 1.7978 - lr: 4.0068e-05\n",
      "Epoch 117/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5172\n",
      "Epoch 117: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 3.927109017240582e-05.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 0.5180 - val_loss: 1.7593 - lr: 3.9668e-05\n",
      "Epoch 118/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5176\n",
      "Epoch 118: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 3.887837901856983e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.5176 - val_loss: 1.7631 - lr: 3.9271e-05\n",
      "Epoch 119/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5106\n",
      "Epoch 119: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 3.848959360766457e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.5107 - val_loss: 1.7519 - lr: 3.8878e-05\n",
      "Epoch 120/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5099\n",
      "Epoch 120: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 3.810469792369986e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.5107 - val_loss: 1.8883 - lr: 3.8490e-05\n",
      "Epoch 121/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5105\n",
      "Epoch 121: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 3.772365234908648e-05.\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.5091 - val_loss: 1.9118 - lr: 3.8105e-05\n",
      "Epoch 122/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5021\n",
      "Epoch 122: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 3.734641726623522e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.5037 - val_loss: 1.8676 - lr: 3.7724e-05\n",
      "Epoch 123/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4983\n",
      "Epoch 123: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 123: ReduceLROnPlateau reducing learning rate to 3.6972953057556876e-05.\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.5030 - val_loss: 1.8219 - lr: 3.7346e-05\n",
      "Epoch 124/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5126\n",
      "Epoch 124: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 124: ReduceLROnPlateau reducing learning rate to 3.660322370706126e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.5120 - val_loss: 1.8016 - lr: 3.6973e-05\n",
      "Epoch 125/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5073\n",
      "Epoch 125: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 3.623719319875818e-05.\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.5089 - val_loss: 1.8316 - lr: 3.6603e-05\n",
      "Epoch 126/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5035\n",
      "Epoch 126: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 3.5874821915058416e-05.\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.5033 - val_loss: 1.8829 - lr: 3.6237e-05\n",
      "Epoch 127/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5097\n",
      "Epoch 127: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 127: ReduceLROnPlateau reducing learning rate to 3.551607383997179e-05.\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.5113 - val_loss: 1.9257 - lr: 3.5875e-05\n",
      "Epoch 128/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5089\n",
      "Epoch 128: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 128: ReduceLROnPlateau reducing learning rate to 3.516091295750812e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.5081 - val_loss: 1.7043 - lr: 3.5516e-05\n",
      "Epoch 129/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5106\n",
      "Epoch 129: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 129: ReduceLROnPlateau reducing learning rate to 3.480930325167719e-05.\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.5118 - val_loss: 1.8307 - lr: 3.5161e-05\n",
      "Epoch 130/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5091\n",
      "Epoch 130: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 130: ReduceLROnPlateau reducing learning rate to 3.446120870648883e-05.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 0.5093 - val_loss: 1.8364 - lr: 3.4809e-05\n",
      "Epoch 131/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5072\n",
      "Epoch 131: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 131: ReduceLROnPlateau reducing learning rate to 3.4116596907551866e-05.\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.5060 - val_loss: 1.8973 - lr: 3.4461e-05\n",
      "Epoch 132/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5168\n",
      "Epoch 132: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 132: ReduceLROnPlateau reducing learning rate to 3.37754318388761e-05.\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.5183 - val_loss: 1.8430 - lr: 3.4117e-05\n",
      "Epoch 133/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/66 [============================>.] - ETA: 0s - loss: 0.5090\n",
      "Epoch 133: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 133: ReduceLROnPlateau reducing learning rate to 3.3437677484471354e-05.\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.5096 - val_loss: 1.8781 - lr: 3.3775e-05\n",
      "Epoch 134/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5127\n",
      "Epoch 134: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 134: ReduceLROnPlateau reducing learning rate to 3.310330142994644e-05.\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.5107 - val_loss: 1.8840 - lr: 3.3438e-05\n",
      "Epoch 135/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5075\n",
      "Epoch 135: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 135: ReduceLROnPlateau reducing learning rate to 3.277226765931118e-05.\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.5081 - val_loss: 1.8964 - lr: 3.3103e-05\n",
      "Epoch 136/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5054\n",
      "Epoch 136: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 3.24445437581744e-05.\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.5042 - val_loss: 1.9463 - lr: 3.2772e-05\n",
      "Epoch 137/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4980\n",
      "Epoch 137: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 137: ReduceLROnPlateau reducing learning rate to 3.212009731214493e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.4987 - val_loss: 1.9282 - lr: 3.2445e-05\n",
      "Epoch 138/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5065\n",
      "Epoch 138: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 138: ReduceLROnPlateau reducing learning rate to 3.17988959068316e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.5065 - val_loss: 1.9559 - lr: 3.2120e-05\n",
      "Epoch 139/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4976\n",
      "Epoch 139: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 139: ReduceLROnPlateau reducing learning rate to 3.1480907127843236e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.4971 - val_loss: 1.9781 - lr: 3.1799e-05\n",
      "Epoch 140/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5127\n",
      "Epoch 140: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 140: ReduceLROnPlateau reducing learning rate to 3.116609856078867e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.5119 - val_loss: 1.8836 - lr: 3.1481e-05\n",
      "Epoch 141/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5052\n",
      "Epoch 141: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 141: ReduceLROnPlateau reducing learning rate to 3.085443779127672e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.5043 - val_loss: 1.8435 - lr: 3.1166e-05\n",
      "Epoch 142/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5084\n",
      "Epoch 142: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 142: ReduceLROnPlateau reducing learning rate to 3.054589240491623e-05.\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.5087 - val_loss: 2.0513 - lr: 3.0854e-05\n",
      "Epoch 143/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5044\n",
      "Epoch 143: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 143: ReduceLROnPlateau reducing learning rate to 3.024043358891504e-05.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 0.5052 - val_loss: 1.9250 - lr: 3.0546e-05\n",
      "Epoch 144/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5058\n",
      "Epoch 144: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 144: ReduceLROnPlateau reducing learning rate to 2.9938028928881975e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.5056 - val_loss: 1.8942 - lr: 3.0240e-05\n",
      "Epoch 145/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5043\n",
      "Epoch 145: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 145: ReduceLROnPlateau reducing learning rate to 2.963864781122538e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.5028 - val_loss: 2.0045 - lr: 2.9938e-05\n",
      "Epoch 146/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5056\n",
      "Epoch 146: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 146: ReduceLROnPlateau reducing learning rate to 2.9342261423153105e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.5060 - val_loss: 1.9875 - lr: 2.9639e-05\n",
      "Epoch 147/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4994\n",
      "Epoch 147: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 147: ReduceLROnPlateau reducing learning rate to 2.9048839151073478e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.4975 - val_loss: 1.9620 - lr: 2.9342e-05\n",
      "Epoch 148/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5020\n",
      "Epoch 148: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 148: ReduceLROnPlateau reducing learning rate to 2.875835038139485e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.5036 - val_loss: 1.9002 - lr: 2.9049e-05\n",
      "Epoch 149/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5008\n",
      "Epoch 149: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 149: ReduceLROnPlateau reducing learning rate to 2.8470766301325058e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.4997 - val_loss: 1.9458 - lr: 2.8758e-05\n",
      "Epoch 150/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4950\n",
      "Epoch 150: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 150: ReduceLROnPlateau reducing learning rate to 2.8186058098071954e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.4950 - val_loss: 1.9955 - lr: 2.8471e-05\n",
      "Epoch 151/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4956\n",
      "Epoch 151: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 151: ReduceLROnPlateau reducing learning rate to 2.7904196958843385e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.4958 - val_loss: 1.9097 - lr: 2.8186e-05\n",
      "Epoch 152/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5042\n",
      "Epoch 152: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 152: ReduceLROnPlateau reducing learning rate to 2.7625155871646713e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.5056 - val_loss: 2.0669 - lr: 2.7904e-05\n",
      "Epoch 153/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4995\n",
      "Epoch 153: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 153: ReduceLROnPlateau reducing learning rate to 2.734890422289027e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.4993 - val_loss: 1.9619 - lr: 2.7625e-05\n",
      "Epoch 154/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5079\n",
      "Epoch 154: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 154: ReduceLROnPlateau reducing learning rate to 2.7075415000581416e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.5071 - val_loss: 1.9299 - lr: 2.7349e-05\n",
      "Epoch 155/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5035\n",
      "Epoch 155: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 155: ReduceLROnPlateau reducing learning rate to 2.6804661192727508e-05.\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.5060 - val_loss: 1.9242 - lr: 2.7075e-05\n",
      "Epoch 156/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4964\n",
      "Epoch 156: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 156: ReduceLROnPlateau reducing learning rate to 2.6536613986536395e-05.\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.4990 - val_loss: 1.9191 - lr: 2.6805e-05\n",
      "Epoch 157/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4985\n",
      "Epoch 157: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 157: ReduceLROnPlateau reducing learning rate to 2.6271248170814942e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.4983 - val_loss: 1.9327 - lr: 2.6537e-05\n",
      "Epoch 158/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4959\n",
      "Epoch 158: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 158: ReduceLROnPlateau reducing learning rate to 2.6008534932770998e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.4938 - val_loss: 1.9840 - lr: 2.6271e-05\n",
      "Epoch 159/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5006\n",
      "Epoch 159: val_loss did not improve from 1.67873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 159: ReduceLROnPlateau reducing learning rate to 2.5748449061211432e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.5028 - val_loss: 1.9745 - lr: 2.6009e-05\n",
      "Epoch 160/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4939\n",
      "Epoch 160: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 160: ReduceLROnPlateau reducing learning rate to 2.5490965344943107e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.4922 - val_loss: 2.0198 - lr: 2.5748e-05\n",
      "Epoch 161/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4965\n",
      "Epoch 161: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 161: ReduceLROnPlateau reducing learning rate to 2.523605497117387e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.4950 - val_loss: 1.8990 - lr: 2.5491e-05\n",
      "Epoch 162/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4964\n",
      "Epoch 162: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 162: ReduceLROnPlateau reducing learning rate to 2.4983694529510104e-05.\n",
      "66/66 [==============================] - 5s 70ms/step - loss: 0.4962 - val_loss: 1.9641 - lr: 2.5236e-05\n",
      "Epoch 163/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4917\n",
      "Epoch 163: val_loss did not improve from 1.67873\n",
      "\n",
      "Epoch 163: ReduceLROnPlateau reducing learning rate to 2.4733857007959158e-05.\n",
      "66/66 [==============================] - 5s 71ms/step - loss: 0.4909 - val_loss: 2.0137 - lr: 2.4984e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_19\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_20 (InputLayer)          [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_57 (LSTM)                 (None, 45, 24)       3744        ['input_20[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_38 (Dropout)           (None, 45, 24)       0           ['lstm_57[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_58 (LSTM)                 (None, 45, 16)       2624        ['dropout_38[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_39 (Dropout)           (None, 45, 16)       0           ['lstm_58[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_59 (LSTM)                 (None, 32)           6272        ['dropout_39[0][0]']             \n",
      "                                                                                                  \n",
      " dense_38 (Dense)               (None, 40)           1320        ['lstm_59[0][0]']                \n",
      "                                                                                                  \n",
      " dense_39 (Dense)               (None, 5)            205         ['dense_38[0][0]']               \n",
      "                                                                                                  \n",
      " tf.unstack_19 (TFOpLambda)     [(None,),            0           ['dense_39[0][0]']               \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_95 (TFOpLambda)  (None, 1)           0           ['tf.unstack_19[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_38 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_95[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_99 (TFOpLambda)  (None, 1)           0           ['tf.unstack_19[0][4]']          \n",
      "                                                                                                  \n",
      " tf.math.multiply_57 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_38[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_39 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_99[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_58 (TFOpLambd  (None, 1)           0           ['tf.math.multiply_57[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_96 (TFOpLambda)  (None, 1)           0           ['tf.unstack_19[0][1]']          \n",
      "                                                                                                  \n",
      " tf.expand_dims_98 (TFOpLambda)  (None, 1)           0           ['tf.unstack_19[0][3]']          \n",
      "                                                                                                  \n",
      " tf.math.multiply_59 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_39[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_38 (TFOpL  (None, 1)           0           ['tf.math.multiply_58[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.softplus_38 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_96[0][0]']      \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_97 (TFOpLambda)  (None, 1)           0           ['tf.unstack_19[0][2]']          \n",
      "                                                                                                  \n",
      " tf.math.softplus_39 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_98[0][0]']      \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_39 (TFOpL  (None, 1)           0           ['tf.math.multiply_59[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack_19 (TFOpLambda)       (None, 5, 1)         0           ['tf.__operators__.add_38[0][0]',\n",
      "                                                                  'tf.math.softplus_38[0][0]',    \n",
      "                                                                  'tf.expand_dims_97[0][0]',      \n",
      "                                                                  'tf.math.softplus_39[0][0]',    \n",
      "                                                                  'tf.__operators__.add_39[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _2_CCMP_t+1_0.09\n",
      "Epoch 1/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 3.4212\n",
      "Epoch 1: val_loss improved from inf to 4.26262, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 10s 88ms/step - loss: 3.4202 - val_loss: 4.2626 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 2.7726\n",
      "Epoch 2: val_loss improved from 4.26262 to 3.81313, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 2.7726 - val_loss: 3.8131 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.8923\n",
      "Epoch 3: val_loss did not improve from 3.81313\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 1.8923 - val_loss: 4.2634 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.4119\n",
      "Epoch 4: val_loss did not improve from 3.81313\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 5s 81ms/step - loss: 1.4078 - val_loss: 3.9528 - lr: 9.9000e-05\n",
      "Epoch 5/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1412\n",
      "Epoch 5: val_loss improved from 3.81313 to 3.69446, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 1.1384 - val_loss: 3.6945 - lr: 9.8010e-05\n",
      "Epoch 6/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0237\n",
      "Epoch 6: val_loss did not improve from 3.69446\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 1.0232 - val_loss: 3.8288 - lr: 9.8010e-05\n",
      "Epoch 7/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9522\n",
      "Epoch 7: val_loss improved from 3.69446 to 3.48007, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.9534 - val_loss: 3.4801 - lr: 9.7030e-05\n",
      "Epoch 8/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9016\n",
      "Epoch 8: val_loss did not improve from 3.48007\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.8999 - val_loss: 3.9403 - lr: 9.7030e-05\n",
      "Epoch 9/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8695\n",
      "Epoch 9: val_loss did not improve from 3.48007\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.8685 - val_loss: 3.5658 - lr: 9.6060e-05\n",
      "Epoch 10/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8365\n",
      "Epoch 10: val_loss improved from 3.48007 to 3.25324, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.8368 - val_loss: 3.2532 - lr: 9.5099e-05\n",
      "Epoch 11/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8194\n",
      "Epoch 11: val_loss did not improve from 3.25324\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.8162 - val_loss: 3.5371 - lr: 9.5099e-05\n",
      "Epoch 12/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7886\n",
      "Epoch 12: val_loss did not improve from 3.25324\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.7880 - val_loss: 3.8705 - lr: 9.4148e-05\n",
      "Epoch 13/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7743\n",
      "Epoch 13: val_loss did not improve from 3.25324\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.7730 - val_loss: 3.5030 - lr: 9.3207e-05\n",
      "Epoch 14/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7664\n",
      "Epoch 14: val_loss improved from 3.25324 to 3.02866, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.7664 - val_loss: 3.0287 - lr: 9.2274e-05\n",
      "Epoch 15/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7525\n",
      "Epoch 15: val_loss did not improve from 3.02866\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.7530 - val_loss: 3.5857 - lr: 9.2274e-05\n",
      "Epoch 16/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7435\n",
      "Epoch 16: val_loss did not improve from 3.02866\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.7428 - val_loss: 3.2368 - lr: 9.1352e-05\n",
      "Epoch 17/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7330\n",
      "Epoch 17: val_loss did not improve from 3.02866\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.7345 - val_loss: 3.5623 - lr: 9.0438e-05\n",
      "Epoch 18/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7292\n",
      "Epoch 18: val_loss did not improve from 3.02866\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.7270 - val_loss: 3.2083 - lr: 8.9534e-05\n",
      "Epoch 19/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7203\n",
      "Epoch 19: val_loss did not improve from 3.02866\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.7209 - val_loss: 3.1293 - lr: 8.8638e-05\n",
      "Epoch 20/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7186\n",
      "Epoch 20: val_loss improved from 3.02866 to 2.85318, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.7160 - val_loss: 2.8532 - lr: 8.7752e-05\n",
      "Epoch 21/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7096\n",
      "Epoch 21: val_loss did not improve from 2.85318\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.7062 - val_loss: 2.9654 - lr: 8.7752e-05\n",
      "Epoch 22/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7097\n",
      "Epoch 22: val_loss did not improve from 2.85318\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.7085 - val_loss: 2.9138 - lr: 8.6875e-05\n",
      "Epoch 23/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6890\n",
      "Epoch 23: val_loss improved from 2.85318 to 2.63223, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6898 - val_loss: 2.6322 - lr: 8.6006e-05\n",
      "Epoch 24/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6979\n",
      "Epoch 24: val_loss did not improve from 2.63223\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.7017 - val_loss: 3.0221 - lr: 8.6006e-05\n",
      "Epoch 25/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6952\n",
      "Epoch 25: val_loss did not improve from 2.63223\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6941 - val_loss: 3.0976 - lr: 8.5146e-05\n",
      "Epoch 26/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6865\n",
      "Epoch 26: val_loss did not improve from 2.63223\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6860 - val_loss: 2.8355 - lr: 8.4294e-05\n",
      "Epoch 27/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6704\n",
      "Epoch 27: val_loss did not improve from 2.63223\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 5s 81ms/step - loss: 0.6681 - val_loss: 2.9153 - lr: 8.3451e-05\n",
      "Epoch 28/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6834\n",
      "Epoch 28: val_loss did not improve from 2.63223\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6830 - val_loss: 2.7635 - lr: 8.2617e-05\n",
      "Epoch 29/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6638\n",
      "Epoch 29: val_loss improved from 2.63223 to 2.46256, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.09.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6703 - val_loss: 2.4626 - lr: 8.1791e-05\n",
      "Epoch 30/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6959\n",
      "Epoch 30: val_loss improved from 2.46256 to 2.46182, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6953 - val_loss: 2.4618 - lr: 8.1791e-05\n",
      "Epoch 31/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6668\n",
      "Epoch 31: val_loss did not improve from 2.46182\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6651 - val_loss: 2.5432 - lr: 8.1791e-05\n",
      "Epoch 32/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6697\n",
      "Epoch 32: val_loss did not improve from 2.46182\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6679 - val_loss: 2.5416 - lr: 8.0973e-05\n",
      "Epoch 33/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6633\n",
      "Epoch 33: val_loss did not improve from 2.46182\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6624 - val_loss: 2.4691 - lr: 8.0163e-05\n",
      "Epoch 34/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6625\n",
      "Epoch 34: val_loss did not improve from 2.46182\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6604 - val_loss: 2.5860 - lr: 7.9361e-05\n",
      "Epoch 35/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6655\n",
      "Epoch 35: val_loss did not improve from 2.46182\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6667 - val_loss: 2.5751 - lr: 7.8568e-05\n",
      "Epoch 36/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6488\n",
      "Epoch 36: val_loss improved from 2.46182 to 2.29591, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.6497 - val_loss: 2.2959 - lr: 7.7782e-05\n",
      "Epoch 37/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6508\n",
      "Epoch 37: val_loss did not improve from 2.29591\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6508 - val_loss: 2.3366 - lr: 7.7782e-05\n",
      "Epoch 38/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6461\n",
      "Epoch 38: val_loss did not improve from 2.29591\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6453 - val_loss: 2.3833 - lr: 7.7004e-05\n",
      "Epoch 39/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6404\n",
      "Epoch 39: val_loss did not improve from 2.29591\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6396 - val_loss: 2.3277 - lr: 7.6234e-05\n",
      "Epoch 40/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6527\n",
      "Epoch 40: val_loss did not improve from 2.29591\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6542 - val_loss: 2.4262 - lr: 7.5472e-05\n",
      "Epoch 41/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6522\n",
      "Epoch 41: val_loss improved from 2.29591 to 2.20973, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6509 - val_loss: 2.2097 - lr: 7.4717e-05\n",
      "Epoch 42/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6346\n",
      "Epoch 42: val_loss did not improve from 2.20973\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6349 - val_loss: 2.3357 - lr: 7.4717e-05\n",
      "Epoch 43/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6460\n",
      "Epoch 43: val_loss did not improve from 2.20973\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6459 - val_loss: 2.2245 - lr: 7.3970e-05\n",
      "Epoch 44/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6360\n",
      "Epoch 44: val_loss did not improve from 2.20973\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6344 - val_loss: 2.3428 - lr: 7.3230e-05\n",
      "Epoch 45/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6318\n",
      "Epoch 45: val_loss did not improve from 2.20973\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6301 - val_loss: 2.2667 - lr: 7.2498e-05\n",
      "Epoch 46/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6332\n",
      "Epoch 46: val_loss did not improve from 2.20973\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6322 - val_loss: 2.2151 - lr: 7.1773e-05\n",
      "Epoch 47/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6327\n",
      "Epoch 47: val_loss improved from 2.20973 to 2.14542, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.6330 - val_loss: 2.1454 - lr: 7.1055e-05\n",
      "Epoch 48/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6299\n",
      "Epoch 48: val_loss did not improve from 2.14542\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6294 - val_loss: 2.1472 - lr: 7.1055e-05\n",
      "Epoch 49/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6285\n",
      "Epoch 49: val_loss improved from 2.14542 to 2.13293, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6266 - val_loss: 2.1329 - lr: 7.0345e-05\n",
      "Epoch 50/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6236\n",
      "Epoch 50: val_loss did not improve from 2.13293\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.6236 - val_loss: 2.1458 - lr: 7.0345e-05\n",
      "Epoch 51/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6153\n",
      "Epoch 51: val_loss did not improve from 2.13293\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6164 - val_loss: 2.2037 - lr: 6.9641e-05\n",
      "Epoch 52/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6295\n",
      "Epoch 52: val_loss did not improve from 2.13293\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6283 - val_loss: 2.1461 - lr: 6.8945e-05\n",
      "Epoch 53/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6166\n",
      "Epoch 53: val_loss improved from 2.13293 to 2.08446, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.6164 - val_loss: 2.0845 - lr: 6.8255e-05\n",
      "Epoch 54/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6193\n",
      "Epoch 54: val_loss did not improve from 2.08446\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6201 - val_loss: 2.1270 - lr: 6.8255e-05\n",
      "Epoch 55/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6138\n",
      "Epoch 55: val_loss improved from 2.08446 to 2.04330, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.09.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 5s 79ms/step - loss: 0.6161 - val_loss: 2.0433 - lr: 6.7573e-05\n",
      "Epoch 56/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6135\n",
      "Epoch 56: val_loss did not improve from 2.04330\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6104 - val_loss: 2.2007 - lr: 6.7573e-05\n",
      "Epoch 57/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6161\n",
      "Epoch 57: val_loss did not improve from 2.04330\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 5s 81ms/step - loss: 0.6166 - val_loss: 2.1012 - lr: 6.6897e-05\n",
      "Epoch 58/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6055\n",
      "Epoch 58: val_loss did not improve from 2.04330\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6058 - val_loss: 2.1220 - lr: 6.6228e-05\n",
      "Epoch 59/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6133\n",
      "Epoch 59: val_loss improved from 2.04330 to 2.03861, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6122 - val_loss: 2.0386 - lr: 6.5566e-05\n",
      "Epoch 60/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6311\n",
      "Epoch 60: val_loss did not improve from 2.03861\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6297 - val_loss: 2.2206 - lr: 6.5566e-05\n",
      "Epoch 61/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6046\n",
      "Epoch 61: val_loss did not improve from 2.03861\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6046 - val_loss: 2.2513 - lr: 6.4910e-05\n",
      "Epoch 62/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6172\n",
      "Epoch 62: val_loss did not improve from 2.03861\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6159 - val_loss: 2.0632 - lr: 6.4261e-05\n",
      "Epoch 63/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6064\n",
      "Epoch 63: val_loss improved from 2.03861 to 1.99443, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6064 - val_loss: 1.9944 - lr: 6.3619e-05\n",
      "Epoch 64/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5957\n",
      "Epoch 64: val_loss improved from 1.99443 to 1.87496, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6031 - val_loss: 1.8750 - lr: 6.3619e-05\n",
      "Epoch 65/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6113\n",
      "Epoch 65: val_loss did not improve from 1.87496\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6097 - val_loss: 1.9188 - lr: 6.3619e-05\n",
      "Epoch 66/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6033\n",
      "Epoch 66: val_loss did not improve from 1.87496\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6019 - val_loss: 2.0380 - lr: 6.2982e-05\n",
      "Epoch 67/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5986\n",
      "Epoch 67: val_loss did not improve from 1.87496\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5986 - val_loss: 1.9875 - lr: 6.2353e-05\n",
      "Epoch 68/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6039\n",
      "Epoch 68: val_loss did not improve from 1.87496\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6021 - val_loss: 1.9850 - lr: 6.1729e-05\n",
      "Epoch 69/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6029\n",
      "Epoch 69: val_loss did not improve from 1.87496\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.6034 - val_loss: 2.0557 - lr: 6.1112e-05\n",
      "Epoch 70/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6069\n",
      "Epoch 70: val_loss improved from 1.87496 to 1.87391, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6066 - val_loss: 1.8739 - lr: 6.0501e-05\n",
      "Epoch 71/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6021\n",
      "Epoch 71: val_loss did not improve from 1.87391\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6008 - val_loss: 1.9657 - lr: 6.0501e-05\n",
      "Epoch 72/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5979\n",
      "Epoch 72: val_loss did not improve from 1.87391\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5969 - val_loss: 1.9084 - lr: 5.9896e-05\n",
      "Epoch 73/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5888\n",
      "Epoch 73: val_loss did not improve from 1.87391\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 5.8703650938696225e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.5906 - val_loss: 1.8996 - lr: 5.9297e-05\n",
      "Epoch 74/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6070\n",
      "Epoch 74: val_loss did not improve from 1.87391\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 5.811661550978897e-05.\n",
      "66/66 [==============================] - 5s 81ms/step - loss: 0.6079 - val_loss: 1.9755 - lr: 5.8704e-05\n",
      "Epoch 75/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5989\n",
      "Epoch 75: val_loss did not improve from 1.87391\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 5.753545068728272e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5971 - val_loss: 1.9013 - lr: 5.8117e-05\n",
      "Epoch 76/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5834\n",
      "Epoch 76: val_loss did not improve from 1.87391\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 5.6960095243994146e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5843 - val_loss: 2.0116 - lr: 5.7535e-05\n",
      "Epoch 77/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5909\n",
      "Epoch 77: val_loss did not improve from 1.87391\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 5.639049515593797e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5899 - val_loss: 1.9530 - lr: 5.6960e-05\n",
      "Epoch 78/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5843\n",
      "Epoch 78: val_loss improved from 1.87391 to 1.75209, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5835 - val_loss: 1.7521 - lr: 5.6390e-05\n",
      "Epoch 79/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5791\n",
      "Epoch 79: val_loss did not improve from 1.75209\n",
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 5.5826589195930865e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5799 - val_loss: 1.9327 - lr: 5.6390e-05\n",
      "Epoch 80/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5803\n",
      "Epoch 80: val_loss did not improve from 1.75209\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 5.526832333998754e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5799 - val_loss: 1.8820 - lr: 5.5827e-05\n",
      "Epoch 81/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5795\n",
      "Epoch 81: val_loss did not improve from 1.75209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 5.471563996252371e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5779 - val_loss: 1.7679 - lr: 5.5268e-05\n",
      "Epoch 82/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5842\n",
      "Epoch 82: val_loss did not improve from 1.75209\n",
      "\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 5.416848503955407e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5846 - val_loss: 1.8261 - lr: 5.4716e-05\n",
      "Epoch 83/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5793\n",
      "Epoch 83: val_loss did not improve from 1.75209\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 5.362680094549433e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5774 - val_loss: 1.8515 - lr: 5.4168e-05\n",
      "Epoch 84/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5906\n",
      "Epoch 84: val_loss did not improve from 1.75209\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 5.309053365635918e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5920 - val_loss: 1.7932 - lr: 5.3627e-05\n",
      "Epoch 85/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5735\n",
      "Epoch 85: val_loss did not improve from 1.75209\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 5.2559629148163366e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5748 - val_loss: 1.8113 - lr: 5.3091e-05\n",
      "Epoch 86/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5873\n",
      "Epoch 86: val_loss improved from 1.75209 to 1.74182, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5874 - val_loss: 1.7418 - lr: 5.2560e-05\n",
      "Epoch 87/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5802\n",
      "Epoch 87: val_loss did not improve from 1.74182\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 5.2034033396921584e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5780 - val_loss: 1.7829 - lr: 5.2560e-05\n",
      "Epoch 88/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5702\n",
      "Epoch 88: val_loss did not improve from 1.74182\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 5.1513692378648555e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5737 - val_loss: 1.7572 - lr: 5.2034e-05\n",
      "Epoch 89/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5830\n",
      "Epoch 89: val_loss improved from 1.74182 to 1.71537, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5824 - val_loss: 1.7154 - lr: 5.1514e-05\n",
      "Epoch 90/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5785\n",
      "Epoch 90: val_loss did not improve from 1.71537\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 5.099855567095801e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5791 - val_loss: 1.7861 - lr: 5.1514e-05\n",
      "Epoch 91/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5778\n",
      "Epoch 91: val_loss did not improve from 1.71537\n",
      "\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 5.048856924986467e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.5760 - val_loss: 1.8169 - lr: 5.0999e-05\n",
      "Epoch 92/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5896\n",
      "Epoch 92: val_loss did not improve from 1.71537\n",
      "\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 4.998368269298226e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.5885 - val_loss: 1.7197 - lr: 5.0489e-05\n",
      "Epoch 93/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5661\n",
      "Epoch 93: val_loss did not improve from 1.71537\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 4.9483845577924514e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5650 - val_loss: 1.8089 - lr: 4.9984e-05\n",
      "Epoch 94/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5728\n",
      "Epoch 94: val_loss improved from 1.71537 to 1.70567, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5749 - val_loss: 1.7057 - lr: 4.9484e-05\n",
      "Epoch 95/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5643\n",
      "Epoch 95: val_loss did not improve from 1.70567\n",
      "\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 4.898900748230517e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5660 - val_loss: 1.8813 - lr: 4.9484e-05\n",
      "Epoch 96/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5657\n",
      "Epoch 96: val_loss did not improve from 1.70567\n",
      "\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 4.849911798373796e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5690 - val_loss: 1.7850 - lr: 4.8989e-05\n",
      "Epoch 97/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5600\n",
      "Epoch 97: val_loss improved from 1.70567 to 1.69404, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 5s 81ms/step - loss: 0.5603 - val_loss: 1.6940 - lr: 4.8499e-05\n",
      "Epoch 98/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5612\n",
      "Epoch 98: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 4.801412665983662e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5612 - val_loss: 1.7756 - lr: 4.8499e-05\n",
      "Epoch 99/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5778\n",
      "Epoch 99: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 4.75339866898139e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5767 - val_loss: 1.8430 - lr: 4.8014e-05\n",
      "Epoch 100/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5704\n",
      "Epoch 100: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 4.705864765128354e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5677 - val_loss: 1.7689 - lr: 4.7534e-05\n",
      "Epoch 101/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5819\n",
      "Epoch 101: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 4.658806272345828e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5813 - val_loss: 1.7654 - lr: 4.7059e-05\n",
      "Epoch 102/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5713\n",
      "Epoch 102: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 4.612218148395186e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5708 - val_loss: 1.8700 - lr: 4.6588e-05\n",
      "Epoch 103/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5675\n",
      "Epoch 103: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 4.566096071357606e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5673 - val_loss: 1.7809 - lr: 4.6122e-05\n",
      "Epoch 104/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5580\n",
      "Epoch 104: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 4.52043499899446e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5600 - val_loss: 1.7242 - lr: 4.5661e-05\n",
      "Epoch 105/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5695\n",
      "Epoch 105: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 4.475230609386927e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5711 - val_loss: 1.8174 - lr: 4.5204e-05\n",
      "Epoch 106/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5725\n",
      "Epoch 106: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 4.43047822045628e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5720 - val_loss: 1.7636 - lr: 4.4752e-05\n",
      "Epoch 107/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5514\n",
      "Epoch 107: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 4.386173510283697e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5514 - val_loss: 1.8644 - lr: 4.4305e-05\n",
      "Epoch 108/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5694\n",
      "Epoch 108: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 4.342311796790454e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5726 - val_loss: 1.8095 - lr: 4.3862e-05\n",
      "Epoch 109/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5685\n",
      "Epoch 109: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 4.298888758057728e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5682 - val_loss: 1.7174 - lr: 4.3423e-05\n",
      "Epoch 110/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5566\n",
      "Epoch 110: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 4.2558997120067946e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.5604 - val_loss: 1.7974 - lr: 4.2989e-05\n",
      "Epoch 111/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5578\n",
      "Epoch 111: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 4.2133406968787315e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5593 - val_loss: 1.8235 - lr: 4.2559e-05\n",
      "Epoch 112/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5613\n",
      "Epoch 112: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 4.1712073907547166e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5631 - val_loss: 1.8472 - lr: 4.2133e-05\n",
      "Epoch 113/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5654\n",
      "Epoch 113: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 4.1294954717159274e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5650 - val_loss: 1.7492 - lr: 4.1712e-05\n",
      "Epoch 114/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5711\n",
      "Epoch 114: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 4.08820061784354e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5697 - val_loss: 1.7347 - lr: 4.1295e-05\n",
      "Epoch 115/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5640\n",
      "Epoch 115: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 4.0473185072187336e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5711 - val_loss: 1.7125 - lr: 4.0882e-05\n",
      "Epoch 116/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5501\n",
      "Epoch 116: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 4.0068451780825854e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5498 - val_loss: 1.8121 - lr: 4.0473e-05\n",
      "Epoch 117/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5682\n",
      "Epoch 117: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 3.966776668676175e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5660 - val_loss: 1.8057 - lr: 4.0068e-05\n",
      "Epoch 118/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5710\n",
      "Epoch 118: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 3.927109017240582e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5714 - val_loss: 1.7231 - lr: 3.9668e-05\n",
      "Epoch 119/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5617\n",
      "Epoch 119: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 3.887837901856983e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5604 - val_loss: 1.7776 - lr: 3.9271e-05\n",
      "Epoch 120/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5670\n",
      "Epoch 120: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 3.848959360766457e-05.\n",
      "66/66 [==============================] - 5s 81ms/step - loss: 0.5670 - val_loss: 1.8204 - lr: 3.8878e-05\n",
      "Epoch 121/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5579\n",
      "Epoch 121: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 3.810469792369986e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5599 - val_loss: 1.7166 - lr: 3.8490e-05\n",
      "Epoch 122/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5552\n",
      "Epoch 122: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 3.772365234908648e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5557 - val_loss: 1.7851 - lr: 3.8105e-05\n",
      "Epoch 123/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5533\n",
      "Epoch 123: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 123: ReduceLROnPlateau reducing learning rate to 3.734641726623522e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5530 - val_loss: 1.7686 - lr: 3.7724e-05\n",
      "Epoch 124/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5534\n",
      "Epoch 124: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 124: ReduceLROnPlateau reducing learning rate to 3.6972953057556876e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5540 - val_loss: 1.7726 - lr: 3.7346e-05\n",
      "Epoch 125/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5509\n",
      "Epoch 125: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 3.660322370706126e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5522 - val_loss: 1.7777 - lr: 3.6973e-05\n",
      "Epoch 126/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5593\n",
      "Epoch 126: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 3.623719319875818e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5593 - val_loss: 1.8086 - lr: 3.6603e-05\n",
      "Epoch 127/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5412\n",
      "Epoch 127: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 127: ReduceLROnPlateau reducing learning rate to 3.5874821915058416e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5423 - val_loss: 1.8173 - lr: 3.6237e-05\n",
      "Epoch 128/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5526\n",
      "Epoch 128: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 128: ReduceLROnPlateau reducing learning rate to 3.551607383997179e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5522 - val_loss: 1.7954 - lr: 3.5875e-05\n",
      "Epoch 129/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5575\n",
      "Epoch 129: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 129: ReduceLROnPlateau reducing learning rate to 3.516091295750812e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.5566 - val_loss: 1.7645 - lr: 3.5516e-05\n",
      "Epoch 130/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5612\n",
      "Epoch 130: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 130: ReduceLROnPlateau reducing learning rate to 3.480930325167719e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5591 - val_loss: 1.7877 - lr: 3.5161e-05\n",
      "Epoch 131/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5616\n",
      "Epoch 131: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 131: ReduceLROnPlateau reducing learning rate to 3.446120870648883e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5644 - val_loss: 1.7589 - lr: 3.4809e-05\n",
      "Epoch 132/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5529\n",
      "Epoch 132: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 132: ReduceLROnPlateau reducing learning rate to 3.4116596907551866e-05.\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.5529 - val_loss: 1.7673 - lr: 3.4461e-05\n",
      "Epoch 133/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5518\n",
      "Epoch 133: val_loss did not improve from 1.69404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 133: ReduceLROnPlateau reducing learning rate to 3.37754318388761e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5532 - val_loss: 1.7896 - lr: 3.4117e-05\n",
      "Epoch 134/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5450\n",
      "Epoch 134: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 134: ReduceLROnPlateau reducing learning rate to 3.3437677484471354e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5468 - val_loss: 1.7576 - lr: 3.3775e-05\n",
      "Epoch 135/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5466\n",
      "Epoch 135: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 135: ReduceLROnPlateau reducing learning rate to 3.310330142994644e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5464 - val_loss: 1.7761 - lr: 3.3438e-05\n",
      "Epoch 136/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5475\n",
      "Epoch 136: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 3.277226765931118e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5449 - val_loss: 1.8374 - lr: 3.3103e-05\n",
      "Epoch 137/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5490\n",
      "Epoch 137: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 137: ReduceLROnPlateau reducing learning rate to 3.24445437581744e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.5490 - val_loss: 1.8331 - lr: 3.2772e-05\n",
      "Epoch 138/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5519\n",
      "Epoch 138: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 138: ReduceLROnPlateau reducing learning rate to 3.212009731214493e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5522 - val_loss: 1.7664 - lr: 3.2445e-05\n",
      "Epoch 139/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5533\n",
      "Epoch 139: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 139: ReduceLROnPlateau reducing learning rate to 3.17988959068316e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5557 - val_loss: 1.7314 - lr: 3.2120e-05\n",
      "Epoch 140/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5445\n",
      "Epoch 140: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 140: ReduceLROnPlateau reducing learning rate to 3.1480907127843236e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5426 - val_loss: 1.8210 - lr: 3.1799e-05\n",
      "Epoch 141/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5393\n",
      "Epoch 141: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 141: ReduceLROnPlateau reducing learning rate to 3.116609856078867e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5405 - val_loss: 1.8365 - lr: 3.1481e-05\n",
      "Epoch 142/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5489\n",
      "Epoch 142: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 142: ReduceLROnPlateau reducing learning rate to 3.085443779127672e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5489 - val_loss: 1.8952 - lr: 3.1166e-05\n",
      "Epoch 143/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5511\n",
      "Epoch 143: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 143: ReduceLROnPlateau reducing learning rate to 3.054589240491623e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5523 - val_loss: 1.8638 - lr: 3.0854e-05\n",
      "Epoch 144/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5497\n",
      "Epoch 144: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 144: ReduceLROnPlateau reducing learning rate to 3.024043358891504e-05.\n",
      "66/66 [==============================] - 5s 82ms/step - loss: 0.5507 - val_loss: 1.7683 - lr: 3.0546e-05\n",
      "Epoch 145/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5538\n",
      "Epoch 145: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 145: ReduceLROnPlateau reducing learning rate to 2.9938028928881975e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5533 - val_loss: 1.7604 - lr: 3.0240e-05\n",
      "Epoch 146/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5450\n",
      "Epoch 146: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 146: ReduceLROnPlateau reducing learning rate to 2.963864781122538e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5447 - val_loss: 1.7843 - lr: 2.9938e-05\n",
      "Epoch 147/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5503\n",
      "Epoch 147: val_loss did not improve from 1.69404\n",
      "\n",
      "Epoch 147: ReduceLROnPlateau reducing learning rate to 2.9342261423153105e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5490 - val_loss: 1.7333 - lr: 2.9639e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_20\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_21 (InputLayer)          [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_60 (LSTM)                 (None, 45, 24)       3744        ['input_21[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_40 (Dropout)           (None, 45, 24)       0           ['lstm_60[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_61 (LSTM)                 (None, 45, 16)       2624        ['dropout_40[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_41 (Dropout)           (None, 45, 16)       0           ['lstm_61[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_62 (LSTM)                 (None, 32)           6272        ['dropout_41[0][0]']             \n",
      "                                                                                                  \n",
      " dense_40 (Dense)               (None, 40)           1320        ['lstm_62[0][0]']                \n",
      "                                                                                                  \n",
      " dense_41 (Dense)               (None, 5)            205         ['dense_40[0][0]']               \n",
      "                                                                                                  \n",
      " tf.unstack_20 (TFOpLambda)     [(None,),            0           ['dense_41[0][0]']               \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_100 (TFOpLambda  (None, 1)           0           ['tf.unstack_20[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_40 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_100[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_104 (TFOpLambda  (None, 1)           0           ['tf.unstack_20[0][4]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_60 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_40[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_41 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_104[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_61 (TFOpLambd  (None, 1)           0           ['tf.math.multiply_60[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_101 (TFOpLambda  (None, 1)           0           ['tf.unstack_20[0][1]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_103 (TFOpLambda  (None, 1)           0           ['tf.unstack_20[0][3]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_62 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_41[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_40 (TFOpL  (None, 1)           0           ['tf.math.multiply_61[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.softplus_40 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_101[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_102 (TFOpLambda  (None, 1)           0           ['tf.unstack_20[0][2]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.softplus_41 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_103[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_41 (TFOpL  (None, 1)           0           ['tf.math.multiply_62[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack_20 (TFOpLambda)       (None, 5, 1)         0           ['tf.__operators__.add_40[0][0]',\n",
      "                                                                  'tf.math.softplus_40[0][0]',    \n",
      "                                                                  'tf.expand_dims_102[0][0]',     \n",
      "                                                                  'tf.math.softplus_41[0][0]',    \n",
      "                                                                  'tf.__operators__.add_41[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _2_CCMP_t+1_0.1\n",
      "Epoch 1/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 3.4244\n",
      "Epoch 1: val_loss improved from inf to 4.02951, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 11s 90ms/step - loss: 3.4240 - val_loss: 4.0295 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 2.9134\n",
      "Epoch 2: val_loss improved from 4.02951 to 3.04017, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 5s 77ms/step - loss: 2.9113 - val_loss: 3.0402 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.8449\n",
      "Epoch 3: val_loss improved from 3.04017 to 2.50065, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 1.8515 - val_loss: 2.5006 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.3564\n",
      "Epoch 4: val_loss improved from 2.50065 to 2.22253, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 1.3566 - val_loss: 2.2225 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1651\n",
      "Epoch 5: val_loss improved from 2.22253 to 2.03145, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 1.1631 - val_loss: 2.0315 - lr: 1.0000e-04\n",
      "Epoch 6/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0483\n",
      "Epoch 6: val_loss improved from 2.03145 to 1.89177, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 1.0460 - val_loss: 1.8918 - lr: 1.0000e-04\n",
      "Epoch 7/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9783\n",
      "Epoch 7: val_loss improved from 1.89177 to 1.79704, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.9786 - val_loss: 1.7970 - lr: 1.0000e-04\n",
      "Epoch 8/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9567\n",
      "Epoch 8: val_loss improved from 1.79704 to 1.72832, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.9543 - val_loss: 1.7283 - lr: 1.0000e-04\n",
      "Epoch 9/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9158\n",
      "Epoch 9: val_loss improved from 1.72832 to 1.68960, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.9162 - val_loss: 1.6896 - lr: 1.0000e-04\n",
      "Epoch 10/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8937\n",
      "Epoch 10: val_loss improved from 1.68960 to 1.63601, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.8953 - val_loss: 1.6360 - lr: 1.0000e-04\n",
      "Epoch 11/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8778\n",
      "Epoch 11: val_loss improved from 1.63601 to 1.61395, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.8750 - val_loss: 1.6139 - lr: 1.0000e-04\n",
      "Epoch 12/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8645\n",
      "Epoch 12: val_loss improved from 1.61395 to 1.57375, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.8675 - val_loss: 1.5737 - lr: 1.0000e-04\n",
      "Epoch 13/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8465\n",
      "Epoch 13: val_loss improved from 1.57375 to 1.53720, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.8493 - val_loss: 1.5372 - lr: 1.0000e-04\n",
      "Epoch 14/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8405\n",
      "Epoch 14: val_loss improved from 1.53720 to 1.51684, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.8407 - val_loss: 1.5168 - lr: 1.0000e-04\n",
      "Epoch 15/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8132\n",
      "Epoch 15: val_loss improved from 1.51684 to 1.49462, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.8120 - val_loss: 1.4946 - lr: 1.0000e-04\n",
      "Epoch 16/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8069\n",
      "Epoch 16: val_loss improved from 1.49462 to 1.48004, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.8055 - val_loss: 1.4800 - lr: 1.0000e-04\n",
      "Epoch 17/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8035\n",
      "Epoch 17: val_loss improved from 1.48004 to 1.44745, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 83ms/step - loss: 0.8028 - val_loss: 1.4474 - lr: 1.0000e-04\n",
      "Epoch 18/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8032\n",
      "Epoch 18: val_loss improved from 1.44745 to 1.44456, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.8019 - val_loss: 1.4446 - lr: 1.0000e-04\n",
      "Epoch 19/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7886\n",
      "Epoch 19: val_loss improved from 1.44456 to 1.42735, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 81ms/step - loss: 0.7869 - val_loss: 1.4274 - lr: 1.0000e-04\n",
      "Epoch 20/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7605\n",
      "Epoch 20: val_loss improved from 1.42735 to 1.40469, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.7630 - val_loss: 1.4047 - lr: 1.0000e-04\n",
      "Epoch 21/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7874\n",
      "Epoch 21: val_loss improved from 1.40469 to 1.38848, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.7862 - val_loss: 1.3885 - lr: 1.0000e-04\n",
      "Epoch 22/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7536\n",
      "Epoch 22: val_loss did not improve from 1.38848\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.7562 - val_loss: 1.3924 - lr: 1.0000e-04\n",
      "Epoch 23/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7740\n",
      "Epoch 23: val_loss improved from 1.38848 to 1.36944, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.7732 - val_loss: 1.3694 - lr: 9.9000e-05\n",
      "Epoch 24/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7532\n",
      "Epoch 24: val_loss improved from 1.36944 to 1.35376, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.7525 - val_loss: 1.3538 - lr: 9.9000e-05\n",
      "Epoch 25/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7548\n",
      "Epoch 25: val_loss improved from 1.35376 to 1.34260, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.7535 - val_loss: 1.3426 - lr: 9.9000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7528\n",
      "Epoch 26: val_loss did not improve from 1.34260\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.7508 - val_loss: 1.3484 - lr: 9.9000e-05\n",
      "Epoch 27/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7174\n",
      "Epoch 27: val_loss did not improve from 1.34260\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.7187 - val_loss: 1.3426 - lr: 9.8010e-05\n",
      "Epoch 28/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7298\n",
      "Epoch 28: val_loss improved from 1.34260 to 1.31934, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.7302 - val_loss: 1.3193 - lr: 9.7030e-05\n",
      "Epoch 29/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7316\n",
      "Epoch 29: val_loss did not improve from 1.31934\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.7303 - val_loss: 1.3229 - lr: 9.7030e-05\n",
      "Epoch 30/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7171\n",
      "Epoch 30: val_loss improved from 1.31934 to 1.30895, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.7166 - val_loss: 1.3089 - lr: 9.6060e-05\n",
      "Epoch 31/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7122\n",
      "Epoch 31: val_loss did not improve from 1.30895\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.7131 - val_loss: 1.3124 - lr: 9.6060e-05\n",
      "Epoch 32/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7120\n",
      "Epoch 32: val_loss improved from 1.30895 to 1.30179, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.7114 - val_loss: 1.3018 - lr: 9.5099e-05\n",
      "Epoch 33/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7048\n",
      "Epoch 33: val_loss improved from 1.30179 to 1.29469, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.7061 - val_loss: 1.2947 - lr: 9.5099e-05\n",
      "Epoch 34/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7116\n",
      "Epoch 34: val_loss did not improve from 1.29469\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.7127 - val_loss: 1.2997 - lr: 9.5099e-05\n",
      "Epoch 35/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7015\n",
      "Epoch 35: val_loss improved from 1.29469 to 1.29020, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.7011 - val_loss: 1.2902 - lr: 9.4148e-05\n",
      "Epoch 36/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7001\n",
      "Epoch 36: val_loss improved from 1.29020 to 1.27308, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.7004 - val_loss: 1.2731 - lr: 9.4148e-05\n",
      "Epoch 37/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7213\n",
      "Epoch 37: val_loss did not improve from 1.27308\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 0.7221 - val_loss: 1.2906 - lr: 9.4148e-05\n",
      "Epoch 38/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7107\n",
      "Epoch 38: val_loss did not improve from 1.27308\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.7092 - val_loss: 1.2798 - lr: 9.3207e-05\n",
      "Epoch 39/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6911\n",
      "Epoch 39: val_loss improved from 1.27308 to 1.27137, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6916 - val_loss: 1.2714 - lr: 9.2274e-05\n",
      "Epoch 40/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7081\n",
      "Epoch 40: val_loss improved from 1.27137 to 1.26738, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.7061 - val_loss: 1.2674 - lr: 9.2274e-05\n",
      "Epoch 41/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6916\n",
      "Epoch 41: val_loss improved from 1.26738 to 1.26401, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6883 - val_loss: 1.2640 - lr: 9.2274e-05\n",
      "Epoch 42/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6961\n",
      "Epoch 42: val_loss improved from 1.26401 to 1.26044, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.6956 - val_loss: 1.2604 - lr: 9.2274e-05\n",
      "Epoch 43/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6929\n",
      "Epoch 43: val_loss improved from 1.26044 to 1.25909, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6921 - val_loss: 1.2591 - lr: 9.2274e-05\n",
      "Epoch 44/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6864\n",
      "Epoch 44: val_loss improved from 1.25909 to 1.25183, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6861 - val_loss: 1.2518 - lr: 9.2274e-05\n",
      "Epoch 45/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6845\n",
      "Epoch 45: val_loss did not improve from 1.25183\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6831 - val_loss: 1.2662 - lr: 9.2274e-05\n",
      "Epoch 46/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6882\n",
      "Epoch 46: val_loss did not improve from 1.25183\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6861 - val_loss: 1.2567 - lr: 9.1352e-05\n",
      "Epoch 47/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6927\n",
      "Epoch 47: val_loss did not improve from 1.25183\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6920 - val_loss: 1.2571 - lr: 9.0438e-05\n",
      "Epoch 48/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6800\n",
      "Epoch 48: val_loss improved from 1.25183 to 1.24742, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6810 - val_loss: 1.2474 - lr: 8.9534e-05\n",
      "Epoch 49/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6901\n",
      "Epoch 49: val_loss did not improve from 1.24742\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6907 - val_loss: 1.2474 - lr: 8.9534e-05\n",
      "Epoch 50/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6702\n",
      "Epoch 50: val_loss did not improve from 1.24742\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6702 - val_loss: 1.2477 - lr: 8.8638e-05\n",
      "Epoch 51/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6694\n",
      "Epoch 51: val_loss improved from 1.24742 to 1.23520, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6684 - val_loss: 1.2352 - lr: 8.7752e-05\n",
      "Epoch 52/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6713\n",
      "Epoch 52: val_loss did not improve from 1.23520\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6699 - val_loss: 1.2396 - lr: 8.7752e-05\n",
      "Epoch 53/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6789\n",
      "Epoch 53: val_loss did not improve from 1.23520\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6773 - val_loss: 1.2359 - lr: 8.6875e-05\n",
      "Epoch 54/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6698\n",
      "Epoch 54: val_loss did not improve from 1.23520\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6723 - val_loss: 1.2357 - lr: 8.6006e-05\n",
      "Epoch 55/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6775\n",
      "Epoch 55: val_loss improved from 1.23520 to 1.22446, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6752 - val_loss: 1.2245 - lr: 8.5146e-05\n",
      "Epoch 56/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6656\n",
      "Epoch 56: val_loss did not improve from 1.22446\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6662 - val_loss: 1.2311 - lr: 8.5146e-05\n",
      "Epoch 57/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6502\n",
      "Epoch 57: val_loss did not improve from 1.22446\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6515 - val_loss: 1.2319 - lr: 8.4294e-05\n",
      "Epoch 58/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6743\n",
      "Epoch 58: val_loss did not improve from 1.22446\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6736 - val_loss: 1.2281 - lr: 8.3451e-05\n",
      "Epoch 59/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6608\n",
      "Epoch 59: val_loss improved from 1.22446 to 1.22428, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6582 - val_loss: 1.2243 - lr: 8.2617e-05\n",
      "Epoch 60/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6530\n",
      "Epoch 60: val_loss did not improve from 1.22428\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6574 - val_loss: 1.2269 - lr: 8.2617e-05\n",
      "Epoch 61/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6581\n",
      "Epoch 61: val_loss improved from 1.22428 to 1.21987, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6557 - val_loss: 1.2199 - lr: 8.1791e-05\n",
      "Epoch 62/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6614\n",
      "Epoch 62: val_loss improved from 1.21987 to 1.21278, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6612 - val_loss: 1.2128 - lr: 8.1791e-05\n",
      "Epoch 63/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6670\n",
      "Epoch 63: val_loss did not improve from 1.21278\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 0.6652 - val_loss: 1.2186 - lr: 8.1791e-05\n",
      "Epoch 64/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6609\n",
      "Epoch 64: val_loss did not improve from 1.21278\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.6607 - val_loss: 1.2299 - lr: 8.0973e-05\n",
      "Epoch 65/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6552\n",
      "Epoch 65: val_loss did not improve from 1.21278\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6549 - val_loss: 1.2173 - lr: 8.0163e-05\n",
      "Epoch 66/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6555\n",
      "Epoch 66: val_loss did not improve from 1.21278\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.6554 - val_loss: 1.2156 - lr: 7.9361e-05\n",
      "Epoch 67/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6529\n",
      "Epoch 67: val_loss did not improve from 1.21278\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6527 - val_loss: 1.2155 - lr: 7.8568e-05\n",
      "Epoch 68/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6542\n",
      "Epoch 68: val_loss did not improve from 1.21278\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6537 - val_loss: 1.2148 - lr: 7.7782e-05\n",
      "Epoch 69/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6408\n",
      "Epoch 69: val_loss did not improve from 1.21278\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6408 - val_loss: 1.2144 - lr: 7.7004e-05\n",
      "Epoch 70/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6513\n",
      "Epoch 70: val_loss did not improve from 1.21278\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6508 - val_loss: 1.2173 - lr: 7.6234e-05\n",
      "Epoch 71/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6319\n",
      "Epoch 71: val_loss did not improve from 1.21278\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6302 - val_loss: 1.2143 - lr: 7.5472e-05\n",
      "Epoch 72/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6392\n",
      "Epoch 72: val_loss improved from 1.21278 to 1.21086, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6392 - val_loss: 1.2109 - lr: 7.4717e-05\n",
      "Epoch 73/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6466\n",
      "Epoch 73: val_loss did not improve from 1.21086\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6461 - val_loss: 1.2134 - lr: 7.4717e-05\n",
      "Epoch 74/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6331\n",
      "Epoch 74: val_loss did not improve from 1.21086\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6325 - val_loss: 1.2160 - lr: 7.3970e-05\n",
      "Epoch 75/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6326\n",
      "Epoch 75: val_loss did not improve from 1.21086\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6349 - val_loss: 1.2186 - lr: 7.3230e-05\n",
      "Epoch 76/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6459\n",
      "Epoch 76: val_loss did not improve from 1.21086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6446 - val_loss: 1.2210 - lr: 7.2498e-05\n",
      "Epoch 77/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6429\n",
      "Epoch 77: val_loss did not improve from 1.21086\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6435 - val_loss: 1.2156 - lr: 7.1773e-05\n",
      "Epoch 78/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6454\n",
      "Epoch 78: val_loss did not improve from 1.21086\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6430 - val_loss: 1.2149 - lr: 7.1055e-05\n",
      "Epoch 79/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6435\n",
      "Epoch 79: val_loss did not improve from 1.21086\n",
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6427 - val_loss: 1.2133 - lr: 7.0345e-05\n",
      "Epoch 80/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6267\n",
      "Epoch 80: val_loss did not improve from 1.21086\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6273 - val_loss: 1.2127 - lr: 6.9641e-05\n",
      "Epoch 81/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6362\n",
      "Epoch 81: val_loss did not improve from 1.21086\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6363 - val_loss: 1.2117 - lr: 6.8945e-05\n",
      "Epoch 82/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6319\n",
      "Epoch 82: val_loss improved from 1.21086 to 1.20962, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6335 - val_loss: 1.2096 - lr: 6.8255e-05\n",
      "Epoch 83/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6419\n",
      "Epoch 83: val_loss improved from 1.20962 to 1.20845, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6400 - val_loss: 1.2084 - lr: 6.8255e-05\n",
      "Epoch 84/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6319\n",
      "Epoch 84: val_loss improved from 1.20845 to 1.20668, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6317 - val_loss: 1.2067 - lr: 6.8255e-05\n",
      "Epoch 85/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6188\n",
      "Epoch 85: val_loss did not improve from 1.20668\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6179 - val_loss: 1.2087 - lr: 6.8255e-05\n",
      "Epoch 86/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6473\n",
      "Epoch 86: val_loss improved from 1.20668 to 1.20563, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6474 - val_loss: 1.2056 - lr: 6.7573e-05\n",
      "Epoch 87/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6187\n",
      "Epoch 87: val_loss improved from 1.20563 to 1.20348, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6210 - val_loss: 1.2035 - lr: 6.7573e-05\n",
      "Epoch 88/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6325\n",
      "Epoch 88: val_loss did not improve from 1.20348\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6349 - val_loss: 1.2325 - lr: 6.7573e-05\n",
      "Epoch 89/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6389\n",
      "Epoch 89: val_loss did not improve from 1.20348\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.6392 - val_loss: 1.2036 - lr: 6.6897e-05\n",
      "Epoch 90/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6219\n",
      "Epoch 90: val_loss did not improve from 1.20348\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 0.6235 - val_loss: 1.2084 - lr: 6.6228e-05\n",
      "Epoch 91/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6126\n",
      "Epoch 91: val_loss did not improve from 1.20348\n",
      "\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6116 - val_loss: 1.2374 - lr: 6.5566e-05\n",
      "Epoch 92/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6337\n",
      "Epoch 92: val_loss did not improve from 1.20348\n",
      "\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6330 - val_loss: 1.2081 - lr: 6.4910e-05\n",
      "Epoch 93/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6177\n",
      "Epoch 93: val_loss did not improve from 1.20348\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6183 - val_loss: 1.2114 - lr: 6.4261e-05\n",
      "Epoch 94/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6364\n",
      "Epoch 94: val_loss did not improve from 1.20348\n",
      "\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6362 - val_loss: 1.2089 - lr: 6.3619e-05\n",
      "Epoch 95/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6149\n",
      "Epoch 95: val_loss did not improve from 1.20348\n",
      "\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6164 - val_loss: 1.2147 - lr: 6.2982e-05\n",
      "Epoch 96/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6337\n",
      "Epoch 96: val_loss did not improve from 1.20348\n",
      "\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6314 - val_loss: 1.2152 - lr: 6.2353e-05\n",
      "Epoch 97/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6306\n",
      "Epoch 97: val_loss did not improve from 1.20348\n",
      "\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6287 - val_loss: 1.2155 - lr: 6.1729e-05\n",
      "Epoch 98/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6199\n",
      "Epoch 98: val_loss did not improve from 1.20348\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6192 - val_loss: 1.2179 - lr: 6.1112e-05\n",
      "Epoch 99/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6189\n",
      "Epoch 99: val_loss improved from 1.20348 to 1.20240, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6181 - val_loss: 1.2024 - lr: 6.0501e-05\n",
      "Epoch 100/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6153\n",
      "Epoch 100: val_loss did not improve from 1.20240\n",
      "\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6185 - val_loss: 1.2048 - lr: 6.0501e-05\n",
      "Epoch 101/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6193\n",
      "Epoch 101: val_loss did not improve from 1.20240\n",
      "\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6198 - val_loss: 1.2081 - lr: 5.9896e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6158\n",
      "Epoch 102: val_loss did not improve from 1.20240\n",
      "\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 5.8703650938696225e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6141 - val_loss: 1.2118 - lr: 5.9297e-05\n",
      "Epoch 103/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6178\n",
      "Epoch 103: val_loss did not improve from 1.20240\n",
      "\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 5.811661550978897e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6178 - val_loss: 1.2033 - lr: 5.8704e-05\n",
      "Epoch 104/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6087\n",
      "Epoch 104: val_loss did not improve from 1.20240\n",
      "\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 5.753545068728272e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6087 - val_loss: 1.2072 - lr: 5.8117e-05\n",
      "Epoch 105/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6064\n",
      "Epoch 105: val_loss did not improve from 1.20240\n",
      "\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 5.6960095243994146e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6063 - val_loss: 1.2062 - lr: 5.7535e-05\n",
      "Epoch 106/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6161\n",
      "Epoch 106: val_loss did not improve from 1.20240\n",
      "\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 5.639049515593797e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6166 - val_loss: 1.2046 - lr: 5.6960e-05\n",
      "Epoch 107/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6209\n",
      "Epoch 107: val_loss did not improve from 1.20240\n",
      "\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 5.5826589195930865e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6198 - val_loss: 1.2150 - lr: 5.6390e-05\n",
      "Epoch 108/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6080\n",
      "Epoch 108: val_loss improved from 1.20240 to 1.19999, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6098 - val_loss: 1.2000 - lr: 5.5827e-05\n",
      "Epoch 109/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6083\n",
      "Epoch 109: val_loss improved from 1.19999 to 1.19705, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6138 - val_loss: 1.1971 - lr: 5.5827e-05\n",
      "Epoch 110/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6074\n",
      "Epoch 110: val_loss did not improve from 1.19705\n",
      "\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 5.526832333998754e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6091 - val_loss: 1.2047 - lr: 5.5827e-05\n",
      "Epoch 111/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6093\n",
      "Epoch 111: val_loss did not improve from 1.19705\n",
      "\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 5.471563996252371e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6095 - val_loss: 1.2042 - lr: 5.5268e-05\n",
      "Epoch 112/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6083\n",
      "Epoch 112: val_loss did not improve from 1.19705\n",
      "\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 5.416848503955407e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6082 - val_loss: 1.2025 - lr: 5.4716e-05\n",
      "Epoch 113/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6052\n",
      "Epoch 113: val_loss did not improve from 1.19705\n",
      "\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 5.362680094549433e-05.\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.6059 - val_loss: 1.2124 - lr: 5.4168e-05\n",
      "Epoch 114/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6045\n",
      "Epoch 114: val_loss did not improve from 1.19705\n",
      "\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 5.309053365635918e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6068 - val_loss: 1.2033 - lr: 5.3627e-05\n",
      "Epoch 115/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6199\n",
      "Epoch 115: val_loss did not improve from 1.19705\n",
      "\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 5.2559629148163366e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6183 - val_loss: 1.2040 - lr: 5.3091e-05\n",
      "Epoch 116/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6157\n",
      "Epoch 116: val_loss did not improve from 1.19705\n",
      "\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 5.2034033396921584e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6143 - val_loss: 1.2022 - lr: 5.2560e-05\n",
      "Epoch 117/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5968\n",
      "Epoch 117: val_loss did not improve from 1.19705\n",
      "\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 5.1513692378648555e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6032 - val_loss: 1.1994 - lr: 5.2034e-05\n",
      "Epoch 118/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6179\n",
      "Epoch 118: val_loss did not improve from 1.19705\n",
      "\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 5.099855567095801e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6152 - val_loss: 1.2002 - lr: 5.1514e-05\n",
      "Epoch 119/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6106\n",
      "Epoch 119: val_loss did not improve from 1.19705\n",
      "\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 5.048856924986467e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6123 - val_loss: 1.2005 - lr: 5.0999e-05\n",
      "Epoch 120/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5970\n",
      "Epoch 120: val_loss did not improve from 1.19705\n",
      "\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 4.998368269298226e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5972 - val_loss: 1.2089 - lr: 5.0489e-05\n",
      "Epoch 121/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6007\n",
      "Epoch 121: val_loss did not improve from 1.19705\n",
      "\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 4.9483845577924514e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6030 - val_loss: 1.1998 - lr: 4.9984e-05\n",
      "Epoch 122/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6090\n",
      "Epoch 122: val_loss did not improve from 1.19705\n",
      "\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 4.898900748230517e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6087 - val_loss: 1.2012 - lr: 4.9484e-05\n",
      "Epoch 123/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6100\n",
      "Epoch 123: val_loss did not improve from 1.19705\n",
      "\n",
      "Epoch 123: ReduceLROnPlateau reducing learning rate to 4.849911798373796e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6093 - val_loss: 1.1995 - lr: 4.8989e-05\n",
      "Epoch 124/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5996\n",
      "Epoch 124: val_loss did not improve from 1.19705\n",
      "\n",
      "Epoch 124: ReduceLROnPlateau reducing learning rate to 4.801412665983662e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5996 - val_loss: 1.2135 - lr: 4.8499e-05\n",
      "Epoch 125/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6016\n",
      "Epoch 125: val_loss did not improve from 1.19705\n",
      "\n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 4.75339866898139e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6009 - val_loss: 1.2019 - lr: 4.8014e-05\n",
      "Epoch 126/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6147\n",
      "Epoch 126: val_loss did not improve from 1.19705\n",
      "\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 4.705864765128354e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6132 - val_loss: 1.1985 - lr: 4.7534e-05\n",
      "Epoch 127/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6015\n",
      "Epoch 127: val_loss did not improve from 1.19705\n",
      "\n",
      "Epoch 127: ReduceLROnPlateau reducing learning rate to 4.658806272345828e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6000 - val_loss: 1.2030 - lr: 4.7059e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6066\n",
      "Epoch 128: val_loss improved from 1.19705 to 1.19168, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6074 - val_loss: 1.1917 - lr: 4.6588e-05\n",
      "Epoch 129/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6102\n",
      "Epoch 129: val_loss did not improve from 1.19168\n",
      "\n",
      "Epoch 129: ReduceLROnPlateau reducing learning rate to 4.612218148395186e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6111 - val_loss: 1.1958 - lr: 4.6588e-05\n",
      "Epoch 130/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6067\n",
      "Epoch 130: val_loss did not improve from 1.19168\n",
      "\n",
      "Epoch 130: ReduceLROnPlateau reducing learning rate to 4.566096071357606e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6061 - val_loss: 1.1969 - lr: 4.6122e-05\n",
      "Epoch 131/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5880\n",
      "Epoch 131: val_loss did not improve from 1.19168\n",
      "\n",
      "Epoch 131: ReduceLROnPlateau reducing learning rate to 4.52043499899446e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5895 - val_loss: 1.1951 - lr: 4.5661e-05\n",
      "Epoch 132/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5967\n",
      "Epoch 132: val_loss did not improve from 1.19168\n",
      "\n",
      "Epoch 132: ReduceLROnPlateau reducing learning rate to 4.475230609386927e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6002 - val_loss: 1.1944 - lr: 4.5204e-05\n",
      "Epoch 133/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6069\n",
      "Epoch 133: val_loss improved from 1.19168 to 1.18887, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6081 - val_loss: 1.1889 - lr: 4.4752e-05\n",
      "Epoch 134/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6017\n",
      "Epoch 134: val_loss did not improve from 1.18887\n",
      "\n",
      "Epoch 134: ReduceLROnPlateau reducing learning rate to 4.43047822045628e-05.\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 0.5996 - val_loss: 1.2022 - lr: 4.4752e-05\n",
      "Epoch 135/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6015\n",
      "Epoch 135: val_loss did not improve from 1.18887\n",
      "\n",
      "Epoch 135: ReduceLROnPlateau reducing learning rate to 4.386173510283697e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6015 - val_loss: 1.1978 - lr: 4.4305e-05\n",
      "Epoch 136/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5986\n",
      "Epoch 136: val_loss did not improve from 1.18887\n",
      "\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 4.342311796790454e-05.\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.5986 - val_loss: 1.1937 - lr: 4.3862e-05\n",
      "Epoch 137/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5999\n",
      "Epoch 137: val_loss did not improve from 1.18887\n",
      "\n",
      "Epoch 137: ReduceLROnPlateau reducing learning rate to 4.298888758057728e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5973 - val_loss: 1.2091 - lr: 4.3423e-05\n",
      "Epoch 138/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6050\n",
      "Epoch 138: val_loss did not improve from 1.18887\n",
      "\n",
      "Epoch 138: ReduceLROnPlateau reducing learning rate to 4.2558997120067946e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6050 - val_loss: 1.1939 - lr: 4.2989e-05\n",
      "Epoch 139/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6008\n",
      "Epoch 139: val_loss did not improve from 1.18887\n",
      "\n",
      "Epoch 139: ReduceLROnPlateau reducing learning rate to 4.2133406968787315e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6007 - val_loss: 1.2015 - lr: 4.2559e-05\n",
      "Epoch 140/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6002\n",
      "Epoch 140: val_loss did not improve from 1.18887\n",
      "\n",
      "Epoch 140: ReduceLROnPlateau reducing learning rate to 4.1712073907547166e-05.\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 0.5993 - val_loss: 1.1975 - lr: 4.2133e-05\n",
      "Epoch 141/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6025\n",
      "Epoch 141: val_loss did not improve from 1.18887\n",
      "\n",
      "Epoch 141: ReduceLROnPlateau reducing learning rate to 4.1294954717159274e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6015 - val_loss: 1.1921 - lr: 4.1712e-05\n",
      "Epoch 142/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5997\n",
      "Epoch 142: val_loss did not improve from 1.18887\n",
      "\n",
      "Epoch 142: ReduceLROnPlateau reducing learning rate to 4.08820061784354e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6021 - val_loss: 1.1931 - lr: 4.1295e-05\n",
      "Epoch 143/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6015\n",
      "Epoch 143: val_loss did not improve from 1.18887\n",
      "\n",
      "Epoch 143: ReduceLROnPlateau reducing learning rate to 4.0473185072187336e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6033 - val_loss: 1.1923 - lr: 4.0882e-05\n",
      "Epoch 144/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6063\n",
      "Epoch 144: val_loss did not improve from 1.18887\n",
      "\n",
      "Epoch 144: ReduceLROnPlateau reducing learning rate to 4.0068451780825854e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6048 - val_loss: 1.2096 - lr: 4.0473e-05\n",
      "Epoch 145/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6116\n",
      "Epoch 145: val_loss did not improve from 1.18887\n",
      "\n",
      "Epoch 145: ReduceLROnPlateau reducing learning rate to 3.966776668676175e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6124 - val_loss: 1.2048 - lr: 4.0068e-05\n",
      "Epoch 146/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5946\n",
      "Epoch 146: val_loss did not improve from 1.18887\n",
      "\n",
      "Epoch 146: ReduceLROnPlateau reducing learning rate to 3.927109017240582e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5947 - val_loss: 1.2187 - lr: 3.9668e-05\n",
      "Epoch 147/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5911\n",
      "Epoch 147: val_loss did not improve from 1.18887\n",
      "\n",
      "Epoch 147: ReduceLROnPlateau reducing learning rate to 3.887837901856983e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5905 - val_loss: 1.1919 - lr: 3.9271e-05\n",
      "Epoch 148/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6006\n",
      "Epoch 148: val_loss did not improve from 1.18887\n",
      "\n",
      "Epoch 148: ReduceLROnPlateau reducing learning rate to 3.848959360766457e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6001 - val_loss: 1.2048 - lr: 3.8878e-05\n",
      "Epoch 149/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5878\n",
      "Epoch 149: val_loss did not improve from 1.18887\n",
      "\n",
      "Epoch 149: ReduceLROnPlateau reducing learning rate to 3.810469792369986e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5852 - val_loss: 1.1891 - lr: 3.8490e-05\n",
      "Epoch 150/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6034\n",
      "Epoch 150: val_loss did not improve from 1.18887\n",
      "\n",
      "Epoch 150: ReduceLROnPlateau reducing learning rate to 3.772365234908648e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6039 - val_loss: 1.1928 - lr: 3.8105e-05\n",
      "Epoch 151/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5987\n",
      "Epoch 151: val_loss did not improve from 1.18887\n",
      "\n",
      "Epoch 151: ReduceLROnPlateau reducing learning rate to 3.734641726623522e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5994 - val_loss: 1.1901 - lr: 3.7724e-05\n",
      "Epoch 152/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6009\n",
      "Epoch 152: val_loss improved from 1.18887 to 1.18794, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5991 - val_loss: 1.1879 - lr: 3.7346e-05\n",
      "Epoch 153/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5917\n",
      "Epoch 153: val_loss did not improve from 1.18794\n",
      "\n",
      "Epoch 153: ReduceLROnPlateau reducing learning rate to 3.6972953057556876e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5906 - val_loss: 1.1926 - lr: 3.7346e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5874\n",
      "Epoch 154: val_loss did not improve from 1.18794\n",
      "\n",
      "Epoch 154: ReduceLROnPlateau reducing learning rate to 3.660322370706126e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5865 - val_loss: 1.1887 - lr: 3.6973e-05\n",
      "Epoch 155/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5907\n",
      "Epoch 155: val_loss did not improve from 1.18794\n",
      "\n",
      "Epoch 155: ReduceLROnPlateau reducing learning rate to 3.623719319875818e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5903 - val_loss: 1.1901 - lr: 3.6603e-05\n",
      "Epoch 156/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5925\n",
      "Epoch 156: val_loss did not improve from 1.18794\n",
      "\n",
      "Epoch 156: ReduceLROnPlateau reducing learning rate to 3.5874821915058416e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5919 - val_loss: 1.1907 - lr: 3.6237e-05\n",
      "Epoch 157/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5889\n",
      "Epoch 157: val_loss did not improve from 1.18794\n",
      "\n",
      "Epoch 157: ReduceLROnPlateau reducing learning rate to 3.551607383997179e-05.\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 0.5879 - val_loss: 1.1882 - lr: 3.5875e-05\n",
      "Epoch 158/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5950\n",
      "Epoch 158: val_loss did not improve from 1.18794\n",
      "\n",
      "Epoch 158: ReduceLROnPlateau reducing learning rate to 3.516091295750812e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5941 - val_loss: 1.1941 - lr: 3.5516e-05\n",
      "Epoch 159/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5970\n",
      "Epoch 159: val_loss did not improve from 1.18794\n",
      "\n",
      "Epoch 159: ReduceLROnPlateau reducing learning rate to 3.480930325167719e-05.\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 0.5959 - val_loss: 1.1986 - lr: 3.5161e-05\n",
      "Epoch 160/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5931\n",
      "Epoch 160: val_loss did not improve from 1.18794\n",
      "\n",
      "Epoch 160: ReduceLROnPlateau reducing learning rate to 3.446120870648883e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.5917 - val_loss: 1.1982 - lr: 3.4809e-05\n",
      "Epoch 161/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5907\n",
      "Epoch 161: val_loss did not improve from 1.18794\n",
      "\n",
      "Epoch 161: ReduceLROnPlateau reducing learning rate to 3.4116596907551866e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5914 - val_loss: 1.1896 - lr: 3.4461e-05\n",
      "Epoch 162/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6003\n",
      "Epoch 162: val_loss did not improve from 1.18794\n",
      "\n",
      "Epoch 162: ReduceLROnPlateau reducing learning rate to 3.37754318388761e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6014 - val_loss: 1.1958 - lr: 3.4117e-05\n",
      "Epoch 163/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5902\n",
      "Epoch 163: val_loss did not improve from 1.18794\n",
      "\n",
      "Epoch 163: ReduceLROnPlateau reducing learning rate to 3.3437677484471354e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5910 - val_loss: 1.1916 - lr: 3.3775e-05\n",
      "Epoch 164/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5839\n",
      "Epoch 164: val_loss improved from 1.18794 to 1.18732, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5850 - val_loss: 1.1873 - lr: 3.3438e-05\n",
      "Epoch 165/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5975\n",
      "Epoch 165: val_loss improved from 1.18732 to 1.18470, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5958 - val_loss: 1.1847 - lr: 3.3438e-05\n",
      "Epoch 166/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5873\n",
      "Epoch 166: val_loss did not improve from 1.18470\n",
      "\n",
      "Epoch 166: ReduceLROnPlateau reducing learning rate to 3.310330142994644e-05.\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 0.5878 - val_loss: 1.1905 - lr: 3.3438e-05\n",
      "Epoch 167/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5872\n",
      "Epoch 167: val_loss did not improve from 1.18470\n",
      "\n",
      "Epoch 167: ReduceLROnPlateau reducing learning rate to 3.277226765931118e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5848 - val_loss: 1.2055 - lr: 3.3103e-05\n",
      "Epoch 168/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5785\n",
      "Epoch 168: val_loss did not improve from 1.18470\n",
      "\n",
      "Epoch 168: ReduceLROnPlateau reducing learning rate to 3.24445437581744e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5766 - val_loss: 1.1897 - lr: 3.2772e-05\n",
      "Epoch 169/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5839\n",
      "Epoch 169: val_loss did not improve from 1.18470\n",
      "\n",
      "Epoch 169: ReduceLROnPlateau reducing learning rate to 3.212009731214493e-05.\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 0.5879 - val_loss: 1.1898 - lr: 3.2445e-05\n",
      "Epoch 170/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5886\n",
      "Epoch 170: val_loss did not improve from 1.18470\n",
      "\n",
      "Epoch 170: ReduceLROnPlateau reducing learning rate to 3.17988959068316e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5907 - val_loss: 1.1992 - lr: 3.2120e-05\n",
      "Epoch 171/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5869\n",
      "Epoch 171: val_loss did not improve from 1.18470\n",
      "\n",
      "Epoch 171: ReduceLROnPlateau reducing learning rate to 3.1480907127843236e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5857 - val_loss: 1.1848 - lr: 3.1799e-05\n",
      "Epoch 172/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5900\n",
      "Epoch 172: val_loss did not improve from 1.18470\n",
      "\n",
      "Epoch 172: ReduceLROnPlateau reducing learning rate to 3.116609856078867e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5899 - val_loss: 1.1939 - lr: 3.1481e-05\n",
      "Epoch 173/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5938\n",
      "Epoch 173: val_loss improved from 1.18470 to 1.18214, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.5971 - val_loss: 1.1821 - lr: 3.1166e-05\n",
      "Epoch 174/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5863\n",
      "Epoch 174: val_loss improved from 1.18214 to 1.17867, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5854 - val_loss: 1.1787 - lr: 3.1166e-05\n",
      "Epoch 175/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5954\n",
      "Epoch 175: val_loss did not improve from 1.17867\n",
      "\n",
      "Epoch 175: ReduceLROnPlateau reducing learning rate to 3.085443779127672e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5945 - val_loss: 1.1944 - lr: 3.1166e-05\n",
      "Epoch 176/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5891\n",
      "Epoch 176: val_loss did not improve from 1.17867\n",
      "\n",
      "Epoch 176: ReduceLROnPlateau reducing learning rate to 3.054589240491623e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5886 - val_loss: 1.1828 - lr: 3.0854e-05\n",
      "Epoch 177/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5915\n",
      "Epoch 177: val_loss improved from 1.17867 to 1.17711, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5900 - val_loss: 1.1771 - lr: 3.0546e-05\n",
      "Epoch 178/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5853\n",
      "Epoch 178: val_loss did not improve from 1.17711\n",
      "\n",
      "Epoch 178: ReduceLROnPlateau reducing learning rate to 3.024043358891504e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5870 - val_loss: 1.1832 - lr: 3.0546e-05\n",
      "Epoch 179/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5976\n",
      "Epoch 179: val_loss did not improve from 1.17711\n",
      "\n",
      "Epoch 179: ReduceLROnPlateau reducing learning rate to 2.9938028928881975e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5968 - val_loss: 1.1911 - lr: 3.0240e-05\n",
      "Epoch 180/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5805\n",
      "Epoch 180: val_loss did not improve from 1.17711\n",
      "\n",
      "Epoch 180: ReduceLROnPlateau reducing learning rate to 2.963864781122538e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5845 - val_loss: 1.1812 - lr: 2.9938e-05\n",
      "Epoch 181/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5800\n",
      "Epoch 181: val_loss did not improve from 1.17711\n",
      "\n",
      "Epoch 181: ReduceLROnPlateau reducing learning rate to 2.9342261423153105e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5791 - val_loss: 1.1931 - lr: 2.9639e-05\n",
      "Epoch 182/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5772\n",
      "Epoch 182: val_loss did not improve from 1.17711\n",
      "\n",
      "Epoch 182: ReduceLROnPlateau reducing learning rate to 2.9048839151073478e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5775 - val_loss: 1.1870 - lr: 2.9342e-05\n",
      "Epoch 183/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5874\n",
      "Epoch 183: val_loss did not improve from 1.17711\n",
      "\n",
      "Epoch 183: ReduceLROnPlateau reducing learning rate to 2.875835038139485e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5878 - val_loss: 1.1855 - lr: 2.9049e-05\n",
      "Epoch 184/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5800\n",
      "Epoch 184: val_loss did not improve from 1.17711\n",
      "\n",
      "Epoch 184: ReduceLROnPlateau reducing learning rate to 2.8470766301325058e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.5800 - val_loss: 1.2017 - lr: 2.8758e-05\n",
      "Epoch 185/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5884\n",
      "Epoch 185: val_loss did not improve from 1.17711\n",
      "\n",
      "Epoch 185: ReduceLROnPlateau reducing learning rate to 2.8186058098071954e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5875 - val_loss: 1.1875 - lr: 2.8471e-05\n",
      "Epoch 186/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5860\n",
      "Epoch 186: val_loss did not improve from 1.17711\n",
      "\n",
      "Epoch 186: ReduceLROnPlateau reducing learning rate to 2.7904196958843385e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5859 - val_loss: 1.1876 - lr: 2.8186e-05\n",
      "Epoch 187/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5940\n",
      "Epoch 187: val_loss did not improve from 1.17711\n",
      "\n",
      "Epoch 187: ReduceLROnPlateau reducing learning rate to 2.7625155871646713e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5936 - val_loss: 1.1837 - lr: 2.7904e-05\n",
      "Epoch 188/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5907\n",
      "Epoch 188: val_loss did not improve from 1.17711\n",
      "\n",
      "Epoch 188: ReduceLROnPlateau reducing learning rate to 2.734890422289027e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5884 - val_loss: 1.1821 - lr: 2.7625e-05\n",
      "Epoch 189/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5930\n",
      "Epoch 189: val_loss did not improve from 1.17711\n",
      "\n",
      "Epoch 189: ReduceLROnPlateau reducing learning rate to 2.7075415000581416e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5903 - val_loss: 1.1802 - lr: 2.7349e-05\n",
      "Epoch 190/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5967\n",
      "Epoch 190: val_loss did not improve from 1.17711\n",
      "\n",
      "Epoch 190: ReduceLROnPlateau reducing learning rate to 2.6804661192727508e-05.\n",
      "66/66 [==============================] - 5s 76ms/step - loss: 0.5991 - val_loss: 1.1780 - lr: 2.7075e-05\n",
      "Epoch 191/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5892\n",
      "Epoch 191: val_loss did not improve from 1.17711\n",
      "\n",
      "Epoch 191: ReduceLROnPlateau reducing learning rate to 2.6536613986536395e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5900 - val_loss: 1.1841 - lr: 2.6805e-05\n",
      "Epoch 192/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5962\n",
      "Epoch 192: val_loss did not improve from 1.17711\n",
      "\n",
      "Epoch 192: ReduceLROnPlateau reducing learning rate to 2.6271248170814942e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5966 - val_loss: 1.1871 - lr: 2.6537e-05\n",
      "Epoch 193/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5822\n",
      "Epoch 193: val_loss improved from 1.17711 to 1.17655, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5837 - val_loss: 1.1765 - lr: 2.6271e-05\n",
      "Epoch 194/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5778\n",
      "Epoch 194: val_loss did not improve from 1.17655\n",
      "\n",
      "Epoch 194: ReduceLROnPlateau reducing learning rate to 2.6008534932770998e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5756 - val_loss: 1.1796 - lr: 2.6271e-05\n",
      "Epoch 195/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5884\n",
      "Epoch 195: val_loss did not improve from 1.17655\n",
      "\n",
      "Epoch 195: ReduceLROnPlateau reducing learning rate to 2.5748449061211432e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5875 - val_loss: 1.1778 - lr: 2.6009e-05\n",
      "Epoch 196/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5800\n",
      "Epoch 196: val_loss did not improve from 1.17655\n",
      "\n",
      "Epoch 196: ReduceLROnPlateau reducing learning rate to 2.5490965344943107e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5811 - val_loss: 1.1784 - lr: 2.5748e-05\n",
      "Epoch 197/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5865\n",
      "Epoch 197: val_loss did not improve from 1.17655\n",
      "\n",
      "Epoch 197: ReduceLROnPlateau reducing learning rate to 2.523605497117387e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5865 - val_loss: 1.1869 - lr: 2.5491e-05\n",
      "Epoch 198/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5871\n",
      "Epoch 198: val_loss did not improve from 1.17655\n",
      "\n",
      "Epoch 198: ReduceLROnPlateau reducing learning rate to 2.4983694529510104e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5881 - val_loss: 1.1922 - lr: 2.5236e-05\n",
      "Epoch 199/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5895\n",
      "Epoch 199: val_loss did not improve from 1.17655\n",
      "\n",
      "Epoch 199: ReduceLROnPlateau reducing learning rate to 2.4733857007959158e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5888 - val_loss: 1.1905 - lr: 2.4984e-05\n",
      "Epoch 200/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5842\n",
      "Epoch 200: val_loss did not improve from 1.17655\n",
      "\n",
      "Epoch 200: ReduceLROnPlateau reducing learning rate to 2.4486518996127415e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5844 - val_loss: 1.1802 - lr: 2.4734e-05\n",
      "Epoch 201/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5746\n",
      "Epoch 201: val_loss did not improve from 1.17655\n",
      "\n",
      "Epoch 201: ReduceLROnPlateau reducing learning rate to 2.424165348202223e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5781 - val_loss: 1.1845 - lr: 2.4487e-05\n",
      "Epoch 202/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5916\n",
      "Epoch 202: val_loss did not improve from 1.17655\n",
      "\n",
      "Epoch 202: ReduceLROnPlateau reducing learning rate to 2.3999237055249977e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5934 - val_loss: 1.1787 - lr: 2.4242e-05\n",
      "Epoch 203/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5855\n",
      "Epoch 203: val_loss did not improve from 1.17655\n",
      "\n",
      "Epoch 203: ReduceLROnPlateau reducing learning rate to 2.375924450461753e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5859 - val_loss: 1.1894 - lr: 2.3999e-05\n",
      "Epoch 204/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5893\n",
      "Epoch 204: val_loss did not improve from 1.17655\n",
      "\n",
      "Epoch 204: ReduceLROnPlateau reducing learning rate to 2.3521652419731254e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5912 - val_loss: 1.1891 - lr: 2.3759e-05\n",
      "Epoch 205/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5922\n",
      "Epoch 205: val_loss did not improve from 1.17655\n",
      "\n",
      "Epoch 205: ReduceLROnPlateau reducing learning rate to 2.3286435589398024e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5968 - val_loss: 1.1822 - lr: 2.3522e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 206/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5848\n",
      "Epoch 206: val_loss did not improve from 1.17655\n",
      "\n",
      "Epoch 206: ReduceLROnPlateau reducing learning rate to 2.3053570603224217e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5855 - val_loss: 1.1773 - lr: 2.3286e-05\n",
      "Epoch 207/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5846\n",
      "Epoch 207: val_loss did not improve from 1.17655\n",
      "\n",
      "Epoch 207: ReduceLROnPlateau reducing learning rate to 2.2823034050816205e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5834 - val_loss: 1.1803 - lr: 2.3054e-05\n",
      "Epoch 208/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5787\n",
      "Epoch 208: val_loss did not improve from 1.17655\n",
      "\n",
      "Epoch 208: ReduceLROnPlateau reducing learning rate to 2.2594804322579874e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5762 - val_loss: 1.1817 - lr: 2.2823e-05\n",
      "Epoch 209/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5803\n",
      "Epoch 209: val_loss did not improve from 1.17655\n",
      "\n",
      "Epoch 209: ReduceLROnPlateau reducing learning rate to 2.2368856207322096e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5791 - val_loss: 1.1904 - lr: 2.2595e-05\n",
      "Epoch 210/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5751\n",
      "Epoch 210: val_loss did not improve from 1.17655\n",
      "\n",
      "Epoch 210: ReduceLROnPlateau reducing learning rate to 2.2145168095448753e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5768 - val_loss: 1.1878 - lr: 2.2369e-05\n",
      "Epoch 211/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5723\n",
      "Epoch 211: val_loss did not improve from 1.17655\n",
      "\n",
      "Epoch 211: ReduceLROnPlateau reducing learning rate to 2.1923716576566222e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5727 - val_loss: 1.1805 - lr: 2.2145e-05\n",
      "Epoch 212/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5772\n",
      "Epoch 212: val_loss improved from 1.17655 to 1.17363, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5785 - val_loss: 1.1736 - lr: 2.1924e-05\n",
      "Epoch 213/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5776\n",
      "Epoch 213: val_loss did not improve from 1.17363\n",
      "\n",
      "Epoch 213: ReduceLROnPlateau reducing learning rate to 2.1704480041080387e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5785 - val_loss: 1.1768 - lr: 2.1924e-05\n",
      "Epoch 214/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5774\n",
      "Epoch 214: val_loss did not improve from 1.17363\n",
      "\n",
      "Epoch 214: ReduceLROnPlateau reducing learning rate to 2.1487435078597626e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5806 - val_loss: 1.1755 - lr: 2.1704e-05\n",
      "Epoch 215/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5774\n",
      "Epoch 215: val_loss did not improve from 1.17363\n",
      "\n",
      "Epoch 215: ReduceLROnPlateau reducing learning rate to 2.1272560079523828e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5764 - val_loss: 1.1764 - lr: 2.1487e-05\n",
      "Epoch 216/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5825\n",
      "Epoch 216: val_loss improved from 1.17363 to 1.17175, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5831 - val_loss: 1.1718 - lr: 2.1273e-05\n",
      "Epoch 217/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5834\n",
      "Epoch 217: val_loss did not improve from 1.17175\n",
      "\n",
      "Epoch 217: ReduceLROnPlateau reducing learning rate to 2.1059835235064383e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5827 - val_loss: 1.1737 - lr: 2.1273e-05\n",
      "Epoch 218/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5686\n",
      "Epoch 218: val_loss did not improve from 1.17175\n",
      "\n",
      "Epoch 218: ReduceLROnPlateau reducing learning rate to 2.084923713482567e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5720 - val_loss: 1.1803 - lr: 2.1060e-05\n",
      "Epoch 219/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5890\n",
      "Epoch 219: val_loss did not improve from 1.17175\n",
      "\n",
      "Epoch 219: ReduceLROnPlateau reducing learning rate to 2.0640744169213575e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5877 - val_loss: 1.1779 - lr: 2.0849e-05\n",
      "Epoch 220/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5821\n",
      "Epoch 220: val_loss did not improve from 1.17175\n",
      "\n",
      "Epoch 220: ReduceLROnPlateau reducing learning rate to 2.0434336529433495e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5829 - val_loss: 1.1759 - lr: 2.0641e-05\n",
      "Epoch 221/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5897\n",
      "Epoch 221: val_loss did not improve from 1.17175\n",
      "\n",
      "Epoch 221: ReduceLROnPlateau reducing learning rate to 2.0229992605891313e-05.\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.5889 - val_loss: 1.1780 - lr: 2.0434e-05\n",
      "Epoch 222/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5927\n",
      "Epoch 222: val_loss did not improve from 1.17175\n",
      "\n",
      "Epoch 222: ReduceLROnPlateau reducing learning rate to 2.0027692589792422e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5934 - val_loss: 1.1768 - lr: 2.0230e-05\n",
      "Epoch 223/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5745\n",
      "Epoch 223: val_loss did not improve from 1.17175\n",
      "\n",
      "Epoch 223: ReduceLROnPlateau reducing learning rate to 1.9827414871542714e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5762 - val_loss: 1.1821 - lr: 2.0028e-05\n",
      "Epoch 224/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5725\n",
      "Epoch 224: val_loss did not improve from 1.17175\n",
      "\n",
      "Epoch 224: ReduceLROnPlateau reducing learning rate to 1.9629141443147093e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5749 - val_loss: 1.1783 - lr: 1.9827e-05\n",
      "Epoch 225/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5855\n",
      "Epoch 225: val_loss did not improve from 1.17175\n",
      "\n",
      "Epoch 225: ReduceLROnPlateau reducing learning rate to 1.943285069501144e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5856 - val_loss: 1.1813 - lr: 1.9629e-05\n",
      "Epoch 226/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5817\n",
      "Epoch 226: val_loss did not improve from 1.17175\n",
      "\n",
      "Epoch 226: ReduceLROnPlateau reducing learning rate to 1.9238522818341152e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5814 - val_loss: 1.1872 - lr: 1.9433e-05\n",
      "Epoch 227/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5745\n",
      "Epoch 227: val_loss did not improve from 1.17175\n",
      "\n",
      "Epoch 227: ReduceLROnPlateau reducing learning rate to 1.9046138004341628e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5740 - val_loss: 1.1865 - lr: 1.9239e-05\n",
      "Epoch 228/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5735\n",
      "Epoch 228: val_loss did not improve from 1.17175\n",
      "\n",
      "Epoch 228: ReduceLROnPlateau reducing learning rate to 1.885567644421826e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5738 - val_loss: 1.1772 - lr: 1.9046e-05\n",
      "Epoch 229/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5725\n",
      "Epoch 229: val_loss did not improve from 1.17175\n",
      "\n",
      "Epoch 229: ReduceLROnPlateau reducing learning rate to 1.8667120129975956e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5708 - val_loss: 1.1800 - lr: 1.8856e-05\n",
      "Epoch 230/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5821\n",
      "Epoch 230: val_loss did not improve from 1.17175\n",
      "\n",
      "Epoch 230: ReduceLROnPlateau reducing learning rate to 1.8480449252820108e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5813 - val_loss: 1.1821 - lr: 1.8667e-05\n",
      "Epoch 231/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5808\n",
      "Epoch 231: val_loss did not improve from 1.17175\n",
      "\n",
      "Epoch 231: ReduceLROnPlateau reducing learning rate to 1.8295644003956113e-05.\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.5803 - val_loss: 1.1762 - lr: 1.8480e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 232/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5737\n",
      "Epoch 232: val_loss did not improve from 1.17175\n",
      "\n",
      "Epoch 232: ReduceLROnPlateau reducing learning rate to 1.8112688176188387e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5734 - val_loss: 1.1820 - lr: 1.8296e-05\n",
      "Epoch 233/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5826\n",
      "Epoch 233: val_loss did not improve from 1.17175\n",
      "\n",
      "Epoch 233: ReduceLROnPlateau reducing learning rate to 1.793156196072232e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5812 - val_loss: 1.1804 - lr: 1.8113e-05\n",
      "Epoch 234/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5738\n",
      "Epoch 234: val_loss did not improve from 1.17175\n",
      "\n",
      "Epoch 234: ReduceLROnPlateau reducing learning rate to 1.775224554876331e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5763 - val_loss: 1.1789 - lr: 1.7932e-05\n",
      "Epoch 235/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5841\n",
      "Epoch 235: val_loss did not improve from 1.17175\n",
      "\n",
      "Epoch 235: ReduceLROnPlateau reducing learning rate to 1.7574722733115777e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5812 - val_loss: 1.1777 - lr: 1.7752e-05\n",
      "Epoch 236/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5903\n",
      "Epoch 236: val_loss did not improve from 1.17175\n",
      "\n",
      "Epoch 236: ReduceLROnPlateau reducing learning rate to 1.739897550578462e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5903 - val_loss: 1.1818 - lr: 1.7575e-05\n",
      "Epoch 237/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5800\n",
      "Epoch 237: val_loss did not improve from 1.17175\n",
      "\n",
      "Epoch 237: ReduceLROnPlateau reducing learning rate to 1.7224985858774742e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5804 - val_loss: 1.1814 - lr: 1.7399e-05\n",
      "Epoch 238/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5858\n",
      "Epoch 238: val_loss did not improve from 1.17175\n",
      "\n",
      "Epoch 238: ReduceLROnPlateau reducing learning rate to 1.7052735784091056e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5882 - val_loss: 1.1735 - lr: 1.7225e-05\n",
      "Epoch 239/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5703\n",
      "Epoch 239: val_loss did not improve from 1.17175\n",
      "\n",
      "Epoch 239: ReduceLROnPlateau reducing learning rate to 1.6882209074537968e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5684 - val_loss: 1.1871 - lr: 1.7053e-05\n",
      "Epoch 240/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5783\n",
      "Epoch 240: val_loss did not improve from 1.17175\n",
      "\n",
      "Epoch 240: ReduceLROnPlateau reducing learning rate to 1.6713387722120388e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5818 - val_loss: 1.1823 - lr: 1.6882e-05\n",
      "Epoch 241/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5707\n",
      "Epoch 241: val_loss did not improve from 1.17175\n",
      "\n",
      "Epoch 241: ReduceLROnPlateau reducing learning rate to 1.6546253718843217e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5757 - val_loss: 1.1819 - lr: 1.6713e-05\n",
      "Epoch 242/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5753\n",
      "Epoch 242: val_loss did not improve from 1.17175\n",
      "\n",
      "Epoch 242: ReduceLROnPlateau reducing learning rate to 1.6380790857510874e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5768 - val_loss: 1.1797 - lr: 1.6546e-05\n",
      "Epoch 243/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5804\n",
      "Epoch 243: val_loss did not improve from 1.17175\n",
      "\n",
      "Epoch 243: ReduceLROnPlateau reducing learning rate to 1.621698293092777e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5790 - val_loss: 1.1774 - lr: 1.6381e-05\n",
      "Epoch 244/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5784\n",
      "Epoch 244: val_loss did not improve from 1.17175\n",
      "\n",
      "Epoch 244: ReduceLROnPlateau reducing learning rate to 1.605481373189832e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5773 - val_loss: 1.1831 - lr: 1.6217e-05\n",
      "Epoch 245/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5837\n",
      "Epoch 245: val_loss did not improve from 1.17175\n",
      "\n",
      "Epoch 245: ReduceLROnPlateau reducing learning rate to 1.589426525242743e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5828 - val_loss: 1.1791 - lr: 1.6055e-05\n",
      "Epoch 246/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5802\n",
      "Epoch 246: val_loss did not improve from 1.17175\n",
      "\n",
      "Epoch 246: ReduceLROnPlateau reducing learning rate to 1.5735323086119023e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5809 - val_loss: 1.1813 - lr: 1.5894e-05\n",
      "Epoch 247/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5825\n",
      "Epoch 247: val_loss did not improve from 1.17175\n",
      "\n",
      "Epoch 247: ReduceLROnPlateau reducing learning rate to 1.5577969224978006e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5834 - val_loss: 1.1779 - lr: 1.5735e-05\n",
      "Epoch 248/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5705\n",
      "Epoch 248: val_loss did not improve from 1.17175\n",
      "\n",
      "Epoch 248: ReduceLROnPlateau reducing learning rate to 1.5422189262608297e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5702 - val_loss: 1.1798 - lr: 1.5578e-05\n",
      "Epoch 249/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5655\n",
      "Epoch 249: val_loss did not improve from 1.17175\n",
      "\n",
      "Epoch 249: ReduceLROnPlateau reducing learning rate to 1.5267966991814318e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5646 - val_loss: 1.1813 - lr: 1.5422e-05\n",
      "Epoch 250/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5847\n",
      "Epoch 250: val_loss did not improve from 1.17175\n",
      "\n",
      "Epoch 250: ReduceLROnPlateau reducing learning rate to 1.511528800619999e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5860 - val_loss: 1.1722 - lr: 1.5268e-05\n",
      "Epoch 251/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5798\n",
      "Epoch 251: val_loss improved from 1.17175 to 1.17082, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5822 - val_loss: 1.1708 - lr: 1.5115e-05\n",
      "Epoch 252/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5794\n",
      "Epoch 252: val_loss did not improve from 1.17082\n",
      "\n",
      "Epoch 252: ReduceLROnPlateau reducing learning rate to 1.496413519816997e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5795 - val_loss: 1.1723 - lr: 1.5115e-05\n",
      "Epoch 253/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5774\n",
      "Epoch 253: val_loss did not improve from 1.17082\n",
      "\n",
      "Epoch 253: ReduceLROnPlateau reducing learning rate to 1.4814494161328184e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5754 - val_loss: 1.1783 - lr: 1.4964e-05\n",
      "Epoch 254/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5815\n",
      "Epoch 254: val_loss did not improve from 1.17082\n",
      "\n",
      "Epoch 254: ReduceLROnPlateau reducing learning rate to 1.4666349588878802e-05.\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.5795 - val_loss: 1.1762 - lr: 1.4814e-05\n",
      "Epoch 255/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5851\n",
      "Epoch 255: val_loss did not improve from 1.17082\n",
      "\n",
      "Epoch 255: ReduceLROnPlateau reducing learning rate to 1.4519686174025991e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5859 - val_loss: 1.1770 - lr: 1.4666e-05\n",
      "Epoch 256/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5718\n",
      "Epoch 256: val_loss did not improve from 1.17082\n",
      "\n",
      "Epoch 256: ReduceLROnPlateau reducing learning rate to 1.4374489510373678e-05.\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.5694 - val_loss: 1.1773 - lr: 1.4520e-05\n",
      "Epoch 257/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5772\n",
      "Epoch 257: val_loss did not improve from 1.17082\n",
      "\n",
      "Epoch 257: ReduceLROnPlateau reducing learning rate to 1.423074429112603e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5759 - val_loss: 1.1758 - lr: 1.4374e-05\n",
      "Epoch 258/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5723\n",
      "Epoch 258: val_loss did not improve from 1.17082\n",
      "\n",
      "Epoch 258: ReduceLROnPlateau reducing learning rate to 1.4088437010286726e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5734 - val_loss: 1.1746 - lr: 1.4231e-05\n",
      "Epoch 259/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5696\n",
      "Epoch 259: val_loss did not improve from 1.17082\n",
      "\n",
      "Epoch 259: ReduceLROnPlateau reducing learning rate to 1.3947552361059934e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5695 - val_loss: 1.1756 - lr: 1.4088e-05\n",
      "Epoch 260/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5853\n",
      "Epoch 260: val_loss did not improve from 1.17082\n",
      "\n",
      "Epoch 260: ReduceLROnPlateau reducing learning rate to 1.3808076837449334e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5843 - val_loss: 1.1754 - lr: 1.3948e-05\n",
      "Epoch 261/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5761\n",
      "Epoch 261: val_loss did not improve from 1.17082\n",
      "\n",
      "Epoch 261: ReduceLROnPlateau reducing learning rate to 1.3669996033058851e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5736 - val_loss: 1.1783 - lr: 1.3808e-05\n",
      "Epoch 262/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5772\n",
      "Epoch 262: val_loss did not improve from 1.17082\n",
      "\n",
      "Epoch 262: ReduceLROnPlateau reducing learning rate to 1.3533296441892163e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5797 - val_loss: 1.1739 - lr: 1.3670e-05\n",
      "Epoch 263/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5639\n",
      "Epoch 263: val_loss did not improve from 1.17082\n",
      "\n",
      "Epoch 263: ReduceLROnPlateau reducing learning rate to 1.3397963657553191e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5657 - val_loss: 1.1723 - lr: 1.3533e-05\n",
      "Epoch 264/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5740\n",
      "Epoch 264: val_loss did not improve from 1.17082\n",
      "\n",
      "Epoch 264: ReduceLROnPlateau reducing learning rate to 1.3263984174045618e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5751 - val_loss: 1.1727 - lr: 1.3398e-05\n",
      "Epoch 265/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5824\n",
      "Epoch 265: val_loss did not improve from 1.17082\n",
      "\n",
      "Epoch 265: ReduceLROnPlateau reducing learning rate to 1.3131344485373119e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5807 - val_loss: 1.1764 - lr: 1.3264e-05\n",
      "Epoch 266/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5891\n",
      "Epoch 266: val_loss did not improve from 1.17082\n",
      "\n",
      "Epoch 266: ReduceLROnPlateau reducing learning rate to 1.3000031085539376e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5892 - val_loss: 1.1729 - lr: 1.3131e-05\n",
      "Epoch 267/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5813\n",
      "Epoch 267: val_loss did not improve from 1.17082\n",
      "\n",
      "Epoch 267: ReduceLROnPlateau reducing learning rate to 1.2870030468548066e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5800 - val_loss: 1.1723 - lr: 1.3000e-05\n",
      "Epoch 268/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5826\n",
      "Epoch 268: val_loss did not improve from 1.17082\n",
      "\n",
      "Epoch 268: ReduceLROnPlateau reducing learning rate to 1.2741330028802622e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5807 - val_loss: 1.1713 - lr: 1.2870e-05\n",
      "Epoch 269/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5792\n",
      "Epoch 269: val_loss did not improve from 1.17082\n",
      "\n",
      "Epoch 269: ReduceLROnPlateau reducing learning rate to 1.2613917160706478e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5787 - val_loss: 1.1763 - lr: 1.2741e-05\n",
      "Epoch 270/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5703\n",
      "Epoch 270: val_loss did not improve from 1.17082\n",
      "\n",
      "Epoch 270: ReduceLROnPlateau reducing learning rate to 1.2487778358263313e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5709 - val_loss: 1.1799 - lr: 1.2614e-05\n",
      "Epoch 271/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5701\n",
      "Epoch 271: val_loss did not improve from 1.17082\n",
      "\n",
      "Epoch 271: ReduceLROnPlateau reducing learning rate to 1.236290101587656e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5703 - val_loss: 1.1736 - lr: 1.2488e-05\n",
      "Epoch 272/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5788\n",
      "Epoch 272: val_loss did not improve from 1.17082\n",
      "\n",
      "Epoch 272: ReduceLROnPlateau reducing learning rate to 1.2239271627549898e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5771 - val_loss: 1.1743 - lr: 1.2363e-05\n",
      "Epoch 273/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5821\n",
      "Epoch 273: val_loss did not improve from 1.17082\n",
      "\n",
      "Epoch 273: ReduceLROnPlateau reducing learning rate to 1.2116878488086512e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5846 - val_loss: 1.1714 - lr: 1.2239e-05\n",
      "Epoch 274/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5683\n",
      "Epoch 274: val_loss did not improve from 1.17082\n",
      "\n",
      "Epoch 274: ReduceLROnPlateau reducing learning rate to 1.1995709892289596e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5700 - val_loss: 1.1761 - lr: 1.2117e-05\n",
      "Epoch 275/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5731\n",
      "Epoch 275: val_loss did not improve from 1.17082\n",
      "\n",
      "Epoch 275: ReduceLROnPlateau reducing learning rate to 1.187575323456258e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5717 - val_loss: 1.1776 - lr: 1.1996e-05\n",
      "Epoch 276/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5712\n",
      "Epoch 276: val_loss did not improve from 1.17082\n",
      "\n",
      "Epoch 276: ReduceLROnPlateau reducing learning rate to 1.1756995909308897e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5719 - val_loss: 1.1755 - lr: 1.1876e-05\n",
      "Epoch 277/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5796\n",
      "Epoch 277: val_loss did not improve from 1.17082\n",
      "\n",
      "Epoch 277: ReduceLROnPlateau reducing learning rate to 1.1639426211331737e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.5795 - val_loss: 1.1753 - lr: 1.1757e-05\n",
      "Epoch 278/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5757\n",
      "Epoch 278: val_loss did not improve from 1.17082\n",
      "\n",
      "Epoch 278: ReduceLROnPlateau reducing learning rate to 1.1523031535034534e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.5732 - val_loss: 1.1734 - lr: 1.1639e-05\n",
      "Epoch 279/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5787\n",
      "Epoch 279: val_loss did not improve from 1.17082\n",
      "\n",
      "Epoch 279: ReduceLROnPlateau reducing learning rate to 1.1407801075620227e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5784 - val_loss: 1.1729 - lr: 1.1523e-05\n",
      "Epoch 280/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5699\n",
      "Epoch 280: val_loss did not improve from 1.17082\n",
      "\n",
      "Epoch 280: ReduceLROnPlateau reducing learning rate to 1.1293723127892008e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5703 - val_loss: 1.1741 - lr: 1.1408e-05\n",
      "Epoch 281/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5787\n",
      "Epoch 281: val_loss did not improve from 1.17082\n",
      "\n",
      "Epoch 281: ReduceLROnPlateau reducing learning rate to 1.1180785986653063e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5804 - val_loss: 1.1743 - lr: 1.1294e-05\n",
      "Epoch 282/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5811\n",
      "Epoch 282: val_loss did not improve from 1.17082\n",
      "\n",
      "Epoch 282: ReduceLROnPlateau reducing learning rate to 1.1068977946706581e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5781 - val_loss: 1.1782 - lr: 1.1181e-05\n",
      "Epoch 283/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5708\n",
      "Epoch 283: val_loss did not improve from 1.17082\n",
      "\n",
      "Epoch 283: ReduceLROnPlateau reducing learning rate to 1.0958288203255507e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5715 - val_loss: 1.1769 - lr: 1.1069e-05\n",
      "Epoch 284/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/66 [============================>.] - ETA: 0s - loss: 0.5835\n",
      "Epoch 284: val_loss did not improve from 1.17082\n",
      "\n",
      "Epoch 284: ReduceLROnPlateau reducing learning rate to 1.0848705051103024e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5837 - val_loss: 1.1746 - lr: 1.0958e-05\n",
      "Epoch 285/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5692\n",
      "Epoch 285: val_loss did not improve from 1.17082\n",
      "\n",
      "Epoch 285: ReduceLROnPlateau reducing learning rate to 1.074021768545208e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5713 - val_loss: 1.1732 - lr: 1.0849e-05\n",
      "Epoch 286/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5690\n",
      "Epoch 286: val_loss did not improve from 1.17082\n",
      "\n",
      "Epoch 286: ReduceLROnPlateau reducing learning rate to 1.0632815301505616e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.5719 - val_loss: 1.1736 - lr: 1.0740e-05\n",
      "Epoch 287/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5730\n",
      "Epoch 287: val_loss improved from 1.17082 to 1.17043, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 6s 84ms/step - loss: 0.5730 - val_loss: 1.1704 - lr: 1.0633e-05\n",
      "Epoch 288/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5696\n",
      "Epoch 288: val_loss improved from 1.17043 to 1.17024, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.5698 - val_loss: 1.1702 - lr: 1.0633e-05\n",
      "Epoch 289/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5648\n",
      "Epoch 289: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 289: ReduceLROnPlateau reducing learning rate to 1.0526487094466574e-05.\n",
      "66/66 [==============================] - 5s 72ms/step - loss: 0.5649 - val_loss: 1.1740 - lr: 1.0633e-05\n",
      "Epoch 290/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5698\n",
      "Epoch 290: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 290: ReduceLROnPlateau reducing learning rate to 1.04212222595379e-05.\n",
      "66/66 [==============================] - 5s 75ms/step - loss: 0.5741 - val_loss: 1.1725 - lr: 1.0526e-05\n",
      "Epoch 291/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5801\n",
      "Epoch 291: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 291: ReduceLROnPlateau reducing learning rate to 1.0317009991922532e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.5811 - val_loss: 1.1709 - lr: 1.0421e-05\n",
      "Epoch 292/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5658\n",
      "Epoch 292: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 292: ReduceLROnPlateau reducing learning rate to 1.0213839486823417e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.5647 - val_loss: 1.1741 - lr: 1.0317e-05\n",
      "Epoch 293/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5682\n",
      "Epoch 293: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 293: ReduceLROnPlateau reducing learning rate to 1.0111700839843252e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.5713 - val_loss: 1.1724 - lr: 1.0214e-05\n",
      "Epoch 294/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5761\n",
      "Epoch 294: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 294: ReduceLROnPlateau reducing learning rate to 1.0010584146584733e-05.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.5770 - val_loss: 1.1725 - lr: 1.0112e-05\n",
      "Epoch 295/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5710\n",
      "Epoch 295: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 295: ReduceLROnPlateau reducing learning rate to 9.910478602250804e-06.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.5706 - val_loss: 1.1743 - lr: 1.0011e-05\n",
      "Epoch 296/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5730\n",
      "Epoch 296: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 296: ReduceLROnPlateau reducing learning rate to 9.81137340204441e-06.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.5702 - val_loss: 1.1752 - lr: 9.9105e-06\n",
      "Epoch 297/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5906\n",
      "Epoch 297: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 297: ReduceLROnPlateau reducing learning rate to 9.713259541968e-06.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 0.5894 - val_loss: 1.1714 - lr: 9.8114e-06\n",
      "Epoch 298/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5704\n",
      "Epoch 298: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 298: ReduceLROnPlateau reducing learning rate to 9.616127117624274e-06.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.5710 - val_loss: 1.1752 - lr: 9.7133e-06\n",
      "Epoch 299/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5885\n",
      "Epoch 299: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 299: ReduceLROnPlateau reducing learning rate to 9.519966224615928e-06.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.5858 - val_loss: 1.1744 - lr: 9.6161e-06\n",
      "Epoch 300/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5641\n",
      "Epoch 300: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 300: ReduceLROnPlateau reducing learning rate to 9.42476695854566e-06.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.5633 - val_loss: 1.1758 - lr: 9.5200e-06\n",
      "Epoch 301/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5957\n",
      "Epoch 301: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 301: ReduceLROnPlateau reducing learning rate to 9.33051941501617e-06.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.5933 - val_loss: 1.1750 - lr: 9.4248e-06\n",
      "Epoch 302/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5628\n",
      "Epoch 302: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 302: ReduceLROnPlateau reducing learning rate to 9.237214590029907e-06.\n",
      "66/66 [==============================] - 5s 75ms/step - loss: 0.5629 - val_loss: 1.1764 - lr: 9.3305e-06\n",
      "Epoch 303/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5733\n",
      "Epoch 303: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 303: ReduceLROnPlateau reducing learning rate to 9.144842579189572e-06.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.5727 - val_loss: 1.1766 - lr: 9.2372e-06\n",
      "Epoch 304/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5741\n",
      "Epoch 304: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 304: ReduceLROnPlateau reducing learning rate to 9.053394378497615e-06.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.5743 - val_loss: 1.1783 - lr: 9.1448e-06\n",
      "Epoch 305/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5741\n",
      "Epoch 305: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 305: ReduceLROnPlateau reducing learning rate to 8.962860083556735e-06.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 0.5756 - val_loss: 1.1765 - lr: 9.0534e-06\n",
      "Epoch 306/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5821\n",
      "Epoch 306: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 306: ReduceLROnPlateau reducing learning rate to 8.873231590769138e-06.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 0.5859 - val_loss: 1.1772 - lr: 8.9629e-06\n",
      "Epoch 307/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5763\n",
      "Epoch 307: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 307: ReduceLROnPlateau reducing learning rate to 8.784498995737521e-06.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.5741 - val_loss: 1.1762 - lr: 8.8732e-06\n",
      "Epoch 308/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5798\n",
      "Epoch 308: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 308: ReduceLROnPlateau reducing learning rate to 8.696654194864095e-06.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.5801 - val_loss: 1.1744 - lr: 8.7845e-06\n",
      "Epoch 309/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5747\n",
      "Epoch 309: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 309: ReduceLROnPlateau reducing learning rate to 8.609687283751554e-06.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.5782 - val_loss: 1.1758 - lr: 8.6967e-06\n",
      "Epoch 310/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/66 [============================>.] - ETA: 0s - loss: 0.5780\n",
      "Epoch 310: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 310: ReduceLROnPlateau reducing learning rate to 8.523590158802108e-06.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.5789 - val_loss: 1.1748 - lr: 8.6097e-06\n",
      "Epoch 311/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5658\n",
      "Epoch 311: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 311: ReduceLROnPlateau reducing learning rate to 8.438353816018208e-06.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 0.5669 - val_loss: 1.1750 - lr: 8.5236e-06\n",
      "Epoch 312/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5770\n",
      "Epoch 312: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 312: ReduceLROnPlateau reducing learning rate to 8.35397015180206e-06.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 0.5760 - val_loss: 1.1753 - lr: 8.4384e-06\n",
      "Epoch 313/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5697\n",
      "Epoch 313: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 313: ReduceLROnPlateau reducing learning rate to 8.270430162156118e-06.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 0.5716 - val_loss: 1.1744 - lr: 8.3540e-06\n",
      "Epoch 314/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5673\n",
      "Epoch 314: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 314: ReduceLROnPlateau reducing learning rate to 8.187725743482588e-06.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.5684 - val_loss: 1.1742 - lr: 8.2704e-06\n",
      "Epoch 315/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5841\n",
      "Epoch 315: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 315: ReduceLROnPlateau reducing learning rate to 8.105848792183679e-06.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.5847 - val_loss: 1.1738 - lr: 8.1877e-06\n",
      "Epoch 316/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5665\n",
      "Epoch 316: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 316: ReduceLROnPlateau reducing learning rate to 8.024790304261842e-06.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 0.5678 - val_loss: 1.1732 - lr: 8.1058e-06\n",
      "Epoch 317/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5784\n",
      "Epoch 317: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 317: ReduceLROnPlateau reducing learning rate to 7.944542176119285e-06.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.5784 - val_loss: 1.1731 - lr: 8.0248e-06\n",
      "Epoch 318/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5631\n",
      "Epoch 318: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 318: ReduceLROnPlateau reducing learning rate to 7.865096304158214e-06.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 0.5684 - val_loss: 1.1748 - lr: 7.9445e-06\n",
      "Epoch 319/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5611\n",
      "Epoch 319: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 319: ReduceLROnPlateau reducing learning rate to 7.786445485180593e-06.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.5604 - val_loss: 1.1763 - lr: 7.8651e-06\n",
      "Epoch 320/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5717\n",
      "Epoch 320: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 320: ReduceLROnPlateau reducing learning rate to 7.708580715188874e-06.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.5712 - val_loss: 1.1748 - lr: 7.7864e-06\n",
      "Epoch 321/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5826\n",
      "Epoch 321: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 321: ReduceLROnPlateau reducing learning rate to 7.631494790985016e-06.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 0.5838 - val_loss: 1.1744 - lr: 7.7086e-06\n",
      "Epoch 322/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5586\n",
      "Epoch 322: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 322: ReduceLROnPlateau reducing learning rate to 7.55517960897123e-06.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 0.5586 - val_loss: 1.1735 - lr: 7.6315e-06\n",
      "Epoch 323/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5806\n",
      "Epoch 323: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 323: ReduceLROnPlateau reducing learning rate to 7.479627965949476e-06.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.5804 - val_loss: 1.1726 - lr: 7.5552e-06\n",
      "Epoch 324/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5796\n",
      "Epoch 324: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 324: ReduceLROnPlateau reducing learning rate to 7.4048317583219615e-06.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 0.5781 - val_loss: 1.1730 - lr: 7.4796e-06\n",
      "Epoch 325/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5789\n",
      "Epoch 325: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 325: ReduceLROnPlateau reducing learning rate to 7.330783332690771e-06.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 0.5801 - val_loss: 1.1739 - lr: 7.4048e-06\n",
      "Epoch 326/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5695\n",
      "Epoch 326: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 326: ReduceLROnPlateau reducing learning rate to 7.257475485857867e-06.\n",
      "66/66 [==============================] - 5s 75ms/step - loss: 0.5743 - val_loss: 1.1744 - lr: 7.3308e-06\n",
      "Epoch 327/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5716\n",
      "Epoch 327: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 327: ReduceLROnPlateau reducing learning rate to 7.184900564425334e-06.\n",
      "66/66 [==============================] - 5s 75ms/step - loss: 0.5705 - val_loss: 1.1733 - lr: 7.2575e-06\n",
      "Epoch 328/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5782\n",
      "Epoch 328: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 328: ReduceLROnPlateau reducing learning rate to 7.113051365195133e-06.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 0.5779 - val_loss: 1.1730 - lr: 7.1849e-06\n",
      "Epoch 329/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5668\n",
      "Epoch 329: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 329: ReduceLROnPlateau reducing learning rate to 7.041920684969227e-06.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 0.5666 - val_loss: 1.1737 - lr: 7.1131e-06\n",
      "Epoch 330/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5727\n",
      "Epoch 330: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 330: ReduceLROnPlateau reducing learning rate to 6.971501320549578e-06.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 0.5736 - val_loss: 1.1730 - lr: 7.0419e-06\n",
      "Epoch 331/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5754\n",
      "Epoch 331: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 331: ReduceLROnPlateau reducing learning rate to 6.901786518938025e-06.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 0.5742 - val_loss: 1.1716 - lr: 6.9715e-06\n",
      "Epoch 332/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5587\n",
      "Epoch 332: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 332: ReduceLROnPlateau reducing learning rate to 6.832768626736651e-06.\n",
      "66/66 [==============================] - 5s 74ms/step - loss: 0.5613 - val_loss: 1.1745 - lr: 6.9018e-06\n",
      "Epoch 333/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5775\n",
      "Epoch 333: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 333: ReduceLROnPlateau reducing learning rate to 6.764440890947299e-06.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 0.5791 - val_loss: 1.1713 - lr: 6.8328e-06\n",
      "Epoch 334/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5729\n",
      "Epoch 334: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 334: ReduceLROnPlateau reducing learning rate to 6.696796558571804e-06.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 0.5710 - val_loss: 1.1715 - lr: 6.7644e-06\n",
      "Epoch 335/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5823\n",
      "Epoch 335: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 335: ReduceLROnPlateau reducing learning rate to 6.629828426412132e-06.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 0.5835 - val_loss: 1.1725 - lr: 6.6968e-06\n",
      "Epoch 336/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5673\n",
      "Epoch 336: val_loss did not improve from 1.17024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 336: ReduceLROnPlateau reducing learning rate to 6.563530191669997e-06.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 0.5679 - val_loss: 1.1715 - lr: 6.6298e-06\n",
      "Epoch 337/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5725\n",
      "Epoch 337: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 337: ReduceLROnPlateau reducing learning rate to 6.4978951013472395e-06.\n",
      "66/66 [==============================] - 5s 75ms/step - loss: 0.5715 - val_loss: 1.1743 - lr: 6.5635e-06\n",
      "Epoch 338/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5746\n",
      "Epoch 338: val_loss did not improve from 1.17024\n",
      "\n",
      "Epoch 338: ReduceLROnPlateau reducing learning rate to 6.432915952245821e-06.\n",
      "66/66 [==============================] - 5s 73ms/step - loss: 0.5727 - val_loss: 1.1744 - lr: 6.4979e-06\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAD7CAYAAACL+TRnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA5sklEQVR4nO3deZwcdZ3/8VdV9Tn3PZPJnZB8k5BAEkKAHHIqbrhkBUQUZUFUFlx1BUUQ5PixeCy4HqASiCCuIi7qygKCiAIhIASScOabkPuYJHOf3dNH1e+P6hkmkzl6MlfX5PN8PHgwXVVd/e6ayae+9a3jaziOgxBCiLHNHO0AQgghhp8UeyGEOAJIsRdCiCOAFHshhDgCSLEXQogjgG+UPz8IHA9UAclRziKEEF5hAeOA14D2dN4w2sX+eODFUc4ghBBetRxYnc6Co13sqwDq61ux7YFf719cnENtbcuQhxpOXswM3swtmUeGZB4ZXTObpkFhYTakamg6RrvYJwFs2zmsYt/xXq/xYmbwZm7JPDIk88joIXPa3d9yglYIIY4AUuyFEOIIMNrdOEKIEeQ4DvX11cRiUWBoujEOHDCxbXtI1jVSvJHZIBAIUVhYimEYg16bFHshjiAtLY0YhkF5+QQMY2gO7H0+k0Qi0wvnwbyQ2XFsGhpqaGlpJDe3YNDrk24cIY4gkUgLubkFQ1boxfAxDJPc3EIikaG5aijtlr1S6j+BEq31Zd2mzwfuB/KAF4Avaq0TQ5JOCDGkbDuJZckBvVdYlg/bHpr7TdPavSulTgc+28vsXwHXaK1nAgZw5ZAk60di53p2r/wazhBtCCGOFEPR/ytGxlD+rvot9kqpIuAO4D96mDcZCGutX0lNehC4cMjS9cFu2EfswHZIxEbi44QQQ+yuu77LZZddwqc/fSGnnHIil112CZdddglPPPGntNdx2WWX9Dl/9ernuf/+nw02KnfccQtPPvn4oNczmtI5nvs5cCMwsYd5lRx8B1cVMGGgIYqLcwb6FhrzsmgHiovCWOHcAb9/NJWWeitvBy/mlswHO3DAxOcb+v76w1nnN77xTQD27t3Lv/7rlfzqV48MeB39veeUU07llFNO7XHeQDIbhoFpGsOy7fpjmmbn38Rg/jb6LPZKqc8Bu7TWf1VKXdZTDg6+fssABnyKu7a2ZcB3s8Va4wDUVDdhhgf6iaOntDSX6urm0Y4xYF7MLZkPZdv2kF+FMtgrW5JJ971d13HBBecwZ85cNm/W3Hvv/Tz66G94/fXXaGpqoqSkhNtuu5OiomKWLVvE6tVreeCBn1NTU82uXTvZv38fZ599Hp/97BU8+eTjrFv3OjfeeAsXXHAOZ565gldffZloNMqNN97KrFmz2br1fe6441aSySTHHjufV15Zw29/+8eDMjqOe5d/ImHzxBN/4pFHfoVhGCg1m69+9esEAgHuvPNWtm7dAsD551/IueeezzPP/Jlf//qXmKZJZWUlN910O8FgcEDbx7ZtqqubD/rbME1jwI3k/lr2nwDGKaXWA0VAjlLqB1rrr6bm78Z98lqHCmDvgBIcLtNy/y999kIclpfeqmL1m2k/WqVXhgHdh7Jedsw4ls4b1/Mb0nTiiUu47bY72b17Fzt3budnP1uFaZrcfvvNPP30U3zyk58+aPn339/MvffeT0tLMxdd9DH++Z8vOmSd+fn5rFz5S37/+0d5+OFV3HHH9/l//+8Wrrzyi5x00jJ++9v/JpnsvaZs2fI+v/zlKu6770Hy8wu4667v8otfrGTJkmU0NTXxi1/8mpqaan760x9z7rnns3LlT7nvvl9QWFjEPff8kJ07tzNjhhrUdjlcfR6TaK0/rLWeq7WeD9wM/KlLoUdrvQOIKqWWpiZdCjw1XGG76rx0zMnsa2WFEIdnzpy5AEyYMJFrrvkqjz/+R3784x/wzjtvEYm0HbL8woWL8Pv9FBYWkZeXR2vroZcsnnDCEgCmT59OU1MTTU2N7NtXxUknLQPgrLPO6zPT+vWvs3TpcvLzCwA499zzef31V5k2bTo7d+7g3//9Gp577lmuvvrLACxdupyrrrqCe+/9ISeffNqoFXo4zJuqlFJPAjdrrdcCnwJWKqXygDeAHw1hvt5Jy16IQVk6b/Ctbxi+G5Q6ujs2bnyPW265kYsvvoRTTz0dyzJxuh9KAIFAoPNnwzD6Wcadb5pWj8v15tDuZodkMkl+fgEPP/wor732D15++SUuv/zTPPzwo3zlK9fy/vvn8fLLq7n99pu4/PLPc+aZK9L+vKGU9tkGrfWDHdfYa61XpAo9WusNWuvFWutZWutLtNZpPUh/0MxU9Iy/5VkIMRjr17/OggXH8bGPXcDEiZNYs2b1kD3qICcnh/HjJ/Dyyy8B8Je//LnPyx0XLDiO1atfoKmpEYA//emPLFiwiNWrn+f2229myZJlfOUr1xIOhzlwYD8XX3w+BQUFXHrpv/DRj57Fpk16SHIfDu/eXWG4LXvHkZa9EGPZ6ad/hBtuuI7PfOYTACg1m6qqoTs1+K1v3cqdd97GypX3Mn36jD5PoB511AwuvfRfuOaaz5NIJFBqNtdd900CgSB///tzXHrpRQQCAc48cwXTpx/FFVd8ga985WqCwSCFhYXceOMtQ5Z7oIyBHMIMgynAtsO5Gie+bS3Rv/yErI/fhlU8aVjCDQcvXiEC3swtmQ+1b98OKiomD+k6vfCcme66Zv7FL1ZyzjnnU1JSwvPPP8czzzzFHXd8f5QTfqDjd9bL1ThTge3prMezLXvD6Oiz99YfmRAis5SXV/DVr/4rPp+P3Nw8rr/+ptGONCw8W+w7T9BKN44QYhBWrDiHFSvOGe0Yw867j75LnaB1pGUvhBD98nCxl0svhRAiXd4t9nJTlRBCpM2zxd6Qlr0QQqTNs8X+g5a9FHshhOiPd4t9qmUvJ2iF8KarrrqCZ599+qBpkUiEFStOp6Ghocf3dDxXvqammmuv/bcel1m2bFGfn7t37x7uuONWADZufJfvfOf2gYfv5oEHfs4DD/x80OsZTh4u9h2PS5CWvRBedNZZ5/LMM38+aNrzzz/HwoWLKCgo6PO9JSWl/Od/Ht5juPbtq2L37t0AzJo1Z8xeV9+dd6+zN6TPXojBiG96ibh+YdDr6emhY371Ifwzl/byDtdpp32Ye+75IU1NjeTl5QPw9NNPctFFl7Bu3evcd9+9tLdHaW5u4d/+7assX35K53urqvbypS99gf/5n8epqtrLbbfdRCQS4eij53YuU119gDvvvJ2WlmZqaqpZseIcPve5L/LDH/4ne/fu4a67vsupp57OqlX38ZOf3MfOnTv43vfuoLm5iVAozFe+ci2zZx/NHXfcQnZ2Dlq/R01NNZdd9jnOOuvcXr/XSy+9yMqVP8VxbCorx3PddTdQVFTMT37yX7z22j8wTYPly0/h8ss/z9q1r3LvvT/CMAxyc3O55Zb/6HdHd7g827I3TLkaRwgvy8rKYvnyk3nuuWcBqKmpZufOHSxefCKPPfZbrr/+Jlat+m+uv/5brFz5017X84MffI8VK87hwQd/zbx5x3ZO/8tfnubDHz6T++57kF/+8rc8+uhvaGho4MtfvpZZs+bwta9946D13H77TVx44cU89NAjfOlL/863vvUNYjF32NMDB/Zz7733853v3M099/yw1yz19XV8//v/wZ13/icPPfQI8+Ydy913f499+6p45ZU1PPTQb/jpT1exffs22tvbeeihB7juum/ywAMPc/zxJ7Bp08bBbNI+ScteiCOUf+bSflvf6RjMs3FWrDiH++//GR/72Md55pmnOPPMFViWxU033c6aNS/yt789m3p+faTXdaxb9zq33HIHAB/5yD919sFfcsmlvPHGWn7964fZtm0LiUScaLTn9bS1tbF7925OPvk0AObOnUdeXh47d+4AYPHiEzAMg2nTpnc+8bIn7777DrNnH824cZUAnHvuP/Pwww9SUlJKMBjkqqsuZ8mS5Vx11ZcIBoMsW/YhbrjhOpYvP5nly0/m+ONPHPhGTJNnW/adJ2ilZS+EZ82fv5Da2hr279/H008/1dk9cvXVV/Lee++g1Cw+85nL+3nmvNH5IEV3rFi3Nvz4xz/gd797hIqKcXz2s1eQn1/Q63p6qiOOQ+eoVYFAsHP9fem+Hsdxn3fv8/m4774H+dznrqKxsZEvfvFf2LlzB5/4xKf48Y9/zoQJE7n33h/x0EMP9Ln+wfBwsZcTtEKMBR/96Fn88peryMvLY/z4CTQ1NbJr1w6uuOKLnHjiUl588fk+n1+/aNFinn76ScA9wRuLuUNqrF37Dy655FJOO+0Mdu7cQXX1AWzbxrJ8hww9mJ2dQ2XleJ5//jkA3n77Lerqapk2bfqAvsucOXN59923Oh/B/Kc//Z6FC49j06aNXHPN5zn22AVcc81XmDJlGjt37uDKKz9LW1srF110CRdddIl04/TIlKdeCjEWrFhxDhdccA7f/ObNAOTl5XP22edx6aUX4fP5WLjweKLRaK9dOf/+71/n9ttv5k9/+gOzZs0mKysbgE9/+jJuv/1mgsEgZWUVzJo1h7179zBzpqKlpZnbb7/poGEIb775dr7//f/ggQd+jt8f4I47voff7x/QdykqKua6627khhuuJR5PUFFRwfXX30xJSQlz5x7DZz7zCUKhEPPmHcuJJy4hFApxxx23YlkWWVlZfOMb3zrMrdg/zz7P3mlvpeWhqwme9EkC884clnDDwYvPWAdv5pbMh5Ln2bu8lHmonmfv4W4cadkLIUS60urGUUrdBlwAOMADWuu7u83/NnA5UJ+atFJrfc9QBj1E6nEJMiyhEEL0r99ir5Q6GTgNOAbwA+8qpZ7QWncdOXcRcLHW+uXhidkDadkLcVgcx+n3qhKRGYaym73fbhyt9fPAqVrrBFCGu4No7bbYIuAGpdSbSqmfKKVCQ5awN4ZcjSPEQJmmRTKZGO0YIk3JZKLzUtLBSqvPXmsdV0rdCrwL/BXY0zFPKZUDrAOuAxYCBcCwP2zCMAy34EuxFyJt4XAOzc0Ncn+KBziOTXNzPeFwzpCsb0BX4yilsoDHgd9qre/rZZkFwCqt9YI0VjkF2JZ2gG62fedi8hafRfFplx7uKoQ4oti2za5du2htbWV0L8QT/TEMyM7OZuLEiZhmr+3ytK/GSafPfhYQ0lqv11q3KaV+j9t/3zF/EnCG1npVR0Ygns6HdzicSy8BsCzaWiLYHrq8zouXA4I3c0vmnmVlFZGVVTRk65PtPLxqa91e814uvUxbOlfjTANuVUotw70a5zxgVZf5EeB7Sqm/4e5hrgb+MKAUh8kwLXkQmhBCpCGdE7RPAk/g9su/DqzRWj+ilHpSKbVIa10NfAG3e0fjtuzvGsbMADS2xojbSJ+9EEKkIa3r7LXWtwC3dJu2osvPjwGPDWWw/rz67n5mR238CbmyQAgh+uPZO2hN08B2jEMeaCSEEOJQni32fp+JjYkjxV4IIfrl3WJvmdgY2FLshRCiX94t9j6TJCa23A0ohBD98myx9/lMbMfAkatxhBCiX54t9m6fvRR7IYRIh3eLveV240ixF0KI/nm32PtMbMfEScodtEII0R9PF/ukdOMIIURaPF3sbUyQkaqEEKJf3i32VscJWunGEUKI/ni22PtSffbyIDQhhOifZ4u9ezWOIY84FkKINHi32Hf02UvLXggh+uXZYm+ZBg6GnKAVQog0eLbYG4aBY1gY0o0jhBD98myxB8A0pc9eCCHS4O1iLy17IYRIi7eLvWlKsRdCiDR4vNhLy14IIdKR1oDjSqnbgAsAB3hAa313t/nzgfuBPOAF4Ita62EfVcQwLQx5EJoQQvSr35a9Uupk4DTgGGAR8CWllOq22K+Aa7TWMwEDuHKog/bItDDl0kshhOhXv8Vea/08cGqqpV6GezTQ2jFfKTUZCGutX0lNehC4cOij9sD0YyHFXggh+pNWN47WOq6UuhW4FvgdsKfL7EqgqsvrKmDCQEIUF+cMZPFOjuXDxKGkOAvDtA5rHaOhtDR3tCMcFi/mlswjQzKPjMFkTqvYA2itv62U+i7wOG43zX2pWSZuX34HAxhQR3ptbQu27fS/YDeG5Qegel8dhj804PePhtLSXKqrm0c7xoB5MbdkHhmSeWR0zWyaxoAbyen02c9KnYBFa90G/B63/77DbmBcl9cVwN4BpThcVmpflRz2c8FCCOFp6Vx6OQ1YqZQKKqUCwHnA6o6ZWusdQFQptTQ16VLgqSFP2gPDFwDAScZH4uOEEMKz0jlB+yTwBLAOeB1Yo7V+RCn1pFJqUWqxTwE/UEptBHKAHw1X4K4Mn9uNgxR7IYToU7onaG8Bbuk2bUWXnzcAi4cyWDo6ir207IUQom8ev4O2o2UvffZCCNEXTxf7jqtxpBtHCCH65uli33E1jnTjCCFE3zxd7Dta9nZCir0QQvTF08WejhO0UuyFEKJPni72nS37eGyUkwghRGbzdrFPtext6bMXQog+ebrYY0k3jhBCpMPTxd6UPnshhEiLp4t9ZzdOQvrshRCiL94u9lbHg9DkDlohhOiLp4u95fORdAwcadkLIUSfvF3sLYMElrTshRCiH54u9qZhkHBMeVyCEEL0w9PFvqNlLw9CE0KIvnm72JsGcceSSy+FEKIfHi/2ptuyt6XPXggh+uLpYm+abp+9dOMIIUTfPF3sLVP67IUQIh1pjUGrlPo2cFHq5RNa66/3MP9yoD41aaXW+p4hS9kLy3L77GVYQiGE6Fu/xV4pdQbwEWAB4AB/Vkqdr7X+Q5fFFgEXa61fHp6YPXMvvbTAlpa9EEL0JZ2WfRXwNa11DEAp9R4wqdsyi4AblFKTgReAa7XW0SFN2gP3BK2JISdohRCiT/322Wut39FavwKglJqB253zZMd8pVQOsA64DlgIFAA3DUfY7qQbRwgh0pNWnz2AUupo4AngOq315o7pWusWYEWX5e4CVgE3prvu4uKcdBc9SOPuBmL4sexGSktzD2sdo8FLWbvyYm7JPDIk88gYTOZ0T9AuBR4DvqK1fqTbvEnAGVrrValJBjCgTvTa2hZs2xnIWwD3apyo44d4hOrq5gG/fzSUluZ6JmtXXswtmUeGZB4ZXTObpjHgRnI6J2gnAn8EPqG1fq6HRSLA95RSfwO2A1cDf+hhuSFnmQYRx4+VbMdxbAzD01eSCiHEsEmnZX8tEALuVkp1TPsZcC5ws9Z6rVLqC8DjQABYDdw1DFkPYZoGUcd9pj3xKASyRuJjhRDCc/ot9lrrLwNf7mHWz7os8xhuN8+IskyTqJ0amjAWwZBiL4QQPfJ0v4fbjZMarSoWGeU0QgiRubxd7K3UCVqk2AshRF88XexN84NiT6xtdMMIIUQG83Sxt0xTWvZCCJEGTxd7U/rshRAiLZ4u9lbXbpy4FHshhOiN54t9DB8OhrTshRCiD54v9mCQsII4coJWCCF65elib5oGAAkziBMb9icqCyGEZ3m62BuG4Q5gYobk0kshhOiDp4s9pAYdNwPSZy+EEH0YA8Ue2s0snGjLaEcRQoiM5flib5kG7WYYJ9o02lGEECJjeb7Ym4bR2bJ3HHu04wghREbyfLG3TIOomQWODe1yklYIIXri+WJvmgZRIwyALV05QgjRI88Xe8s0iKSKvRPx1piSQggxUjxf7A3DIGK4I1Q5EWnZCyFETzxf7A9q2UelZS+EED3xfLE3TYM2QoAUeyGE6E2/A44DKKW+DVyUevmE1vrr3ebPB+4H8oAXgC9qrRNDmLNXlmmQdEwIZEmfvRBC9KLflr1S6gzgI8ACYD5wnFLq/G6L/Qq4Rms9EzCAK4c4Z69M0yBpOxjhPOmzF0KIXqTTjVMFfE1rHdNax4H3gEkdM5VSk4Gw1vqV1KQHgQuHOmhvLNPAdhzM7ELs1rqR+lghhPCUfrtxtNbvdPyslJqB252ztMsilbg7hA5VwISBhCguzhnI4gcJBnxYlkm4uJzIjrcpLc097HWNFC9k7IkXc0vmkSGZR8ZgMqfVZw+glDoaeAK4Tmu9ucssE3C6vDaAAT23oLa2Bdt2+l+wm9LSXJJJm2h7gpgvl2RzHQf2N2KYmXveubQ0l+pq751b8GJuyTwyJPPI6JrZNI0BN5LTqopKqaXAX4HrtdYPdZu9GxjX5XUFsHdAKQbB6uizzy4Cx8ZpaxipjxZCCM9I5wTtROCPwCVa60e6z9da7wCiqR0CwKXAU0MZsi9mR599ThEAjvTbCyHEIdLpxrkWCAF3K6U6pv0MOBe4WWu9FvgUsFIplQe8AfxoGLL2yDQNbNvBSBV7u6UOq3ykPl0IIbwhnRO0Xwa+3MOsn3VZZgOweAhzpc0y3G4cM7ujZV87GjGEECKjZe6ZzDR1tOwJZIE/jN1cM9qRhBAi44yZYm8YBmbBOOz6ETs3LIQQnuH5Yt9xNQ6AWTgeu37PKCcSQojM4/li77NMEkn3sn6rsBIn0iSDjwshRDeeL/Z+n0k84RZ7s3A8AElp3QshxEG8X+ytLsW+yC32dt3u0YwkhBAZx/vF3m8ST3XjGNlFGKFcktVbRzmVEEJkFu8X+1TL3nHcK3Ks8qNI7nt/tGMJIURG8X6x97lfoeMkrVk+A6dpP3Zb42jGEkKIjDIGir0F0Nlvb1XMACC5T49aJiGEyDRjoNi7X6Gz2JdNw8gqIK5Xj2YsIYTIKJ4v9oFuxd4wLfyzTia56y3spurRjCaEEBnD88W+s2Wf/GC8FL9aBjgktr8+SqmEECKzeL/YW+5XiMU/KPZmbilm4QQSOzeMViwhhMgo3i/2PbTsAXyTjyVZtQk76q2hx4QQYjiMnWKf6FbsjzoJgPbVv8RxBj6+rRBCjCVjoNgffOllB6toAoHjziOx9TVsuaNWCHGEGwPFvueWPUBg7ofBHyL27nMjHUsIITLKGCr2yUPmGYEw/hlLSGz5hzz2WAhxREtnwHFSA4mvAc7WWm/vNu/bwOVAfWrSSq31PUMZsi8dV+P01LIH8M85lfi7zxHXLxI49p9GKpYQQmSUfou9UuoEYCUws5dFFgEXa61fHspg6ertapwOVtFErIqZxN7+C/5ZH8IIZo9kPCGEyAjpdONcCVwN9Da46yLgBqXUm0qpnyilQkOWLg199dl3CJ74CZy2RqIv/EKuzBFCHJH6LfZa689prV/saZ5SKgdYB1wHLAQKgJuGMmB/0in2Vtl0gosvILFtLfG3nh6paEIIkTHS6rPvjda6BVjR8VopdRewCrhxIOspLs457AwV5XmYBvgDPkpLc3tdzjn9AvY3bKftlUfILS4kb/4Zh/2Zg9VXzkzmxdySeWRI5pExmMyDKvZKqUnAGVrrValJBhAf6Hpqa1uw7YF3r5SW5lJT04LPZ9LYFKW6uu+7Zc3lV2I1NVD73H8TLZ+P4QsO+DMHq7Q0t9+cmciLuSXzyJDMI6NrZtM0BtxIHuyllxHge0qpqUopA7dv/w+DXOeAdR2Hti+G6SOw6J9xIk20PfZtEnveHYF0Qggx+g6r2CulnlRKLdJaVwNfAB4HNG7L/q4hzJcWv88knjz0Ovue+MYpgss+gwNEnv4hydpdwxtOCCEyQNrdOFrrKV1+XtHl58eAx4Y21sAEfFZaLfvO5eechm/KQtoe+zbR535G1vk3j0qXjhBCjBTP30ELbss+NoBiD2BmFRA65XPYDXtp+/0tRJ7+IU4iNkwJhRBidI2JYu/zpddnf8j7Js4jdMqVYFokdqyj/R+/HYZ0Qggx+gZ1NU6m8B9msQfwz1iCf8YSomt+TfztZzD8YXxTj8MIZmPmlQ1xUiGEGB1jotiHAz6a2gbXBRM88WJIRImt/z9i6/8PgMDiCwjOP3soIgohxKgaE8U+N8vP3prBPdXSME2Cy/8F3+SFOO2tJLa/Qey135PY/gaBuR/GLJqAWTgBwzCGKLUQQoycMVPsm9sGfC/XIQzDwDd5PgC+yfOJPHsPdnMN0ed+npq2AGviPAxfwB0JyzCl+AshPGGMFPsAsYRNeyxJMGANyTqNYDZZZ30dJ9FOYvsb2I0HiL35FIkd6wCwNr6A3daIWVBBaNlnMHOKh+RzhRBiOIyNYh/2A9DcFiMYCA/pug1fEH9qPNvAvA/jtDWS3LeZ6Au/AH+QZGsdrY98A6tiBlb5UZilU7EKK0nu34ITbcY3bTFGVgGGOSYufBJCeNTYKPZZAQCaI3FKCoa22HdlBLIwAlmYBeMwcoowwvkYwSxibz1DskoTW/8EOAdfFdT+yiOYhROwKmeB5SP+ofNw2m15rr4QYkSNkWL/Qct+pPgmzO38OXTSJwFw4lHs+j0k9rwHiXas8XOwa3cSf+/vxN9/GeJRdr35NOBghHIxCyuxxs3CbqjCyC7EKj8K7CRW5WzMrHwSVRq7bhf+2af1e2Tg2DYYYBj9H0G0r38Sq2gCZl4pkWd+RHDppfjGzxnU9hBCZLYxVuwHf5J2MAx/CKtsOlbZ9A8mVs4mMO9MAOyGKnw71hCxAzjNtSS2rSVZtQkjrwxnxxsfPGvf8mGVTSdZtQlwiL35NEZWPoY/BKaF01qPmVuCkVeO01yN3bAPu3Efhj+EWTrFPQIJukchRlYBRnah+zgIO4mTjBF79VGw/FilU7Ebqog8dTeBheeCnSB5YCtOeytOrA3f5AUEZp9KsnYnDZsbSBbOwIlFMYLZ7rkLwyQw/yyIteHYSZz2FpI7NuCbvAAjlAPBLAzDxHEckvs3Y+aWYmYXkqzbQ2LzS/jnnYmZld/vdrVbanEiTVilUw+a7iRiJHasw67bTbJKEz7zy2D5MXyBw/4dOrYN8YgceYkxZ4wU+1Q3zigX+/6YBeMonnFZ52NKnRMvwom3Y2YX4iTj2LW7AIf4ppdI7t1I4Nh/wsyvILFzA06sDScWgWQcI5xHsn4Pzo71GLmlmAUV+CfOw2lrwG7aj91c4xbg9lawexiIPZjjFuZ9m7DGH01yzzvE1v4eMDBLJmGE8zACWcTf/DPxN/8MQLSX75R4/xXshipwPviczjuRDQMjmAOGgRNpcgtxIIwTi0IyRuzd5zD8IYxAFtbEeTitde4y4TwM00d802rMognYNTtw2lsxC8eDYeCbeAxGdgGJ7etI7nmn83Nb/vurGKFcAvPPxvAFiLRNJLZzm3vy3E6CY7s7iG2vY7fW4auYiePYGKE8fBUzMHKKiP5tJcnqrYSWXkpizzuYOSXuUVflLIxQLolNL+FXyzFCOSTr9oABTkstpI6ooqsfJnDMmW7GnOKDrtZy2lshEO48+nIcB/vAFuz6ve65nuKJAO7vtr0Vq2RK547LbmvArt+LVTL5sHZEdlsDhi+AEchKa3kn0S7PixpjxkSxDwUsfJYxot04Q6HjHACAYfmxyqYBHHxkAPhnfajH9zuO0+eln47j4LS34LTWQ7wdTBO7oQqzeBJOWwNYAaxxCqe1zs1hJ90Weeq9iW2v4cQiWEUTKR4/jgNvvYqZU4wTi7gt9ANbSOxYj79yFmZ+OSQTWOMUyQNb3cIabcaJNoOdxCydhl2/B5LuDtk39TgS29/AiUVI7n+f+Dt/xcwtwUnG3R1DMo5VMdPNFs5LZW50z5FseNI9N2KYBBZfiFV+FInNa9z1xaO0r34IgKretns4DzO/gtiGp8A0wU7S+ZdjmG7Rf/4BMK0PdpaWHyOUi9NaR+ytpzHzykju23Toyk0f7at/STvuTtXIK8XMLiJZvxuncT9m8WTM/DLwBbDr92JXb+v4YIzsAiKBIIn6fe6kQBjfhHk47a0k922GZAwsP/6ZSzGyCt1nzAJmQSV2QxXJA1vcHZvlh2Qcu243+IM4LfXY9bvBMPFNXuD+Tiy/uxNub+3cudsNVRiWH0yLZNVGzIJKd152AXb9ntTfiI2RU+TuuEyLwJxTibQV0v7OWpxEDMO0cJKJ1JFlGKe9zf08XxCrbCqG6SfZUIUZzsXIygfbJlG1ESfajFU+AyOYjRNpxG484O6cwnmpnZvjHj221OFEmjDCuRihPPf/4TwMy4/dXO02LhzH/Ux/EJJxnGgrRnah++8AByfRTkNOmPa6OpxoK1i+1FGz6X5+Iu52m5o+9+/DsDAsn/v3YFo40RacaLO7MwyEO4+4sZPuTtIwwRcEw3DXaZg4sQhGdhEkY+4zuFLP4fJNPW7EdqrGKI/JOgXYNpjBSzpayV+75yXmTCnkirMyu+/Zi4MmwPDmdmwb7ERnK9ZJxLCbDmAWVvZ4DsKJR93CkjpSAHfnhJOEWNTdyeCQZ7TSZGfjpFq1qQUxy6ZjmCZ2S51bkJJxkvs24zQdwDd5PkZOEXbtLoy8MpJ73sXwh0hsfx27YR++GSe5O5WWWnwzl0OszS1s+RU4rfX4piwkWb3NPcqq2eF2QTXXuEcHFTOJb37JPQpIFUT/0Wfgq5xFfOML2M01+O0IibwJ+CpmEt/2Gsm972FkFWAVT8Y3ZQGJ7W+460gmDtkuZsE47LYGSCbB58fMKXF34OFcfBOPwW7aT2Lnm+5ONdKE4ziYeaU4kUacaCtmwTg3d0MV/lknu8tEm7Fbat2dvJ3AMH3YrfUYoRycVPca4H4nywe27f4/3nEsaLgFNB4F+9DMbnAf+AIQa/tgmi/ofken25GpYWAEst2dDYOsXabVWdxJxA79rGFnED776/gqZ6e1dC+Dl0wFtqf1aWOl2N/+0FrCQYtrL14wtAmHmBT7kTNWMzuxSKq4uq1Ju24PRnah2x2Y+vd8uDf7OY4N7W2dR3h9LhuPkqzeRn5ekGaz+KDzL46dgFjUbfmaFk6iHbvxANgJzNxSd4fc3opjJ7FKprgt5rZ690gjkIWZV9aZxWlvdb9vquVt+EM4tu0etUaacaJNEIti5JW628axMbMK3B2Mz+/uHJqrMXKK3QLvD1FSnEVNQ/zgbrbU5+ELuEeOyYQ7zU6m/kvg2Ek3QygXEjGcWAQnEXV3coaJ4Q+CbaeeoGu705Nx9wgr0uw2OnwB9/+BLMxwXtq/m8EW+zHRjQNQVhhmy57G0Y4hxLAzut5LYvo6u//g8Iv8B+83IY1CD+4FCb7K2WSV5tLabQdlmL6D1mP4gp3nJIAedyZGbinklh6SpcdlTRMjnAfhPGB8/2G7XQhgBsIYxsFHGod8d39nT1nPAuGDfxcZbszc6VNaEKKuqZ1E8vCefimEEGPZGCr2YWzHoa6pt+tGhBDiyDVmin1Z6s7ZAw2RUU4ihBCZZ8wU+9JUsa9ukJa9EEJ0l9YJWqVUHrAGOFtrvb3bvPnA/UAe8ALwRa11L9dYDZ+C3CB+n8n+urb+FxZCiCNMvy17pdQJwGpgZi+L/Aq4Rms9E/fk9ZVDFy99pmEwuSKXLXvlihwhhOgunW6cK4Grgb3dZyilJgNhrfUrqUkPAhcOWboBmjEhn+1VzcTiI31zhBBCZLZ+u3G01p8DUEr1NLuSg+9KrwImDDRE6uaAw1Jamtv586Kjx/HUKzupiySYV1lw2Oscbl0ze4kXc0vmkSGZR8ZgMg/2piqTg+9ZNoABX+g+FHfQApTmBDANgydf3EpFXmY+xMmLd3WCN3NL5pEhmUdGL3fQpm2wV+PsBsZ1eV1BD909IyUn7OfMEyay+q0q9M760YohhBAZZ1DFXmu9A4gqpZamJl0KPDXoVINw3tKp5Gb5eea1XaMZQwghMsphFXul1JNKqUWpl58CfqCU2gjkAD8aqnCHI+C3WH5MJevfr5EbrIQQIiXtPnut9ZQuP6/o8vMGYPHQxhqc0xaO52/rdvPz/32H6z+1EL9vzNw7JoQQh2VMVsGivBCXr5jNtqomfvPsJpK2PBxNCHFkG5PFHuA4VcZHF0/i7+v3csuq12jy2ChWQggxlMZssQe48NTpXPWxuRxoiPD936yTu2uFEEesMV3sDcPg+FllXPPP82iLJvjh796kLTrij+0RQohRN6aLfYd504r50sfn0RKJ88AT77J24wG272tilIdkFEKIETNmhiXsz5SKPD62bCpPvLKDdZtrAFg8u4wrzppNXVM75UVZo5xQCCGGzxFT7AHOXTaVDx8/keqGCOs21/C/q7exZU8TtU1RPvXhmWQFfZw0t2K0YwohxJA7ooo9QDjoY1J5LpPKczlQH+Hld/YB8N9/2QTA3tpWKkuymX9UCeHgEbd5hBBj1BFdzT79kZnMnJiPzzJ5YYP7SJ8nXt4BwNxpRVx13lziSZu8rMBoxhRCiEE7oot9OOjj5PnjAVg6bxy249DYEuPFDXv54+ptXP2DFwA4/bgJfOT4iZTkh3AcMAz3/X/+x04mlOUwb1rxaH0FIYRIyxFd7LszDYPC3CAfWTyRl96uYnxJDnnZAf76+m7++vpu8rMDOLjPcT72qGJe2FCFAZy9ZAq1TVHOXjKF7JCPnLAfo2OPIIQQGUCKfQ9CAR/f+cJJnQX79OMmsHl3A1v2NBJP2FTVtfHChioWzCghaTs8vmY7AP94dz9J26EkP8RxqhTTNHh7ax3jirOob27nnCVTOLU0F8dxSCQdeWaPEGLESLHvRdeW+cSyHCaW5XDaQncQLttxaIsmyA75iCVsnnltF+WFYda8vY/p4/PZtKuBZ9fuBmBSeS5rN1bj4PCz/32HN7fVs3F7LXtr2sjPCZCXFeDjJ0/jmbW7OHpKEWcunoTtOOzY10xtY5TZUwrJDvlHZRsIIcYOKfaHwTQMcsJuAQ76Lc5ZMgWAxbPLO5dJJN2Hr/ksk/rmdqKxBA8+tZE39AEKcwKcftwEWiIxNu9u5O5HNwDw9tY6/nf1NizToDV1p69lGpQVhskK+QgFfMw/qoTWSJxlx4wjFLDYVxdhWmUeLZE4++vbCPotQgGL4rzQIV1JdU1R2uNJxhVnD/cmEkJkGCn2w8RnfdBFU5gbBIJ889PHHTIcWms0zqon3uOo8fmEgj6qaluJxZPMmFBAWWGYDe/Xsr+ujZZInJqGSOclomve2UdLW5y29gTHTi9m0+4GIu0fDLR+zPRiTMMgkbTZV9fGqQvG8/yGvbS0xVl+7Dje3lrHiUeXc8qC8bywfi/F+SHe3V7HsmMq2bW/mVmTC2WnIMQYIsV+lGWH/Hzp48f0On/GhILOn23bYc3b+4jEEjz63PscM72Y/OwAb2yuYdakQpbNG0fSdti+r5knX9lBfk6AUMBHYW6Q3/19C+CeXH761V1MKM3msee38vd1e6htau/8jBc2uOPHW6bBlHG5WKZJPJFkxqQiWlvbWX5sJdUNEV55Zx+FuSGmjc8jLytAcV6IgN8kPzvIhi01bHi/hrlTiwn4TaobIiyeXU5Jfoh9dW0kbYfxJdkcaIiwv66N2ZML2bq3iZywn8qSbAzDoC2aICvU/59nfXM7O/c3c8z0YjkpLkQfjFF+PswUYNtQDTjuBUOVOZ6w+zzBW1XbSkl+GL/PxHEcnl27mwMNEWZMyAdg0awyfv/8Vp56ZQcnz6+krT1BZUk2W/Y08fGTp7H6zSr21raSTLq/l+37mzGAaMw9eqgoyqKmMUIi2fPvLTvk6+yKAnd84Oywn/11bQBMqcilqq6N9liSYMCiPbXeE+aU09Qa470d9Uwdl0fQb1KcF2JvbRvNbTEmluUQT9o0t8ZpaovR2BLDdhz+6cRJZAV9JG2HzbsbaY8nmVCeyzFTipg7rYjqhgjJpIPtOGzZ20RW0Edulp+/r9vD3GnFHHtUCXpnPXOnFpEV8uM4Dk1tcYJ+k7Zogg3v13DUhAJqm6JMLs/F7zPZXtVEPGGzYGbpId+/uS1GVW0b5YVh/vHeASzTIBSwmDouj8qSg4+YOsZbsEyTtoRDeyRGU2uM9/c0cvysMvKyB3afh+04bK9qpjgvSH5OMO339fc31Rsv/DtM2jaW6X63aCyBbVpk+bzVOOhlwPGpwPZ03i/FfoRlWuZ0W9Clpbns3F3Phi01+EyThaqUptYY7fEkjS0xWiJx2mNJGltjFOQEOH52GWve3ke0PcnRU4u45w9vkZsV4MQ55URjSV59bz8lBWFK8kJUN0ZYMreCzbsa+fOrO8kO+ThxTgVbqxqxTJPapiihgEVFURbVDRF8lkletntyuzA36B5pvLu/M6sBTK7IpaElRkNLO4YBvf2ZW6ZB0nYwAAfw+0yOGp9PLJ5ky96mzvl9WTCjhIllOax5ex9FuUF2V7cSSyR73BEG/CZL5o7jrS21FOe5y7a1JwgHLWZPLmL9+zUH/VvIDvmYMaGA2ZMLcYDJ5TnUNEZpaGmnuS1OTtjPlj2NhEM+Aj6Tt7fV4TjuEU9Jfoizl0zhhQ17KckPsWTuOIrzQ+za38zGnQ1MLMuhJRLHZxlsr2rm9U3VTCjNpqktzlknTaapNcbk8lziCZtILMGmXQ2dXYPnLpvK21vreOntKmZNKWJ8cRZNrTHe0NWcsmA8ZYVhSgrCbNxRz9tba5k9pYiCnAAl+WH+tm4PL71VhZpUQHlhFuWFYU6aW8Hemlb+snY3pQUhzls6lfqWdjZsrqG0MExrJOF+172NTCrP5b0d9ZTmh1gyt4L2hE1JXgiA1zYeoLYpykeOn8j7uxsJBS1icZv/+t0GFswopboxQtBnsnFnA7f8y/EkbYfqhgh/fnUnMyYU4LdM/D6TrJCPY6YXU9sYZXxpDo8+9z7jirPIzwkwfXw+5YVZvPLOPt7ZVscJc8rRqW2TnxNg9uRC9ta0MWNiPk2tMQI+k2gsya4DLeypbiUvO8BpC8cTjSUHtCOXYp9BhTMdXswMI5PbcRze2lrHpPIcCgbQInUct+sKYFtVE9khPyfMKaewKJunX9rKjv3NjCvKxjIN2hNJ5k4toiUSp6k1xtRxeeidDehdDcyaVMjm3Q28tvEA7bEkZ500mZZInHDQx8yJBazdeAA1qYC65naSSYdQwKKhpZ2/r9tDU1ucyeW5RNoTTB+fR25WgGmVeTS2xJhWmQcGxOI2T72yg7e31TGlIpdE0mZaZT6FuUFqGiOs21TD9AkFTC7PoSQ/xLjiLP62bg96ZwP1ze2HfG+fZZBIOpQVhGmOxIi0J5k9uZCskI+ZEwr4w4tbicaSlBWEaWtP0BKJd743HPQRaU907uTCQR/zjypm14EWGlI77+7yswP4fe6RTiJpE0/aVBRl0RJJ0JwaHKggJ0BDy8EDBXV8Rtfcx0wvYcueRmKJ5EHnmroe5Q1EOOgjO+SjpjF60Pfr0N9Ou+NItHvWnlimwVHj89G7Gg6a3v29XT8z6Ldoj3/wvToaIF+7eD5HTylK4xuOULFXSl0CfAvwA/+ltb6n2/xvA5cD9alJK7sv04spSLH3BC/mPtzMiaRNPGGn/Wwkx3GIxpKEAlZa5w1qGiIU5Ycwuy1r2w6lpbnU1rYckqepNYZlGuzY30JO2M+E0mz8PpOm1hh52W6B3VPdwtwud3O3ROJUN0SYWJbjdm/taqC5LY5pGhw/u4w91a2EAhYBv0V2yNd5UcHO/c28tbWWZcdUdrZM/T6TgtwgpmFQ1xTl8TXbaWhu5/PnHs3E8QW8u/kAoaCPkN/iL2t3Mb4kh4ZWd6e4cKZ7FPjSW1UE/BYfPWFS59VsADv2uZ+Xlx3ghNnlbNhSw/76CNkhH2pSIfXNUQI+i5rGCMepMjbvbiAvK0BNY5TG1HbZureJ6oYIJ8+v5EB9hPf3NLJkbgXRWJLGlnYWzyln654mDANe2LCXY2aW8cbG/Zy2cDxFuSEmV+RS0xChMDeEzzJ4Z3sdb26pZd60Ynbsa2ZSeQ57alopzQ/z3s56Xn57H2edNJnapijrN9dw42cWUVoQ4kB9hI076snNCvDW1lqmj89n8+4G9M4GPn7ydHKy/OypbqUlEqeyOItFs8oOupijL8Ne7JVS44HVwHFAO7AG+KTW+t0uyzwO/IfW+uW0Un9gClLsPcGLuSXzyDgSM3ecA+jY0ffXMHAcZ9AXEAy22KezSzkDeE5rXae1bgX+B7ig2zKLgBuUUm8qpX6ilAqlmV8IITyn42SvYRhpHQFmwpVi6RT7SqCqy+sqYELHC6VUDrAOuA5YCBQANw1dRCGEEIOVTqekycHnHQzA7nihtW4BVnS8VkrdBawCbkw3ROpw5LCUluYe9ntHixczgzdzS+aRIZlHxmAyp1PsdwPLu7yuAPZ2vFBKTQLO0FqvSk0ygENP5fdB+uwznxdzS+aRIZlHRi999mlLp9g/C9yilCoFWoGPA5/vMj8CfE8p9TfcEwVXA38YUAohhBDDqt8+e631Htwumb8B64Ffa61fVUo9qZRapLWuBr4APA5o3Jb9XcMXWQghxECldSGx1vrXwK+7TVvR5efHgMeGNpoQQoihMtoPQrPA7X86XIN572jxYmbwZm7JPDIk88joyNwlu5Xue0f7cQnLgBdHM4AQQnjYctybXvs12sU+CByPe+3+wB+IIYQQRyYLGAe8hvtkg36NdrEXQggxAmTEayGEOAJIsRdCiCOAFHshhDgCSLEXQogjgBR7IYQ4AkixF0KII4AUeyGEOAKM9uMSDlt/4+JmitTTQMv44LHPXwBygbuBMPBbrfW3RineQZRSebjDTp6ttd6ulDqDHnIqpeYD9wN5wAvAF7XWiZ7XOuKZf4F7Z3ZrapFbtdZ/yJTMqfGaL0q9fEJr/fVM3869ZM7o7QyglLoNd1Q9B3hAa323B7Z1T5mHZFt7smWfGhf3DtwNMB/4vFJqzqiG6oFSygBmAsdqredrrecDb+IO7nIeMBs4Xin1T6OX0qWUOgH3tuuZqddhes/5K+AarfVM3KecXjnyiQ/NnLII+FDH9tZadzxue9QzpwrNR4AFuH+3xymlPkkGb+deMp9PBm9nAKXUycBpwDGprF9SSh1LZm/rnjIrhmhbe7LYk964uJlApf7/jFJqg1LqGmAxsFlrvS21F/4VcOGoJfzAlbhjEXQMTNNjTqXUZCCstX4ltdyDjF7+gzIrpbKAScCq1HjItyqlzAzKXAV8TWsd01rHgfdwd1SZvJ17yjyJzN7OaK2fB05NbdMy3F6MAjJ4W/eSOcIQbWuvduP0NC7u4lHK0pdC4K/Al3C7m/4OfJc+xvQdLVrrzwG4DQmg97GH+xyTeCT1kLkCeA74V6AR+D/gCuBtMiCz1vqdjp+VUjNwu0Z+TAZv514yLwdOIUO3cwetdVwpdStwLfA7vPE33T2znyH6m/Zqse9zXNxMobV+GXi547VS6gHgNg5+Sl1GZqf3bZyx215rvRU4v+O1UurHwGeAd8mgzEqpo4EngOuABAd3Q2Xkdu6aWWut8cB2BtBaf1sp9V3cwZVm4oG/6W6ZT9daD8m29mo3zm7cJ751OGhc3EyhlFqmlDq9yyQDd+jGjM9O79s4Y7e9UmqeUurjXSZ1jIecMZmVUktxj/au11o/1Ee2jM3ske08K3UCE611G/B73KORjN3WvWT+xFBta68W+2eB05VSpal+2o8Dfx7lTD0pAL6vlAoppXKBzwI3AEopdZRSygIuAZ4axYy9+Qc95NRa7wCiqQIAcCmZk98A/kspVaiU8uOOlfyHTMmslJoI/BG4RGv9SGpyRm/nXjJn9HZOmQasVEoFlVIB3JOyPyeDt3UvmZ9niLa1J4t9b+PijmqoHmit/w/30Hcd8DqwKtW1cxnuMI7vAhtxTzBnFK11lN5zfgr4gVJqI5AD/Gg0MnantX4TuBN4CTfzeq31b1KzMyHztUAIuFsptV4ptR53G19G5m7nnjIvIbO3M1rrJzn4396a1M7qMjJ0W/eS+TaGaFvL8+yFEOII4MmWvRBCiIGRYi+EEEcAKfZCCHEEkGIvhBBHACn2QghxBJBiL4QQRwAp9kIIcQSQYi+EEEeA/w/Ux6A5wnP/iwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_21\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_22 (InputLayer)          [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_63 (LSTM)                 (None, 45, 24)       3744        ['input_22[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_42 (Dropout)           (None, 45, 24)       0           ['lstm_63[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_64 (LSTM)                 (None, 45, 16)       2624        ['dropout_42[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_43 (Dropout)           (None, 45, 16)       0           ['lstm_64[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_65 (LSTM)                 (None, 32)           6272        ['dropout_43[0][0]']             \n",
      "                                                                                                  \n",
      " dense_42 (Dense)               (None, 40)           1320        ['lstm_65[0][0]']                \n",
      "                                                                                                  \n",
      " dense_43 (Dense)               (None, 5)            205         ['dense_42[0][0]']               \n",
      "                                                                                                  \n",
      " tf.unstack_21 (TFOpLambda)     [(None,),            0           ['dense_43[0][0]']               \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_105 (TFOpLambda  (None, 1)           0           ['tf.unstack_21[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_42 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_105[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_109 (TFOpLambda  (None, 1)           0           ['tf.unstack_21[0][4]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_63 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_42[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_43 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_109[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_64 (TFOpLambd  (None, 1)           0           ['tf.math.multiply_63[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_106 (TFOpLambda  (None, 1)           0           ['tf.unstack_21[0][1]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_108 (TFOpLambda  (None, 1)           0           ['tf.unstack_21[0][3]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_65 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_43[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_42 (TFOpL  (None, 1)           0           ['tf.math.multiply_64[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.softplus_42 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_106[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_107 (TFOpLambda  (None, 1)           0           ['tf.unstack_21[0][2]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.softplus_43 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_108[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_43 (TFOpL  (None, 1)           0           ['tf.math.multiply_65[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack_21 (TFOpLambda)       (None, 5, 1)         0           ['tf.__operators__.add_42[0][0]',\n",
      "                                                                  'tf.math.softplus_42[0][0]',    \n",
      "                                                                  'tf.expand_dims_107[0][0]',     \n",
      "                                                                  'tf.math.softplus_43[0][0]',    \n",
      "                                                                  'tf.__operators__.add_43[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _2_CCMP_t+1_0.11\n",
      "Epoch 1/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 3.3695\n",
      "Epoch 1: val_loss improved from inf to 3.88567, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.11.hdf5\n",
      "66/66 [==============================] - 10s 84ms/step - loss: 3.3728 - val_loss: 3.8857 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 2.6658\n",
      "Epoch 2: val_loss improved from 3.88567 to 3.06452, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.11.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 5s 75ms/step - loss: 2.6643 - val_loss: 3.0645 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.7102\n",
      "Epoch 3: val_loss improved from 3.06452 to 2.83567, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.11.hdf5\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 1.7102 - val_loss: 2.8357 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.3663\n",
      "Epoch 4: val_loss did not improve from 2.83567\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 5s 81ms/step - loss: 1.3663 - val_loss: 2.8539 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.2213\n",
      "Epoch 5: val_loss did not improve from 2.83567\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 5s 81ms/step - loss: 1.2213 - val_loss: 2.8506 - lr: 9.9000e-05\n",
      "Epoch 6/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1421\n",
      "Epoch 6: val_loss did not improve from 2.83567\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 1.1401 - val_loss: 2.9694 - lr: 9.8010e-05\n",
      "Epoch 7/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0950\n",
      "Epoch 7: val_loss did not improve from 2.83567\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 1.0934 - val_loss: 2.9419 - lr: 9.7030e-05\n",
      "Epoch 8/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0541\n",
      "Epoch 8: val_loss did not improve from 2.83567\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 1.0529 - val_loss: 3.0695 - lr: 9.6060e-05\n",
      "Epoch 9/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0280\n",
      "Epoch 9: val_loss did not improve from 2.83567\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 1.0272 - val_loss: 3.0231 - lr: 9.5099e-05\n",
      "Epoch 10/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9926\n",
      "Epoch 10: val_loss did not improve from 2.83567\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.9929 - val_loss: 3.1485 - lr: 9.4148e-05\n",
      "Epoch 11/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9723\n",
      "Epoch 11: val_loss did not improve from 2.83567\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.9781 - val_loss: 3.1147 - lr: 9.3207e-05\n",
      "Epoch 12/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9631\n",
      "Epoch 12: val_loss did not improve from 2.83567\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 5s 81ms/step - loss: 0.9631 - val_loss: 3.0441 - lr: 9.2274e-05\n",
      "Epoch 13/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9406\n",
      "Epoch 13: val_loss did not improve from 2.83567\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.9402 - val_loss: 3.0699 - lr: 9.1352e-05\n",
      "Epoch 14/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9137\n",
      "Epoch 14: val_loss did not improve from 2.83567\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.9116 - val_loss: 2.9122 - lr: 9.0438e-05\n",
      "Epoch 15/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9065\n",
      "Epoch 15: val_loss improved from 2.83567 to 2.76890, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.11.hdf5\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.9050 - val_loss: 2.7689 - lr: 8.9534e-05\n",
      "Epoch 16/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9123\n",
      "Epoch 16: val_loss improved from 2.76890 to 2.54868, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.11.hdf5\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.9117 - val_loss: 2.5487 - lr: 8.9534e-05\n",
      "Epoch 17/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8832\n",
      "Epoch 17: val_loss improved from 2.54868 to 2.51214, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.11.hdf5\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.8847 - val_loss: 2.5121 - lr: 8.9534e-05\n",
      "Epoch 18/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8711\n",
      "Epoch 18: val_loss improved from 2.51214 to 2.30373, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.11.hdf5\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.8715 - val_loss: 2.3037 - lr: 8.9534e-05\n",
      "Epoch 19/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8690\n",
      "Epoch 19: val_loss improved from 2.30373 to 2.29936, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.11.hdf5\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8690 - val_loss: 2.2994 - lr: 8.9534e-05\n",
      "Epoch 20/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8496\n",
      "Epoch 20: val_loss improved from 2.29936 to 2.25570, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.11.hdf5\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.8497 - val_loss: 2.2557 - lr: 8.9534e-05\n",
      "Epoch 21/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8585\n",
      "Epoch 21: val_loss did not improve from 2.25570\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8585 - val_loss: 2.3054 - lr: 8.9534e-05\n",
      "Epoch 22/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8379\n",
      "Epoch 22: val_loss improved from 2.25570 to 2.16699, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.11.hdf5\n",
      "66/66 [==============================] - 5s 82ms/step - loss: 0.8379 - val_loss: 2.1670 - lr: 8.8638e-05\n",
      "Epoch 23/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8444\n",
      "Epoch 23: val_loss did not improve from 2.16699\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 5s 81ms/step - loss: 0.8444 - val_loss: 2.1893 - lr: 8.8638e-05\n",
      "Epoch 24/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8205\n",
      "Epoch 24: val_loss improved from 2.16699 to 2.14959, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.11.hdf5\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.8233 - val_loss: 2.1496 - lr: 8.7752e-05\n",
      "Epoch 25/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8280\n",
      "Epoch 25: val_loss improved from 2.14959 to 1.99241, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.11.hdf5\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.8285 - val_loss: 1.9924 - lr: 8.7752e-05\n",
      "Epoch 26/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8250\n",
      "Epoch 26: val_loss did not improve from 1.99241\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.8242 - val_loss: 2.0277 - lr: 8.7752e-05\n",
      "Epoch 27/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8140\n",
      "Epoch 27: val_loss improved from 1.99241 to 1.88638, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.11.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8113 - val_loss: 1.8864 - lr: 8.6875e-05\n",
      "Epoch 28/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7993\n",
      "Epoch 28: val_loss did not improve from 1.88638\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.8026 - val_loss: 2.0734 - lr: 8.6875e-05\n",
      "Epoch 29/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8265\n",
      "Epoch 29: val_loss did not improve from 1.88638\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.8270 - val_loss: 2.1991 - lr: 8.6006e-05\n",
      "Epoch 30/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8103\n",
      "Epoch 30: val_loss did not improve from 1.88638\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.8094 - val_loss: 1.9868 - lr: 8.5146e-05\n",
      "Epoch 31/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8008\n",
      "Epoch 31: val_loss did not improve from 1.88638\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.8016 - val_loss: 1.9522 - lr: 8.4294e-05\n",
      "Epoch 32/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7926\n",
      "Epoch 32: val_loss did not improve from 1.88638\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.7929 - val_loss: 2.0392 - lr: 8.3451e-05\n",
      "Epoch 33/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7826\n",
      "Epoch 33: val_loss improved from 1.88638 to 1.88113, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.11.hdf5\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.7833 - val_loss: 1.8811 - lr: 8.2617e-05\n",
      "Epoch 34/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7938\n",
      "Epoch 34: val_loss did not improve from 1.88113\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.7952 - val_loss: 1.9571 - lr: 8.2617e-05\n",
      "Epoch 35/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7863\n",
      "Epoch 35: val_loss did not improve from 1.88113\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.7872 - val_loss: 1.9722 - lr: 8.1791e-05\n",
      "Epoch 36/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7831\n",
      "Epoch 36: val_loss did not improve from 1.88113\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.7830 - val_loss: 1.9009 - lr: 8.0973e-05\n",
      "Epoch 37/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7843\n",
      "Epoch 37: val_loss improved from 1.88113 to 1.84259, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.11.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.7814 - val_loss: 1.8426 - lr: 8.0163e-05\n",
      "Epoch 38/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7745\n",
      "Epoch 38: val_loss did not improve from 1.84259\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.7753 - val_loss: 1.9003 - lr: 8.0163e-05\n",
      "Epoch 39/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7748\n",
      "Epoch 39: val_loss did not improve from 1.84259\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.7740 - val_loss: 1.8856 - lr: 7.9361e-05\n",
      "Epoch 40/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7513\n",
      "Epoch 40: val_loss did not improve from 1.84259\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.7499 - val_loss: 1.9220 - lr: 7.8568e-05\n",
      "Epoch 41/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7699\n",
      "Epoch 41: val_loss did not improve from 1.84259\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.7704 - val_loss: 1.8771 - lr: 7.7782e-05\n",
      "Epoch 42/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7555\n",
      "Epoch 42: val_loss improved from 1.84259 to 1.82796, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.11.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.7546 - val_loss: 1.8280 - lr: 7.7004e-05\n",
      "Epoch 43/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7583\n",
      "Epoch 43: val_loss did not improve from 1.82796\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.7575 - val_loss: 1.9761 - lr: 7.7004e-05\n",
      "Epoch 44/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7733\n",
      "Epoch 44: val_loss did not improve from 1.82796\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.7720 - val_loss: 1.9319 - lr: 7.6234e-05\n",
      "Epoch 45/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7636\n",
      "Epoch 45: val_loss did not improve from 1.82796\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.7636 - val_loss: 1.9998 - lr: 7.5472e-05\n",
      "Epoch 46/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7709\n",
      "Epoch 46: val_loss did not improve from 1.82796\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.7690 - val_loss: 1.9884 - lr: 7.4717e-05\n",
      "Epoch 47/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7556\n",
      "Epoch 47: val_loss did not improve from 1.82796\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.7552 - val_loss: 1.9381 - lr: 7.3970e-05\n",
      "Epoch 48/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7497\n",
      "Epoch 48: val_loss improved from 1.82796 to 1.79759, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.11.hdf5\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.7507 - val_loss: 1.7976 - lr: 7.3230e-05\n",
      "Epoch 49/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7514\n",
      "Epoch 49: val_loss did not improve from 1.79759\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.7516 - val_loss: 1.8422 - lr: 7.3230e-05\n",
      "Epoch 50/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7485\n",
      "Epoch 50: val_loss did not improve from 1.79759\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.7496 - val_loss: 1.8958 - lr: 7.2498e-05\n",
      "Epoch 51/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7538\n",
      "Epoch 51: val_loss did not improve from 1.79759\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.7519 - val_loss: 1.8664 - lr: 7.1773e-05\n",
      "Epoch 52/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7419\n",
      "Epoch 52: val_loss did not improve from 1.79759\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.7413 - val_loss: 1.8665 - lr: 7.1055e-05\n",
      "Epoch 53/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7549\n",
      "Epoch 53: val_loss did not improve from 1.79759\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 5s 79ms/step - loss: 0.7549 - val_loss: 1.8710 - lr: 7.0345e-05\n",
      "Epoch 54/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7424\n",
      "Epoch 54: val_loss did not improve from 1.79759\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.7405 - val_loss: 1.9824 - lr: 6.9641e-05\n",
      "Epoch 55/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7389\n",
      "Epoch 55: val_loss did not improve from 1.79759\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.7391 - val_loss: 1.8521 - lr: 6.8945e-05\n",
      "Epoch 56/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7493\n",
      "Epoch 56: val_loss improved from 1.79759 to 1.77790, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.11.hdf5\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.7482 - val_loss: 1.7779 - lr: 6.8255e-05\n",
      "Epoch 57/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7362\n",
      "Epoch 57: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.7360 - val_loss: 1.7895 - lr: 6.8255e-05\n",
      "Epoch 58/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7294\n",
      "Epoch 58: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 5s 81ms/step - loss: 0.7290 - val_loss: 1.9395 - lr: 6.7573e-05\n",
      "Epoch 59/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7466\n",
      "Epoch 59: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.7463 - val_loss: 1.8894 - lr: 6.6897e-05\n",
      "Epoch 60/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7207\n",
      "Epoch 60: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.7221 - val_loss: 1.9370 - lr: 6.6228e-05\n",
      "Epoch 61/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7353\n",
      "Epoch 61: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.7366 - val_loss: 1.8122 - lr: 6.5566e-05\n",
      "Epoch 62/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7333\n",
      "Epoch 62: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.7333 - val_loss: 1.8532 - lr: 6.4910e-05\n",
      "Epoch 63/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7157\n",
      "Epoch 63: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.7148 - val_loss: 1.8710 - lr: 6.4261e-05\n",
      "Epoch 64/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7148\n",
      "Epoch 64: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.7154 - val_loss: 1.8660 - lr: 6.3619e-05\n",
      "Epoch 65/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7322\n",
      "Epoch 65: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.7316 - val_loss: 1.8304 - lr: 6.2982e-05\n",
      "Epoch 66/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7266\n",
      "Epoch 66: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.7265 - val_loss: 1.9204 - lr: 6.2353e-05\n",
      "Epoch 67/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7239\n",
      "Epoch 67: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.7275 - val_loss: 1.8808 - lr: 6.1729e-05\n",
      "Epoch 68/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7140\n",
      "Epoch 68: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.7123 - val_loss: 1.8543 - lr: 6.1112e-05\n",
      "Epoch 69/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7130\n",
      "Epoch 69: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 5s 81ms/step - loss: 0.7160 - val_loss: 1.7816 - lr: 6.0501e-05\n",
      "Epoch 70/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7198\n",
      "Epoch 70: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.7181 - val_loss: 1.9221 - lr: 5.9896e-05\n",
      "Epoch 71/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7255\n",
      "Epoch 71: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 5.8703650938696225e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.7248 - val_loss: 1.8149 - lr: 5.9297e-05\n",
      "Epoch 72/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7210\n",
      "Epoch 72: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 5.811661550978897e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.7189 - val_loss: 1.9135 - lr: 5.8704e-05\n",
      "Epoch 73/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7116\n",
      "Epoch 73: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 5.753545068728272e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.7116 - val_loss: 1.8536 - lr: 5.8117e-05\n",
      "Epoch 74/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7121\n",
      "Epoch 74: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 5.6960095243994146e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.7107 - val_loss: 1.7993 - lr: 5.7535e-05\n",
      "Epoch 75/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7183\n",
      "Epoch 75: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 5.639049515593797e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.7163 - val_loss: 1.8778 - lr: 5.6960e-05\n",
      "Epoch 76/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7047\n",
      "Epoch 76: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 5.5826589195930865e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.7035 - val_loss: 1.8936 - lr: 5.6390e-05\n",
      "Epoch 77/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7184\n",
      "Epoch 77: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 5.526832333998754e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.7193 - val_loss: 1.9354 - lr: 5.5827e-05\n",
      "Epoch 78/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7134\n",
      "Epoch 78: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 5.471563996252371e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.7123 - val_loss: 1.8504 - lr: 5.5268e-05\n",
      "Epoch 79/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6984\n",
      "Epoch 79: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 5.416848503955407e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6984 - val_loss: 1.9427 - lr: 5.4716e-05\n",
      "Epoch 80/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/66 [============================>.] - ETA: 0s - loss: 0.7171\n",
      "Epoch 80: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 5.362680094549433e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.7171 - val_loss: 1.9267 - lr: 5.4168e-05\n",
      "Epoch 81/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7018\n",
      "Epoch 81: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 5.309053365635918e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.7010 - val_loss: 1.9174 - lr: 5.3627e-05\n",
      "Epoch 82/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7051\n",
      "Epoch 82: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 5.2559629148163366e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.7031 - val_loss: 1.9387 - lr: 5.3091e-05\n",
      "Epoch 83/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7024\n",
      "Epoch 83: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 5.2034033396921584e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.7009 - val_loss: 1.8715 - lr: 5.2560e-05\n",
      "Epoch 84/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6919\n",
      "Epoch 84: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 5.1513692378648555e-05.\n",
      "66/66 [==============================] - 5s 77ms/step - loss: 0.6913 - val_loss: 1.9790 - lr: 5.2034e-05\n",
      "Epoch 85/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7033\n",
      "Epoch 85: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 5.099855567095801e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.7033 - val_loss: 1.8942 - lr: 5.1514e-05\n",
      "Epoch 86/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7065\n",
      "Epoch 86: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 5.048856924986467e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.7071 - val_loss: 1.8353 - lr: 5.0999e-05\n",
      "Epoch 87/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7021\n",
      "Epoch 87: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 4.998368269298226e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.7001 - val_loss: 1.8676 - lr: 5.0489e-05\n",
      "Epoch 88/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7021\n",
      "Epoch 88: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 4.9483845577924514e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.7026 - val_loss: 2.0116 - lr: 4.9984e-05\n",
      "Epoch 89/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7033\n",
      "Epoch 89: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 4.898900748230517e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.7030 - val_loss: 1.8645 - lr: 4.9484e-05\n",
      "Epoch 90/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6970\n",
      "Epoch 90: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 4.849911798373796e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6974 - val_loss: 1.9120 - lr: 4.8989e-05\n",
      "Epoch 91/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6940\n",
      "Epoch 91: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 4.801412665983662e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6965 - val_loss: 1.8852 - lr: 4.8499e-05\n",
      "Epoch 92/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7001\n",
      "Epoch 92: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 4.75339866898139e-05.\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.6994 - val_loss: 1.8247 - lr: 4.8014e-05\n",
      "Epoch 93/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7003\n",
      "Epoch 93: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 4.705864765128354e-05.\n",
      "66/66 [==============================] - 6s 84ms/step - loss: 0.7017 - val_loss: 1.8131 - lr: 4.7534e-05\n",
      "Epoch 94/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7010\n",
      "Epoch 94: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 4.658806272345828e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.7008 - val_loss: 1.8817 - lr: 4.7059e-05\n",
      "Epoch 95/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6935\n",
      "Epoch 95: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 4.612218148395186e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.6937 - val_loss: 1.9423 - lr: 4.6588e-05\n",
      "Epoch 96/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6985\n",
      "Epoch 96: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 4.566096071357606e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.6985 - val_loss: 1.8978 - lr: 4.6122e-05\n",
      "Epoch 97/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6952\n",
      "Epoch 97: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 4.52043499899446e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.6952 - val_loss: 1.9030 - lr: 4.5661e-05\n",
      "Epoch 98/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6775\n",
      "Epoch 98: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 4.475230609386927e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.6782 - val_loss: 1.9371 - lr: 4.5204e-05\n",
      "Epoch 99/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6862\n",
      "Epoch 99: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 4.43047822045628e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.6875 - val_loss: 1.8156 - lr: 4.4752e-05\n",
      "Epoch 100/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6946\n",
      "Epoch 100: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 4.386173510283697e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.6927 - val_loss: 1.8935 - lr: 4.4305e-05\n",
      "Epoch 101/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6847\n",
      "Epoch 101: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 4.342311796790454e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.6846 - val_loss: 1.9982 - lr: 4.3862e-05\n",
      "Epoch 102/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6930\n",
      "Epoch 102: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 4.298888758057728e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6901 - val_loss: 1.8855 - lr: 4.3423e-05\n",
      "Epoch 103/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7020\n",
      "Epoch 103: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 4.2558997120067946e-05.\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.7020 - val_loss: 1.8515 - lr: 4.2989e-05\n",
      "Epoch 104/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6922\n",
      "Epoch 104: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 4.2133406968787315e-05.\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.6939 - val_loss: 1.8696 - lr: 4.2559e-05\n",
      "Epoch 105/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6840\n",
      "Epoch 105: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 4.1712073907547166e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6840 - val_loss: 1.8637 - lr: 4.2133e-05\n",
      "Epoch 106/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6874\n",
      "Epoch 106: val_loss did not improve from 1.77790\n",
      "\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 4.1294954717159274e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.6880 - val_loss: 1.8553 - lr: 4.1712e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_22\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_23 (InputLayer)          [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_66 (LSTM)                 (None, 45, 24)       3744        ['input_23[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_44 (Dropout)           (None, 45, 24)       0           ['lstm_66[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_67 (LSTM)                 (None, 45, 16)       2624        ['dropout_44[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_45 (Dropout)           (None, 45, 16)       0           ['lstm_67[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_68 (LSTM)                 (None, 32)           6272        ['dropout_45[0][0]']             \n",
      "                                                                                                  \n",
      " dense_44 (Dense)               (None, 40)           1320        ['lstm_68[0][0]']                \n",
      "                                                                                                  \n",
      " dense_45 (Dense)               (None, 5)            205         ['dense_44[0][0]']               \n",
      "                                                                                                  \n",
      " tf.unstack_22 (TFOpLambda)     [(None,),            0           ['dense_45[0][0]']               \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_110 (TFOpLambda  (None, 1)           0           ['tf.unstack_22[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_44 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_110[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_114 (TFOpLambda  (None, 1)           0           ['tf.unstack_22[0][4]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_66 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_44[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_45 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_114[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_67 (TFOpLambd  (None, 1)           0           ['tf.math.multiply_66[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_111 (TFOpLambda  (None, 1)           0           ['tf.unstack_22[0][1]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_113 (TFOpLambda  (None, 1)           0           ['tf.unstack_22[0][3]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_68 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_45[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_44 (TFOpL  (None, 1)           0           ['tf.math.multiply_67[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.softplus_44 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_111[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_112 (TFOpLambda  (None, 1)           0           ['tf.unstack_22[0][2]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.softplus_45 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_113[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_45 (TFOpL  (None, 1)           0           ['tf.math.multiply_68[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack_22 (TFOpLambda)       (None, 5, 1)         0           ['tf.__operators__.add_44[0][0]',\n",
      "                                                                  'tf.math.softplus_44[0][0]',    \n",
      "                                                                  'tf.expand_dims_112[0][0]',     \n",
      "                                                                  'tf.math.softplus_45[0][0]',    \n",
      "                                                                  'tf.__operators__.add_45[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _2_CCMP_t+1_0.12\n",
      "Epoch 1/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 3.4267\n",
      "Epoch 1: val_loss improved from inf to 4.34473, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 11s 97ms/step - loss: 3.4238 - val_loss: 4.3447 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 2.7376\n",
      "Epoch 2: val_loss improved from 4.34473 to 3.55674, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.12.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 5s 80ms/step - loss: 2.7357 - val_loss: 3.5567 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.8334\n",
      "Epoch 3: val_loss improved from 3.55674 to 3.23390, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 5s 83ms/step - loss: 1.8334 - val_loss: 3.2339 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.4930\n",
      "Epoch 4: val_loss improved from 3.23390 to 2.99595, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 1.4930 - val_loss: 2.9960 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.3352\n",
      "Epoch 5: val_loss improved from 2.99595 to 2.82721, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 1.3352 - val_loss: 2.8272 - lr: 1.0000e-04\n",
      "Epoch 6/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.2279\n",
      "Epoch 6: val_loss improved from 2.82721 to 2.74372, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 1.2279 - val_loss: 2.7437 - lr: 1.0000e-04\n",
      "Epoch 7/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1678\n",
      "Epoch 7: val_loss did not improve from 2.74372\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 1.1678 - val_loss: 2.7468 - lr: 1.0000e-04\n",
      "Epoch 8/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1331\n",
      "Epoch 8: val_loss did not improve from 2.74372\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 1.1309 - val_loss: 2.8347 - lr: 9.9000e-05\n",
      "Epoch 9/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0885\n",
      "Epoch 9: val_loss improved from 2.74372 to 2.66614, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 1.0885 - val_loss: 2.6661 - lr: 9.8010e-05\n",
      "Epoch 10/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0702\n",
      "Epoch 10: val_loss improved from 2.66614 to 2.65895, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 1.0702 - val_loss: 2.6590 - lr: 9.8010e-05\n",
      "Epoch 11/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0547\n",
      "Epoch 11: val_loss did not improve from 2.65895\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 1.0547 - val_loss: 2.8738 - lr: 9.8010e-05\n",
      "Epoch 12/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0299\n",
      "Epoch 12: val_loss improved from 2.65895 to 2.51775, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 1.0299 - val_loss: 2.5178 - lr: 9.7030e-05\n",
      "Epoch 13/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0055\n",
      "Epoch 13: val_loss improved from 2.51775 to 2.46076, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 1.0055 - val_loss: 2.4608 - lr: 9.7030e-05\n",
      "Epoch 14/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0015\n",
      "Epoch 14: val_loss improved from 2.46076 to 2.32856, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 1.0015 - val_loss: 2.3286 - lr: 9.7030e-05\n",
      "Epoch 15/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9959\n",
      "Epoch 15: val_loss improved from 2.32856 to 2.08724, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9959 - val_loss: 2.0872 - lr: 9.7030e-05\n",
      "Epoch 16/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9767\n",
      "Epoch 16: val_loss did not improve from 2.08724\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.9767 - val_loss: 2.3692 - lr: 9.7030e-05\n",
      "Epoch 17/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9684\n",
      "Epoch 17: val_loss did not improve from 2.08724\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9672 - val_loss: 2.4158 - lr: 9.6060e-05\n",
      "Epoch 18/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9481\n",
      "Epoch 18: val_loss improved from 2.08724 to 2.03169, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.9481 - val_loss: 2.0317 - lr: 9.5099e-05\n",
      "Epoch 19/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9482\n",
      "Epoch 19: val_loss did not improve from 2.03169\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9477 - val_loss: 2.0695 - lr: 9.5099e-05\n",
      "Epoch 20/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9254\n",
      "Epoch 20: val_loss did not improve from 2.03169\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9254 - val_loss: 2.1003 - lr: 9.4148e-05\n",
      "Epoch 21/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9088\n",
      "Epoch 21: val_loss improved from 2.03169 to 1.95876, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9088 - val_loss: 1.9588 - lr: 9.3207e-05\n",
      "Epoch 22/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9137\n",
      "Epoch 22: val_loss improved from 1.95876 to 1.94713, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9137 - val_loss: 1.9471 - lr: 9.3207e-05\n",
      "Epoch 23/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8929\n",
      "Epoch 23: val_loss did not improve from 1.94713\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.8919 - val_loss: 1.9720 - lr: 9.3207e-05\n",
      "Epoch 24/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8807\n",
      "Epoch 24: val_loss improved from 1.94713 to 1.92194, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.8807 - val_loss: 1.9219 - lr: 9.2274e-05\n",
      "Epoch 25/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8717\n",
      "Epoch 25: val_loss did not improve from 1.92194\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.8717 - val_loss: 1.9700 - lr: 9.2274e-05\n",
      "Epoch 26/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8558\n",
      "Epoch 26: val_loss improved from 1.92194 to 1.90850, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.8558 - val_loss: 1.9085 - lr: 9.1352e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8616\n",
      "Epoch 27: val_loss did not improve from 1.90850\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.8616 - val_loss: 1.9921 - lr: 9.1352e-05\n",
      "Epoch 28/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8460\n",
      "Epoch 28: val_loss did not improve from 1.90850\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.8460 - val_loss: 1.9302 - lr: 9.0438e-05\n",
      "Epoch 29/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8478\n",
      "Epoch 29: val_loss improved from 1.90850 to 1.85365, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 6s 98ms/step - loss: 0.8478 - val_loss: 1.8536 - lr: 8.9534e-05\n",
      "Epoch 30/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8378\n",
      "Epoch 30: val_loss did not improve from 1.85365\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.8378 - val_loss: 1.8694 - lr: 8.9534e-05\n",
      "Epoch 31/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8330\n",
      "Epoch 31: val_loss improved from 1.85365 to 1.83189, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.8330 - val_loss: 1.8319 - lr: 8.8638e-05\n",
      "Epoch 32/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8254\n",
      "Epoch 32: val_loss did not improve from 1.83189\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.8254 - val_loss: 2.0034 - lr: 8.8638e-05\n",
      "Epoch 33/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8268\n",
      "Epoch 33: val_loss improved from 1.83189 to 1.81637, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.8268 - val_loss: 1.8164 - lr: 8.7752e-05\n",
      "Epoch 34/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8127\n",
      "Epoch 34: val_loss improved from 1.81637 to 1.77853, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.8127 - val_loss: 1.7785 - lr: 8.7752e-05\n",
      "Epoch 35/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8148\n",
      "Epoch 35: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.8148 - val_loss: 1.8423 - lr: 8.7752e-05\n",
      "Epoch 36/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8102\n",
      "Epoch 36: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.8102 - val_loss: 1.7873 - lr: 8.6875e-05\n",
      "Epoch 37/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8203\n",
      "Epoch 37: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.8203 - val_loss: 1.9219 - lr: 8.6006e-05\n",
      "Epoch 38/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8099\n",
      "Epoch 38: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.8099 - val_loss: 1.8275 - lr: 8.5146e-05\n",
      "Epoch 39/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7970\n",
      "Epoch 39: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.7970 - val_loss: 1.8052 - lr: 8.4294e-05\n",
      "Epoch 40/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7976\n",
      "Epoch 40: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.7976 - val_loss: 1.8582 - lr: 8.3451e-05\n",
      "Epoch 41/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8005\n",
      "Epoch 41: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.8005 - val_loss: 1.8038 - lr: 8.2617e-05\n",
      "Epoch 42/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7868\n",
      "Epoch 42: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.7868 - val_loss: 1.8075 - lr: 8.1791e-05\n",
      "Epoch 43/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7791\n",
      "Epoch 43: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.7791 - val_loss: 1.9068 - lr: 8.0973e-05\n",
      "Epoch 44/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7834\n",
      "Epoch 44: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.7834 - val_loss: 1.8959 - lr: 8.0163e-05\n",
      "Epoch 45/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7885\n",
      "Epoch 45: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.7885 - val_loss: 1.8833 - lr: 7.9361e-05\n",
      "Epoch 46/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7940\n",
      "Epoch 46: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.7940 - val_loss: 1.7908 - lr: 7.8568e-05\n",
      "Epoch 47/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7981\n",
      "Epoch 47: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.7981 - val_loss: 1.8001 - lr: 7.7782e-05\n",
      "Epoch 48/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7809\n",
      "Epoch 48: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.7809 - val_loss: 1.9028 - lr: 7.7004e-05\n",
      "Epoch 49/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7706\n",
      "Epoch 49: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.7706 - val_loss: 1.8738 - lr: 7.6234e-05\n",
      "Epoch 50/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7699\n",
      "Epoch 50: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.7699 - val_loss: 1.9233 - lr: 7.5472e-05\n",
      "Epoch 51/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7640\n",
      "Epoch 51: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.7640 - val_loss: 2.0227 - lr: 7.4717e-05\n",
      "Epoch 52/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7691\n",
      "Epoch 52: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.7691 - val_loss: 1.8993 - lr: 7.3970e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7745\n",
      "Epoch 53: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.7745 - val_loss: 1.9690 - lr: 7.3230e-05\n",
      "Epoch 54/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7685\n",
      "Epoch 54: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.7685 - val_loss: 1.9016 - lr: 7.2498e-05\n",
      "Epoch 55/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7720\n",
      "Epoch 55: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.7720 - val_loss: 1.8770 - lr: 7.1773e-05\n",
      "Epoch 56/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7739\n",
      "Epoch 56: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.7739 - val_loss: 1.9056 - lr: 7.1055e-05\n",
      "Epoch 57/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7662\n",
      "Epoch 57: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.7662 - val_loss: 1.9444 - lr: 7.0345e-05\n",
      "Epoch 58/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7558\n",
      "Epoch 58: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.7558 - val_loss: 1.8689 - lr: 6.9641e-05\n",
      "Epoch 59/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7751\n",
      "Epoch 59: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.7751 - val_loss: 1.9550 - lr: 6.8945e-05\n",
      "Epoch 60/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7574\n",
      "Epoch 60: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.7574 - val_loss: 1.9688 - lr: 6.8255e-05\n",
      "Epoch 61/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7584\n",
      "Epoch 61: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.7584 - val_loss: 1.9660 - lr: 6.7573e-05\n",
      "Epoch 62/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7646\n",
      "Epoch 62: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.7646 - val_loss: 1.9758 - lr: 6.6897e-05\n",
      "Epoch 63/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7588\n",
      "Epoch 63: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.7588 - val_loss: 2.0001 - lr: 6.6228e-05\n",
      "Epoch 64/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7541\n",
      "Epoch 64: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.7541 - val_loss: 1.9184 - lr: 6.5566e-05\n",
      "Epoch 65/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7502\n",
      "Epoch 65: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.7502 - val_loss: 2.0658 - lr: 6.4910e-05\n",
      "Epoch 66/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7519\n",
      "Epoch 66: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.7526 - val_loss: 2.0450 - lr: 6.4261e-05\n",
      "Epoch 67/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7568\n",
      "Epoch 67: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.7568 - val_loss: 2.0334 - lr: 6.3619e-05\n",
      "Epoch 68/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7540\n",
      "Epoch 68: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.7540 - val_loss: 2.0018 - lr: 6.2982e-05\n",
      "Epoch 69/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7614\n",
      "Epoch 69: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.7614 - val_loss: 1.9141 - lr: 6.2353e-05\n",
      "Epoch 70/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7527\n",
      "Epoch 70: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.7527 - val_loss: 1.9779 - lr: 6.1729e-05\n",
      "Epoch 71/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7567\n",
      "Epoch 71: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.7567 - val_loss: 1.8841 - lr: 6.1112e-05\n",
      "Epoch 72/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7523\n",
      "Epoch 72: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.7523 - val_loss: 2.0266 - lr: 6.0501e-05\n",
      "Epoch 73/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7447\n",
      "Epoch 73: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.7441 - val_loss: 2.0315 - lr: 5.9896e-05\n",
      "Epoch 74/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7443\n",
      "Epoch 74: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 5.8703650938696225e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.7443 - val_loss: 2.0180 - lr: 5.9297e-05\n",
      "Epoch 75/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7487\n",
      "Epoch 75: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 5.811661550978897e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.7487 - val_loss: 2.0243 - lr: 5.8704e-05\n",
      "Epoch 76/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7397\n",
      "Epoch 76: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 5.753545068728272e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.7397 - val_loss: 1.9087 - lr: 5.8117e-05\n",
      "Epoch 77/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7442\n",
      "Epoch 77: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 5.6960095243994146e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.7442 - val_loss: 2.0361 - lr: 5.7535e-05\n",
      "Epoch 78/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7480\n",
      "Epoch 78: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 5.639049515593797e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.7480 - val_loss: 2.0938 - lr: 5.6960e-05\n",
      "Epoch 79/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7444\n",
      "Epoch 79: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 5.5826589195930865e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 6s 87ms/step - loss: 0.7437 - val_loss: 2.1716 - lr: 5.6390e-05\n",
      "Epoch 80/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7322\n",
      "Epoch 80: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 5.526832333998754e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 0.7331 - val_loss: 2.0800 - lr: 5.5827e-05\n",
      "Epoch 81/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7356\n",
      "Epoch 81: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 5.471563996252371e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.7356 - val_loss: 2.0721 - lr: 5.5268e-05\n",
      "Epoch 82/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7479\n",
      "Epoch 82: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 5.416848503955407e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.7479 - val_loss: 1.9866 - lr: 5.4716e-05\n",
      "Epoch 83/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7456\n",
      "Epoch 83: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 5.362680094549433e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.7456 - val_loss: 2.1119 - lr: 5.4168e-05\n",
      "Epoch 84/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7396\n",
      "Epoch 84: val_loss did not improve from 1.77853\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 5.309053365635918e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.7394 - val_loss: 2.1133 - lr: 5.3627e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_23\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_24 (InputLayer)          [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_69 (LSTM)                 (None, 45, 24)       3744        ['input_24[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_46 (Dropout)           (None, 45, 24)       0           ['lstm_69[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_70 (LSTM)                 (None, 45, 16)       2624        ['dropout_46[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_47 (Dropout)           (None, 45, 16)       0           ['lstm_70[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_71 (LSTM)                 (None, 32)           6272        ['dropout_47[0][0]']             \n",
      "                                                                                                  \n",
      " dense_46 (Dense)               (None, 40)           1320        ['lstm_71[0][0]']                \n",
      "                                                                                                  \n",
      " dense_47 (Dense)               (None, 5)            205         ['dense_46[0][0]']               \n",
      "                                                                                                  \n",
      " tf.unstack_23 (TFOpLambda)     [(None,),            0           ['dense_47[0][0]']               \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_115 (TFOpLambda  (None, 1)           0           ['tf.unstack_23[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_46 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_115[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_119 (TFOpLambda  (None, 1)           0           ['tf.unstack_23[0][4]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_69 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_46[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_47 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_119[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_70 (TFOpLambd  (None, 1)           0           ['tf.math.multiply_69[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_116 (TFOpLambda  (None, 1)           0           ['tf.unstack_23[0][1]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_118 (TFOpLambda  (None, 1)           0           ['tf.unstack_23[0][3]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_71 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_47[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_46 (TFOpL  (None, 1)           0           ['tf.math.multiply_70[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.softplus_46 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_116[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_117 (TFOpLambda  (None, 1)           0           ['tf.unstack_23[0][2]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.softplus_47 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_118[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_47 (TFOpL  (None, 1)           0           ['tf.math.multiply_71[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack_23 (TFOpLambda)       (None, 5, 1)         0           ['tf.__operators__.add_46[0][0]',\n",
      "                                                                  'tf.math.softplus_46[0][0]',    \n",
      "                                                                  'tf.expand_dims_117[0][0]',     \n",
      "                                                                  'tf.math.softplus_47[0][0]',    \n",
      "                                                                  'tf.__operators__.add_47[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _2_CCMP_t+1_0.13\n",
      "Epoch 1/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 3.4831\n",
      "Epoch 1: val_loss improved from inf to 4.37622, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 10s 87ms/step - loss: 3.4843 - val_loss: 4.3762 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 2.9711\n",
      "Epoch 2: val_loss improved from 4.37622 to 3.98210, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.13.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 5s 83ms/step - loss: 2.9718 - val_loss: 3.9821 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 2.0493\n",
      "Epoch 3: val_loss improved from 3.98210 to 2.92940, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 2.0473 - val_loss: 2.9294 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.5240\n",
      "Epoch 4: val_loss improved from 2.92940 to 2.44254, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 5s 82ms/step - loss: 1.5244 - val_loss: 2.4425 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.3065\n",
      "Epoch 5: val_loss did not improve from 2.44254\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 1.3065 - val_loss: 2.4437 - lr: 1.0000e-04\n",
      "Epoch 6/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1936\n",
      "Epoch 6: val_loss improved from 2.44254 to 2.38430, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 1.1936 - val_loss: 2.3843 - lr: 9.9000e-05\n",
      "Epoch 7/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1401\n",
      "Epoch 7: val_loss improved from 2.38430 to 2.36425, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 1.1401 - val_loss: 2.3643 - lr: 9.9000e-05\n",
      "Epoch 8/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0906\n",
      "Epoch 8: val_loss did not improve from 2.36425\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 1.0919 - val_loss: 2.5976 - lr: 9.9000e-05\n",
      "Epoch 9/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0665\n",
      "Epoch 9: val_loss did not improve from 2.36425\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 1.0684 - val_loss: 2.5755 - lr: 9.8010e-05\n",
      "Epoch 10/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0373\n",
      "Epoch 10: val_loss did not improve from 2.36425\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 1.0426 - val_loss: 2.5597 - lr: 9.7030e-05\n",
      "Epoch 11/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0407\n",
      "Epoch 11: val_loss did not improve from 2.36425\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 6s 83ms/step - loss: 1.0423 - val_loss: 2.7427 - lr: 9.6060e-05\n",
      "Epoch 12/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0242\n",
      "Epoch 12: val_loss did not improve from 2.36425\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 6s 84ms/step - loss: 1.0249 - val_loss: 2.7030 - lr: 9.5099e-05\n",
      "Epoch 13/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0103\n",
      "Epoch 13: val_loss did not improve from 2.36425\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 6s 84ms/step - loss: 1.0103 - val_loss: 3.0084 - lr: 9.4148e-05\n",
      "Epoch 14/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0038\n",
      "Epoch 14: val_loss did not improve from 2.36425\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 1.0038 - val_loss: 2.5417 - lr: 9.3207e-05\n",
      "Epoch 15/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0063\n",
      "Epoch 15: val_loss improved from 2.36425 to 2.34413, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 1.0063 - val_loss: 2.3441 - lr: 9.2274e-05\n",
      "Epoch 16/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9856\n",
      "Epoch 16: val_loss did not improve from 2.34413\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9856 - val_loss: 2.7168 - lr: 9.2274e-05\n",
      "Epoch 17/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9682\n",
      "Epoch 17: val_loss did not improve from 2.34413\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9682 - val_loss: 2.8627 - lr: 9.1352e-05\n",
      "Epoch 18/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9558\n",
      "Epoch 18: val_loss did not improve from 2.34413\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9558 - val_loss: 2.6987 - lr: 9.0438e-05\n",
      "Epoch 19/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9573\n",
      "Epoch 19: val_loss did not improve from 2.34413\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9564 - val_loss: 2.5990 - lr: 8.9534e-05\n",
      "Epoch 20/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9537\n",
      "Epoch 20: val_loss did not improve from 2.34413\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9537 - val_loss: 2.7812 - lr: 8.8638e-05\n",
      "Epoch 21/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9507\n",
      "Epoch 21: val_loss did not improve from 2.34413\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.9507 - val_loss: 2.4837 - lr: 8.7752e-05\n",
      "Epoch 22/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9485\n",
      "Epoch 22: val_loss did not improve from 2.34413\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9485 - val_loss: 2.4532 - lr: 8.6875e-05\n",
      "Epoch 23/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9244\n",
      "Epoch 23: val_loss did not improve from 2.34413\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9244 - val_loss: 2.4135 - lr: 8.6006e-05\n",
      "Epoch 24/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9362\n",
      "Epoch 24: val_loss did not improve from 2.34413\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9362 - val_loss: 2.4786 - lr: 8.5146e-05\n",
      "Epoch 25/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9145\n",
      "Epoch 25: val_loss did not improve from 2.34413\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.9145 - val_loss: 2.5364 - lr: 8.4294e-05\n",
      "Epoch 26/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9221\n",
      "Epoch 26: val_loss did not improve from 2.34413\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.9224 - val_loss: 2.6350 - lr: 8.3451e-05\n",
      "Epoch 27/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9183\n",
      "Epoch 27: val_loss did not improve from 2.34413\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9183 - val_loss: 2.4701 - lr: 8.2617e-05\n",
      "Epoch 28/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9038\n",
      "Epoch 28: val_loss did not improve from 2.34413\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9038 - val_loss: 2.6859 - lr: 8.1791e-05\n",
      "Epoch 29/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9137\n",
      "Epoch 29: val_loss did not improve from 2.34413\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.9137 - val_loss: 2.3793 - lr: 8.0973e-05\n",
      "Epoch 30/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9029\n",
      "Epoch 30: val_loss did not improve from 2.34413\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 7s 101ms/step - loss: 0.9029 - val_loss: 2.3791 - lr: 8.0163e-05\n",
      "Epoch 31/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9024\n",
      "Epoch 31: val_loss did not improve from 2.34413\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 8s 121ms/step - loss: 0.9024 - val_loss: 2.4386 - lr: 7.9361e-05\n",
      "Epoch 32/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9006\n",
      "Epoch 32: val_loss improved from 2.34413 to 2.18520, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 8s 115ms/step - loss: 0.9006 - val_loss: 2.1852 - lr: 7.8568e-05\n",
      "Epoch 33/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8901\n",
      "Epoch 33: val_loss did not improve from 2.18520\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.8901 - val_loss: 2.2764 - lr: 7.8568e-05\n",
      "Epoch 34/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9002\n",
      "Epoch 34: val_loss did not improve from 2.18520\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.9002 - val_loss: 2.2724 - lr: 7.7782e-05\n",
      "Epoch 35/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8932\n",
      "Epoch 35: val_loss did not improve from 2.18520\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.8932 - val_loss: 2.3376 - lr: 7.7004e-05\n",
      "Epoch 36/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8922\n",
      "Epoch 36: val_loss did not improve from 2.18520\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.8922 - val_loss: 2.4136 - lr: 7.6234e-05\n",
      "Epoch 37/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8760\n",
      "Epoch 37: val_loss did not improve from 2.18520\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.8760 - val_loss: 2.5163 - lr: 7.5472e-05\n",
      "Epoch 38/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8850\n",
      "Epoch 38: val_loss did not improve from 2.18520\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.8850 - val_loss: 2.3127 - lr: 7.4717e-05\n",
      "Epoch 39/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8680\n",
      "Epoch 39: val_loss did not improve from 2.18520\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.8680 - val_loss: 2.2269 - lr: 7.3970e-05\n",
      "Epoch 40/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8606\n",
      "Epoch 40: val_loss did not improve from 2.18520\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.8606 - val_loss: 2.3337 - lr: 7.3230e-05\n",
      "Epoch 41/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8660\n",
      "Epoch 41: val_loss did not improve from 2.18520\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.8660 - val_loss: 2.3315 - lr: 7.2498e-05\n",
      "Epoch 42/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8753\n",
      "Epoch 42: val_loss did not improve from 2.18520\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.8753 - val_loss: 2.3743 - lr: 7.1773e-05\n",
      "Epoch 43/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8659\n",
      "Epoch 43: val_loss did not improve from 2.18520\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.8659 - val_loss: 2.2215 - lr: 7.1055e-05\n",
      "Epoch 44/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8623\n",
      "Epoch 44: val_loss did not improve from 2.18520\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.8623 - val_loss: 2.2926 - lr: 7.0345e-05\n",
      "Epoch 45/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8547\n",
      "Epoch 45: val_loss did not improve from 2.18520\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 7s 103ms/step - loss: 0.8547 - val_loss: 2.2007 - lr: 6.9641e-05\n",
      "Epoch 46/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8612\n",
      "Epoch 46: val_loss did not improve from 2.18520\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.8584 - val_loss: 2.2227 - lr: 6.8945e-05\n",
      "Epoch 47/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8680\n",
      "Epoch 47: val_loss did not improve from 2.18520\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.8670 - val_loss: 2.3126 - lr: 6.8255e-05\n",
      "Epoch 48/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8530\n",
      "Epoch 48: val_loss did not improve from 2.18520\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.8518 - val_loss: 2.2564 - lr: 6.7573e-05\n",
      "Epoch 49/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8524\n",
      "Epoch 49: val_loss improved from 2.18520 to 2.18118, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.8516 - val_loss: 2.1812 - lr: 6.6897e-05\n",
      "Epoch 50/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8404\n",
      "Epoch 50: val_loss did not improve from 2.18118\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.8396 - val_loss: 2.3568 - lr: 6.6897e-05\n",
      "Epoch 51/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8624\n",
      "Epoch 51: val_loss improved from 2.18118 to 2.13314, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8616 - val_loss: 2.1331 - lr: 6.6228e-05\n",
      "Epoch 52/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8705\n",
      "Epoch 52: val_loss did not improve from 2.13314\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.8701 - val_loss: 2.2024 - lr: 6.6228e-05\n",
      "Epoch 53/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8472\n",
      "Epoch 53: val_loss did not improve from 2.13314\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.8475 - val_loss: 2.2203 - lr: 6.5566e-05\n",
      "Epoch 54/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8645\n",
      "Epoch 54: val_loss did not improve from 2.13314\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.8628 - val_loss: 2.3264 - lr: 6.4910e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8501\n",
      "Epoch 55: val_loss improved from 2.13314 to 2.09333, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 5s 81ms/step - loss: 0.8493 - val_loss: 2.0933 - lr: 6.4261e-05\n",
      "Epoch 56/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8315\n",
      "Epoch 56: val_loss did not improve from 2.09333\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8322 - val_loss: 2.4045 - lr: 6.4261e-05\n",
      "Epoch 57/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8508\n",
      "Epoch 57: val_loss did not improve from 2.09333\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 5s 78ms/step - loss: 0.8488 - val_loss: 2.3331 - lr: 6.3619e-05\n",
      "Epoch 58/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8295\n",
      "Epoch 58: val_loss did not improve from 2.09333\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8319 - val_loss: 2.1100 - lr: 6.2982e-05\n",
      "Epoch 59/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8292\n",
      "Epoch 59: val_loss did not improve from 2.09333\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8300 - val_loss: 2.1608 - lr: 6.2353e-05\n",
      "Epoch 60/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8391\n",
      "Epoch 60: val_loss did not improve from 2.09333\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8377 - val_loss: 2.2174 - lr: 6.1729e-05\n",
      "Epoch 61/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8313\n",
      "Epoch 61: val_loss did not improve from 2.09333\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8319 - val_loss: 2.2041 - lr: 6.1112e-05\n",
      "Epoch 62/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8386\n",
      "Epoch 62: val_loss did not improve from 2.09333\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8357 - val_loss: 2.2801 - lr: 6.0501e-05\n",
      "Epoch 63/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8380\n",
      "Epoch 63: val_loss improved from 2.09333 to 2.05769, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8378 - val_loss: 2.0577 - lr: 5.9896e-05\n",
      "Epoch 64/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8255\n",
      "Epoch 64: val_loss did not improve from 2.05769\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8276 - val_loss: 2.1215 - lr: 5.9896e-05\n",
      "Epoch 65/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8365\n",
      "Epoch 65: val_loss did not improve from 2.05769\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 5.8703650938696225e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8363 - val_loss: 2.3758 - lr: 5.9297e-05\n",
      "Epoch 66/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8245\n",
      "Epoch 66: val_loss did not improve from 2.05769\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 5.811661550978897e-05.\n",
      "66/66 [==============================] - 5s 82ms/step - loss: 0.8245 - val_loss: 2.1398 - lr: 5.8704e-05\n",
      "Epoch 67/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8317\n",
      "Epoch 67: val_loss did not improve from 2.05769\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 5.753545068728272e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8308 - val_loss: 2.1558 - lr: 5.8117e-05\n",
      "Epoch 68/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8334\n",
      "Epoch 68: val_loss did not improve from 2.05769\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 5.6960095243994146e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8344 - val_loss: 2.1964 - lr: 5.7535e-05\n",
      "Epoch 69/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8317\n",
      "Epoch 69: val_loss did not improve from 2.05769\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 5.639049515593797e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8317 - val_loss: 2.0920 - lr: 5.6960e-05\n",
      "Epoch 70/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8341\n",
      "Epoch 70: val_loss did not improve from 2.05769\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 5.5826589195930865e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8314 - val_loss: 2.1365 - lr: 5.6390e-05\n",
      "Epoch 71/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8224\n",
      "Epoch 71: val_loss improved from 2.05769 to 2.04664, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.8219 - val_loss: 2.0466 - lr: 5.5827e-05\n",
      "Epoch 72/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8156\n",
      "Epoch 72: val_loss did not improve from 2.04664\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 5.526832333998754e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8170 - val_loss: 2.2307 - lr: 5.5827e-05\n",
      "Epoch 73/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8256\n",
      "Epoch 73: val_loss did not improve from 2.04664\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 5.471563996252371e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8239 - val_loss: 2.1949 - lr: 5.5268e-05\n",
      "Epoch 74/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8111\n",
      "Epoch 74: val_loss improved from 2.04664 to 2.03178, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.8109 - val_loss: 2.0318 - lr: 5.4716e-05\n",
      "Epoch 75/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8104\n",
      "Epoch 75: val_loss did not improve from 2.03178\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 5.416848503955407e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8111 - val_loss: 2.2154 - lr: 5.4716e-05\n",
      "Epoch 76/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8223\n",
      "Epoch 76: val_loss did not improve from 2.03178\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 5.362680094549433e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8217 - val_loss: 2.2084 - lr: 5.4168e-05\n",
      "Epoch 77/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8118\n",
      "Epoch 77: val_loss did not improve from 2.03178\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 5.309053365635918e-05.\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.8148 - val_loss: 2.2434 - lr: 5.3627e-05\n",
      "Epoch 78/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8310\n",
      "Epoch 78: val_loss did not improve from 2.03178\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 5.2559629148163366e-05.\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.8314 - val_loss: 2.3181 - lr: 5.3091e-05\n",
      "Epoch 79/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8168\n",
      "Epoch 79: val_loss did not improve from 2.03178\n",
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 5.2034033396921584e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8146 - val_loss: 2.1223 - lr: 5.2560e-05\n",
      "Epoch 80/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8137\n",
      "Epoch 80: val_loss improved from 2.03178 to 2.00076, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8146 - val_loss: 2.0008 - lr: 5.2034e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8164\n",
      "Epoch 81: val_loss did not improve from 2.00076\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 5.1513692378648555e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8180 - val_loss: 2.0532 - lr: 5.2034e-05\n",
      "Epoch 82/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8147\n",
      "Epoch 82: val_loss did not improve from 2.00076\n",
      "\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 5.099855567095801e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8142 - val_loss: 2.0016 - lr: 5.1514e-05\n",
      "Epoch 83/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8087\n",
      "Epoch 83: val_loss did not improve from 2.00076\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 5.048856924986467e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8107 - val_loss: 2.2378 - lr: 5.0999e-05\n",
      "Epoch 84/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8173\n",
      "Epoch 84: val_loss did not improve from 2.00076\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 4.998368269298226e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8153 - val_loss: 2.0735 - lr: 5.0489e-05\n",
      "Epoch 85/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8118\n",
      "Epoch 85: val_loss did not improve from 2.00076\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 4.9483845577924514e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8106 - val_loss: 2.0576 - lr: 4.9984e-05\n",
      "Epoch 86/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8061\n",
      "Epoch 86: val_loss did not improve from 2.00076\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 4.898900748230517e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8040 - val_loss: 2.1818 - lr: 4.9484e-05\n",
      "Epoch 87/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8057\n",
      "Epoch 87: val_loss did not improve from 2.00076\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 4.849911798373796e-05.\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.8051 - val_loss: 2.1245 - lr: 4.8989e-05\n",
      "Epoch 88/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8033\n",
      "Epoch 88: val_loss did not improve from 2.00076\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 4.801412665983662e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8033 - val_loss: 2.1531 - lr: 4.8499e-05\n",
      "Epoch 89/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8139\n",
      "Epoch 89: val_loss did not improve from 2.00076\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 4.75339866898139e-05.\n",
      "66/66 [==============================] - 5s 82ms/step - loss: 0.8125 - val_loss: 2.1438 - lr: 4.8014e-05\n",
      "Epoch 90/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8146\n",
      "Epoch 90: val_loss did not improve from 2.00076\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 4.705864765128354e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8130 - val_loss: 2.0659 - lr: 4.7534e-05\n",
      "Epoch 91/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8005\n",
      "Epoch 91: val_loss did not improve from 2.00076\n",
      "\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 4.658806272345828e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8008 - val_loss: 2.0865 - lr: 4.7059e-05\n",
      "Epoch 92/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8006\n",
      "Epoch 92: val_loss did not improve from 2.00076\n",
      "\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 4.612218148395186e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.7997 - val_loss: 2.0801 - lr: 4.6588e-05\n",
      "Epoch 93/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7994\n",
      "Epoch 93: val_loss did not improve from 2.00076\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 4.566096071357606e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.7999 - val_loss: 2.1277 - lr: 4.6122e-05\n",
      "Epoch 94/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8067\n",
      "Epoch 94: val_loss did not improve from 2.00076\n",
      "\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 4.52043499899446e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8065 - val_loss: 2.0489 - lr: 4.5661e-05\n",
      "Epoch 95/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8050\n",
      "Epoch 95: val_loss did not improve from 2.00076\n",
      "\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 4.475230609386927e-05.\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.8045 - val_loss: 2.0713 - lr: 4.5204e-05\n",
      "Epoch 96/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8082\n",
      "Epoch 96: val_loss did not improve from 2.00076\n",
      "\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 4.43047822045628e-05.\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.8082 - val_loss: 2.0431 - lr: 4.4752e-05\n",
      "Epoch 97/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8048\n",
      "Epoch 97: val_loss did not improve from 2.00076\n",
      "\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 4.386173510283697e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8062 - val_loss: 2.1349 - lr: 4.4305e-05\n",
      "Epoch 98/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8021\n",
      "Epoch 98: val_loss did not improve from 2.00076\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 4.342311796790454e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8029 - val_loss: 2.0908 - lr: 4.3862e-05\n",
      "Epoch 99/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7936\n",
      "Epoch 99: val_loss did not improve from 2.00076\n",
      "\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 4.298888758057728e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.7935 - val_loss: 2.1933 - lr: 4.3423e-05\n",
      "Epoch 100/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7989\n",
      "Epoch 100: val_loss did not improve from 2.00076\n",
      "\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 4.2558997120067946e-05.\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.7989 - val_loss: 2.1406 - lr: 4.2989e-05\n",
      "Epoch 101/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7926\n",
      "Epoch 101: val_loss did not improve from 2.00076\n",
      "\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 4.2133406968787315e-05.\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.7947 - val_loss: 2.0445 - lr: 4.2559e-05\n",
      "Epoch 102/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7927\n",
      "Epoch 102: val_loss did not improve from 2.00076\n",
      "\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 4.1712073907547166e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.7915 - val_loss: 2.0597 - lr: 4.2133e-05\n",
      "Epoch 103/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8044\n",
      "Epoch 103: val_loss did not improve from 2.00076\n",
      "\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 4.1294954717159274e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8030 - val_loss: 2.1678 - lr: 4.1712e-05\n",
      "Epoch 104/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7949\n",
      "Epoch 104: val_loss did not improve from 2.00076\n",
      "\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 4.08820061784354e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.7964 - val_loss: 2.1470 - lr: 4.1295e-05\n",
      "Epoch 105/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7930\n",
      "Epoch 105: val_loss did not improve from 2.00076\n",
      "\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 4.0473185072187336e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.7931 - val_loss: 2.1099 - lr: 4.0882e-05\n",
      "Epoch 106/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7997\n",
      "Epoch 106: val_loss did not improve from 2.00076\n",
      "\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 4.0068451780825854e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.7977 - val_loss: 2.1053 - lr: 4.0473e-05\n",
      "Epoch 107/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8043\n",
      "Epoch 107: val_loss did not improve from 2.00076\n",
      "\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 3.966776668676175e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 5s 79ms/step - loss: 0.8062 - val_loss: 2.0659 - lr: 4.0068e-05\n",
      "Epoch 108/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7883\n",
      "Epoch 108: val_loss did not improve from 2.00076\n",
      "\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 3.927109017240582e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.7893 - val_loss: 2.0470 - lr: 3.9668e-05\n",
      "Epoch 109/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7877\n",
      "Epoch 109: val_loss did not improve from 2.00076\n",
      "\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 3.887837901856983e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.7868 - val_loss: 2.0410 - lr: 3.9271e-05\n",
      "Epoch 110/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7889\n",
      "Epoch 110: val_loss did not improve from 2.00076\n",
      "\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 3.848959360766457e-05.\n",
      "66/66 [==============================] - 5s 79ms/step - loss: 0.7872 - val_loss: 2.0699 - lr: 3.8878e-05\n",
      "Epoch 111/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7781\n",
      "Epoch 111: val_loss improved from 2.00076 to 1.98219, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 5s 80ms/step - loss: 0.7778 - val_loss: 1.9822 - lr: 3.8490e-05\n",
      "Epoch 112/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7865\n",
      "Epoch 112: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 3.810469792369986e-05.\n",
      "66/66 [==============================] - 5s 83ms/step - loss: 0.7865 - val_loss: 2.0819 - lr: 3.8490e-05\n",
      "Epoch 113/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7846\n",
      "Epoch 113: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 3.772365234908648e-05.\n",
      "66/66 [==============================] - 9s 134ms/step - loss: 0.7846 - val_loss: 2.0962 - lr: 3.8105e-05\n",
      "Epoch 114/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7924\n",
      "Epoch 114: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 3.734641726623522e-05.\n",
      "66/66 [==============================] - 8s 120ms/step - loss: 0.7924 - val_loss: 2.1137 - lr: 3.7724e-05\n",
      "Epoch 115/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7916\n",
      "Epoch 115: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 3.6972953057556876e-05.\n",
      "66/66 [==============================] - 6s 98ms/step - loss: 0.7916 - val_loss: 2.0367 - lr: 3.7346e-05\n",
      "Epoch 116/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7914\n",
      "Epoch 116: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 3.660322370706126e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.7914 - val_loss: 2.0757 - lr: 3.6973e-05\n",
      "Epoch 117/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7902\n",
      "Epoch 117: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 3.623719319875818e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.7902 - val_loss: 2.1202 - lr: 3.6603e-05\n",
      "Epoch 118/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7835\n",
      "Epoch 118: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 3.5874821915058416e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.7835 - val_loss: 2.0542 - lr: 3.6237e-05\n",
      "Epoch 119/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7845\n",
      "Epoch 119: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 3.551607383997179e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.7845 - val_loss: 2.1155 - lr: 3.5875e-05\n",
      "Epoch 120/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7819\n",
      "Epoch 120: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 3.516091295750812e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.7819 - val_loss: 2.0829 - lr: 3.5516e-05\n",
      "Epoch 121/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7929\n",
      "Epoch 121: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 3.480930325167719e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.7929 - val_loss: 2.0729 - lr: 3.5161e-05\n",
      "Epoch 122/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7822\n",
      "Epoch 122: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 3.446120870648883e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.7822 - val_loss: 2.1209 - lr: 3.4809e-05\n",
      "Epoch 123/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7740\n",
      "Epoch 123: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 123: ReduceLROnPlateau reducing learning rate to 3.4116596907551866e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.7740 - val_loss: 2.1058 - lr: 3.4461e-05\n",
      "Epoch 124/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7817\n",
      "Epoch 124: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 124: ReduceLROnPlateau reducing learning rate to 3.37754318388761e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.7817 - val_loss: 2.1323 - lr: 3.4117e-05\n",
      "Epoch 125/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7922\n",
      "Epoch 125: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 3.3437677484471354e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.7922 - val_loss: 2.0381 - lr: 3.3775e-05\n",
      "Epoch 126/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7876\n",
      "Epoch 126: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 3.310330142994644e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.7876 - val_loss: 2.0956 - lr: 3.3438e-05\n",
      "Epoch 127/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7830\n",
      "Epoch 127: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 127: ReduceLROnPlateau reducing learning rate to 3.277226765931118e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.7830 - val_loss: 2.0442 - lr: 3.3103e-05\n",
      "Epoch 128/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7782\n",
      "Epoch 128: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 128: ReduceLROnPlateau reducing learning rate to 3.24445437581744e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.7782 - val_loss: 2.0049 - lr: 3.2772e-05\n",
      "Epoch 129/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8006\n",
      "Epoch 129: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 129: ReduceLROnPlateau reducing learning rate to 3.212009731214493e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.8006 - val_loss: 2.0636 - lr: 3.2445e-05\n",
      "Epoch 130/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7760\n",
      "Epoch 130: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 130: ReduceLROnPlateau reducing learning rate to 3.17988959068316e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.7760 - val_loss: 1.9843 - lr: 3.2120e-05\n",
      "Epoch 131/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7793\n",
      "Epoch 131: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 131: ReduceLROnPlateau reducing learning rate to 3.1480907127843236e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.7793 - val_loss: 2.0893 - lr: 3.1799e-05\n",
      "Epoch 132/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7790\n",
      "Epoch 132: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 132: ReduceLROnPlateau reducing learning rate to 3.116609856078867e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.7790 - val_loss: 2.0947 - lr: 3.1481e-05\n",
      "Epoch 133/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7753\n",
      "Epoch 133: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 133: ReduceLROnPlateau reducing learning rate to 3.085443779127672e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.7753 - val_loss: 2.0693 - lr: 3.1166e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7840\n",
      "Epoch 134: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 134: ReduceLROnPlateau reducing learning rate to 3.054589240491623e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.7840 - val_loss: 2.0195 - lr: 3.0854e-05\n",
      "Epoch 135/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7816\n",
      "Epoch 135: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 135: ReduceLROnPlateau reducing learning rate to 3.024043358891504e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.7816 - val_loss: 2.0148 - lr: 3.0546e-05\n",
      "Epoch 136/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7807\n",
      "Epoch 136: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 2.9938028928881975e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.7807 - val_loss: 2.0557 - lr: 3.0240e-05\n",
      "Epoch 137/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7739\n",
      "Epoch 137: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 137: ReduceLROnPlateau reducing learning rate to 2.963864781122538e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.7739 - val_loss: 2.0953 - lr: 2.9938e-05\n",
      "Epoch 138/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7752\n",
      "Epoch 138: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 138: ReduceLROnPlateau reducing learning rate to 2.9342261423153105e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.7749 - val_loss: 2.0581 - lr: 2.9639e-05\n",
      "Epoch 139/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7826\n",
      "Epoch 139: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 139: ReduceLROnPlateau reducing learning rate to 2.9048839151073478e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.7826 - val_loss: 2.0816 - lr: 2.9342e-05\n",
      "Epoch 140/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7831\n",
      "Epoch 140: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 140: ReduceLROnPlateau reducing learning rate to 2.875835038139485e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.7831 - val_loss: 1.9825 - lr: 2.9049e-05\n",
      "Epoch 141/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7756\n",
      "Epoch 141: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 141: ReduceLROnPlateau reducing learning rate to 2.8470766301325058e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.7748 - val_loss: 2.0094 - lr: 2.8758e-05\n",
      "Epoch 142/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7791\n",
      "Epoch 142: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 142: ReduceLROnPlateau reducing learning rate to 2.8186058098071954e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.7791 - val_loss: 2.0667 - lr: 2.8471e-05\n",
      "Epoch 143/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7788\n",
      "Epoch 143: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 143: ReduceLROnPlateau reducing learning rate to 2.7904196958843385e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.7788 - val_loss: 2.0240 - lr: 2.8186e-05\n",
      "Epoch 144/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7752\n",
      "Epoch 144: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 144: ReduceLROnPlateau reducing learning rate to 2.7625155871646713e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.7752 - val_loss: 2.0544 - lr: 2.7904e-05\n",
      "Epoch 145/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7918\n",
      "Epoch 145: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 145: ReduceLROnPlateau reducing learning rate to 2.734890422289027e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.7918 - val_loss: 2.0408 - lr: 2.7625e-05\n",
      "Epoch 146/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7828\n",
      "Epoch 146: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 146: ReduceLROnPlateau reducing learning rate to 2.7075415000581416e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.7830 - val_loss: 2.0456 - lr: 2.7349e-05\n",
      "Epoch 147/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7854\n",
      "Epoch 147: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 147: ReduceLROnPlateau reducing learning rate to 2.6804661192727508e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.7854 - val_loss: 2.1020 - lr: 2.7075e-05\n",
      "Epoch 148/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7816\n",
      "Epoch 148: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 148: ReduceLROnPlateau reducing learning rate to 2.6536613986536395e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.7816 - val_loss: 2.0234 - lr: 2.6805e-05\n",
      "Epoch 149/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7716\n",
      "Epoch 149: val_loss did not improve from 1.98219\n",
      "\n",
      "Epoch 149: ReduceLROnPlateau reducing learning rate to 2.6271248170814942e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.7716 - val_loss: 2.0891 - lr: 2.6537e-05\n",
      "Epoch 150/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7665\n",
      "Epoch 150: val_loss improved from 1.98219 to 1.96693, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.7665 - val_loss: 1.9669 - lr: 2.6271e-05\n",
      "Epoch 151/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7729\n",
      "Epoch 151: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 151: ReduceLROnPlateau reducing learning rate to 2.6008534932770998e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.7729 - val_loss: 2.0756 - lr: 2.6271e-05\n",
      "Epoch 152/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7706\n",
      "Epoch 152: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 152: ReduceLROnPlateau reducing learning rate to 2.5748449061211432e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.7690 - val_loss: 2.1474 - lr: 2.6009e-05\n",
      "Epoch 153/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7780\n",
      "Epoch 153: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 153: ReduceLROnPlateau reducing learning rate to 2.5490965344943107e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.7780 - val_loss: 2.0758 - lr: 2.5748e-05\n",
      "Epoch 154/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7806\n",
      "Epoch 154: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 154: ReduceLROnPlateau reducing learning rate to 2.523605497117387e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.7806 - val_loss: 2.0173 - lr: 2.5491e-05\n",
      "Epoch 155/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7797\n",
      "Epoch 155: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 155: ReduceLROnPlateau reducing learning rate to 2.4983694529510104e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.7797 - val_loss: 2.0190 - lr: 2.5236e-05\n",
      "Epoch 156/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7742\n",
      "Epoch 156: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 156: ReduceLROnPlateau reducing learning rate to 2.4733857007959158e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.7742 - val_loss: 2.0347 - lr: 2.4984e-05\n",
      "Epoch 157/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7761\n",
      "Epoch 157: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 157: ReduceLROnPlateau reducing learning rate to 2.4486518996127415e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.7747 - val_loss: 2.0794 - lr: 2.4734e-05\n",
      "Epoch 158/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7757\n",
      "Epoch 158: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 158: ReduceLROnPlateau reducing learning rate to 2.424165348202223e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.7757 - val_loss: 2.0131 - lr: 2.4487e-05\n",
      "Epoch 159/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7731\n",
      "Epoch 159: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 159: ReduceLROnPlateau reducing learning rate to 2.3999237055249977e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.7731 - val_loss: 2.1224 - lr: 2.4242e-05\n",
      "Epoch 160/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7645\n",
      "Epoch 160: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 160: ReduceLROnPlateau reducing learning rate to 2.375924450461753e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.7645 - val_loss: 2.0344 - lr: 2.3999e-05\n",
      "Epoch 161/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7728\n",
      "Epoch 161: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 161: ReduceLROnPlateau reducing learning rate to 2.3521652419731254e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.7728 - val_loss: 2.0642 - lr: 2.3759e-05\n",
      "Epoch 162/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7696\n",
      "Epoch 162: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 162: ReduceLROnPlateau reducing learning rate to 2.3286435589398024e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.7696 - val_loss: 2.1153 - lr: 2.3522e-05\n",
      "Epoch 163/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7814\n",
      "Epoch 163: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 163: ReduceLROnPlateau reducing learning rate to 2.3053570603224217e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.7826 - val_loss: 2.1042 - lr: 2.3286e-05\n",
      "Epoch 164/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7689\n",
      "Epoch 164: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 164: ReduceLROnPlateau reducing learning rate to 2.2823034050816205e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.7697 - val_loss: 2.0475 - lr: 2.3054e-05\n",
      "Epoch 165/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7675\n",
      "Epoch 165: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 165: ReduceLROnPlateau reducing learning rate to 2.2594804322579874e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.7666 - val_loss: 2.0583 - lr: 2.2823e-05\n",
      "Epoch 166/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7668\n",
      "Epoch 166: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 166: ReduceLROnPlateau reducing learning rate to 2.2368856207322096e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.7668 - val_loss: 2.0950 - lr: 2.2595e-05\n",
      "Epoch 167/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7719\n",
      "Epoch 167: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 167: ReduceLROnPlateau reducing learning rate to 2.2145168095448753e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.7719 - val_loss: 2.1373 - lr: 2.2369e-05\n",
      "Epoch 168/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7706\n",
      "Epoch 168: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 168: ReduceLROnPlateau reducing learning rate to 2.1923716576566222e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.7706 - val_loss: 2.0592 - lr: 2.2145e-05\n",
      "Epoch 169/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7675\n",
      "Epoch 169: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 169: ReduceLROnPlateau reducing learning rate to 2.1704480041080387e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.7675 - val_loss: 2.0746 - lr: 2.1924e-05\n",
      "Epoch 170/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7730\n",
      "Epoch 170: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 170: ReduceLROnPlateau reducing learning rate to 2.1487435078597626e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.7730 - val_loss: 2.0029 - lr: 2.1704e-05\n",
      "Epoch 171/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7770\n",
      "Epoch 171: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 171: ReduceLROnPlateau reducing learning rate to 2.1272560079523828e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.7770 - val_loss: 2.0946 - lr: 2.1487e-05\n",
      "Epoch 172/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7669\n",
      "Epoch 172: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 172: ReduceLROnPlateau reducing learning rate to 2.1059835235064383e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.7669 - val_loss: 2.1149 - lr: 2.1273e-05\n",
      "Epoch 173/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7611\n",
      "Epoch 173: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 173: ReduceLROnPlateau reducing learning rate to 2.084923713482567e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.7611 - val_loss: 2.0320 - lr: 2.1060e-05\n",
      "Epoch 174/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7697\n",
      "Epoch 174: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 174: ReduceLROnPlateau reducing learning rate to 2.0640744169213575e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.7697 - val_loss: 2.1337 - lr: 2.0849e-05\n",
      "Epoch 175/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7671\n",
      "Epoch 175: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 175: ReduceLROnPlateau reducing learning rate to 2.0434336529433495e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.7671 - val_loss: 2.1312 - lr: 2.0641e-05\n",
      "Epoch 176/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7698\n",
      "Epoch 176: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 176: ReduceLROnPlateau reducing learning rate to 2.0229992605891313e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.7698 - val_loss: 2.0834 - lr: 2.0434e-05\n",
      "Epoch 177/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7659\n",
      "Epoch 177: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 177: ReduceLROnPlateau reducing learning rate to 2.0027692589792422e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.7659 - val_loss: 2.0582 - lr: 2.0230e-05\n",
      "Epoch 178/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7609\n",
      "Epoch 178: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 178: ReduceLROnPlateau reducing learning rate to 1.9827414871542714e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.7609 - val_loss: 2.0914 - lr: 2.0028e-05\n",
      "Epoch 179/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7669\n",
      "Epoch 179: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 179: ReduceLROnPlateau reducing learning rate to 1.9629141443147093e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.7669 - val_loss: 2.1579 - lr: 1.9827e-05\n",
      "Epoch 180/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7667\n",
      "Epoch 180: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 180: ReduceLROnPlateau reducing learning rate to 1.943285069501144e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.7667 - val_loss: 2.1046 - lr: 1.9629e-05\n",
      "Epoch 181/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7691\n",
      "Epoch 181: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 181: ReduceLROnPlateau reducing learning rate to 1.9238522818341152e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.7675 - val_loss: 2.0779 - lr: 1.9433e-05\n",
      "Epoch 182/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7704\n",
      "Epoch 182: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 182: ReduceLROnPlateau reducing learning rate to 1.9046138004341628e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.7704 - val_loss: 2.0824 - lr: 1.9239e-05\n",
      "Epoch 183/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7752\n",
      "Epoch 183: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 183: ReduceLROnPlateau reducing learning rate to 1.885567644421826e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 0.7752 - val_loss: 2.0793 - lr: 1.9046e-05\n",
      "Epoch 184/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7652\n",
      "Epoch 184: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 184: ReduceLROnPlateau reducing learning rate to 1.8667120129975956e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.7652 - val_loss: 2.0401 - lr: 1.8856e-05\n",
      "Epoch 185/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7735\n",
      "Epoch 185: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 185: ReduceLROnPlateau reducing learning rate to 1.8480449252820108e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.7735 - val_loss: 2.0539 - lr: 1.8667e-05\n",
      "Epoch 186/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - ETA: 0s - loss: 0.7689\n",
      "Epoch 186: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 186: ReduceLROnPlateau reducing learning rate to 1.8295644003956113e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.7689 - val_loss: 2.0973 - lr: 1.8480e-05\n",
      "Epoch 187/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7664\n",
      "Epoch 187: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 187: ReduceLROnPlateau reducing learning rate to 1.8112688176188387e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.7664 - val_loss: 2.0963 - lr: 1.8296e-05\n",
      "Epoch 188/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7544\n",
      "Epoch 188: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 188: ReduceLROnPlateau reducing learning rate to 1.793156196072232e-05.\n",
      "66/66 [==============================] - 7s 100ms/step - loss: 0.7544 - val_loss: 2.1498 - lr: 1.8113e-05\n",
      "Epoch 189/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7684\n",
      "Epoch 189: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 189: ReduceLROnPlateau reducing learning rate to 1.775224554876331e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.7679 - val_loss: 2.0758 - lr: 1.7932e-05\n",
      "Epoch 190/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7606\n",
      "Epoch 190: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 190: ReduceLROnPlateau reducing learning rate to 1.7574722733115777e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.7605 - val_loss: 2.0810 - lr: 1.7752e-05\n",
      "Epoch 191/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7706\n",
      "Epoch 191: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 191: ReduceLROnPlateau reducing learning rate to 1.739897550578462e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.7706 - val_loss: 2.0931 - lr: 1.7575e-05\n",
      "Epoch 192/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7631\n",
      "Epoch 192: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 192: ReduceLROnPlateau reducing learning rate to 1.7224985858774742e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.7631 - val_loss: 2.0732 - lr: 1.7399e-05\n",
      "Epoch 193/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7653\n",
      "Epoch 193: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 193: ReduceLROnPlateau reducing learning rate to 1.7052735784091056e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.7653 - val_loss: 2.0459 - lr: 1.7225e-05\n",
      "Epoch 194/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7639\n",
      "Epoch 194: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 194: ReduceLROnPlateau reducing learning rate to 1.6882209074537968e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.7639 - val_loss: 2.0358 - lr: 1.7053e-05\n",
      "Epoch 195/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7747\n",
      "Epoch 195: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 195: ReduceLROnPlateau reducing learning rate to 1.6713387722120388e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.7770 - val_loss: 2.0663 - lr: 1.6882e-05\n",
      "Epoch 196/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7618\n",
      "Epoch 196: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 196: ReduceLROnPlateau reducing learning rate to 1.6546253718843217e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.7631 - val_loss: 2.0537 - lr: 1.6713e-05\n",
      "Epoch 197/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7562\n",
      "Epoch 197: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 197: ReduceLROnPlateau reducing learning rate to 1.6380790857510874e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.7560 - val_loss: 2.1317 - lr: 1.6546e-05\n",
      "Epoch 198/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7713\n",
      "Epoch 198: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 198: ReduceLROnPlateau reducing learning rate to 1.621698293092777e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.7713 - val_loss: 2.0647 - lr: 1.6381e-05\n",
      "Epoch 199/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7624\n",
      "Epoch 199: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 199: ReduceLROnPlateau reducing learning rate to 1.605481373189832e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.7624 - val_loss: 2.0984 - lr: 1.6217e-05\n",
      "Epoch 200/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7618\n",
      "Epoch 200: val_loss did not improve from 1.96693\n",
      "\n",
      "Epoch 200: ReduceLROnPlateau reducing learning rate to 1.589426525242743e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.7618 - val_loss: 2.1323 - lr: 1.6055e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_24\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_25 (InputLayer)          [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_72 (LSTM)                 (None, 45, 24)       3744        ['input_25[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_48 (Dropout)           (None, 45, 24)       0           ['lstm_72[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_73 (LSTM)                 (None, 45, 16)       2624        ['dropout_48[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_49 (Dropout)           (None, 45, 16)       0           ['lstm_73[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_74 (LSTM)                 (None, 32)           6272        ['dropout_49[0][0]']             \n",
      "                                                                                                  \n",
      " dense_48 (Dense)               (None, 40)           1320        ['lstm_74[0][0]']                \n",
      "                                                                                                  \n",
      " dense_49 (Dense)               (None, 5)            205         ['dense_48[0][0]']               \n",
      "                                                                                                  \n",
      " tf.unstack_24 (TFOpLambda)     [(None,),            0           ['dense_49[0][0]']               \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_120 (TFOpLambda  (None, 1)           0           ['tf.unstack_24[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_48 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_120[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_124 (TFOpLambda  (None, 1)           0           ['tf.unstack_24[0][4]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_72 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_48[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_49 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_124[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_73 (TFOpLambd  (None, 1)           0           ['tf.math.multiply_72[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_121 (TFOpLambda  (None, 1)           0           ['tf.unstack_24[0][1]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_123 (TFOpLambda  (None, 1)           0           ['tf.unstack_24[0][3]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_74 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_49[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_48 (TFOpL  (None, 1)           0           ['tf.math.multiply_73[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.softplus_48 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_121[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_122 (TFOpLambda  (None, 1)           0           ['tf.unstack_24[0][2]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.softplus_49 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_123[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_49 (TFOpL  (None, 1)           0           ['tf.math.multiply_74[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack_24 (TFOpLambda)       (None, 5, 1)         0           ['tf.__operators__.add_48[0][0]',\n",
      "                                                                  'tf.math.softplus_48[0][0]',    \n",
      "                                                                  'tf.expand_dims_122[0][0]',     \n",
      "                                                                  'tf.math.softplus_49[0][0]',    \n",
      "                                                                  'tf.__operators__.add_49[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _2_CCMP_t+1_0.14\n",
      "Epoch 1/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 3.4772\n",
      "Epoch 1: val_loss improved from inf to 4.55640, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 10s 92ms/step - loss: 3.4754 - val_loss: 4.5564 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 2.9014\n",
      "Epoch 2: val_loss improved from 4.55640 to 4.10206, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.14.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 6s 84ms/step - loss: 2.9014 - val_loss: 4.1021 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 2.0559\n",
      "Epoch 3: val_loss improved from 4.10206 to 3.33954, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 2.0559 - val_loss: 3.3395 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.7038\n",
      "Epoch 4: val_loss improved from 3.33954 to 2.93465, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 1.7038 - val_loss: 2.9347 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.5077\n",
      "Epoch 5: val_loss improved from 2.93465 to 2.77192, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 1.5077 - val_loss: 2.7719 - lr: 1.0000e-04\n",
      "Epoch 6/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.3910\n",
      "Epoch 6: val_loss improved from 2.77192 to 2.56698, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 6s 84ms/step - loss: 1.3910 - val_loss: 2.5670 - lr: 1.0000e-04\n",
      "Epoch 7/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.3042\n",
      "Epoch 7: val_loss improved from 2.56698 to 2.48440, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 1.3042 - val_loss: 2.4844 - lr: 1.0000e-04\n",
      "Epoch 8/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.2449\n",
      "Epoch 8: val_loss improved from 2.48440 to 2.43804, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 6s 84ms/step - loss: 1.2449 - val_loss: 2.4380 - lr: 1.0000e-04\n",
      "Epoch 9/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1858\n",
      "Epoch 9: val_loss improved from 2.43804 to 2.34555, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 5s 83ms/step - loss: 1.1860 - val_loss: 2.3455 - lr: 1.0000e-04\n",
      "Epoch 10/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1550\n",
      "Epoch 10: val_loss did not improve from 2.34555\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 5s 83ms/step - loss: 1.1550 - val_loss: 2.5882 - lr: 1.0000e-04\n",
      "Epoch 11/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1117\n",
      "Epoch 11: val_loss improved from 2.34555 to 2.30654, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 1.1117 - val_loss: 2.3065 - lr: 9.9000e-05\n",
      "Epoch 12/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1036\n",
      "Epoch 12: val_loss did not improve from 2.30654\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 1.1036 - val_loss: 2.3170 - lr: 9.9000e-05\n",
      "Epoch 13/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0706\n",
      "Epoch 13: val_loss did not improve from 2.30654\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 6s 84ms/step - loss: 1.0700 - val_loss: 2.4253 - lr: 9.8010e-05\n",
      "Epoch 14/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0424\n",
      "Epoch 14: val_loss improved from 2.30654 to 2.22916, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 6s 84ms/step - loss: 1.0424 - val_loss: 2.2292 - lr: 9.7030e-05\n",
      "Epoch 15/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0796\n",
      "Epoch 15: val_loss did not improve from 2.22916\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 5s 82ms/step - loss: 1.0783 - val_loss: 2.3914 - lr: 9.7030e-05\n",
      "Epoch 16/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0222\n",
      "Epoch 16: val_loss did not improve from 2.22916\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 5s 83ms/step - loss: 1.0220 - val_loss: 2.2971 - lr: 9.6060e-05\n",
      "Epoch 17/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0063\n",
      "Epoch 17: val_loss did not improve from 2.22916\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 6s 83ms/step - loss: 1.0063 - val_loss: 2.3196 - lr: 9.5099e-05\n",
      "Epoch 18/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9810\n",
      "Epoch 18: val_loss did not improve from 2.22916\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 5s 82ms/step - loss: 0.9822 - val_loss: 2.2670 - lr: 9.4148e-05\n",
      "Epoch 19/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9801\n",
      "Epoch 19: val_loss did not improve from 2.22916\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 5s 83ms/step - loss: 0.9826 - val_loss: 2.2435 - lr: 9.3207e-05\n",
      "Epoch 20/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9849\n",
      "Epoch 20: val_loss improved from 2.22916 to 2.18093, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 6s 84ms/step - loss: 0.9849 - val_loss: 2.1809 - lr: 9.2274e-05\n",
      "Epoch 21/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9869\n",
      "Epoch 21: val_loss improved from 2.18093 to 2.15146, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 6s 84ms/step - loss: 0.9869 - val_loss: 2.1515 - lr: 9.2274e-05\n",
      "Epoch 22/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9593\n",
      "Epoch 22: val_loss did not improve from 2.15146\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 6s 83ms/step - loss: 0.9608 - val_loss: 2.2160 - lr: 9.2274e-05\n",
      "Epoch 23/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9589\n",
      "Epoch 23: val_loss improved from 2.15146 to 2.06461, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 6s 84ms/step - loss: 0.9577 - val_loss: 2.0646 - lr: 9.1352e-05\n",
      "Epoch 24/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9543\n",
      "Epoch 24: val_loss did not improve from 2.06461\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 5s 83ms/step - loss: 0.9543 - val_loss: 2.2241 - lr: 9.1352e-05\n",
      "Epoch 25/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9481\n",
      "Epoch 25: val_loss did not improve from 2.06461\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 5s 83ms/step - loss: 0.9479 - val_loss: 2.2549 - lr: 9.0438e-05\n",
      "Epoch 26/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9413\n",
      "Epoch 26: val_loss did not improve from 2.06461\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 5s 83ms/step - loss: 0.9412 - val_loss: 2.2224 - lr: 8.9534e-05\n",
      "Epoch 27/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9332\n",
      "Epoch 27: val_loss did not improve from 2.06461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 5s 83ms/step - loss: 0.9319 - val_loss: 2.1589 - lr: 8.8638e-05\n",
      "Epoch 28/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9369\n",
      "Epoch 28: val_loss did not improve from 2.06461\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 5s 82ms/step - loss: 0.9350 - val_loss: 2.2642 - lr: 8.7752e-05\n",
      "Epoch 29/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9235\n",
      "Epoch 29: val_loss did not improve from 2.06461\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 6s 84ms/step - loss: 0.9235 - val_loss: 2.3550 - lr: 8.6875e-05\n",
      "Epoch 30/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9262\n",
      "Epoch 30: val_loss did not improve from 2.06461\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 6s 84ms/step - loss: 0.9262 - val_loss: 2.2050 - lr: 8.6006e-05\n",
      "Epoch 31/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9132\n",
      "Epoch 31: val_loss did not improve from 2.06461\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 6s 84ms/step - loss: 0.9132 - val_loss: 2.2325 - lr: 8.5146e-05\n",
      "Epoch 32/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9163\n",
      "Epoch 32: val_loss did not improve from 2.06461\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 6s 83ms/step - loss: 0.9163 - val_loss: 2.0712 - lr: 8.4294e-05\n",
      "Epoch 33/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9216\n",
      "Epoch 33: val_loss did not improve from 2.06461\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9202 - val_loss: 2.1815 - lr: 8.3451e-05\n",
      "Epoch 34/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9201\n",
      "Epoch 34: val_loss did not improve from 2.06461\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.9201 - val_loss: 2.1017 - lr: 8.2617e-05\n",
      "Epoch 35/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9255\n",
      "Epoch 35: val_loss did not improve from 2.06461\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.9257 - val_loss: 2.1829 - lr: 8.1791e-05\n",
      "Epoch 36/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9081\n",
      "Epoch 36: val_loss did not improve from 2.06461\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9081 - val_loss: 2.2154 - lr: 8.0973e-05\n",
      "Epoch 37/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9054\n",
      "Epoch 37: val_loss did not improve from 2.06461\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.9054 - val_loss: 2.2192 - lr: 8.0163e-05\n",
      "Epoch 38/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9191\n",
      "Epoch 38: val_loss did not improve from 2.06461\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.9191 - val_loss: 2.1475 - lr: 7.9361e-05\n",
      "Epoch 39/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9172\n",
      "Epoch 39: val_loss did not improve from 2.06461\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.9172 - val_loss: 2.1169 - lr: 7.8568e-05\n",
      "Epoch 40/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8936\n",
      "Epoch 40: val_loss did not improve from 2.06461\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.8936 - val_loss: 2.1172 - lr: 7.7782e-05\n",
      "Epoch 41/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9036\n",
      "Epoch 41: val_loss did not improve from 2.06461\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9042 - val_loss: 2.2590 - lr: 7.7004e-05\n",
      "Epoch 42/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8884\n",
      "Epoch 42: val_loss did not improve from 2.06461\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.8884 - val_loss: 2.1791 - lr: 7.6234e-05\n",
      "Epoch 43/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8883\n",
      "Epoch 43: val_loss did not improve from 2.06461\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.8883 - val_loss: 2.2472 - lr: 7.5472e-05\n",
      "Epoch 44/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8959\n",
      "Epoch 44: val_loss improved from 2.06461 to 2.06391, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.8959 - val_loss: 2.0639 - lr: 7.4717e-05\n",
      "Epoch 45/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8953\n",
      "Epoch 45: val_loss did not improve from 2.06391\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.8953 - val_loss: 2.1530 - lr: 7.4717e-05\n",
      "Epoch 46/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8953\n",
      "Epoch 46: val_loss did not improve from 2.06391\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.8953 - val_loss: 2.1174 - lr: 7.3970e-05\n",
      "Epoch 47/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8902\n",
      "Epoch 47: val_loss improved from 2.06391 to 2.04122, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.8902 - val_loss: 2.0412 - lr: 7.3230e-05\n",
      "Epoch 48/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8826\n",
      "Epoch 48: val_loss did not improve from 2.04122\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.8826 - val_loss: 2.1554 - lr: 7.3230e-05\n",
      "Epoch 49/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8861\n",
      "Epoch 49: val_loss did not improve from 2.04122\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.8861 - val_loss: 2.1028 - lr: 7.2498e-05\n",
      "Epoch 50/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8910\n",
      "Epoch 50: val_loss did not improve from 2.04122\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.8910 - val_loss: 2.0927 - lr: 7.1773e-05\n",
      "Epoch 51/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9004\n",
      "Epoch 51: val_loss did not improve from 2.04122\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9004 - val_loss: 2.1563 - lr: 7.1055e-05\n",
      "Epoch 52/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8924\n",
      "Epoch 52: val_loss did not improve from 2.04122\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.8924 - val_loss: 2.0748 - lr: 7.0345e-05\n",
      "Epoch 53/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8770\n",
      "Epoch 53: val_loss did not improve from 2.04122\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 6s 86ms/step - loss: 0.8770 - val_loss: 2.0966 - lr: 6.9641e-05\n",
      "Epoch 54/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8807\n",
      "Epoch 54: val_loss did not improve from 2.04122\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.8807 - val_loss: 2.1375 - lr: 6.8945e-05\n",
      "Epoch 55/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8733\n",
      "Epoch 55: val_loss did not improve from 2.04122\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.8733 - val_loss: 2.1550 - lr: 6.8255e-05\n",
      "Epoch 56/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8803\n",
      "Epoch 56: val_loss did not improve from 2.04122\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.8803 - val_loss: 2.0814 - lr: 6.7573e-05\n",
      "Epoch 57/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8786\n",
      "Epoch 57: val_loss did not improve from 2.04122\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 0.8786 - val_loss: 2.0863 - lr: 6.6897e-05\n",
      "Epoch 58/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8693\n",
      "Epoch 58: val_loss did not improve from 2.04122\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.8693 - val_loss: 2.1490 - lr: 6.6228e-05\n",
      "Epoch 59/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8699\n",
      "Epoch 59: val_loss did not improve from 2.04122\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.8699 - val_loss: 2.1391 - lr: 6.5566e-05\n",
      "Epoch 60/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8682\n",
      "Epoch 60: val_loss did not improve from 2.04122\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.8682 - val_loss: 2.2131 - lr: 6.4910e-05\n",
      "Epoch 61/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8775\n",
      "Epoch 61: val_loss did not improve from 2.04122\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 0.8775 - val_loss: 2.1818 - lr: 6.4261e-05\n",
      "Epoch 62/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8681\n",
      "Epoch 62: val_loss did not improve from 2.04122\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 0.8681 - val_loss: 2.1461 - lr: 6.3619e-05\n",
      "Epoch 63/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8634\n",
      "Epoch 63: val_loss did not improve from 2.04122\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.8634 - val_loss: 2.2678 - lr: 6.2982e-05\n",
      "Epoch 64/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8644\n",
      "Epoch 64: val_loss did not improve from 2.04122\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.8644 - val_loss: 2.1789 - lr: 6.2353e-05\n",
      "Epoch 65/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8730\n",
      "Epoch 65: val_loss did not improve from 2.04122\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.8730 - val_loss: 2.1188 - lr: 6.1729e-05\n",
      "Epoch 66/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8797\n",
      "Epoch 66: val_loss did not improve from 2.04122\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.8797 - val_loss: 2.0955 - lr: 6.1112e-05\n",
      "Epoch 67/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8655\n",
      "Epoch 67: val_loss did not improve from 2.04122\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 0.8655 - val_loss: 2.1577 - lr: 6.0501e-05\n",
      "Epoch 68/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8716\n",
      "Epoch 68: val_loss did not improve from 2.04122\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 0.8709 - val_loss: 2.1107 - lr: 5.9896e-05\n",
      "Epoch 69/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8608\n",
      "Epoch 69: val_loss did not improve from 2.04122\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 5.8703650938696225e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 0.8612 - val_loss: 2.1410 - lr: 5.9297e-05\n",
      "Epoch 70/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8658\n",
      "Epoch 70: val_loss did not improve from 2.04122\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 5.811661550978897e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 0.8658 - val_loss: 2.1302 - lr: 5.8704e-05\n",
      "Epoch 71/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8669\n",
      "Epoch 71: val_loss did not improve from 2.04122\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 5.753545068728272e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 0.8665 - val_loss: 2.1784 - lr: 5.8117e-05\n",
      "Epoch 72/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8719\n",
      "Epoch 72: val_loss did not improve from 2.04122\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 5.6960095243994146e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 0.8719 - val_loss: 2.1857 - lr: 5.7535e-05\n",
      "Epoch 73/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8607\n",
      "Epoch 73: val_loss improved from 2.04122 to 1.99925, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 0.8607 - val_loss: 1.9992 - lr: 5.6960e-05\n",
      "Epoch 74/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8724\n",
      "Epoch 74: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 5.639049515593797e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 0.8724 - val_loss: 2.0408 - lr: 5.6960e-05\n",
      "Epoch 75/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8586\n",
      "Epoch 75: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 5.5826589195930865e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.8586 - val_loss: 2.0827 - lr: 5.6390e-05\n",
      "Epoch 76/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8481\n",
      "Epoch 76: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 5.526832333998754e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 0.8481 - val_loss: 2.1374 - lr: 5.5827e-05\n",
      "Epoch 77/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8621\n",
      "Epoch 77: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 5.471563996252371e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 0.8621 - val_loss: 2.0638 - lr: 5.5268e-05\n",
      "Epoch 78/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8539\n",
      "Epoch 78: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 5.416848503955407e-05.\n",
      "66/66 [==============================] - 6s 84ms/step - loss: 0.8556 - val_loss: 2.0497 - lr: 5.4716e-05\n",
      "Epoch 79/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8539\n",
      "Epoch 79: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 5.362680094549433e-05.\n",
      "66/66 [==============================] - 6s 84ms/step - loss: 0.8539 - val_loss: 2.1767 - lr: 5.4168e-05\n",
      "Epoch 80/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - ETA: 0s - loss: 0.8584\n",
      "Epoch 80: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 5.309053365635918e-05.\n",
      "66/66 [==============================] - 5s 83ms/step - loss: 0.8584 - val_loss: 2.1774 - lr: 5.3627e-05\n",
      "Epoch 81/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8578\n",
      "Epoch 81: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 5.2559629148163366e-05.\n",
      "66/66 [==============================] - 6s 84ms/step - loss: 0.8567 - val_loss: 2.1655 - lr: 5.3091e-05\n",
      "Epoch 82/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8625\n",
      "Epoch 82: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 5.2034033396921584e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 0.8608 - val_loss: 2.1468 - lr: 5.2560e-05\n",
      "Epoch 83/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8456\n",
      "Epoch 83: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 5.1513692378648555e-05.\n",
      "66/66 [==============================] - 6s 83ms/step - loss: 0.8456 - val_loss: 2.1295 - lr: 5.2034e-05\n",
      "Epoch 84/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8519\n",
      "Epoch 84: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 5.099855567095801e-05.\n",
      "66/66 [==============================] - 5s 83ms/step - loss: 0.8519 - val_loss: 2.0817 - lr: 5.1514e-05\n",
      "Epoch 85/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8532\n",
      "Epoch 85: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 5.048856924986467e-05.\n",
      "66/66 [==============================] - 5s 83ms/step - loss: 0.8516 - val_loss: 2.1468 - lr: 5.0999e-05\n",
      "Epoch 86/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8481\n",
      "Epoch 86: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 4.998368269298226e-05.\n",
      "66/66 [==============================] - 5s 83ms/step - loss: 0.8466 - val_loss: 2.0945 - lr: 5.0489e-05\n",
      "Epoch 87/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8541\n",
      "Epoch 87: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 4.9483845577924514e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.8541 - val_loss: 2.0893 - lr: 4.9984e-05\n",
      "Epoch 88/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8504\n",
      "Epoch 88: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 4.898900748230517e-05.\n",
      "66/66 [==============================] - 5s 82ms/step - loss: 0.8502 - val_loss: 2.1320 - lr: 4.9484e-05\n",
      "Epoch 89/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8621\n",
      "Epoch 89: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 4.849911798373796e-05.\n",
      "66/66 [==============================] - 6s 83ms/step - loss: 0.8611 - val_loss: 2.0485 - lr: 4.8989e-05\n",
      "Epoch 90/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8480\n",
      "Epoch 90: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 4.801412665983662e-05.\n",
      "66/66 [==============================] - 5s 83ms/step - loss: 0.8480 - val_loss: 2.0901 - lr: 4.8499e-05\n",
      "Epoch 91/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8452\n",
      "Epoch 91: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 4.75339866898139e-05.\n",
      "66/66 [==============================] - 5s 83ms/step - loss: 0.8452 - val_loss: 2.1264 - lr: 4.8014e-05\n",
      "Epoch 92/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8474\n",
      "Epoch 92: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 4.705864765128354e-05.\n",
      "66/66 [==============================] - 5s 83ms/step - loss: 0.8474 - val_loss: 2.1120 - lr: 4.7534e-05\n",
      "Epoch 93/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8454\n",
      "Epoch 93: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 4.658806272345828e-05.\n",
      "66/66 [==============================] - 5s 82ms/step - loss: 0.8443 - val_loss: 2.1323 - lr: 4.7059e-05\n",
      "Epoch 94/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8489\n",
      "Epoch 94: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 4.612218148395186e-05.\n",
      "66/66 [==============================] - 5s 83ms/step - loss: 0.8489 - val_loss: 2.1251 - lr: 4.6588e-05\n",
      "Epoch 95/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8527\n",
      "Epoch 95: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 4.566096071357606e-05.\n",
      "66/66 [==============================] - 5s 81ms/step - loss: 0.8520 - val_loss: 2.0835 - lr: 4.6122e-05\n",
      "Epoch 96/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8432\n",
      "Epoch 96: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 4.52043499899446e-05.\n",
      "66/66 [==============================] - 5s 82ms/step - loss: 0.8463 - val_loss: 2.1192 - lr: 4.5661e-05\n",
      "Epoch 97/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8522\n",
      "Epoch 97: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 4.475230609386927e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.8522 - val_loss: 2.0267 - lr: 4.5204e-05\n",
      "Epoch 98/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8503\n",
      "Epoch 98: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 4.43047822045628e-05.\n",
      "66/66 [==============================] - 5s 82ms/step - loss: 0.8503 - val_loss: 2.0906 - lr: 4.4752e-05\n",
      "Epoch 99/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8362\n",
      "Epoch 99: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 4.386173510283697e-05.\n",
      "66/66 [==============================] - 6s 84ms/step - loss: 0.8362 - val_loss: 2.1472 - lr: 4.4305e-05\n",
      "Epoch 100/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8380\n",
      "Epoch 100: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 4.342311796790454e-05.\n",
      "66/66 [==============================] - 5s 83ms/step - loss: 0.8382 - val_loss: 2.1054 - lr: 4.3862e-05\n",
      "Epoch 101/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8450\n",
      "Epoch 101: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 4.298888758057728e-05.\n",
      "66/66 [==============================] - 5s 82ms/step - loss: 0.8450 - val_loss: 2.1248 - lr: 4.3423e-05\n",
      "Epoch 102/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8331\n",
      "Epoch 102: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 4.2558997120067946e-05.\n",
      "66/66 [==============================] - 5s 83ms/step - loss: 0.8317 - val_loss: 2.1188 - lr: 4.2989e-05\n",
      "Epoch 103/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8418\n",
      "Epoch 103: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 4.2133406968787315e-05.\n",
      "66/66 [==============================] - 5s 81ms/step - loss: 0.8418 - val_loss: 2.1184 - lr: 4.2559e-05\n",
      "Epoch 104/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8332\n",
      "Epoch 104: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 4.1712073907547166e-05.\n",
      "66/66 [==============================] - 5s 82ms/step - loss: 0.8343 - val_loss: 2.1163 - lr: 4.2133e-05\n",
      "Epoch 105/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8408\n",
      "Epoch 105: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 4.1294954717159274e-05.\n",
      "66/66 [==============================] - 5s 82ms/step - loss: 0.8408 - val_loss: 2.0509 - lr: 4.1712e-05\n",
      "Epoch 106/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8349\n",
      "Epoch 106: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 4.08820061784354e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 5s 81ms/step - loss: 0.8332 - val_loss: 2.0661 - lr: 4.1295e-05\n",
      "Epoch 107/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8420\n",
      "Epoch 107: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 4.0473185072187336e-05.\n",
      "66/66 [==============================] - 5s 82ms/step - loss: 0.8429 - val_loss: 2.0913 - lr: 4.0882e-05\n",
      "Epoch 108/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8420\n",
      "Epoch 108: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 4.0068451780825854e-05.\n",
      "66/66 [==============================] - 5s 81ms/step - loss: 0.8390 - val_loss: 2.0564 - lr: 4.0473e-05\n",
      "Epoch 109/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8374\n",
      "Epoch 109: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 3.966776668676175e-05.\n",
      "66/66 [==============================] - 5s 82ms/step - loss: 0.8387 - val_loss: 2.0959 - lr: 4.0068e-05\n",
      "Epoch 110/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8325\n",
      "Epoch 110: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 3.927109017240582e-05.\n",
      "66/66 [==============================] - 5s 81ms/step - loss: 0.8325 - val_loss: 2.0762 - lr: 3.9668e-05\n",
      "Epoch 111/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8426\n",
      "Epoch 111: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 3.887837901856983e-05.\n",
      "66/66 [==============================] - 5s 82ms/step - loss: 0.8419 - val_loss: 2.0657 - lr: 3.9271e-05\n",
      "Epoch 112/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8358\n",
      "Epoch 112: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 3.848959360766457e-05.\n",
      "66/66 [==============================] - 5s 82ms/step - loss: 0.8358 - val_loss: 2.0993 - lr: 3.8878e-05\n",
      "Epoch 113/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8296\n",
      "Epoch 113: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 3.810469792369986e-05.\n",
      "66/66 [==============================] - 5s 82ms/step - loss: 0.8296 - val_loss: 2.0841 - lr: 3.8490e-05\n",
      "Epoch 114/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8359\n",
      "Epoch 114: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 3.772365234908648e-05.\n",
      "66/66 [==============================] - 5s 83ms/step - loss: 0.8359 - val_loss: 2.0533 - lr: 3.8105e-05\n",
      "Epoch 115/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8287\n",
      "Epoch 115: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 3.734641726623522e-05.\n",
      "66/66 [==============================] - 5s 83ms/step - loss: 0.8287 - val_loss: 2.0800 - lr: 3.7724e-05\n",
      "Epoch 116/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8340\n",
      "Epoch 116: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 3.6972953057556876e-05.\n",
      "66/66 [==============================] - 5s 83ms/step - loss: 0.8336 - val_loss: 2.0917 - lr: 3.7346e-05\n",
      "Epoch 117/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8297\n",
      "Epoch 117: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 3.660322370706126e-05.\n",
      "66/66 [==============================] - 6s 84ms/step - loss: 0.8345 - val_loss: 2.0788 - lr: 3.6973e-05\n",
      "Epoch 118/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8249\n",
      "Epoch 118: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 3.623719319875818e-05.\n",
      "66/66 [==============================] - 5s 83ms/step - loss: 0.8254 - val_loss: 2.1046 - lr: 3.6603e-05\n",
      "Epoch 119/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8410\n",
      "Epoch 119: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 3.5874821915058416e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.8410 - val_loss: 2.1068 - lr: 3.6237e-05\n",
      "Epoch 120/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8317\n",
      "Epoch 120: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 3.551607383997179e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.8317 - val_loss: 2.1035 - lr: 3.5875e-05\n",
      "Epoch 121/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8340\n",
      "Epoch 121: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 3.516091295750812e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.8340 - val_loss: 2.1063 - lr: 3.5516e-05\n",
      "Epoch 122/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8388\n",
      "Epoch 122: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 3.480930325167719e-05.\n",
      "66/66 [==============================] - 6s 84ms/step - loss: 0.8388 - val_loss: 2.0961 - lr: 3.5161e-05\n",
      "Epoch 123/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8384\n",
      "Epoch 123: val_loss did not improve from 1.99925\n",
      "\n",
      "Epoch 123: ReduceLROnPlateau reducing learning rate to 3.446120870648883e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 0.8384 - val_loss: 2.0827 - lr: 3.4809e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_25\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_26 (InputLayer)          [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_75 (LSTM)                 (None, 45, 24)       3744        ['input_26[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_50 (Dropout)           (None, 45, 24)       0           ['lstm_75[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_76 (LSTM)                 (None, 45, 16)       2624        ['dropout_50[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_51 (Dropout)           (None, 45, 16)       0           ['lstm_76[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_77 (LSTM)                 (None, 32)           6272        ['dropout_51[0][0]']             \n",
      "                                                                                                  \n",
      " dense_50 (Dense)               (None, 40)           1320        ['lstm_77[0][0]']                \n",
      "                                                                                                  \n",
      " dense_51 (Dense)               (None, 5)            205         ['dense_50[0][0]']               \n",
      "                                                                                                  \n",
      " tf.unstack_25 (TFOpLambda)     [(None,),            0           ['dense_51[0][0]']               \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_125 (TFOpLambda  (None, 1)           0           ['tf.unstack_25[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_50 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_125[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_129 (TFOpLambda  (None, 1)           0           ['tf.unstack_25[0][4]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_75 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_50[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_51 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_129[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_76 (TFOpLambd  (None, 1)           0           ['tf.math.multiply_75[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_126 (TFOpLambda  (None, 1)           0           ['tf.unstack_25[0][1]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_128 (TFOpLambda  (None, 1)           0           ['tf.unstack_25[0][3]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_77 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_51[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_50 (TFOpL  (None, 1)           0           ['tf.math.multiply_76[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.softplus_50 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_126[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_127 (TFOpLambda  (None, 1)           0           ['tf.unstack_25[0][2]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.softplus_51 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_128[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_51 (TFOpL  (None, 1)           0           ['tf.math.multiply_77[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack_25 (TFOpLambda)       (None, 5, 1)         0           ['tf.__operators__.add_50[0][0]',\n",
      "                                                                  'tf.math.softplus_50[0][0]',    \n",
      "                                                                  'tf.expand_dims_127[0][0]',     \n",
      "                                                                  'tf.math.softplus_51[0][0]',    \n",
      "                                                                  'tf.__operators__.add_51[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _2_CCMP_t+1_0.15\n",
      "Epoch 1/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 3.4281\n",
      "Epoch 1: val_loss improved from inf to 4.11821, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 11s 95ms/step - loss: 3.4271 - val_loss: 4.1182 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 2.7253\n",
      "Epoch 2: val_loss improved from 4.11821 to 3.51526, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.15.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 6s 83ms/step - loss: 2.7253 - val_loss: 3.5153 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.8363\n",
      "Epoch 3: val_loss improved from 3.51526 to 2.87570, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 1.8363 - val_loss: 2.8757 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.4996\n",
      "Epoch 4: val_loss improved from 2.87570 to 2.59471, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 1.4996 - val_loss: 2.5947 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.3473\n",
      "Epoch 5: val_loss improved from 2.59471 to 2.47806, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.3473 - val_loss: 2.4781 - lr: 1.0000e-04\n",
      "Epoch 6/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.2853\n",
      "Epoch 6: val_loss improved from 2.47806 to 2.47196, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 1.2853 - val_loss: 2.4720 - lr: 1.0000e-04\n",
      "Epoch 7/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.2449\n",
      "Epoch 7: val_loss improved from 2.47196 to 2.33684, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 1.2431 - val_loss: 2.3368 - lr: 1.0000e-04\n",
      "Epoch 8/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.2277\n",
      "Epoch 8: val_loss did not improve from 2.33684\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 1.2277 - val_loss: 2.5014 - lr: 1.0000e-04\n",
      "Epoch 9/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1911\n",
      "Epoch 9: val_loss did not improve from 2.33684\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 1.1911 - val_loss: 2.3970 - lr: 9.9000e-05\n",
      "Epoch 10/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1691\n",
      "Epoch 10: val_loss improved from 2.33684 to 2.20517, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 1.1691 - val_loss: 2.2052 - lr: 9.8010e-05\n",
      "Epoch 11/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1644\n",
      "Epoch 11: val_loss improved from 2.20517 to 2.15838, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 1.1644 - val_loss: 2.1584 - lr: 9.8010e-05\n",
      "Epoch 12/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1463\n",
      "Epoch 12: val_loss did not improve from 2.15838\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 1.1484 - val_loss: 2.2353 - lr: 9.8010e-05\n",
      "Epoch 13/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1300\n",
      "Epoch 13: val_loss did not improve from 2.15838\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 1.1300 - val_loss: 2.1868 - lr: 9.7030e-05\n",
      "Epoch 14/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1127\n",
      "Epoch 14: val_loss improved from 2.15838 to 2.15366, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 1.1127 - val_loss: 2.1537 - lr: 9.6060e-05\n",
      "Epoch 15/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1016\n",
      "Epoch 15: val_loss improved from 2.15366 to 2.04520, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 1.1017 - val_loss: 2.0452 - lr: 9.6060e-05\n",
      "Epoch 16/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1146\n",
      "Epoch 16: val_loss did not improve from 2.04520\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 1.1146 - val_loss: 2.0785 - lr: 9.6060e-05\n",
      "Epoch 17/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0833\n",
      "Epoch 17: val_loss did not improve from 2.04520\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 1.0833 - val_loss: 2.1677 - lr: 9.5099e-05\n",
      "Epoch 18/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0841\n",
      "Epoch 18: val_loss did not improve from 2.04520\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 1.0829 - val_loss: 2.1712 - lr: 9.4148e-05\n",
      "Epoch 19/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0533\n",
      "Epoch 19: val_loss did not improve from 2.04520\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 1.0533 - val_loss: 2.0710 - lr: 9.3207e-05\n",
      "Epoch 20/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0471\n",
      "Epoch 20: val_loss improved from 2.04520 to 2.02522, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 1.0471 - val_loss: 2.0252 - lr: 9.2274e-05\n",
      "Epoch 21/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0374\n",
      "Epoch 21: val_loss did not improve from 2.02522\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 1.0374 - val_loss: 2.1093 - lr: 9.2274e-05\n",
      "Epoch 22/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0453\n",
      "Epoch 22: val_loss did not improve from 2.02522\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 1.0453 - val_loss: 2.0774 - lr: 9.1352e-05\n",
      "Epoch 23/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0124\n",
      "Epoch 23: val_loss did not improve from 2.02522\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 1.0153 - val_loss: 2.0891 - lr: 9.0438e-05\n",
      "Epoch 24/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0220\n",
      "Epoch 24: val_loss did not improve from 2.02522\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 1.0199 - val_loss: 2.0601 - lr: 8.9534e-05\n",
      "Epoch 25/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0249\n",
      "Epoch 25: val_loss improved from 2.02522 to 1.96232, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 1.0249 - val_loss: 1.9623 - lr: 8.8638e-05\n",
      "Epoch 26/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0064\n",
      "Epoch 26: val_loss did not improve from 1.96232\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 1.0064 - val_loss: 2.0098 - lr: 8.8638e-05\n",
      "Epoch 27/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9978\n",
      "Epoch 27: val_loss did not improve from 1.96232\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9978 - val_loss: 1.9858 - lr: 8.7752e-05\n",
      "Epoch 28/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0031\n",
      "Epoch 28: val_loss did not improve from 1.96232\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 1.0031 - val_loss: 2.1366 - lr: 8.6875e-05\n",
      "Epoch 29/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9986\n",
      "Epoch 29: val_loss improved from 1.96232 to 1.86507, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9978 - val_loss: 1.8651 - lr: 8.6006e-05\n",
      "Epoch 30/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9782\n",
      "Epoch 30: val_loss did not improve from 1.86507\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9782 - val_loss: 1.9984 - lr: 8.6006e-05\n",
      "Epoch 31/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9795\n",
      "Epoch 31: val_loss did not improve from 1.86507\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9795 - val_loss: 1.9897 - lr: 8.5146e-05\n",
      "Epoch 32/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9748\n",
      "Epoch 32: val_loss did not improve from 1.86507\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9748 - val_loss: 1.8828 - lr: 8.4294e-05\n",
      "Epoch 33/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9749\n",
      "Epoch 33: val_loss did not improve from 1.86507\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9749 - val_loss: 1.9331 - lr: 8.3451e-05\n",
      "Epoch 34/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9706\n",
      "Epoch 34: val_loss did not improve from 1.86507\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9706 - val_loss: 1.9051 - lr: 8.2617e-05\n",
      "Epoch 35/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9683\n",
      "Epoch 35: val_loss did not improve from 1.86507\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9698 - val_loss: 1.9253 - lr: 8.1791e-05\n",
      "Epoch 36/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9551\n",
      "Epoch 36: val_loss did not improve from 1.86507\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.9551 - val_loss: 2.0953 - lr: 8.0973e-05\n",
      "Epoch 37/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9605\n",
      "Epoch 37: val_loss did not improve from 1.86507\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9605 - val_loss: 1.9004 - lr: 8.0163e-05\n",
      "Epoch 38/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9658\n",
      "Epoch 38: val_loss did not improve from 1.86507\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9658 - val_loss: 1.9233 - lr: 7.9361e-05\n",
      "Epoch 39/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9540\n",
      "Epoch 39: val_loss did not improve from 1.86507\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9540 - val_loss: 1.9012 - lr: 7.8568e-05\n",
      "Epoch 40/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9421\n",
      "Epoch 40: val_loss did not improve from 1.86507\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9421 - val_loss: 1.8659 - lr: 7.7782e-05\n",
      "Epoch 41/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9533\n",
      "Epoch 41: val_loss did not improve from 1.86507\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9533 - val_loss: 2.0039 - lr: 7.7004e-05\n",
      "Epoch 42/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9429\n",
      "Epoch 42: val_loss improved from 1.86507 to 1.82515, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.9429 - val_loss: 1.8252 - lr: 7.6234e-05\n",
      "Epoch 43/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9373\n",
      "Epoch 43: val_loss did not improve from 1.82515\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.9373 - val_loss: 1.9201 - lr: 7.6234e-05\n",
      "Epoch 44/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9435\n",
      "Epoch 44: val_loss improved from 1.82515 to 1.81996, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.9435 - val_loss: 1.8200 - lr: 7.5472e-05\n",
      "Epoch 45/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9480\n",
      "Epoch 45: val_loss improved from 1.81996 to 1.81653, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.9480 - val_loss: 1.8165 - lr: 7.5472e-05\n",
      "Epoch 46/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9472\n",
      "Epoch 46: val_loss did not improve from 1.81653\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.9472 - val_loss: 1.9591 - lr: 7.5472e-05\n",
      "Epoch 47/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9463\n",
      "Epoch 47: val_loss did not improve from 1.81653\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.9463 - val_loss: 1.9248 - lr: 7.4717e-05\n",
      "Epoch 48/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9321\n",
      "Epoch 48: val_loss did not improve from 1.81653\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.9321 - val_loss: 1.8755 - lr: 7.3970e-05\n",
      "Epoch 49/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9325\n",
      "Epoch 49: val_loss improved from 1.81653 to 1.78399, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.9325 - val_loss: 1.7840 - lr: 7.3230e-05\n",
      "Epoch 50/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9339\n",
      "Epoch 50: val_loss did not improve from 1.78399\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.9339 - val_loss: 1.8452 - lr: 7.3230e-05\n",
      "Epoch 51/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9255\n",
      "Epoch 51: val_loss did not improve from 1.78399\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.9255 - val_loss: 1.8395 - lr: 7.2498e-05\n",
      "Epoch 52/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9262\n",
      "Epoch 52: val_loss did not improve from 1.78399\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.9262 - val_loss: 1.8022 - lr: 7.1773e-05\n",
      "Epoch 53/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9252\n",
      "Epoch 53: val_loss did not improve from 1.78399\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9252 - val_loss: 1.8268 - lr: 7.1055e-05\n",
      "Epoch 54/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9305\n",
      "Epoch 54: val_loss did not improve from 1.78399\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.9305 - val_loss: 1.8152 - lr: 7.0345e-05\n",
      "Epoch 55/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9171\n",
      "Epoch 55: val_loss improved from 1.78399 to 1.78368, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.9171 - val_loss: 1.7837 - lr: 6.9641e-05\n",
      "Epoch 56/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9107\n",
      "Epoch 56: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.9107 - val_loss: 1.8742 - lr: 6.9641e-05\n",
      "Epoch 57/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9176\n",
      "Epoch 57: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9176 - val_loss: 1.9659 - lr: 6.8945e-05\n",
      "Epoch 58/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9107\n",
      "Epoch 58: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9107 - val_loss: 1.8399 - lr: 6.8255e-05\n",
      "Epoch 59/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9163\n",
      "Epoch 59: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9163 - val_loss: 1.8488 - lr: 6.7573e-05\n",
      "Epoch 60/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9257\n",
      "Epoch 60: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9257 - val_loss: 1.9036 - lr: 6.6897e-05\n",
      "Epoch 61/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9232\n",
      "Epoch 61: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.9232 - val_loss: 1.8619 - lr: 6.6228e-05\n",
      "Epoch 62/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9069\n",
      "Epoch 62: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.9069 - val_loss: 1.9538 - lr: 6.5566e-05\n",
      "Epoch 63/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9036\n",
      "Epoch 63: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.9036 - val_loss: 1.8811 - lr: 6.4910e-05\n",
      "Epoch 64/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9105\n",
      "Epoch 64: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9105 - val_loss: 1.9300 - lr: 6.4261e-05\n",
      "Epoch 65/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9078\n",
      "Epoch 65: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.9078 - val_loss: 1.8852 - lr: 6.3619e-05\n",
      "Epoch 66/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9027\n",
      "Epoch 66: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9027 - val_loss: 1.8739 - lr: 6.2982e-05\n",
      "Epoch 67/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9100\n",
      "Epoch 67: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.9100 - val_loss: 1.8773 - lr: 6.2353e-05\n",
      "Epoch 68/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8990\n",
      "Epoch 68: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.8990 - val_loss: 2.0354 - lr: 6.1729e-05\n",
      "Epoch 69/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9143\n",
      "Epoch 69: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9143 - val_loss: 1.8904 - lr: 6.1112e-05\n",
      "Epoch 70/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9061\n",
      "Epoch 70: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9061 - val_loss: 1.8738 - lr: 6.0501e-05\n",
      "Epoch 71/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8945\n",
      "Epoch 71: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.8945 - val_loss: 1.9946 - lr: 5.9896e-05\n",
      "Epoch 72/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8998\n",
      "Epoch 72: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 5.8703650938696225e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.8998 - val_loss: 1.8886 - lr: 5.9297e-05\n",
      "Epoch 73/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8912\n",
      "Epoch 73: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 5.811661550978897e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.8910 - val_loss: 1.9298 - lr: 5.8704e-05\n",
      "Epoch 74/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8972\n",
      "Epoch 74: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 5.753545068728272e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.8972 - val_loss: 1.9052 - lr: 5.8117e-05\n",
      "Epoch 75/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9038\n",
      "Epoch 75: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 5.6960095243994146e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9038 - val_loss: 1.9027 - lr: 5.7535e-05\n",
      "Epoch 76/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9021\n",
      "Epoch 76: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 5.639049515593797e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9006 - val_loss: 1.8591 - lr: 5.6960e-05\n",
      "Epoch 77/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8898\n",
      "Epoch 77: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 5.5826589195930865e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.8898 - val_loss: 1.9404 - lr: 5.6390e-05\n",
      "Epoch 78/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9002\n",
      "Epoch 78: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 5.526832333998754e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.8982 - val_loss: 2.0142 - lr: 5.5827e-05\n",
      "Epoch 79/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8928\n",
      "Epoch 79: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 5.471563996252371e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.8930 - val_loss: 2.0095 - lr: 5.5268e-05\n",
      "Epoch 80/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - ETA: 0s - loss: 0.9050\n",
      "Epoch 80: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 5.416848503955407e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9050 - val_loss: 1.8768 - lr: 5.4716e-05\n",
      "Epoch 81/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8865\n",
      "Epoch 81: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 5.362680094549433e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.8865 - val_loss: 1.9682 - lr: 5.4168e-05\n",
      "Epoch 82/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8889\n",
      "Epoch 82: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 5.309053365635918e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.8878 - val_loss: 1.9726 - lr: 5.3627e-05\n",
      "Epoch 83/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8839\n",
      "Epoch 83: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 5.2559629148163366e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.8839 - val_loss: 1.9431 - lr: 5.3091e-05\n",
      "Epoch 84/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8936\n",
      "Epoch 84: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 5.2034033396921584e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.8936 - val_loss: 2.0272 - lr: 5.2560e-05\n",
      "Epoch 85/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8843\n",
      "Epoch 85: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 5.1513692378648555e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.8843 - val_loss: 1.9727 - lr: 5.2034e-05\n",
      "Epoch 86/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8856\n",
      "Epoch 86: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 5.099855567095801e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.8856 - val_loss: 1.9287 - lr: 5.1514e-05\n",
      "Epoch 87/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8876\n",
      "Epoch 87: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 5.048856924986467e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.8876 - val_loss: 1.9340 - lr: 5.0999e-05\n",
      "Epoch 88/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8932\n",
      "Epoch 88: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 4.998368269298226e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.8932 - val_loss: 1.9992 - lr: 5.0489e-05\n",
      "Epoch 89/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8762\n",
      "Epoch 89: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 4.9483845577924514e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.8777 - val_loss: 1.9270 - lr: 4.9984e-05\n",
      "Epoch 90/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8811\n",
      "Epoch 90: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 4.898900748230517e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.8819 - val_loss: 1.9290 - lr: 4.9484e-05\n",
      "Epoch 91/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8798\n",
      "Epoch 91: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 4.849911798373796e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.8798 - val_loss: 2.0006 - lr: 4.8989e-05\n",
      "Epoch 92/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8815\n",
      "Epoch 92: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 4.801412665983662e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.8815 - val_loss: 1.9506 - lr: 4.8499e-05\n",
      "Epoch 93/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8724\n",
      "Epoch 93: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 4.75339866898139e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.8724 - val_loss: 1.9765 - lr: 4.8014e-05\n",
      "Epoch 94/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8808\n",
      "Epoch 94: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 4.705864765128354e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.8847 - val_loss: 1.8879 - lr: 4.7534e-05\n",
      "Epoch 95/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8808\n",
      "Epoch 95: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 4.658806272345828e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.8823 - val_loss: 2.0030 - lr: 4.7059e-05\n",
      "Epoch 96/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8818\n",
      "Epoch 96: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 4.612218148395186e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.8818 - val_loss: 2.0095 - lr: 4.6588e-05\n",
      "Epoch 97/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8764\n",
      "Epoch 97: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 4.566096071357606e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.8764 - val_loss: 1.9326 - lr: 4.6122e-05\n",
      "Epoch 98/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8796\n",
      "Epoch 98: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 4.52043499899446e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.8796 - val_loss: 2.0176 - lr: 4.5661e-05\n",
      "Epoch 99/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8782\n",
      "Epoch 99: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 4.475230609386927e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.8782 - val_loss: 1.9358 - lr: 4.5204e-05\n",
      "Epoch 100/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8729\n",
      "Epoch 100: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 4.43047822045628e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.8741 - val_loss: 1.9463 - lr: 4.4752e-05\n",
      "Epoch 101/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8739\n",
      "Epoch 101: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 4.386173510283697e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.8744 - val_loss: 1.9390 - lr: 4.4305e-05\n",
      "Epoch 102/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8693\n",
      "Epoch 102: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 4.342311796790454e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.8693 - val_loss: 2.0393 - lr: 4.3862e-05\n",
      "Epoch 103/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8813\n",
      "Epoch 103: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 4.298888758057728e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.8813 - val_loss: 1.9713 - lr: 4.3423e-05\n",
      "Epoch 104/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8825\n",
      "Epoch 104: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 4.2558997120067946e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.8825 - val_loss: 1.9119 - lr: 4.2989e-05\n",
      "Epoch 105/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8771\n",
      "Epoch 105: val_loss did not improve from 1.78368\n",
      "\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 4.2133406968787315e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.8771 - val_loss: 1.9831 - lr: 4.2559e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_26\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_27 (InputLayer)          [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_78 (LSTM)                 (None, 45, 24)       3744        ['input_27[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_52 (Dropout)           (None, 45, 24)       0           ['lstm_78[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_79 (LSTM)                 (None, 45, 16)       2624        ['dropout_52[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_53 (Dropout)           (None, 45, 16)       0           ['lstm_79[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_80 (LSTM)                 (None, 32)           6272        ['dropout_53[0][0]']             \n",
      "                                                                                                  \n",
      " dense_52 (Dense)               (None, 40)           1320        ['lstm_80[0][0]']                \n",
      "                                                                                                  \n",
      " dense_53 (Dense)               (None, 5)            205         ['dense_52[0][0]']               \n",
      "                                                                                                  \n",
      " tf.unstack_26 (TFOpLambda)     [(None,),            0           ['dense_53[0][0]']               \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_130 (TFOpLambda  (None, 1)           0           ['tf.unstack_26[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_52 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_130[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_134 (TFOpLambda  (None, 1)           0           ['tf.unstack_26[0][4]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_78 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_52[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_53 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_134[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_79 (TFOpLambd  (None, 1)           0           ['tf.math.multiply_78[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_131 (TFOpLambda  (None, 1)           0           ['tf.unstack_26[0][1]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_133 (TFOpLambda  (None, 1)           0           ['tf.unstack_26[0][3]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_80 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_53[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_52 (TFOpL  (None, 1)           0           ['tf.math.multiply_79[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.softplus_52 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_131[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_132 (TFOpLambda  (None, 1)           0           ['tf.unstack_26[0][2]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.softplus_53 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_133[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_53 (TFOpL  (None, 1)           0           ['tf.math.multiply_80[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack_26 (TFOpLambda)       (None, 5, 1)         0           ['tf.__operators__.add_52[0][0]',\n",
      "                                                                  'tf.math.softplus_52[0][0]',    \n",
      "                                                                  'tf.expand_dims_132[0][0]',     \n",
      "                                                                  'tf.math.softplus_53[0][0]',    \n",
      "                                                                  'tf.__operators__.add_53[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _2_CCMP_t+1_0.16\n",
      "Epoch 1/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 3.5060\n",
      "Epoch 1: val_loss improved from inf to 4.10905, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 11s 94ms/step - loss: 3.5068 - val_loss: 4.1091 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 3.1120\n",
      "Epoch 2: val_loss improved from 4.10905 to 3.38327, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.16.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 6s 85ms/step - loss: 3.1092 - val_loss: 3.3833 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 2.0431\n",
      "Epoch 3: val_loss improved from 3.38327 to 3.08166, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 2.0431 - val_loss: 3.0817 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.5676\n",
      "Epoch 4: val_loss improved from 3.08166 to 2.96282, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 1.5676 - val_loss: 2.9628 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.4266\n",
      "Epoch 5: val_loss did not improve from 2.96282\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 1.4266 - val_loss: 2.9654 - lr: 1.0000e-04\n",
      "Epoch 6/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.3280\n",
      "Epoch 6: val_loss improved from 2.96282 to 2.82294, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 1.3280 - val_loss: 2.8229 - lr: 9.9000e-05\n",
      "Epoch 7/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.2765\n",
      "Epoch 7: val_loss improved from 2.82294 to 2.66295, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.2765 - val_loss: 2.6629 - lr: 9.9000e-05\n",
      "Epoch 8/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.2448\n",
      "Epoch 8: val_loss did not improve from 2.66295\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 1.2448 - val_loss: 2.7568 - lr: 9.9000e-05\n",
      "Epoch 9/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.2047\n",
      "Epoch 9: val_loss improved from 2.66295 to 2.58481, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 1.2047 - val_loss: 2.5848 - lr: 9.8010e-05\n",
      "Epoch 10/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1748\n",
      "Epoch 10: val_loss did not improve from 2.58481\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 1.1748 - val_loss: 2.6685 - lr: 9.8010e-05\n",
      "Epoch 11/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1671\n",
      "Epoch 11: val_loss improved from 2.58481 to 2.51249, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.1655 - val_loss: 2.5125 - lr: 9.7030e-05\n",
      "Epoch 12/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1304\n",
      "Epoch 12: val_loss improved from 2.51249 to 2.42496, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 7s 102ms/step - loss: 1.1304 - val_loss: 2.4250 - lr: 9.7030e-05\n",
      "Epoch 13/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1100\n",
      "Epoch 13: val_loss did not improve from 2.42496\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 6s 98ms/step - loss: 1.1100 - val_loss: 2.5251 - lr: 9.7030e-05\n",
      "Epoch 14/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1186\n",
      "Epoch 14: val_loss did not improve from 2.42496\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 1.1186 - val_loss: 2.4571 - lr: 9.6060e-05\n",
      "Epoch 15/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0803\n",
      "Epoch 15: val_loss improved from 2.42496 to 2.31920, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 1.0803 - val_loss: 2.3192 - lr: 9.5099e-05\n",
      "Epoch 16/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0846\n",
      "Epoch 16: val_loss did not improve from 2.31920\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 1.0846 - val_loss: 2.3222 - lr: 9.5099e-05\n",
      "Epoch 17/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0606\n",
      "Epoch 17: val_loss did not improve from 2.31920\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 1.0606 - val_loss: 2.3836 - lr: 9.4148e-05\n",
      "Epoch 18/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0517\n",
      "Epoch 18: val_loss improved from 2.31920 to 2.26839, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 1.0517 - val_loss: 2.2684 - lr: 9.3207e-05\n",
      "Epoch 19/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0415\n",
      "Epoch 19: val_loss did not improve from 2.26839\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 1.0415 - val_loss: 2.3907 - lr: 9.3207e-05\n",
      "Epoch 20/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0386\n",
      "Epoch 20: val_loss improved from 2.26839 to 2.25772, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 1.0386 - val_loss: 2.2577 - lr: 9.2274e-05\n",
      "Epoch 21/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0507\n",
      "Epoch 21: val_loss did not improve from 2.25772\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 1.0507 - val_loss: 2.2931 - lr: 9.2274e-05\n",
      "Epoch 22/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0376\n",
      "Epoch 22: val_loss improved from 2.25772 to 2.16513, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 1.0376 - val_loss: 2.1651 - lr: 9.1352e-05\n",
      "Epoch 23/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0128\n",
      "Epoch 23: val_loss did not improve from 2.16513\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 1.0128 - val_loss: 2.2119 - lr: 9.1352e-05\n",
      "Epoch 24/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0185\n",
      "Epoch 24: val_loss did not improve from 2.16513\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 1.0185 - val_loss: 2.2636 - lr: 9.0438e-05\n",
      "Epoch 25/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0093\n",
      "Epoch 25: val_loss did not improve from 2.16513\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 1.0093 - val_loss: 2.2256 - lr: 8.9534e-05\n",
      "Epoch 26/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0084\n",
      "Epoch 26: val_loss did not improve from 2.16513\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.0084 - val_loss: 2.2128 - lr: 8.8638e-05\n",
      "Epoch 27/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0059\n",
      "Epoch 27: val_loss improved from 2.16513 to 2.14782, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.16.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 6s 94ms/step - loss: 1.0059 - val_loss: 2.1478 - lr: 8.7752e-05\n",
      "Epoch 28/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9886\n",
      "Epoch 28: val_loss did not improve from 2.14782\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.9886 - val_loss: 2.2302 - lr: 8.7752e-05\n",
      "Epoch 29/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9944\n",
      "Epoch 29: val_loss did not improve from 2.14782\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.9944 - val_loss: 2.1649 - lr: 8.6875e-05\n",
      "Epoch 30/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9985\n",
      "Epoch 30: val_loss improved from 2.14782 to 2.12904, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.9985 - val_loss: 2.1290 - lr: 8.6006e-05\n",
      "Epoch 31/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9797\n",
      "Epoch 31: val_loss improved from 2.12904 to 1.95961, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.9797 - val_loss: 1.9596 - lr: 8.6006e-05\n",
      "Epoch 32/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9879\n",
      "Epoch 32: val_loss did not improve from 1.95961\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.9879 - val_loss: 1.9817 - lr: 8.6006e-05\n",
      "Epoch 33/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9858\n",
      "Epoch 33: val_loss did not improve from 1.95961\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.9858 - val_loss: 2.0373 - lr: 8.5146e-05\n",
      "Epoch 34/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9871\n",
      "Epoch 34: val_loss did not improve from 1.95961\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.9871 - val_loss: 2.0438 - lr: 8.4294e-05\n",
      "Epoch 35/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9813\n",
      "Epoch 35: val_loss did not improve from 1.95961\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.9813 - val_loss: 2.0875 - lr: 8.3451e-05\n",
      "Epoch 36/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9712\n",
      "Epoch 36: val_loss did not improve from 1.95961\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.9712 - val_loss: 1.9930 - lr: 8.2617e-05\n",
      "Epoch 37/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9727\n",
      "Epoch 37: val_loss improved from 1.95961 to 1.94637, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.9727 - val_loss: 1.9464 - lr: 8.1791e-05\n",
      "Epoch 38/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9646\n",
      "Epoch 38: val_loss did not improve from 1.94637\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.9646 - val_loss: 1.9780 - lr: 8.1791e-05\n",
      "Epoch 39/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9712\n",
      "Epoch 39: val_loss improved from 1.94637 to 1.89223, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.9712 - val_loss: 1.8922 - lr: 8.0973e-05\n",
      "Epoch 40/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9614\n",
      "Epoch 40: val_loss did not improve from 1.89223\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.9614 - val_loss: 2.0433 - lr: 8.0973e-05\n",
      "Epoch 41/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9559\n",
      "Epoch 41: val_loss did not improve from 1.89223\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.9559 - val_loss: 1.9665 - lr: 8.0163e-05\n",
      "Epoch 42/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9632\n",
      "Epoch 42: val_loss did not improve from 1.89223\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.9632 - val_loss: 1.8947 - lr: 7.9361e-05\n",
      "Epoch 43/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9565\n",
      "Epoch 43: val_loss did not improve from 1.89223\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.9565 - val_loss: 1.9694 - lr: 7.8568e-05\n",
      "Epoch 44/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9551\n",
      "Epoch 44: val_loss did not improve from 1.89223\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.9551 - val_loss: 1.9469 - lr: 7.7782e-05\n",
      "Epoch 45/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9502\n",
      "Epoch 45: val_loss improved from 1.89223 to 1.88248, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.9502 - val_loss: 1.8825 - lr: 7.7004e-05\n",
      "Epoch 46/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9365\n",
      "Epoch 46: val_loss did not improve from 1.88248\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.9365 - val_loss: 2.0823 - lr: 7.7004e-05\n",
      "Epoch 47/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9440\n",
      "Epoch 47: val_loss did not improve from 1.88248\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.9440 - val_loss: 2.0065 - lr: 7.6234e-05\n",
      "Epoch 48/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9390\n",
      "Epoch 48: val_loss did not improve from 1.88248\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9404 - val_loss: 1.9416 - lr: 7.5472e-05\n",
      "Epoch 49/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9338\n",
      "Epoch 49: val_loss did not improve from 1.88248\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9344 - val_loss: 2.0333 - lr: 7.4717e-05\n",
      "Epoch 50/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9337\n",
      "Epoch 50: val_loss improved from 1.88248 to 1.86732, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.9337 - val_loss: 1.8673 - lr: 7.3970e-05\n",
      "Epoch 51/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9330\n",
      "Epoch 51: val_loss improved from 1.86732 to 1.84226, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.9330 - val_loss: 1.8423 - lr: 7.3970e-05\n",
      "Epoch 52/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9471\n",
      "Epoch 52: val_loss did not improve from 1.84226\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9471 - val_loss: 1.9426 - lr: 7.3970e-05\n",
      "Epoch 53/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9418\n",
      "Epoch 53: val_loss did not improve from 1.84226\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9403 - val_loss: 1.8787 - lr: 7.3230e-05\n",
      "Epoch 54/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9312\n",
      "Epoch 54: val_loss did not improve from 1.84226\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9327 - val_loss: 1.8867 - lr: 7.2498e-05\n",
      "Epoch 55/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9295\n",
      "Epoch 55: val_loss did not improve from 1.84226\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9295 - val_loss: 1.9563 - lr: 7.1773e-05\n",
      "Epoch 56/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9213\n",
      "Epoch 56: val_loss did not improve from 1.84226\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.9213 - val_loss: 1.8565 - lr: 7.1055e-05\n",
      "Epoch 57/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9245\n",
      "Epoch 57: val_loss did not improve from 1.84226\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9245 - val_loss: 1.8690 - lr: 7.0345e-05\n",
      "Epoch 58/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9241\n",
      "Epoch 58: val_loss did not improve from 1.84226\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9241 - val_loss: 1.8674 - lr: 6.9641e-05\n",
      "Epoch 59/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9262\n",
      "Epoch 59: val_loss did not improve from 1.84226\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9253 - val_loss: 1.8835 - lr: 6.8945e-05\n",
      "Epoch 60/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9289\n",
      "Epoch 60: val_loss did not improve from 1.84226\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9289 - val_loss: 2.0443 - lr: 6.8255e-05\n",
      "Epoch 61/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9293\n",
      "Epoch 61: val_loss did not improve from 1.84226\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.9293 - val_loss: 1.8972 - lr: 6.7573e-05\n",
      "Epoch 62/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9211\n",
      "Epoch 62: val_loss did not improve from 1.84226\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9211 - val_loss: 1.9685 - lr: 6.6897e-05\n",
      "Epoch 63/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9208\n",
      "Epoch 63: val_loss did not improve from 1.84226\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9208 - val_loss: 1.9840 - lr: 6.6228e-05\n",
      "Epoch 64/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9162\n",
      "Epoch 64: val_loss did not improve from 1.84226\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9162 - val_loss: 1.8766 - lr: 6.5566e-05\n",
      "Epoch 65/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9049\n",
      "Epoch 65: val_loss did not improve from 1.84226\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.9057 - val_loss: 1.8662 - lr: 6.4910e-05\n",
      "Epoch 66/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9134\n",
      "Epoch 66: val_loss improved from 1.84226 to 1.80047, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.9134 - val_loss: 1.8005 - lr: 6.4261e-05\n",
      "Epoch 67/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9096\n",
      "Epoch 67: val_loss did not improve from 1.80047\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.9096 - val_loss: 1.8851 - lr: 6.4261e-05\n",
      "Epoch 68/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9106\n",
      "Epoch 68: val_loss did not improve from 1.80047\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.9106 - val_loss: 1.9612 - lr: 6.3619e-05\n",
      "Epoch 69/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8985\n",
      "Epoch 69: val_loss did not improve from 1.80047\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.8985 - val_loss: 1.8741 - lr: 6.2982e-05\n",
      "Epoch 70/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9108\n",
      "Epoch 70: val_loss did not improve from 1.80047\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.9108 - val_loss: 1.8211 - lr: 6.2353e-05\n",
      "Epoch 71/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9034\n",
      "Epoch 71: val_loss did not improve from 1.80047\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.9034 - val_loss: 1.8866 - lr: 6.1729e-05\n",
      "Epoch 72/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8905\n",
      "Epoch 72: val_loss did not improve from 1.80047\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.8905 - val_loss: 1.9371 - lr: 6.1112e-05\n",
      "Epoch 73/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9074\n",
      "Epoch 73: val_loss did not improve from 1.80047\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.9074 - val_loss: 1.8220 - lr: 6.0501e-05\n",
      "Epoch 74/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8962\n",
      "Epoch 74: val_loss did not improve from 1.80047\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.8962 - val_loss: 1.8674 - lr: 5.9896e-05\n",
      "Epoch 75/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8895\n",
      "Epoch 75: val_loss did not improve from 1.80047\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 5.8703650938696225e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.8895 - val_loss: 1.8832 - lr: 5.9297e-05\n",
      "Epoch 76/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9031\n",
      "Epoch 76: val_loss did not improve from 1.80047\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 5.811661550978897e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.9031 - val_loss: 1.9222 - lr: 5.8704e-05\n",
      "Epoch 77/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8965\n",
      "Epoch 77: val_loss did not improve from 1.80047\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 5.753545068728272e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.8965 - val_loss: 1.8193 - lr: 5.8117e-05\n",
      "Epoch 78/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8943\n",
      "Epoch 78: val_loss did not improve from 1.80047\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 5.6960095243994146e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.8943 - val_loss: 1.8467 - lr: 5.7535e-05\n",
      "Epoch 79/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8876\n",
      "Epoch 79: val_loss did not improve from 1.80047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 5.639049515593797e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.8891 - val_loss: 1.8978 - lr: 5.6960e-05\n",
      "Epoch 80/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9035\n",
      "Epoch 80: val_loss did not improve from 1.80047\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 5.5826589195930865e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.9035 - val_loss: 1.8943 - lr: 5.6390e-05\n",
      "Epoch 81/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9037\n",
      "Epoch 81: val_loss did not improve from 1.80047\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 5.526832333998754e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.9037 - val_loss: 1.8030 - lr: 5.5827e-05\n",
      "Epoch 82/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8954\n",
      "Epoch 82: val_loss did not improve from 1.80047\n",
      "\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 5.471563996252371e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.8954 - val_loss: 1.8985 - lr: 5.5268e-05\n",
      "Epoch 83/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8855\n",
      "Epoch 83: val_loss did not improve from 1.80047\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 5.416848503955407e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.8855 - val_loss: 1.8573 - lr: 5.4716e-05\n",
      "Epoch 84/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8900\n",
      "Epoch 84: val_loss did not improve from 1.80047\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 5.362680094549433e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.8900 - val_loss: 1.9260 - lr: 5.4168e-05\n",
      "Epoch 85/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8905\n",
      "Epoch 85: val_loss did not improve from 1.80047\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 5.309053365635918e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.8905 - val_loss: 1.8513 - lr: 5.3627e-05\n",
      "Epoch 86/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9001\n",
      "Epoch 86: val_loss did not improve from 1.80047\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 5.2559629148163366e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.9001 - val_loss: 1.8645 - lr: 5.3091e-05\n",
      "Epoch 87/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8964\n",
      "Epoch 87: val_loss did not improve from 1.80047\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 5.2034033396921584e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.8964 - val_loss: 1.8605 - lr: 5.2560e-05\n",
      "Epoch 88/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8910\n",
      "Epoch 88: val_loss did not improve from 1.80047\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 5.1513692378648555e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.8910 - val_loss: 2.0061 - lr: 5.2034e-05\n",
      "Epoch 89/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8820\n",
      "Epoch 89: val_loss did not improve from 1.80047\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 5.099855567095801e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.8797 - val_loss: 1.9411 - lr: 5.1514e-05\n",
      "Epoch 90/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8792\n",
      "Epoch 90: val_loss did not improve from 1.80047\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 5.048856924986467e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.8792 - val_loss: 1.9165 - lr: 5.0999e-05\n",
      "Epoch 91/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8790\n",
      "Epoch 91: val_loss did not improve from 1.80047\n",
      "\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 4.998368269298226e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.8790 - val_loss: 1.9283 - lr: 5.0489e-05\n",
      "Epoch 92/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9018\n",
      "Epoch 92: val_loss did not improve from 1.80047\n",
      "\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 4.9483845577924514e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9018 - val_loss: 1.9794 - lr: 4.9984e-05\n",
      "Epoch 93/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8884\n",
      "Epoch 93: val_loss improved from 1.80047 to 1.76961, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.8884 - val_loss: 1.7696 - lr: 4.9484e-05\n",
      "Epoch 94/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8831\n",
      "Epoch 94: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 4.898900748230517e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.8831 - val_loss: 1.8541 - lr: 4.9484e-05\n",
      "Epoch 95/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8945\n",
      "Epoch 95: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 4.849911798373796e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.8945 - val_loss: 1.8816 - lr: 4.8989e-05\n",
      "Epoch 96/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8810\n",
      "Epoch 96: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 4.801412665983662e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.8810 - val_loss: 1.9399 - lr: 4.8499e-05\n",
      "Epoch 97/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8865\n",
      "Epoch 97: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 4.75339866898139e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.8865 - val_loss: 1.9267 - lr: 4.8014e-05\n",
      "Epoch 98/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8813\n",
      "Epoch 98: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 4.705864765128354e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.8813 - val_loss: 1.8341 - lr: 4.7534e-05\n",
      "Epoch 99/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8792\n",
      "Epoch 99: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 4.658806272345828e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.8792 - val_loss: 1.9515 - lr: 4.7059e-05\n",
      "Epoch 100/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8879\n",
      "Epoch 100: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 4.612218148395186e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.8879 - val_loss: 1.9097 - lr: 4.6588e-05\n",
      "Epoch 101/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8807\n",
      "Epoch 101: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 4.566096071357606e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.8810 - val_loss: 1.8932 - lr: 4.6122e-05\n",
      "Epoch 102/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8807\n",
      "Epoch 102: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 4.52043499899446e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.8807 - val_loss: 1.9719 - lr: 4.5661e-05\n",
      "Epoch 103/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8767\n",
      "Epoch 103: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 4.475230609386927e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.8767 - val_loss: 1.9398 - lr: 4.5204e-05\n",
      "Epoch 104/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8836\n",
      "Epoch 104: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 4.43047822045628e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.8836 - val_loss: 2.0047 - lr: 4.4752e-05\n",
      "Epoch 105/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8770\n",
      "Epoch 105: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 4.386173510283697e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.8791 - val_loss: 1.8885 - lr: 4.4305e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8844\n",
      "Epoch 106: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 4.342311796790454e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.8844 - val_loss: 1.9322 - lr: 4.3862e-05\n",
      "Epoch 107/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8668\n",
      "Epoch 107: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 4.298888758057728e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.8668 - val_loss: 1.8883 - lr: 4.3423e-05\n",
      "Epoch 108/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8877\n",
      "Epoch 108: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 4.2558997120067946e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.8877 - val_loss: 1.8280 - lr: 4.2989e-05\n",
      "Epoch 109/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8799\n",
      "Epoch 109: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 4.2133406968787315e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.8799 - val_loss: 1.8975 - lr: 4.2559e-05\n",
      "Epoch 110/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8835\n",
      "Epoch 110: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 4.1712073907547166e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.8835 - val_loss: 1.9129 - lr: 4.2133e-05\n",
      "Epoch 111/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8836\n",
      "Epoch 111: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 4.1294954717159274e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.8836 - val_loss: 1.9382 - lr: 4.1712e-05\n",
      "Epoch 112/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8752\n",
      "Epoch 112: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 4.08820061784354e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.8752 - val_loss: 1.9183 - lr: 4.1295e-05\n",
      "Epoch 113/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8712\n",
      "Epoch 113: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 4.0473185072187336e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.8712 - val_loss: 1.9120 - lr: 4.0882e-05\n",
      "Epoch 114/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8680\n",
      "Epoch 114: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 4.0068451780825854e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.8680 - val_loss: 1.9604 - lr: 4.0473e-05\n",
      "Epoch 115/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8645\n",
      "Epoch 115: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 3.966776668676175e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.8641 - val_loss: 1.9419 - lr: 4.0068e-05\n",
      "Epoch 116/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8742\n",
      "Epoch 116: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 3.927109017240582e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.8742 - val_loss: 1.9360 - lr: 3.9668e-05\n",
      "Epoch 117/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8791\n",
      "Epoch 117: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 3.887837901856983e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.8791 - val_loss: 1.9736 - lr: 3.9271e-05\n",
      "Epoch 118/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8797\n",
      "Epoch 118: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 3.848959360766457e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.8797 - val_loss: 1.9360 - lr: 3.8878e-05\n",
      "Epoch 119/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8747\n",
      "Epoch 119: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 3.810469792369986e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.8747 - val_loss: 1.9059 - lr: 3.8490e-05\n",
      "Epoch 120/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8761\n",
      "Epoch 120: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 3.772365234908648e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.8761 - val_loss: 1.8411 - lr: 3.8105e-05\n",
      "Epoch 121/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8706\n",
      "Epoch 121: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 3.734641726623522e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.8706 - val_loss: 1.8814 - lr: 3.7724e-05\n",
      "Epoch 122/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8740\n",
      "Epoch 122: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 3.6972953057556876e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.8740 - val_loss: 1.8999 - lr: 3.7346e-05\n",
      "Epoch 123/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8636\n",
      "Epoch 123: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 123: ReduceLROnPlateau reducing learning rate to 3.660322370706126e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.8636 - val_loss: 1.8928 - lr: 3.6973e-05\n",
      "Epoch 124/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8681\n",
      "Epoch 124: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 124: ReduceLROnPlateau reducing learning rate to 3.623719319875818e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.8681 - val_loss: 1.8770 - lr: 3.6603e-05\n",
      "Epoch 125/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8749\n",
      "Epoch 125: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 3.5874821915058416e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.8749 - val_loss: 1.9012 - lr: 3.6237e-05\n",
      "Epoch 126/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8608\n",
      "Epoch 126: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 3.551607383997179e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.8625 - val_loss: 1.9284 - lr: 3.5875e-05\n",
      "Epoch 127/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8690\n",
      "Epoch 127: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 127: ReduceLROnPlateau reducing learning rate to 3.516091295750812e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.8690 - val_loss: 1.8833 - lr: 3.5516e-05\n",
      "Epoch 128/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8663\n",
      "Epoch 128: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 128: ReduceLROnPlateau reducing learning rate to 3.480930325167719e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.8663 - val_loss: 1.8861 - lr: 3.5161e-05\n",
      "Epoch 129/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8721\n",
      "Epoch 129: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 129: ReduceLROnPlateau reducing learning rate to 3.446120870648883e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.8721 - val_loss: 1.9029 - lr: 3.4809e-05\n",
      "Epoch 130/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8713\n",
      "Epoch 130: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 130: ReduceLROnPlateau reducing learning rate to 3.4116596907551866e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.8713 - val_loss: 1.8993 - lr: 3.4461e-05\n",
      "Epoch 131/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8733\n",
      "Epoch 131: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 131: ReduceLROnPlateau reducing learning rate to 3.37754318388761e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.8709 - val_loss: 1.9231 - lr: 3.4117e-05\n",
      "Epoch 132/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8712\n",
      "Epoch 132: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 132: ReduceLROnPlateau reducing learning rate to 3.3437677484471354e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.8712 - val_loss: 1.9693 - lr: 3.3775e-05\n",
      "Epoch 133/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8626\n",
      "Epoch 133: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 133: ReduceLROnPlateau reducing learning rate to 3.310330142994644e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.8626 - val_loss: 1.9595 - lr: 3.3438e-05\n",
      "Epoch 134/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8676\n",
      "Epoch 134: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 134: ReduceLROnPlateau reducing learning rate to 3.277226765931118e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.8676 - val_loss: 1.8516 - lr: 3.3103e-05\n",
      "Epoch 135/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8582\n",
      "Epoch 135: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 135: ReduceLROnPlateau reducing learning rate to 3.24445437581744e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.8582 - val_loss: 1.9362 - lr: 3.2772e-05\n",
      "Epoch 136/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8653\n",
      "Epoch 136: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 3.212009731214493e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.8653 - val_loss: 1.9802 - lr: 3.2445e-05\n",
      "Epoch 137/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8661\n",
      "Epoch 137: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 137: ReduceLROnPlateau reducing learning rate to 3.17988959068316e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.8652 - val_loss: 1.9168 - lr: 3.2120e-05\n",
      "Epoch 138/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8723\n",
      "Epoch 138: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 138: ReduceLROnPlateau reducing learning rate to 3.1480907127843236e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.8723 - val_loss: 1.9045 - lr: 3.1799e-05\n",
      "Epoch 139/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8734\n",
      "Epoch 139: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 139: ReduceLROnPlateau reducing learning rate to 3.116609856078867e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.8734 - val_loss: 1.9620 - lr: 3.1481e-05\n",
      "Epoch 140/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8715\n",
      "Epoch 140: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 140: ReduceLROnPlateau reducing learning rate to 3.085443779127672e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.8715 - val_loss: 1.9625 - lr: 3.1166e-05\n",
      "Epoch 141/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8561\n",
      "Epoch 141: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 141: ReduceLROnPlateau reducing learning rate to 3.054589240491623e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.8561 - val_loss: 1.9237 - lr: 3.0854e-05\n",
      "Epoch 142/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8709\n",
      "Epoch 142: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 142: ReduceLROnPlateau reducing learning rate to 3.024043358891504e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.8703 - val_loss: 1.8944 - lr: 3.0546e-05\n",
      "Epoch 143/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8650\n",
      "Epoch 143: val_loss did not improve from 1.76961\n",
      "\n",
      "Epoch 143: ReduceLROnPlateau reducing learning rate to 2.9938028928881975e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.8650 - val_loss: 1.8949 - lr: 3.0240e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_27\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_28 (InputLayer)          [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_81 (LSTM)                 (None, 45, 24)       3744        ['input_28[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_54 (Dropout)           (None, 45, 24)       0           ['lstm_81[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_82 (LSTM)                 (None, 45, 16)       2624        ['dropout_54[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_55 (Dropout)           (None, 45, 16)       0           ['lstm_82[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_83 (LSTM)                 (None, 32)           6272        ['dropout_55[0][0]']             \n",
      "                                                                                                  \n",
      " dense_54 (Dense)               (None, 40)           1320        ['lstm_83[0][0]']                \n",
      "                                                                                                  \n",
      " dense_55 (Dense)               (None, 5)            205         ['dense_54[0][0]']               \n",
      "                                                                                                  \n",
      " tf.unstack_27 (TFOpLambda)     [(None,),            0           ['dense_55[0][0]']               \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_135 (TFOpLambda  (None, 1)           0           ['tf.unstack_27[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_54 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_135[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_139 (TFOpLambda  (None, 1)           0           ['tf.unstack_27[0][4]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_81 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_54[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_55 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_139[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_82 (TFOpLambd  (None, 1)           0           ['tf.math.multiply_81[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_136 (TFOpLambda  (None, 1)           0           ['tf.unstack_27[0][1]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_138 (TFOpLambda  (None, 1)           0           ['tf.unstack_27[0][3]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_83 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_55[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_54 (TFOpL  (None, 1)           0           ['tf.math.multiply_82[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.softplus_54 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_136[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_137 (TFOpLambda  (None, 1)           0           ['tf.unstack_27[0][2]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.softplus_55 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_138[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_55 (TFOpL  (None, 1)           0           ['tf.math.multiply_83[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack_27 (TFOpLambda)       (None, 5, 1)         0           ['tf.__operators__.add_54[0][0]',\n",
      "                                                                  'tf.math.softplus_54[0][0]',    \n",
      "                                                                  'tf.expand_dims_137[0][0]',     \n",
      "                                                                  'tf.math.softplus_55[0][0]',    \n",
      "                                                                  'tf.__operators__.add_55[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _2_CCMP_t+1_0.17\n",
      "Epoch 1/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 3.4727\n",
      "Epoch 1: val_loss improved from inf to 3.97030, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.17.hdf5\n",
      "66/66 [==============================] - 11s 100ms/step - loss: 3.4727 - val_loss: 3.9703 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 2.8151\n",
      "Epoch 2: val_loss improved from 3.97030 to 3.43746, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.17.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 6s 92ms/step - loss: 2.8123 - val_loss: 3.4375 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.9174\n",
      "Epoch 3: val_loss improved from 3.43746 to 3.07697, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.17.hdf5\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 1.9174 - val_loss: 3.0770 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.6515\n",
      "Epoch 4: val_loss improved from 3.07697 to 2.91752, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.17.hdf5\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 1.6515 - val_loss: 2.9175 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.5317\n",
      "Epoch 5: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 1.5317 - val_loss: 3.0181 - lr: 1.0000e-04\n",
      "Epoch 6/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.4502\n",
      "Epoch 6: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 1.4502 - val_loss: 3.6034 - lr: 9.9000e-05\n",
      "Epoch 7/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.3745\n",
      "Epoch 7: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 1.3745 - val_loss: 3.5576 - lr: 9.8010e-05\n",
      "Epoch 8/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.3277\n",
      "Epoch 8: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.3277 - val_loss: 3.9493 - lr: 9.7030e-05\n",
      "Epoch 9/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.2750\n",
      "Epoch 9: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 1.2750 - val_loss: 4.4095 - lr: 9.6060e-05\n",
      "Epoch 10/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.2478\n",
      "Epoch 10: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 1.2507 - val_loss: 4.2452 - lr: 9.5099e-05\n",
      "Epoch 11/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.2185\n",
      "Epoch 11: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.2185 - val_loss: 4.7262 - lr: 9.4148e-05\n",
      "Epoch 12/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.2004\n",
      "Epoch 12: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 1.2004 - val_loss: 4.5935 - lr: 9.3207e-05\n",
      "Epoch 13/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1829\n",
      "Epoch 13: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 1.1829 - val_loss: 5.1417 - lr: 9.2274e-05\n",
      "Epoch 14/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1645\n",
      "Epoch 14: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.1652 - val_loss: 4.5186 - lr: 9.1352e-05\n",
      "Epoch 15/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1621\n",
      "Epoch 15: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.1613 - val_loss: 4.7691 - lr: 9.0438e-05\n",
      "Epoch 16/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1444\n",
      "Epoch 16: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.1444 - val_loss: 4.3430 - lr: 8.9534e-05\n",
      "Epoch 17/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1333\n",
      "Epoch 17: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 1.1333 - val_loss: 4.2977 - lr: 8.8638e-05\n",
      "Epoch 18/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1233\n",
      "Epoch 18: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 1.1233 - val_loss: 4.3524 - lr: 8.7752e-05\n",
      "Epoch 19/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1143\n",
      "Epoch 19: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 1.1129 - val_loss: 4.2367 - lr: 8.6875e-05\n",
      "Epoch 20/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1177\n",
      "Epoch 20: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.1177 - val_loss: 4.0470 - lr: 8.6006e-05\n",
      "Epoch 21/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1093\n",
      "Epoch 21: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 1.1093 - val_loss: 4.2083 - lr: 8.5146e-05\n",
      "Epoch 22/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0919\n",
      "Epoch 22: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 1.0919 - val_loss: 3.9178 - lr: 8.4294e-05\n",
      "Epoch 23/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0883\n",
      "Epoch 23: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.0885 - val_loss: 4.1288 - lr: 8.3451e-05\n",
      "Epoch 24/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0788\n",
      "Epoch 24: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.0788 - val_loss: 3.8714 - lr: 8.2617e-05\n",
      "Epoch 25/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0735\n",
      "Epoch 25: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 1.0726 - val_loss: 3.6164 - lr: 8.1791e-05\n",
      "Epoch 26/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0692\n",
      "Epoch 26: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 1.0692 - val_loss: 3.7543 - lr: 8.0973e-05\n",
      "Epoch 27/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0745\n",
      "Epoch 27: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 1.0745 - val_loss: 3.8418 - lr: 8.0163e-05\n",
      "Epoch 28/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0625\n",
      "Epoch 28: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 1.0625 - val_loss: 3.9007 - lr: 7.9361e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0575\n",
      "Epoch 29: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 1.0575 - val_loss: 3.7858 - lr: 7.8568e-05\n",
      "Epoch 30/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0687\n",
      "Epoch 30: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 1.0687 - val_loss: 3.4910 - lr: 7.7782e-05\n",
      "Epoch 31/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0506\n",
      "Epoch 31: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 1.0506 - val_loss: 3.4950 - lr: 7.7004e-05\n",
      "Epoch 32/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0503\n",
      "Epoch 32: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 1.0503 - val_loss: 3.1187 - lr: 7.6234e-05\n",
      "Epoch 33/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0639\n",
      "Epoch 33: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 1.0639 - val_loss: 3.4919 - lr: 7.5472e-05\n",
      "Epoch 34/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0467\n",
      "Epoch 34: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 1.0467 - val_loss: 3.2166 - lr: 7.4717e-05\n",
      "Epoch 35/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0376\n",
      "Epoch 35: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 1.0376 - val_loss: 3.4238 - lr: 7.3970e-05\n",
      "Epoch 36/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0354\n",
      "Epoch 36: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 1.0354 - val_loss: 3.2924 - lr: 7.3230e-05\n",
      "Epoch 37/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0345\n",
      "Epoch 37: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 1.0345 - val_loss: 3.4619 - lr: 7.2498e-05\n",
      "Epoch 38/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0335\n",
      "Epoch 38: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 1.0335 - val_loss: 3.1468 - lr: 7.1773e-05\n",
      "Epoch 39/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0193\n",
      "Epoch 39: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.0225 - val_loss: 3.3763 - lr: 7.1055e-05\n",
      "Epoch 40/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0348\n",
      "Epoch 40: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 1.0348 - val_loss: 3.2460 - lr: 7.0345e-05\n",
      "Epoch 41/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0231\n",
      "Epoch 41: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 1.0231 - val_loss: 3.1116 - lr: 6.9641e-05\n",
      "Epoch 42/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0232\n",
      "Epoch 42: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 1.0232 - val_loss: 3.0050 - lr: 6.8945e-05\n",
      "Epoch 43/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0219\n",
      "Epoch 43: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.0211 - val_loss: 2.9755 - lr: 6.8255e-05\n",
      "Epoch 44/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0094\n",
      "Epoch 44: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 1.0094 - val_loss: 3.0110 - lr: 6.7573e-05\n",
      "Epoch 45/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0228\n",
      "Epoch 45: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 1.0228 - val_loss: 2.9997 - lr: 6.6897e-05\n",
      "Epoch 46/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0060\n",
      "Epoch 46: val_loss did not improve from 2.91752\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 1.0060 - val_loss: 3.1570 - lr: 6.6228e-05\n",
      "Epoch 47/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0003\n",
      "Epoch 47: val_loss improved from 2.91752 to 2.89493, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.17.hdf5\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 1.0024 - val_loss: 2.8949 - lr: 6.5566e-05\n",
      "Epoch 48/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0100\n",
      "Epoch 48: val_loss improved from 2.89493 to 2.87289, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.17.hdf5\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 1.0100 - val_loss: 2.8729 - lr: 6.5566e-05\n",
      "Epoch 49/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0172\n",
      "Epoch 49: val_loss did not improve from 2.87289\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 1.0172 - val_loss: 2.8971 - lr: 6.5566e-05\n",
      "Epoch 50/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9993\n",
      "Epoch 50: val_loss did not improve from 2.87289\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.9993 - val_loss: 2.8815 - lr: 6.4910e-05\n",
      "Epoch 51/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0125\n",
      "Epoch 51: val_loss did not improve from 2.87289\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 1.0125 - val_loss: 2.8811 - lr: 6.4261e-05\n",
      "Epoch 52/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0115\n",
      "Epoch 52: val_loss did not improve from 2.87289\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.0115 - val_loss: 2.8813 - lr: 6.3619e-05\n",
      "Epoch 53/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9895\n",
      "Epoch 53: val_loss did not improve from 2.87289\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.9895 - val_loss: 2.9674 - lr: 6.2982e-05\n",
      "Epoch 54/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0007\n",
      "Epoch 54: val_loss improved from 2.87289 to 2.81034, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.17.hdf5\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 1.0007 - val_loss: 2.8103 - lr: 6.2353e-05\n",
      "Epoch 55/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9968\n",
      "Epoch 55: val_loss improved from 2.81034 to 2.76196, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.17.hdf5\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.9945 - val_loss: 2.7620 - lr: 6.2353e-05\n",
      "Epoch 56/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0076\n",
      "Epoch 56: val_loss did not improve from 2.76196\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 1.0076 - val_loss: 2.8964 - lr: 6.2353e-05\n",
      "Epoch 57/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9929\n",
      "Epoch 57: val_loss improved from 2.76196 to 2.67100, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.17.hdf5\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.9929 - val_loss: 2.6710 - lr: 6.1729e-05\n",
      "Epoch 58/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9859\n",
      "Epoch 58: val_loss improved from 2.67100 to 2.58388, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.17.hdf5\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.9859 - val_loss: 2.5839 - lr: 6.1729e-05\n",
      "Epoch 59/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9996\n",
      "Epoch 59: val_loss did not improve from 2.58388\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.9986 - val_loss: 2.7290 - lr: 6.1729e-05\n",
      "Epoch 60/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9909\n",
      "Epoch 60: val_loss did not improve from 2.58388\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.9909 - val_loss: 2.9553 - lr: 6.1112e-05\n",
      "Epoch 61/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9891\n",
      "Epoch 61: val_loss did not improve from 2.58388\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.9891 - val_loss: 2.7791 - lr: 6.0501e-05\n",
      "Epoch 62/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9824\n",
      "Epoch 62: val_loss did not improve from 2.58388\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.9824 - val_loss: 2.7208 - lr: 5.9896e-05\n",
      "Epoch 63/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9838\n",
      "Epoch 63: val_loss did not improve from 2.58388\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 5.8703650938696225e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.9849 - val_loss: 2.5968 - lr: 5.9297e-05\n",
      "Epoch 64/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9833\n",
      "Epoch 64: val_loss improved from 2.58388 to 2.45208, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.17.hdf5\n",
      "66/66 [==============================] - 6s 98ms/step - loss: 0.9833 - val_loss: 2.4521 - lr: 5.8704e-05\n",
      "Epoch 65/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9925\n",
      "Epoch 65: val_loss did not improve from 2.45208\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 5.811661550978897e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.9925 - val_loss: 2.4640 - lr: 5.8704e-05\n",
      "Epoch 66/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9872\n",
      "Epoch 66: val_loss did not improve from 2.45208\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 5.753545068728272e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.9872 - val_loss: 2.5100 - lr: 5.8117e-05\n",
      "Epoch 67/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9807\n",
      "Epoch 67: val_loss did not improve from 2.45208\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 5.6960095243994146e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.9798 - val_loss: 2.6873 - lr: 5.7535e-05\n",
      "Epoch 68/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9861\n",
      "Epoch 68: val_loss did not improve from 2.45208\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 5.639049515593797e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.9861 - val_loss: 2.5336 - lr: 5.6960e-05\n",
      "Epoch 69/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9792\n",
      "Epoch 69: val_loss did not improve from 2.45208\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 5.5826589195930865e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.9792 - val_loss: 2.6169 - lr: 5.6390e-05\n",
      "Epoch 70/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9702\n",
      "Epoch 70: val_loss did not improve from 2.45208\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 5.526832333998754e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.9702 - val_loss: 2.5433 - lr: 5.5827e-05\n",
      "Epoch 71/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9779\n",
      "Epoch 71: val_loss did not improve from 2.45208\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 5.471563996252371e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.9774 - val_loss: 2.4685 - lr: 5.5268e-05\n",
      "Epoch 72/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9762\n",
      "Epoch 72: val_loss did not improve from 2.45208\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 5.416848503955407e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.9743 - val_loss: 2.4572 - lr: 5.4716e-05\n",
      "Epoch 73/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9763\n",
      "Epoch 73: val_loss improved from 2.45208 to 2.41899, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.17.hdf5\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.9758 - val_loss: 2.4190 - lr: 5.4168e-05\n",
      "Epoch 74/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9653\n",
      "Epoch 74: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 5.362680094549433e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.9653 - val_loss: 2.5903 - lr: 5.4168e-05\n",
      "Epoch 75/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9706\n",
      "Epoch 75: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 5.309053365635918e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.9696 - val_loss: 2.5668 - lr: 5.3627e-05\n",
      "Epoch 76/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9750\n",
      "Epoch 76: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 5.2559629148163366e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.9750 - val_loss: 2.6687 - lr: 5.3091e-05\n",
      "Epoch 77/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9681\n",
      "Epoch 77: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 5.2034033396921584e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.9681 - val_loss: 2.4671 - lr: 5.2560e-05\n",
      "Epoch 78/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9733\n",
      "Epoch 78: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 5.1513692378648555e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.9733 - val_loss: 2.6610 - lr: 5.2034e-05\n",
      "Epoch 79/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9735\n",
      "Epoch 79: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 5.099855567095801e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.9735 - val_loss: 2.5699 - lr: 5.1514e-05\n",
      "Epoch 80/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9651\n",
      "Epoch 80: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 5.048856924986467e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.9651 - val_loss: 2.5108 - lr: 5.0999e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9629\n",
      "Epoch 81: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 4.998368269298226e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.9629 - val_loss: 2.5864 - lr: 5.0489e-05\n",
      "Epoch 82/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9566\n",
      "Epoch 82: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 4.9483845577924514e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.9566 - val_loss: 2.6964 - lr: 4.9984e-05\n",
      "Epoch 83/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9636\n",
      "Epoch 83: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 4.898900748230517e-05.\n",
      "66/66 [==============================] - 6s 98ms/step - loss: 0.9636 - val_loss: 2.5247 - lr: 4.9484e-05\n",
      "Epoch 84/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9706\n",
      "Epoch 84: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 4.849911798373796e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.9706 - val_loss: 2.5716 - lr: 4.8989e-05\n",
      "Epoch 85/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9576\n",
      "Epoch 85: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 4.801412665983662e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.9576 - val_loss: 2.5586 - lr: 4.8499e-05\n",
      "Epoch 86/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9566\n",
      "Epoch 86: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 4.75339866898139e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.9566 - val_loss: 2.6973 - lr: 4.8014e-05\n",
      "Epoch 87/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9601\n",
      "Epoch 87: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 4.705864765128354e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.9601 - val_loss: 2.5353 - lr: 4.7534e-05\n",
      "Epoch 88/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9649\n",
      "Epoch 88: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 4.658806272345828e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.9638 - val_loss: 2.7081 - lr: 4.7059e-05\n",
      "Epoch 89/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9639\n",
      "Epoch 89: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 4.612218148395186e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.9639 - val_loss: 2.6426 - lr: 4.6588e-05\n",
      "Epoch 90/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9595\n",
      "Epoch 90: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 4.566096071357606e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.9595 - val_loss: 2.6699 - lr: 4.6122e-05\n",
      "Epoch 91/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9546\n",
      "Epoch 91: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 4.52043499899446e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.9546 - val_loss: 2.6786 - lr: 4.5661e-05\n",
      "Epoch 92/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9435\n",
      "Epoch 92: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 4.475230609386927e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.9511 - val_loss: 2.6059 - lr: 4.5204e-05\n",
      "Epoch 93/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9521\n",
      "Epoch 93: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 4.43047822045628e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.9521 - val_loss: 2.5951 - lr: 4.4752e-05\n",
      "Epoch 94/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9583\n",
      "Epoch 94: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 4.386173510283697e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.9583 - val_loss: 2.6585 - lr: 4.4305e-05\n",
      "Epoch 95/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9591\n",
      "Epoch 95: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 4.342311796790454e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.9591 - val_loss: 2.5296 - lr: 4.3862e-05\n",
      "Epoch 96/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9438\n",
      "Epoch 96: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 4.298888758057728e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.9423 - val_loss: 2.6234 - lr: 4.3423e-05\n",
      "Epoch 97/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9643\n",
      "Epoch 97: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 4.2558997120067946e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.9643 - val_loss: 2.5729 - lr: 4.2989e-05\n",
      "Epoch 98/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9467\n",
      "Epoch 98: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 4.2133406968787315e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.9467 - val_loss: 2.5715 - lr: 4.2559e-05\n",
      "Epoch 99/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9516\n",
      "Epoch 99: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 4.1712073907547166e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.9516 - val_loss: 2.6518 - lr: 4.2133e-05\n",
      "Epoch 100/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9427\n",
      "Epoch 100: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 4.1294954717159274e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.9427 - val_loss: 2.5273 - lr: 4.1712e-05\n",
      "Epoch 101/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9554\n",
      "Epoch 101: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 4.08820061784354e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.9554 - val_loss: 2.6403 - lr: 4.1295e-05\n",
      "Epoch 102/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9491\n",
      "Epoch 102: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 4.0473185072187336e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.9492 - val_loss: 2.5749 - lr: 4.0882e-05\n",
      "Epoch 103/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9431\n",
      "Epoch 103: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 4.0068451780825854e-05.\n",
      "66/66 [==============================] - 6s 98ms/step - loss: 0.9431 - val_loss: 2.4807 - lr: 4.0473e-05\n",
      "Epoch 104/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9473\n",
      "Epoch 104: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 3.966776668676175e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.9473 - val_loss: 2.6880 - lr: 4.0068e-05\n",
      "Epoch 105/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9395\n",
      "Epoch 105: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 3.927109017240582e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.9395 - val_loss: 2.7560 - lr: 3.9668e-05\n",
      "Epoch 106/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9425\n",
      "Epoch 106: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 3.887837901856983e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.9425 - val_loss: 2.6507 - lr: 3.9271e-05\n",
      "Epoch 107/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9407\n",
      "Epoch 107: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 3.848959360766457e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 6s 95ms/step - loss: 0.9407 - val_loss: 2.6242 - lr: 3.8878e-05\n",
      "Epoch 108/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9407\n",
      "Epoch 108: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 3.810469792369986e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.9407 - val_loss: 2.5491 - lr: 3.8490e-05\n",
      "Epoch 109/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9396\n",
      "Epoch 109: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 3.772365234908648e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.9396 - val_loss: 2.5752 - lr: 3.8105e-05\n",
      "Epoch 110/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9289\n",
      "Epoch 110: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 3.734641726623522e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.9289 - val_loss: 2.7213 - lr: 3.7724e-05\n",
      "Epoch 111/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9429\n",
      "Epoch 111: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 3.6972953057556876e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.9429 - val_loss: 2.6207 - lr: 3.7346e-05\n",
      "Epoch 112/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9386\n",
      "Epoch 112: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 3.660322370706126e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.9386 - val_loss: 2.5490 - lr: 3.6973e-05\n",
      "Epoch 113/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9400\n",
      "Epoch 113: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 3.623719319875818e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.9400 - val_loss: 2.6834 - lr: 3.6603e-05\n",
      "Epoch 114/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9415\n",
      "Epoch 114: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 3.5874821915058416e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.9415 - val_loss: 2.5705 - lr: 3.6237e-05\n",
      "Epoch 115/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9484\n",
      "Epoch 115: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 3.551607383997179e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.9484 - val_loss: 2.6512 - lr: 3.5875e-05\n",
      "Epoch 116/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9272\n",
      "Epoch 116: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 3.516091295750812e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.9256 - val_loss: 2.6903 - lr: 3.5516e-05\n",
      "Epoch 117/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9333\n",
      "Epoch 117: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 3.480930325167719e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.9333 - val_loss: 2.6021 - lr: 3.5161e-05\n",
      "Epoch 118/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9378\n",
      "Epoch 118: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 3.446120870648883e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.9378 - val_loss: 2.6006 - lr: 3.4809e-05\n",
      "Epoch 119/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9324\n",
      "Epoch 119: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 3.4116596907551866e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.9324 - val_loss: 2.6707 - lr: 3.4461e-05\n",
      "Epoch 120/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9390\n",
      "Epoch 120: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 3.37754318388761e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.9393 - val_loss: 2.5669 - lr: 3.4117e-05\n",
      "Epoch 121/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9415\n",
      "Epoch 121: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 3.3437677484471354e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.9415 - val_loss: 2.6858 - lr: 3.3775e-05\n",
      "Epoch 122/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9319\n",
      "Epoch 122: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 3.310330142994644e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.9319 - val_loss: 2.6807 - lr: 3.3438e-05\n",
      "Epoch 123/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9299\n",
      "Epoch 123: val_loss did not improve from 2.41899\n",
      "\n",
      "Epoch 123: ReduceLROnPlateau reducing learning rate to 3.277226765931118e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.9299 - val_loss: 2.5434 - lr: 3.3103e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_28\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_29 (InputLayer)          [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_84 (LSTM)                 (None, 45, 24)       3744        ['input_29[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_56 (Dropout)           (None, 45, 24)       0           ['lstm_84[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_85 (LSTM)                 (None, 45, 16)       2624        ['dropout_56[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_57 (Dropout)           (None, 45, 16)       0           ['lstm_85[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_86 (LSTM)                 (None, 32)           6272        ['dropout_57[0][0]']             \n",
      "                                                                                                  \n",
      " dense_56 (Dense)               (None, 40)           1320        ['lstm_86[0][0]']                \n",
      "                                                                                                  \n",
      " dense_57 (Dense)               (None, 5)            205         ['dense_56[0][0]']               \n",
      "                                                                                                  \n",
      " tf.unstack_28 (TFOpLambda)     [(None,),            0           ['dense_57[0][0]']               \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_140 (TFOpLambda  (None, 1)           0           ['tf.unstack_28[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_56 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_140[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_144 (TFOpLambda  (None, 1)           0           ['tf.unstack_28[0][4]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_84 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_56[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_57 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_144[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_85 (TFOpLambd  (None, 1)           0           ['tf.math.multiply_84[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_141 (TFOpLambda  (None, 1)           0           ['tf.unstack_28[0][1]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_143 (TFOpLambda  (None, 1)           0           ['tf.unstack_28[0][3]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_86 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_57[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_56 (TFOpL  (None, 1)           0           ['tf.math.multiply_85[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.softplus_56 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_141[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_142 (TFOpLambda  (None, 1)           0           ['tf.unstack_28[0][2]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.softplus_57 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_143[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_57 (TFOpL  (None, 1)           0           ['tf.math.multiply_86[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack_28 (TFOpLambda)       (None, 5, 1)         0           ['tf.__operators__.add_56[0][0]',\n",
      "                                                                  'tf.math.softplus_56[0][0]',    \n",
      "                                                                  'tf.expand_dims_142[0][0]',     \n",
      "                                                                  'tf.math.softplus_57[0][0]',    \n",
      "                                                                  'tf.__operators__.add_57[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _2_CCMP_t+1_0.18\n",
      "Epoch 1/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 3.4092\n",
      "Epoch 1: val_loss improved from inf to 3.83782, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 11s 95ms/step - loss: 3.4116 - val_loss: 3.8378 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 2.7442\n",
      "Epoch 2: val_loss improved from 3.83782 to 3.81111, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.18.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 5s 83ms/step - loss: 2.7442 - val_loss: 3.8111 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 2.0106\n",
      "Epoch 3: val_loss improved from 3.81111 to 3.47115, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 2.0106 - val_loss: 3.4712 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.6661\n",
      "Epoch 4: val_loss did not improve from 3.47115\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 1.6661 - val_loss: 4.2312 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.4946\n",
      "Epoch 5: val_loss did not improve from 3.47115\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 1.4946 - val_loss: 3.9468 - lr: 9.9000e-05\n",
      "Epoch 6/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.3938\n",
      "Epoch 6: val_loss improved from 3.47115 to 3.10477, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 1.3938 - val_loss: 3.1048 - lr: 9.8010e-05\n",
      "Epoch 7/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.3429\n",
      "Epoch 7: val_loss did not improve from 3.10477\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 1.3402 - val_loss: 3.4754 - lr: 9.8010e-05\n",
      "Epoch 8/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.3041\n",
      "Epoch 8: val_loss did not improve from 3.10477\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 1.3041 - val_loss: 3.3070 - lr: 9.7030e-05\n",
      "Epoch 9/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.2919\n",
      "Epoch 9: val_loss improved from 3.10477 to 3.09089, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 1.2919 - val_loss: 3.0909 - lr: 9.6060e-05\n",
      "Epoch 10/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.2628\n",
      "Epoch 10: val_loss did not improve from 3.09089\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 1.2628 - val_loss: 3.3566 - lr: 9.6060e-05\n",
      "Epoch 11/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.2289\n",
      "Epoch 11: val_loss improved from 3.09089 to 2.97008, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 1.2289 - val_loss: 2.9701 - lr: 9.5099e-05\n",
      "Epoch 12/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.2236\n",
      "Epoch 12: val_loss improved from 2.97008 to 2.73635, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 1.2236 - val_loss: 2.7363 - lr: 9.5099e-05\n",
      "Epoch 13/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.2114\n",
      "Epoch 13: val_loss improved from 2.73635 to 2.65870, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 1.2114 - val_loss: 2.6587 - lr: 9.5099e-05\n",
      "Epoch 14/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1825\n",
      "Epoch 14: val_loss improved from 2.65870 to 2.65294, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 1.1825 - val_loss: 2.6529 - lr: 9.5099e-05\n",
      "Epoch 15/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1670\n",
      "Epoch 15: val_loss improved from 2.65294 to 2.64892, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 1.1670 - val_loss: 2.6489 - lr: 9.5099e-05\n",
      "Epoch 16/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1620\n",
      "Epoch 16: val_loss improved from 2.64892 to 2.48950, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 1.1620 - val_loss: 2.4895 - lr: 9.5099e-05\n",
      "Epoch 17/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1434\n",
      "Epoch 17: val_loss did not improve from 2.48950\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 1.1434 - val_loss: 2.7443 - lr: 9.5099e-05\n",
      "Epoch 18/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1350\n",
      "Epoch 18: val_loss did not improve from 2.48950\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 1.1350 - val_loss: 2.6454 - lr: 9.4148e-05\n",
      "Epoch 19/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1076\n",
      "Epoch 19: val_loss did not improve from 2.48950\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 1.1076 - val_loss: 2.5430 - lr: 9.3207e-05\n",
      "Epoch 20/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1127\n",
      "Epoch 20: val_loss improved from 2.48950 to 2.34999, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 1.1170 - val_loss: 2.3500 - lr: 9.2274e-05\n",
      "Epoch 21/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1072\n",
      "Epoch 21: val_loss did not improve from 2.34999\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 1.1072 - val_loss: 2.3512 - lr: 9.2274e-05\n",
      "Epoch 22/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0816\n",
      "Epoch 22: val_loss did not improve from 2.34999\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 1.0816 - val_loss: 2.3774 - lr: 9.1352e-05\n",
      "Epoch 23/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0940\n",
      "Epoch 23: val_loss did not improve from 2.34999\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 1.0940 - val_loss: 2.5854 - lr: 9.0438e-05\n",
      "Epoch 24/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0764\n",
      "Epoch 24: val_loss did not improve from 2.34999\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 1.0764 - val_loss: 2.3768 - lr: 8.9534e-05\n",
      "Epoch 25/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0739\n",
      "Epoch 25: val_loss improved from 2.34999 to 2.26445, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 1.0739 - val_loss: 2.2645 - lr: 8.8638e-05\n",
      "Epoch 26/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0782\n",
      "Epoch 26: val_loss did not improve from 2.26445\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 1.0790 - val_loss: 2.2851 - lr: 8.8638e-05\n",
      "Epoch 27/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0873\n",
      "Epoch 27: val_loss improved from 2.26445 to 2.11869, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.18.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 6s 87ms/step - loss: 1.0852 - val_loss: 2.1187 - lr: 8.7752e-05\n",
      "Epoch 28/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0739\n",
      "Epoch 28: val_loss improved from 2.11869 to 2.11322, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 1.0739 - val_loss: 2.1132 - lr: 8.7752e-05\n",
      "Epoch 29/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0708\n",
      "Epoch 29: val_loss did not improve from 2.11322\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 1.0708 - val_loss: 2.2382 - lr: 8.7752e-05\n",
      "Epoch 30/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0519\n",
      "Epoch 30: val_loss improved from 2.11322 to 2.05172, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 1.0519 - val_loss: 2.0517 - lr: 8.6875e-05\n",
      "Epoch 31/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0615\n",
      "Epoch 31: val_loss did not improve from 2.05172\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 1.0615 - val_loss: 2.0619 - lr: 8.6875e-05\n",
      "Epoch 32/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0580\n",
      "Epoch 32: val_loss did not improve from 2.05172\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 1.0580 - val_loss: 2.2070 - lr: 8.6006e-05\n",
      "Epoch 33/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0484\n",
      "Epoch 33: val_loss did not improve from 2.05172\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 1.0480 - val_loss: 2.1663 - lr: 8.5146e-05\n",
      "Epoch 34/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0597\n",
      "Epoch 34: val_loss improved from 2.05172 to 1.96381, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 1.0584 - val_loss: 1.9638 - lr: 8.4294e-05\n",
      "Epoch 35/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0382\n",
      "Epoch 35: val_loss did not improve from 1.96381\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 1.0382 - val_loss: 2.0598 - lr: 8.4294e-05\n",
      "Epoch 36/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0424\n",
      "Epoch 36: val_loss did not improve from 1.96381\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 1.0424 - val_loss: 2.0981 - lr: 8.3451e-05\n",
      "Epoch 37/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0347\n",
      "Epoch 37: val_loss did not improve from 1.96381\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 1.0347 - val_loss: 2.0228 - lr: 8.2617e-05\n",
      "Epoch 38/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0325\n",
      "Epoch 38: val_loss did not improve from 1.96381\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 1.0325 - val_loss: 2.0779 - lr: 8.1791e-05\n",
      "Epoch 39/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0291\n",
      "Epoch 39: val_loss did not improve from 1.96381\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 1.0291 - val_loss: 1.9889 - lr: 8.0973e-05\n",
      "Epoch 40/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0356\n",
      "Epoch 40: val_loss did not improve from 1.96381\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 1.0356 - val_loss: 2.1223 - lr: 8.0163e-05\n",
      "Epoch 41/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0223\n",
      "Epoch 41: val_loss did not improve from 1.96381\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 1.0223 - val_loss: 2.0574 - lr: 7.9361e-05\n",
      "Epoch 42/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0266\n",
      "Epoch 42: val_loss improved from 1.96381 to 1.88389, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 1.0266 - val_loss: 1.8839 - lr: 7.8568e-05\n",
      "Epoch 43/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0224\n",
      "Epoch 43: val_loss did not improve from 1.88389\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 1.0224 - val_loss: 1.9338 - lr: 7.8568e-05\n",
      "Epoch 44/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0200\n",
      "Epoch 44: val_loss did not improve from 1.88389\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 1.0200 - val_loss: 1.9759 - lr: 7.7782e-05\n",
      "Epoch 45/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0120\n",
      "Epoch 45: val_loss did not improve from 1.88389\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 1.0120 - val_loss: 1.9147 - lr: 7.7004e-05\n",
      "Epoch 46/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0102\n",
      "Epoch 46: val_loss did not improve from 1.88389\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 1.0090 - val_loss: 2.0753 - lr: 7.6234e-05\n",
      "Epoch 47/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0158\n",
      "Epoch 47: val_loss did not improve from 1.88389\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 1.0158 - val_loss: 1.9118 - lr: 7.5472e-05\n",
      "Epoch 48/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0150\n",
      "Epoch 48: val_loss did not improve from 1.88389\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 1.0150 - val_loss: 1.9211 - lr: 7.4717e-05\n",
      "Epoch 49/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0126\n",
      "Epoch 49: val_loss did not improve from 1.88389\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 1.0126 - val_loss: 1.9606 - lr: 7.3970e-05\n",
      "Epoch 50/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0087\n",
      "Epoch 50: val_loss did not improve from 1.88389\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 1.0087 - val_loss: 1.9139 - lr: 7.3230e-05\n",
      "Epoch 51/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0152\n",
      "Epoch 51: val_loss did not improve from 1.88389\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 1.0152 - val_loss: 1.9245 - lr: 7.2498e-05\n",
      "Epoch 52/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9929\n",
      "Epoch 52: val_loss did not improve from 1.88389\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9929 - val_loss: 1.9146 - lr: 7.1773e-05\n",
      "Epoch 53/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0007\n",
      "Epoch 53: val_loss improved from 1.88389 to 1.85635, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.18.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 6s 88ms/step - loss: 1.0007 - val_loss: 1.8563 - lr: 7.1055e-05\n",
      "Epoch 54/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0120\n",
      "Epoch 54: val_loss did not improve from 1.85635\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 1.0120 - val_loss: 1.9151 - lr: 7.1055e-05\n",
      "Epoch 55/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0039\n",
      "Epoch 55: val_loss improved from 1.85635 to 1.82823, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 1.0039 - val_loss: 1.8282 - lr: 7.0345e-05\n",
      "Epoch 56/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9983\n",
      "Epoch 56: val_loss did not improve from 1.82823\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9983 - val_loss: 1.8497 - lr: 7.0345e-05\n",
      "Epoch 57/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0011\n",
      "Epoch 57: val_loss did not improve from 1.82823\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 1.0011 - val_loss: 1.8290 - lr: 6.9641e-05\n",
      "Epoch 58/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0035\n",
      "Epoch 58: val_loss did not improve from 1.82823\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 1.0035 - val_loss: 1.8490 - lr: 6.8945e-05\n",
      "Epoch 59/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9899\n",
      "Epoch 59: val_loss did not improve from 1.82823\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9899 - val_loss: 1.8443 - lr: 6.8255e-05\n",
      "Epoch 60/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9895\n",
      "Epoch 60: val_loss improved from 1.82823 to 1.81700, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9895 - val_loss: 1.8170 - lr: 6.7573e-05\n",
      "Epoch 61/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9965\n",
      "Epoch 61: val_loss improved from 1.81700 to 1.76712, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9965 - val_loss: 1.7671 - lr: 6.7573e-05\n",
      "Epoch 62/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9976\n",
      "Epoch 62: val_loss improved from 1.76712 to 1.75918, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9976 - val_loss: 1.7592 - lr: 6.7573e-05\n",
      "Epoch 63/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0006\n",
      "Epoch 63: val_loss did not improve from 1.75918\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 1.0006 - val_loss: 1.7867 - lr: 6.7573e-05\n",
      "Epoch 64/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0037\n",
      "Epoch 64: val_loss did not improve from 1.75918\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 1.0037 - val_loss: 1.7839 - lr: 6.6897e-05\n",
      "Epoch 65/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9981\n",
      "Epoch 65: val_loss did not improve from 1.75918\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9981 - val_loss: 1.8041 - lr: 6.6228e-05\n",
      "Epoch 66/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9950\n",
      "Epoch 66: val_loss did not improve from 1.75918\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9959 - val_loss: 1.8497 - lr: 6.5566e-05\n",
      "Epoch 67/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9881\n",
      "Epoch 67: val_loss did not improve from 1.75918\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9868 - val_loss: 1.7616 - lr: 6.4910e-05\n",
      "Epoch 68/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9928\n",
      "Epoch 68: val_loss did not improve from 1.75918\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9928 - val_loss: 1.7912 - lr: 6.4261e-05\n",
      "Epoch 69/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9857\n",
      "Epoch 69: val_loss did not improve from 1.75918\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9857 - val_loss: 1.8378 - lr: 6.3619e-05\n",
      "Epoch 70/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9863\n",
      "Epoch 70: val_loss did not improve from 1.75918\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9858 - val_loss: 1.8864 - lr: 6.2982e-05\n",
      "Epoch 71/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9785\n",
      "Epoch 71: val_loss did not improve from 1.75918\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.9785 - val_loss: 1.8059 - lr: 6.2353e-05\n",
      "Epoch 72/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9868\n",
      "Epoch 72: val_loss did not improve from 1.75918\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9868 - val_loss: 1.7952 - lr: 6.1729e-05\n",
      "Epoch 73/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9839\n",
      "Epoch 73: val_loss did not improve from 1.75918\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9839 - val_loss: 1.7901 - lr: 6.1112e-05\n",
      "Epoch 74/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9738\n",
      "Epoch 74: val_loss did not improve from 1.75918\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9738 - val_loss: 1.8267 - lr: 6.0501e-05\n",
      "Epoch 75/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9779\n",
      "Epoch 75: val_loss improved from 1.75918 to 1.74906, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9779 - val_loss: 1.7491 - lr: 5.9896e-05\n",
      "Epoch 76/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9755\n",
      "Epoch 76: val_loss improved from 1.74906 to 1.70398, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9755 - val_loss: 1.7040 - lr: 5.9896e-05\n",
      "Epoch 77/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9789\n",
      "Epoch 77: val_loss did not improve from 1.70398\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9789 - val_loss: 1.7090 - lr: 5.9896e-05\n",
      "Epoch 78/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9819\n",
      "Epoch 78: val_loss did not improve from 1.70398\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 5.8703650938696225e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9819 - val_loss: 1.7337 - lr: 5.9297e-05\n",
      "Epoch 79/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9687\n",
      "Epoch 79: val_loss did not improve from 1.70398\n",
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 5.811661550978897e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9674 - val_loss: 1.7531 - lr: 5.8704e-05\n",
      "Epoch 80/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9674\n",
      "Epoch 80: val_loss did not improve from 1.70398\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 5.753545068728272e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9674 - val_loss: 1.7736 - lr: 5.8117e-05\n",
      "Epoch 81/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9694\n",
      "Epoch 81: val_loss did not improve from 1.70398\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 5.6960095243994146e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.9694 - val_loss: 1.7588 - lr: 5.7535e-05\n",
      "Epoch 82/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9752\n",
      "Epoch 82: val_loss did not improve from 1.70398\n",
      "\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 5.639049515593797e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9752 - val_loss: 1.7134 - lr: 5.6960e-05\n",
      "Epoch 83/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9710\n",
      "Epoch 83: val_loss did not improve from 1.70398\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 5.5826589195930865e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9710 - val_loss: 1.7650 - lr: 5.6390e-05\n",
      "Epoch 84/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9647\n",
      "Epoch 84: val_loss did not improve from 1.70398\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 5.526832333998754e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9647 - val_loss: 1.7202 - lr: 5.5827e-05\n",
      "Epoch 85/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9717\n",
      "Epoch 85: val_loss did not improve from 1.70398\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 5.471563996252371e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9718 - val_loss: 1.7862 - lr: 5.5268e-05\n",
      "Epoch 86/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9607\n",
      "Epoch 86: val_loss did not improve from 1.70398\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 5.416848503955407e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9622 - val_loss: 1.8032 - lr: 5.4716e-05\n",
      "Epoch 87/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9756\n",
      "Epoch 87: val_loss did not improve from 1.70398\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 5.362680094549433e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9756 - val_loss: 1.7104 - lr: 5.4168e-05\n",
      "Epoch 88/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9739\n",
      "Epoch 88: val_loss did not improve from 1.70398\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 5.309053365635918e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9739 - val_loss: 1.7064 - lr: 5.3627e-05\n",
      "Epoch 89/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9665\n",
      "Epoch 89: val_loss did not improve from 1.70398\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 5.2559629148163366e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9665 - val_loss: 1.7609 - lr: 5.3091e-05\n",
      "Epoch 90/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9590\n",
      "Epoch 90: val_loss improved from 1.70398 to 1.69527, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9590 - val_loss: 1.6953 - lr: 5.2560e-05\n",
      "Epoch 91/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9641\n",
      "Epoch 91: val_loss did not improve from 1.69527\n",
      "\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 5.2034033396921584e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.9641 - val_loss: 1.7189 - lr: 5.2560e-05\n",
      "Epoch 92/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9594\n",
      "Epoch 92: val_loss did not improve from 1.69527\n",
      "\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 5.1513692378648555e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9617 - val_loss: 1.6961 - lr: 5.2034e-05\n",
      "Epoch 93/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9629\n",
      "Epoch 93: val_loss did not improve from 1.69527\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 5.099855567095801e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9629 - val_loss: 1.7483 - lr: 5.1514e-05\n",
      "Epoch 94/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9565\n",
      "Epoch 94: val_loss did not improve from 1.69527\n",
      "\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 5.048856924986467e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9565 - val_loss: 1.7556 - lr: 5.0999e-05\n",
      "Epoch 95/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9545\n",
      "Epoch 95: val_loss did not improve from 1.69527\n",
      "\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 4.998368269298226e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9545 - val_loss: 1.7375 - lr: 5.0489e-05\n",
      "Epoch 96/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9648\n",
      "Epoch 96: val_loss improved from 1.69527 to 1.68813, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9648 - val_loss: 1.6881 - lr: 4.9984e-05\n",
      "Epoch 97/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9549\n",
      "Epoch 97: val_loss did not improve from 1.68813\n",
      "\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 4.9483845577924514e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9549 - val_loss: 1.8090 - lr: 4.9984e-05\n",
      "Epoch 98/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9660\n",
      "Epoch 98: val_loss did not improve from 1.68813\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 4.898900748230517e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9660 - val_loss: 1.7462 - lr: 4.9484e-05\n",
      "Epoch 99/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9641\n",
      "Epoch 99: val_loss did not improve from 1.68813\n",
      "\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 4.849911798373796e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9637 - val_loss: 1.7306 - lr: 4.8989e-05\n",
      "Epoch 100/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9663\n",
      "Epoch 100: val_loss did not improve from 1.68813\n",
      "\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 4.801412665983662e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9663 - val_loss: 1.7070 - lr: 4.8499e-05\n",
      "Epoch 101/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9562\n",
      "Epoch 101: val_loss did not improve from 1.68813\n",
      "\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 4.75339866898139e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9562 - val_loss: 1.7228 - lr: 4.8014e-05\n",
      "Epoch 102/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9506\n",
      "Epoch 102: val_loss did not improve from 1.68813\n",
      "\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 4.705864765128354e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.9506 - val_loss: 1.7354 - lr: 4.7534e-05\n",
      "Epoch 103/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9652\n",
      "Epoch 103: val_loss improved from 1.68813 to 1.66821, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9652 - val_loss: 1.6682 - lr: 4.7059e-05\n",
      "Epoch 104/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9588\n",
      "Epoch 104: val_loss did not improve from 1.66821\n",
      "\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 4.658806272345828e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9588 - val_loss: 1.7100 - lr: 4.7059e-05\n",
      "Epoch 105/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - ETA: 0s - loss: 0.9581\n",
      "Epoch 105: val_loss did not improve from 1.66821\n",
      "\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 4.612218148395186e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9581 - val_loss: 1.7495 - lr: 4.6588e-05\n",
      "Epoch 106/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9583\n",
      "Epoch 106: val_loss did not improve from 1.66821\n",
      "\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 4.566096071357606e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9583 - val_loss: 1.6878 - lr: 4.6122e-05\n",
      "Epoch 107/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9593\n",
      "Epoch 107: val_loss did not improve from 1.66821\n",
      "\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 4.52043499899446e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9593 - val_loss: 1.6809 - lr: 4.5661e-05\n",
      "Epoch 108/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9615\n",
      "Epoch 108: val_loss did not improve from 1.66821\n",
      "\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 4.475230609386927e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9615 - val_loss: 1.7511 - lr: 4.5204e-05\n",
      "Epoch 109/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9525\n",
      "Epoch 109: val_loss did not improve from 1.66821\n",
      "\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 4.43047822045628e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9525 - val_loss: 1.7338 - lr: 4.4752e-05\n",
      "Epoch 110/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9554\n",
      "Epoch 110: val_loss did not improve from 1.66821\n",
      "\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 4.386173510283697e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9554 - val_loss: 1.7464 - lr: 4.4305e-05\n",
      "Epoch 111/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9561\n",
      "Epoch 111: val_loss did not improve from 1.66821\n",
      "\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 4.342311796790454e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9548 - val_loss: 1.7170 - lr: 4.3862e-05\n",
      "Epoch 112/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9461\n",
      "Epoch 112: val_loss did not improve from 1.66821\n",
      "\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 4.298888758057728e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9461 - val_loss: 1.7322 - lr: 4.3423e-05\n",
      "Epoch 113/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9523\n",
      "Epoch 113: val_loss did not improve from 1.66821\n",
      "\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 4.2558997120067946e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9523 - val_loss: 1.7408 - lr: 4.2989e-05\n",
      "Epoch 114/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9416\n",
      "Epoch 114: val_loss did not improve from 1.66821\n",
      "\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 4.2133406968787315e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9416 - val_loss: 1.7338 - lr: 4.2559e-05\n",
      "Epoch 115/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9513\n",
      "Epoch 115: val_loss did not improve from 1.66821\n",
      "\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 4.1712073907547166e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9513 - val_loss: 1.7088 - lr: 4.2133e-05\n",
      "Epoch 116/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9379\n",
      "Epoch 116: val_loss did not improve from 1.66821\n",
      "\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 4.1294954717159274e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9379 - val_loss: 1.7327 - lr: 4.1712e-05\n",
      "Epoch 117/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9534\n",
      "Epoch 117: val_loss did not improve from 1.66821\n",
      "\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 4.08820061784354e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9511 - val_loss: 1.7255 - lr: 4.1295e-05\n",
      "Epoch 118/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9551\n",
      "Epoch 118: val_loss did not improve from 1.66821\n",
      "\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 4.0473185072187336e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9540 - val_loss: 1.7252 - lr: 4.0882e-05\n",
      "Epoch 119/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9394\n",
      "Epoch 119: val_loss did not improve from 1.66821\n",
      "\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 4.0068451780825854e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9394 - val_loss: 1.7730 - lr: 4.0473e-05\n",
      "Epoch 120/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9388\n",
      "Epoch 120: val_loss did not improve from 1.66821\n",
      "\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 3.966776668676175e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9388 - val_loss: 1.6911 - lr: 4.0068e-05\n",
      "Epoch 121/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9491\n",
      "Epoch 121: val_loss did not improve from 1.66821\n",
      "\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 3.927109017240582e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9491 - val_loss: 1.7225 - lr: 3.9668e-05\n",
      "Epoch 122/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9427\n",
      "Epoch 122: val_loss did not improve from 1.66821\n",
      "\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 3.887837901856983e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9422 - val_loss: 1.7155 - lr: 3.9271e-05\n",
      "Epoch 123/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9433\n",
      "Epoch 123: val_loss did not improve from 1.66821\n",
      "\n",
      "Epoch 123: ReduceLROnPlateau reducing learning rate to 3.848959360766457e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.9433 - val_loss: 1.7942 - lr: 3.8878e-05\n",
      "Epoch 124/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9355\n",
      "Epoch 124: val_loss did not improve from 1.66821\n",
      "\n",
      "Epoch 124: ReduceLROnPlateau reducing learning rate to 3.810469792369986e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9386 - val_loss: 1.7309 - lr: 3.8490e-05\n",
      "Epoch 125/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9533\n",
      "Epoch 125: val_loss did not improve from 1.66821\n",
      "\n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 3.772365234908648e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 0.9533 - val_loss: 1.6921 - lr: 3.8105e-05\n",
      "Epoch 126/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9435\n",
      "Epoch 126: val_loss did not improve from 1.66821\n",
      "\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 3.734641726623522e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9435 - val_loss: 1.7183 - lr: 3.7724e-05\n",
      "Epoch 127/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9443\n",
      "Epoch 127: val_loss did not improve from 1.66821\n",
      "\n",
      "Epoch 127: ReduceLROnPlateau reducing learning rate to 3.6972953057556876e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9443 - val_loss: 1.7319 - lr: 3.7346e-05\n",
      "Epoch 128/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9430\n",
      "Epoch 128: val_loss did not improve from 1.66821\n",
      "\n",
      "Epoch 128: ReduceLROnPlateau reducing learning rate to 3.660322370706126e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9430 - val_loss: 1.7643 - lr: 3.6973e-05\n",
      "Epoch 129/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9456\n",
      "Epoch 129: val_loss did not improve from 1.66821\n",
      "\n",
      "Epoch 129: ReduceLROnPlateau reducing learning rate to 3.623719319875818e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9456 - val_loss: 1.6814 - lr: 3.6603e-05\n",
      "Epoch 130/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9414\n",
      "Epoch 130: val_loss did not improve from 1.66821\n",
      "\n",
      "Epoch 130: ReduceLROnPlateau reducing learning rate to 3.5874821915058416e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9414 - val_loss: 1.7667 - lr: 3.6237e-05\n",
      "Epoch 131/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9477\n",
      "Epoch 131: val_loss did not improve from 1.66821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 131: ReduceLROnPlateau reducing learning rate to 3.551607383997179e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9460 - val_loss: 1.7255 - lr: 3.5875e-05\n",
      "Epoch 132/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9431\n",
      "Epoch 132: val_loss did not improve from 1.66821\n",
      "\n",
      "Epoch 132: ReduceLROnPlateau reducing learning rate to 3.516091295750812e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 0.9406 - val_loss: 1.6996 - lr: 3.5516e-05\n",
      "Epoch 133/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9347\n",
      "Epoch 133: val_loss did not improve from 1.66821\n",
      "\n",
      "Epoch 133: ReduceLROnPlateau reducing learning rate to 3.480930325167719e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9347 - val_loss: 1.7050 - lr: 3.5161e-05\n",
      "Epoch 134/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9513\n",
      "Epoch 134: val_loss did not improve from 1.66821\n",
      "\n",
      "Epoch 134: ReduceLROnPlateau reducing learning rate to 3.446120870648883e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9513 - val_loss: 1.7147 - lr: 3.4809e-05\n",
      "Epoch 135/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9472\n",
      "Epoch 135: val_loss did not improve from 1.66821\n",
      "\n",
      "Epoch 135: ReduceLROnPlateau reducing learning rate to 3.4116596907551866e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9472 - val_loss: 1.7237 - lr: 3.4461e-05\n",
      "Epoch 136/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9405\n",
      "Epoch 136: val_loss did not improve from 1.66821\n",
      "\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 3.37754318388761e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9394 - val_loss: 1.7524 - lr: 3.4117e-05\n",
      "Epoch 137/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9371\n",
      "Epoch 137: val_loss did not improve from 1.66821\n",
      "\n",
      "Epoch 137: ReduceLROnPlateau reducing learning rate to 3.3437677484471354e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9370 - val_loss: 1.7099 - lr: 3.3775e-05\n",
      "Epoch 138/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9345\n",
      "Epoch 138: val_loss did not improve from 1.66821\n",
      "\n",
      "Epoch 138: ReduceLROnPlateau reducing learning rate to 3.310330142994644e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9322 - val_loss: 1.7517 - lr: 3.3438e-05\n",
      "Epoch 139/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9333\n",
      "Epoch 139: val_loss did not improve from 1.66821\n",
      "\n",
      "Epoch 139: ReduceLROnPlateau reducing learning rate to 3.277226765931118e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.9333 - val_loss: 1.7153 - lr: 3.3103e-05\n",
      "Epoch 140/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9407\n",
      "Epoch 140: val_loss did not improve from 1.66821\n",
      "\n",
      "Epoch 140: ReduceLROnPlateau reducing learning rate to 3.24445437581744e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9407 - val_loss: 1.7211 - lr: 3.2772e-05\n",
      "Epoch 141/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9398\n",
      "Epoch 141: val_loss improved from 1.66821 to 1.66622, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.18.hdf5\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9398 - val_loss: 1.6662 - lr: 3.2445e-05\n",
      "Epoch 142/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9487\n",
      "Epoch 142: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 142: ReduceLROnPlateau reducing learning rate to 3.212009731214493e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9487 - val_loss: 1.6918 - lr: 3.2445e-05\n",
      "Epoch 143/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9497\n",
      "Epoch 143: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 143: ReduceLROnPlateau reducing learning rate to 3.17988959068316e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.9497 - val_loss: 1.6998 - lr: 3.2120e-05\n",
      "Epoch 144/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9344\n",
      "Epoch 144: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 144: ReduceLROnPlateau reducing learning rate to 3.1480907127843236e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9344 - val_loss: 1.7232 - lr: 3.1799e-05\n",
      "Epoch 145/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9378\n",
      "Epoch 145: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 145: ReduceLROnPlateau reducing learning rate to 3.116609856078867e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9378 - val_loss: 1.7137 - lr: 3.1481e-05\n",
      "Epoch 146/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9407\n",
      "Epoch 146: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 146: ReduceLROnPlateau reducing learning rate to 3.085443779127672e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9407 - val_loss: 1.7885 - lr: 3.1166e-05\n",
      "Epoch 147/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9334\n",
      "Epoch 147: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 147: ReduceLROnPlateau reducing learning rate to 3.054589240491623e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9334 - val_loss: 1.7281 - lr: 3.0854e-05\n",
      "Epoch 148/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9336\n",
      "Epoch 148: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 148: ReduceLROnPlateau reducing learning rate to 3.024043358891504e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9336 - val_loss: 1.7734 - lr: 3.0546e-05\n",
      "Epoch 149/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9418\n",
      "Epoch 149: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 149: ReduceLROnPlateau reducing learning rate to 2.9938028928881975e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9418 - val_loss: 1.6950 - lr: 3.0240e-05\n",
      "Epoch 150/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9433\n",
      "Epoch 150: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 150: ReduceLROnPlateau reducing learning rate to 2.963864781122538e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9447 - val_loss: 1.7333 - lr: 2.9938e-05\n",
      "Epoch 151/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9374\n",
      "Epoch 151: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 151: ReduceLROnPlateau reducing learning rate to 2.9342261423153105e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9374 - val_loss: 1.7393 - lr: 2.9639e-05\n",
      "Epoch 152/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9329\n",
      "Epoch 152: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 152: ReduceLROnPlateau reducing learning rate to 2.9048839151073478e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9329 - val_loss: 1.7464 - lr: 2.9342e-05\n",
      "Epoch 153/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9335\n",
      "Epoch 153: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 153: ReduceLROnPlateau reducing learning rate to 2.875835038139485e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9335 - val_loss: 1.7359 - lr: 2.9049e-05\n",
      "Epoch 154/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9381\n",
      "Epoch 154: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 154: ReduceLROnPlateau reducing learning rate to 2.8470766301325058e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9381 - val_loss: 1.7287 - lr: 2.8758e-05\n",
      "Epoch 155/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9397\n",
      "Epoch 155: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 155: ReduceLROnPlateau reducing learning rate to 2.8186058098071954e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9397 - val_loss: 1.7256 - lr: 2.8471e-05\n",
      "Epoch 156/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9319\n",
      "Epoch 156: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 156: ReduceLROnPlateau reducing learning rate to 2.7904196958843385e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9319 - val_loss: 1.7379 - lr: 2.8186e-05\n",
      "Epoch 157/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9387\n",
      "Epoch 157: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 157: ReduceLROnPlateau reducing learning rate to 2.7625155871646713e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9370 - val_loss: 1.7645 - lr: 2.7904e-05\n",
      "Epoch 158/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9359\n",
      "Epoch 158: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 158: ReduceLROnPlateau reducing learning rate to 2.734890422289027e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9359 - val_loss: 1.7221 - lr: 2.7625e-05\n",
      "Epoch 159/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9350\n",
      "Epoch 159: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 159: ReduceLROnPlateau reducing learning rate to 2.7075415000581416e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9350 - val_loss: 1.7408 - lr: 2.7349e-05\n",
      "Epoch 160/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9365\n",
      "Epoch 160: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 160: ReduceLROnPlateau reducing learning rate to 2.6804661192727508e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9365 - val_loss: 1.7417 - lr: 2.7075e-05\n",
      "Epoch 161/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9330\n",
      "Epoch 161: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 161: ReduceLROnPlateau reducing learning rate to 2.6536613986536395e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9330 - val_loss: 1.7380 - lr: 2.6805e-05\n",
      "Epoch 162/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9347\n",
      "Epoch 162: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 162: ReduceLROnPlateau reducing learning rate to 2.6271248170814942e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9347 - val_loss: 1.7492 - lr: 2.6537e-05\n",
      "Epoch 163/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9286\n",
      "Epoch 163: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 163: ReduceLROnPlateau reducing learning rate to 2.6008534932770998e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9286 - val_loss: 1.7385 - lr: 2.6271e-05\n",
      "Epoch 164/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9331\n",
      "Epoch 164: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 164: ReduceLROnPlateau reducing learning rate to 2.5748449061211432e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9331 - val_loss: 1.7732 - lr: 2.6009e-05\n",
      "Epoch 165/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9266\n",
      "Epoch 165: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 165: ReduceLROnPlateau reducing learning rate to 2.5490965344943107e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9266 - val_loss: 1.7329 - lr: 2.5748e-05\n",
      "Epoch 166/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9289\n",
      "Epoch 166: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 166: ReduceLROnPlateau reducing learning rate to 2.523605497117387e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9289 - val_loss: 1.7767 - lr: 2.5491e-05\n",
      "Epoch 167/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9327\n",
      "Epoch 167: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 167: ReduceLROnPlateau reducing learning rate to 2.4983694529510104e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9327 - val_loss: 1.7314 - lr: 2.5236e-05\n",
      "Epoch 168/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9262\n",
      "Epoch 168: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 168: ReduceLROnPlateau reducing learning rate to 2.4733857007959158e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9262 - val_loss: 1.7553 - lr: 2.4984e-05\n",
      "Epoch 169/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9377\n",
      "Epoch 169: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 169: ReduceLROnPlateau reducing learning rate to 2.4486518996127415e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9396 - val_loss: 1.7258 - lr: 2.4734e-05\n",
      "Epoch 170/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9279\n",
      "Epoch 170: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 170: ReduceLROnPlateau reducing learning rate to 2.424165348202223e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9278 - val_loss: 1.7374 - lr: 2.4487e-05\n",
      "Epoch 171/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9175\n",
      "Epoch 171: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 171: ReduceLROnPlateau reducing learning rate to 2.3999237055249977e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9175 - val_loss: 1.7777 - lr: 2.4242e-05\n",
      "Epoch 172/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9305\n",
      "Epoch 172: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 172: ReduceLROnPlateau reducing learning rate to 2.375924450461753e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9305 - val_loss: 1.7318 - lr: 2.3999e-05\n",
      "Epoch 173/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9297\n",
      "Epoch 173: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 173: ReduceLROnPlateau reducing learning rate to 2.3521652419731254e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9297 - val_loss: 1.7529 - lr: 2.3759e-05\n",
      "Epoch 174/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9186\n",
      "Epoch 174: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 174: ReduceLROnPlateau reducing learning rate to 2.3286435589398024e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9186 - val_loss: 1.7410 - lr: 2.3522e-05\n",
      "Epoch 175/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9215\n",
      "Epoch 175: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 175: ReduceLROnPlateau reducing learning rate to 2.3053570603224217e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.9215 - val_loss: 1.7357 - lr: 2.3286e-05\n",
      "Epoch 176/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9157\n",
      "Epoch 176: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 176: ReduceLROnPlateau reducing learning rate to 2.2823034050816205e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9159 - val_loss: 1.7554 - lr: 2.3054e-05\n",
      "Epoch 177/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9326\n",
      "Epoch 177: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 177: ReduceLROnPlateau reducing learning rate to 2.2594804322579874e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9300 - val_loss: 1.7035 - lr: 2.2823e-05\n",
      "Epoch 178/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9386\n",
      "Epoch 178: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 178: ReduceLROnPlateau reducing learning rate to 2.2368856207322096e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9386 - val_loss: 1.7149 - lr: 2.2595e-05\n",
      "Epoch 179/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9331\n",
      "Epoch 179: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 179: ReduceLROnPlateau reducing learning rate to 2.2145168095448753e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9331 - val_loss: 1.7264 - lr: 2.2369e-05\n",
      "Epoch 180/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9302\n",
      "Epoch 180: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 180: ReduceLROnPlateau reducing learning rate to 2.1923716576566222e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9302 - val_loss: 1.7205 - lr: 2.2145e-05\n",
      "Epoch 181/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9203\n",
      "Epoch 181: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 181: ReduceLROnPlateau reducing learning rate to 2.1704480041080387e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9203 - val_loss: 1.7796 - lr: 2.1924e-05\n",
      "Epoch 182/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9223\n",
      "Epoch 182: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 182: ReduceLROnPlateau reducing learning rate to 2.1487435078597626e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9223 - val_loss: 1.7561 - lr: 2.1704e-05\n",
      "Epoch 183/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9375\n",
      "Epoch 183: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 183: ReduceLROnPlateau reducing learning rate to 2.1272560079523828e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9385 - val_loss: 1.7250 - lr: 2.1487e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9212\n",
      "Epoch 184: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 184: ReduceLROnPlateau reducing learning rate to 2.1059835235064383e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9212 - val_loss: 1.7245 - lr: 2.1273e-05\n",
      "Epoch 185/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9297\n",
      "Epoch 185: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 185: ReduceLROnPlateau reducing learning rate to 2.084923713482567e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.9297 - val_loss: 1.7674 - lr: 2.1060e-05\n",
      "Epoch 186/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9346\n",
      "Epoch 186: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 186: ReduceLROnPlateau reducing learning rate to 2.0640744169213575e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.9346 - val_loss: 1.7315 - lr: 2.0849e-05\n",
      "Epoch 187/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9328\n",
      "Epoch 187: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 187: ReduceLROnPlateau reducing learning rate to 2.0434336529433495e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9328 - val_loss: 1.7402 - lr: 2.0641e-05\n",
      "Epoch 188/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9303\n",
      "Epoch 188: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 188: ReduceLROnPlateau reducing learning rate to 2.0229992605891313e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9303 - val_loss: 1.7450 - lr: 2.0434e-05\n",
      "Epoch 189/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9268\n",
      "Epoch 189: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 189: ReduceLROnPlateau reducing learning rate to 2.0027692589792422e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9266 - val_loss: 1.7366 - lr: 2.0230e-05\n",
      "Epoch 190/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9299\n",
      "Epoch 190: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 190: ReduceLROnPlateau reducing learning rate to 1.9827414871542714e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9299 - val_loss: 1.7460 - lr: 2.0028e-05\n",
      "Epoch 191/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9209\n",
      "Epoch 191: val_loss did not improve from 1.66622\n",
      "\n",
      "Epoch 191: ReduceLROnPlateau reducing learning rate to 1.9629141443147093e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.9209 - val_loss: 1.7295 - lr: 1.9827e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_29\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_30 (InputLayer)          [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_87 (LSTM)                 (None, 45, 24)       3744        ['input_30[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_58 (Dropout)           (None, 45, 24)       0           ['lstm_87[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_88 (LSTM)                 (None, 45, 16)       2624        ['dropout_58[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_59 (Dropout)           (None, 45, 16)       0           ['lstm_88[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_89 (LSTM)                 (None, 32)           6272        ['dropout_59[0][0]']             \n",
      "                                                                                                  \n",
      " dense_58 (Dense)               (None, 40)           1320        ['lstm_89[0][0]']                \n",
      "                                                                                                  \n",
      " dense_59 (Dense)               (None, 5)            205         ['dense_58[0][0]']               \n",
      "                                                                                                  \n",
      " tf.unstack_29 (TFOpLambda)     [(None,),            0           ['dense_59[0][0]']               \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_145 (TFOpLambda  (None, 1)           0           ['tf.unstack_29[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_58 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_145[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_149 (TFOpLambda  (None, 1)           0           ['tf.unstack_29[0][4]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_87 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_58[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_59 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_149[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_88 (TFOpLambd  (None, 1)           0           ['tf.math.multiply_87[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_146 (TFOpLambda  (None, 1)           0           ['tf.unstack_29[0][1]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_148 (TFOpLambda  (None, 1)           0           ['tf.unstack_29[0][3]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_89 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_59[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_58 (TFOpL  (None, 1)           0           ['tf.math.multiply_88[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.softplus_58 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_146[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_147 (TFOpLambda  (None, 1)           0           ['tf.unstack_29[0][2]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.softplus_59 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_148[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_59 (TFOpL  (None, 1)           0           ['tf.math.multiply_89[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack_29 (TFOpLambda)       (None, 5, 1)         0           ['tf.__operators__.add_58[0][0]',\n",
      "                                                                  'tf.math.softplus_58[0][0]',    \n",
      "                                                                  'tf.expand_dims_147[0][0]',     \n",
      "                                                                  'tf.math.softplus_59[0][0]',    \n",
      "                                                                  'tf.__operators__.add_59[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _2_CCMP_t+1_0.19\n",
      "Epoch 1/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 3.4626\n",
      "Epoch 1: val_loss improved from inf to 3.95743, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 11s 99ms/step - loss: 3.4617 - val_loss: 3.9574 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 2.9119\n",
      "Epoch 2: val_loss did not improve from 3.95743\n",
      "\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 2.9098 - val_loss: 4.2174 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.9526\n",
      "Epoch 3: val_loss did not improve from 3.95743\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 1.9487 - val_loss: 4.1803 - lr: 9.9000e-05\n",
      "Epoch 4/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.6182\n",
      "Epoch 4: val_loss improved from 3.95743 to 3.63808, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 1.6182 - val_loss: 3.6381 - lr: 9.8010e-05\n",
      "Epoch 5/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.4995\n",
      "Epoch 5: val_loss did not improve from 3.63808\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 1.4995 - val_loss: 3.8451 - lr: 9.8010e-05\n",
      "Epoch 6/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.4287\n",
      "Epoch 6: val_loss did not improve from 3.63808\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 1.4287 - val_loss: 3.7813 - lr: 9.7030e-05\n",
      "Epoch 7/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.3522\n",
      "Epoch 7: val_loss did not improve from 3.63808\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 1.3522 - val_loss: 3.8274 - lr: 9.6060e-05\n",
      "Epoch 8/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.2926\n",
      "Epoch 8: val_loss did not improve from 3.63808\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 1.2923 - val_loss: 3.6955 - lr: 9.5099e-05\n",
      "Epoch 9/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.2614\n",
      "Epoch 9: val_loss did not improve from 3.63808\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.2614 - val_loss: 3.8429 - lr: 9.4148e-05\n",
      "Epoch 10/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.2444\n",
      "Epoch 10: val_loss improved from 3.63808 to 3.48237, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 1.2444 - val_loss: 3.4824 - lr: 9.3207e-05\n",
      "Epoch 11/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.2255\n",
      "Epoch 11: val_loss improved from 3.48237 to 3.39189, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 1.2255 - val_loss: 3.3919 - lr: 9.3207e-05\n",
      "Epoch 12/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.2021\n",
      "Epoch 12: val_loss improved from 3.39189 to 3.29309, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 1.2012 - val_loss: 3.2931 - lr: 9.3207e-05\n",
      "Epoch 13/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1939\n",
      "Epoch 13: val_loss did not improve from 3.29309\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.1939 - val_loss: 3.2984 - lr: 9.3207e-05\n",
      "Epoch 14/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1946\n",
      "Epoch 14: val_loss did not improve from 3.29309\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 1.1946 - val_loss: 3.4846 - lr: 9.2274e-05\n",
      "Epoch 15/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1815\n",
      "Epoch 15: val_loss did not improve from 3.29309\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.1815 - val_loss: 3.4398 - lr: 9.1352e-05\n",
      "Epoch 16/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1847\n",
      "Epoch 16: val_loss did not improve from 3.29309\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 1.1847 - val_loss: 3.6785 - lr: 9.0438e-05\n",
      "Epoch 17/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1720\n",
      "Epoch 17: val_loss improved from 3.29309 to 3.14436, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 1.1720 - val_loss: 3.1444 - lr: 8.9534e-05\n",
      "Epoch 18/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1641\n",
      "Epoch 18: val_loss did not improve from 3.14436\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.1641 - val_loss: 3.2187 - lr: 8.9534e-05\n",
      "Epoch 19/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1613\n",
      "Epoch 19: val_loss did not improve from 3.14436\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 1.1613 - val_loss: 3.1588 - lr: 8.8638e-05\n",
      "Epoch 20/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1529\n",
      "Epoch 20: val_loss improved from 3.14436 to 3.02787, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 1.1529 - val_loss: 3.0279 - lr: 8.7752e-05\n",
      "Epoch 21/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1508\n",
      "Epoch 21: val_loss did not improve from 3.02787\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 1.1505 - val_loss: 3.0471 - lr: 8.7752e-05\n",
      "Epoch 22/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1392\n",
      "Epoch 22: val_loss did not improve from 3.02787\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 1.1392 - val_loss: 3.0649 - lr: 8.6875e-05\n",
      "Epoch 23/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1330\n",
      "Epoch 23: val_loss improved from 3.02787 to 2.96178, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 1.1330 - val_loss: 2.9618 - lr: 8.6006e-05\n",
      "Epoch 24/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1276\n",
      "Epoch 24: val_loss did not improve from 2.96178\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 1.1276 - val_loss: 3.0736 - lr: 8.6006e-05\n",
      "Epoch 25/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1323\n",
      "Epoch 25: val_loss did not improve from 2.96178\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 1.1307 - val_loss: 3.1720 - lr: 8.5146e-05\n",
      "Epoch 26/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1282\n",
      "Epoch 26: val_loss did not improve from 2.96178\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 1.1282 - val_loss: 3.1231 - lr: 8.4294e-05\n",
      "Epoch 27/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1166\n",
      "Epoch 27: val_loss improved from 2.96178 to 2.85490, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 1.1166 - val_loss: 2.8549 - lr: 8.3451e-05\n",
      "Epoch 28/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1204\n",
      "Epoch 28: val_loss did not improve from 2.85490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.1204 - val_loss: 2.9707 - lr: 8.3451e-05\n",
      "Epoch 29/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.1152\n",
      "Epoch 29: val_loss did not improve from 2.85490\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 1.1126 - val_loss: 3.0415 - lr: 8.2617e-05\n",
      "Epoch 30/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1146\n",
      "Epoch 30: val_loss improved from 2.85490 to 2.73360, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.1146 - val_loss: 2.7336 - lr: 8.1791e-05\n",
      "Epoch 31/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1119\n",
      "Epoch 31: val_loss improved from 2.73360 to 2.70003, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 1.1119 - val_loss: 2.7000 - lr: 8.1791e-05\n",
      "Epoch 32/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1117\n",
      "Epoch 32: val_loss improved from 2.70003 to 2.69760, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 1.1117 - val_loss: 2.6976 - lr: 8.1791e-05\n",
      "Epoch 33/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0917\n",
      "Epoch 33: val_loss improved from 2.69760 to 2.61047, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 1.0917 - val_loss: 2.6105 - lr: 8.1791e-05\n",
      "Epoch 34/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1058\n",
      "Epoch 34: val_loss did not improve from 2.61047\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 1.1058 - val_loss: 2.6666 - lr: 8.1791e-05\n",
      "Epoch 35/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0975\n",
      "Epoch 35: val_loss did not improve from 2.61047\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.0975 - val_loss: 2.9563 - lr: 8.0973e-05\n",
      "Epoch 36/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1041\n",
      "Epoch 36: val_loss improved from 2.61047 to 2.60965, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.1041 - val_loss: 2.6096 - lr: 8.0163e-05\n",
      "Epoch 37/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0905\n",
      "Epoch 37: val_loss did not improve from 2.60965\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 1.0889 - val_loss: 2.6389 - lr: 8.0163e-05\n",
      "Epoch 38/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0810\n",
      "Epoch 38: val_loss improved from 2.60965 to 2.56455, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.0816 - val_loss: 2.5645 - lr: 7.9361e-05\n",
      "Epoch 39/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0826\n",
      "Epoch 39: val_loss did not improve from 2.56455\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 1.0826 - val_loss: 2.7629 - lr: 7.9361e-05\n",
      "Epoch 40/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0897\n",
      "Epoch 40: val_loss did not improve from 2.56455\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 1.0897 - val_loss: 2.7409 - lr: 7.8568e-05\n",
      "Epoch 41/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0769\n",
      "Epoch 41: val_loss improved from 2.56455 to 2.52940, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 1.0769 - val_loss: 2.5294 - lr: 7.7782e-05\n",
      "Epoch 42/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0938\n",
      "Epoch 42: val_loss improved from 2.52940 to 2.50340, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.0929 - val_loss: 2.5034 - lr: 7.7782e-05\n",
      "Epoch 43/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0763\n",
      "Epoch 43: val_loss did not improve from 2.50340\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 1.0763 - val_loss: 2.5454 - lr: 7.7782e-05\n",
      "Epoch 44/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0838\n",
      "Epoch 44: val_loss did not improve from 2.50340\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.0838 - val_loss: 2.5265 - lr: 7.7004e-05\n",
      "Epoch 45/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0740\n",
      "Epoch 45: val_loss did not improve from 2.50340\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 1.0740 - val_loss: 2.5071 - lr: 7.6234e-05\n",
      "Epoch 46/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0791\n",
      "Epoch 46: val_loss did not improve from 2.50340\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 1.0791 - val_loss: 2.5356 - lr: 7.5472e-05\n",
      "Epoch 47/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0726\n",
      "Epoch 47: val_loss improved from 2.50340 to 2.40714, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 1.0738 - val_loss: 2.4071 - lr: 7.4717e-05\n",
      "Epoch 48/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0809\n",
      "Epoch 48: val_loss improved from 2.40714 to 2.38700, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 1.0809 - val_loss: 2.3870 - lr: 7.4717e-05\n",
      "Epoch 49/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0770\n",
      "Epoch 49: val_loss did not improve from 2.38700\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.0770 - val_loss: 2.3961 - lr: 7.4717e-05\n",
      "Epoch 50/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0611\n",
      "Epoch 50: val_loss did not improve from 2.38700\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.0611 - val_loss: 2.4951 - lr: 7.3970e-05\n",
      "Epoch 51/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0732\n",
      "Epoch 51: val_loss did not improve from 2.38700\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.0723 - val_loss: 2.5033 - lr: 7.3230e-05\n",
      "Epoch 52/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0647\n",
      "Epoch 52: val_loss did not improve from 2.38700\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 1.0647 - val_loss: 2.5086 - lr: 7.2498e-05\n",
      "Epoch 53/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0626\n",
      "Epoch 53: val_loss improved from 2.38700 to 2.33579, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.19.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 6s 95ms/step - loss: 1.0626 - val_loss: 2.3358 - lr: 7.1773e-05\n",
      "Epoch 54/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0616\n",
      "Epoch 54: val_loss did not improve from 2.33579\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 1.0616 - val_loss: 2.3984 - lr: 7.1773e-05\n",
      "Epoch 55/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0591\n",
      "Epoch 55: val_loss did not improve from 2.33579\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.0591 - val_loss: 2.3774 - lr: 7.1055e-05\n",
      "Epoch 56/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0576\n",
      "Epoch 56: val_loss did not improve from 2.33579\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.0576 - val_loss: 2.4749 - lr: 7.0345e-05\n",
      "Epoch 57/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0521\n",
      "Epoch 57: val_loss did not improve from 2.33579\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 1.0521 - val_loss: 2.3937 - lr: 6.9641e-05\n",
      "Epoch 58/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0595\n",
      "Epoch 58: val_loss improved from 2.33579 to 2.23217, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 1.0595 - val_loss: 2.2322 - lr: 6.8945e-05\n",
      "Epoch 59/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0546\n",
      "Epoch 59: val_loss improved from 2.23217 to 2.17367, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 1.0568 - val_loss: 2.1737 - lr: 6.8945e-05\n",
      "Epoch 60/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0489\n",
      "Epoch 60: val_loss did not improve from 2.17367\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 1.0489 - val_loss: 2.3136 - lr: 6.8945e-05\n",
      "Epoch 61/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0581\n",
      "Epoch 61: val_loss did not improve from 2.17367\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 1.0581 - val_loss: 2.2853 - lr: 6.8255e-05\n",
      "Epoch 62/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0618\n",
      "Epoch 62: val_loss did not improve from 2.17367\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 1.0618 - val_loss: 2.3009 - lr: 6.7573e-05\n",
      "Epoch 63/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0502\n",
      "Epoch 63: val_loss did not improve from 2.17367\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 1.0532 - val_loss: 2.2354 - lr: 6.6897e-05\n",
      "Epoch 64/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0480\n",
      "Epoch 64: val_loss did not improve from 2.17367\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 1.0479 - val_loss: 2.2456 - lr: 6.6228e-05\n",
      "Epoch 65/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0511\n",
      "Epoch 65: val_loss did not improve from 2.17367\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 1.0511 - val_loss: 2.1895 - lr: 6.5566e-05\n",
      "Epoch 66/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0412\n",
      "Epoch 66: val_loss did not improve from 2.17367\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 1.0412 - val_loss: 2.2122 - lr: 6.4910e-05\n",
      "Epoch 67/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0377\n",
      "Epoch 67: val_loss did not improve from 2.17367\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.0377 - val_loss: 2.2897 - lr: 6.4261e-05\n",
      "Epoch 68/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0541\n",
      "Epoch 68: val_loss did not improve from 2.17367\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 1.0531 - val_loss: 2.1965 - lr: 6.3619e-05\n",
      "Epoch 69/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0398\n",
      "Epoch 69: val_loss did not improve from 2.17367\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 1.0398 - val_loss: 2.2863 - lr: 6.2982e-05\n",
      "Epoch 70/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0433\n",
      "Epoch 70: val_loss did not improve from 2.17367\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.0433 - val_loss: 2.1947 - lr: 6.2353e-05\n",
      "Epoch 71/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0430\n",
      "Epoch 71: val_loss did not improve from 2.17367\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 1.0430 - val_loss: 2.2156 - lr: 6.1729e-05\n",
      "Epoch 72/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0451\n",
      "Epoch 72: val_loss did not improve from 2.17367\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.0451 - val_loss: 2.2205 - lr: 6.1112e-05\n",
      "Epoch 73/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0476\n",
      "Epoch 73: val_loss improved from 2.17367 to 2.05258, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.0476 - val_loss: 2.0526 - lr: 6.0501e-05\n",
      "Epoch 74/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0372\n",
      "Epoch 74: val_loss did not improve from 2.05258\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.0372 - val_loss: 2.1206 - lr: 6.0501e-05\n",
      "Epoch 75/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0413\n",
      "Epoch 75: val_loss did not improve from 2.05258\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.0413 - val_loss: 2.2772 - lr: 5.9896e-05\n",
      "Epoch 76/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0335\n",
      "Epoch 76: val_loss did not improve from 2.05258\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 5.8703650938696225e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 1.0335 - val_loss: 2.1535 - lr: 5.9297e-05\n",
      "Epoch 77/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0396\n",
      "Epoch 77: val_loss did not improve from 2.05258\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 5.811661550978897e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 1.0391 - val_loss: 2.1088 - lr: 5.8704e-05\n",
      "Epoch 78/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0424\n",
      "Epoch 78: val_loss did not improve from 2.05258\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 5.753545068728272e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.0424 - val_loss: 2.1923 - lr: 5.8117e-05\n",
      "Epoch 79/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0340\n",
      "Epoch 79: val_loss did not improve from 2.05258\n",
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 5.6960095243994146e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 1.0340 - val_loss: 2.2232 - lr: 5.7535e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0289\n",
      "Epoch 80: val_loss did not improve from 2.05258\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 5.639049515593797e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 1.0289 - val_loss: 2.0759 - lr: 5.6960e-05\n",
      "Epoch 81/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0446\n",
      "Epoch 81: val_loss did not improve from 2.05258\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 5.5826589195930865e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 1.0437 - val_loss: 2.1290 - lr: 5.6390e-05\n",
      "Epoch 82/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0318\n",
      "Epoch 82: val_loss did not improve from 2.05258\n",
      "\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 5.526832333998754e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 1.0318 - val_loss: 2.2433 - lr: 5.5827e-05\n",
      "Epoch 83/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0202\n",
      "Epoch 83: val_loss did not improve from 2.05258\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 5.471563996252371e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.0202 - val_loss: 2.1526 - lr: 5.5268e-05\n",
      "Epoch 84/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0281\n",
      "Epoch 84: val_loss did not improve from 2.05258\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 5.416848503955407e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.0281 - val_loss: 2.1395 - lr: 5.4716e-05\n",
      "Epoch 85/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0282\n",
      "Epoch 85: val_loss did not improve from 2.05258\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 5.362680094549433e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 1.0277 - val_loss: 2.2189 - lr: 5.4168e-05\n",
      "Epoch 86/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0292\n",
      "Epoch 86: val_loss did not improve from 2.05258\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 5.309053365635918e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 1.0292 - val_loss: 2.1822 - lr: 5.3627e-05\n",
      "Epoch 87/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0254\n",
      "Epoch 87: val_loss did not improve from 2.05258\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 5.2559629148163366e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 1.0254 - val_loss: 2.1638 - lr: 5.3091e-05\n",
      "Epoch 88/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0181\n",
      "Epoch 88: val_loss did not improve from 2.05258\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 5.2034033396921584e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 1.0181 - val_loss: 2.1880 - lr: 5.2560e-05\n",
      "Epoch 89/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0249\n",
      "Epoch 89: val_loss did not improve from 2.05258\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 5.1513692378648555e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.0249 - val_loss: 2.1420 - lr: 5.2034e-05\n",
      "Epoch 90/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0204\n",
      "Epoch 90: val_loss improved from 2.05258 to 2.04269, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.0178 - val_loss: 2.0427 - lr: 5.1514e-05\n",
      "Epoch 91/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0257\n",
      "Epoch 91: val_loss did not improve from 2.04269\n",
      "\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 5.099855567095801e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 1.0257 - val_loss: 2.0856 - lr: 5.1514e-05\n",
      "Epoch 92/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0252\n",
      "Epoch 92: val_loss did not improve from 2.04269\n",
      "\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 5.048856924986467e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 1.0252 - val_loss: 2.0979 - lr: 5.0999e-05\n",
      "Epoch 93/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0176\n",
      "Epoch 93: val_loss did not improve from 2.04269\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 4.998368269298226e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 1.0176 - val_loss: 2.1493 - lr: 5.0489e-05\n",
      "Epoch 94/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0212\n",
      "Epoch 94: val_loss did not improve from 2.04269\n",
      "\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 4.9483845577924514e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 1.0212 - val_loss: 2.1082 - lr: 4.9984e-05\n",
      "Epoch 95/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0219\n",
      "Epoch 95: val_loss did not improve from 2.04269\n",
      "\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 4.898900748230517e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 1.0219 - val_loss: 2.0761 - lr: 4.9484e-05\n",
      "Epoch 96/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0120\n",
      "Epoch 96: val_loss did not improve from 2.04269\n",
      "\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 4.849911798373796e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 1.0120 - val_loss: 2.1466 - lr: 4.8989e-05\n",
      "Epoch 97/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0175\n",
      "Epoch 97: val_loss did not improve from 2.04269\n",
      "\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 4.801412665983662e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 1.0175 - val_loss: 2.1237 - lr: 4.8499e-05\n",
      "Epoch 98/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0194\n",
      "Epoch 98: val_loss did not improve from 2.04269\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 4.75339866898139e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.0184 - val_loss: 2.1141 - lr: 4.8014e-05\n",
      "Epoch 99/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0175\n",
      "Epoch 99: val_loss did not improve from 2.04269\n",
      "\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 4.705864765128354e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.0175 - val_loss: 2.1279 - lr: 4.7534e-05\n",
      "Epoch 100/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0183\n",
      "Epoch 100: val_loss did not improve from 2.04269\n",
      "\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 4.658806272345828e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 1.0183 - val_loss: 2.0892 - lr: 4.7059e-05\n",
      "Epoch 101/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0111\n",
      "Epoch 101: val_loss did not improve from 2.04269\n",
      "\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 4.612218148395186e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 1.0111 - val_loss: 2.0506 - lr: 4.6588e-05\n",
      "Epoch 102/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0055\n",
      "Epoch 102: val_loss did not improve from 2.04269\n",
      "\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 4.566096071357606e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.0065 - val_loss: 2.1665 - lr: 4.6122e-05\n",
      "Epoch 103/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0180\n",
      "Epoch 103: val_loss did not improve from 2.04269\n",
      "\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 4.52043499899446e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.0180 - val_loss: 2.0656 - lr: 4.5661e-05\n",
      "Epoch 104/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0091\n",
      "Epoch 104: val_loss did not improve from 2.04269\n",
      "\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 4.475230609386927e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.0091 - val_loss: 2.0875 - lr: 4.5204e-05\n",
      "Epoch 105/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0135\n",
      "Epoch 105: val_loss did not improve from 2.04269\n",
      "\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 4.43047822045628e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.0135 - val_loss: 2.1218 - lr: 4.4752e-05\n",
      "Epoch 106/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0169\n",
      "Epoch 106: val_loss did not improve from 2.04269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 4.386173510283697e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.0169 - val_loss: 2.1212 - lr: 4.4305e-05\n",
      "Epoch 107/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0130\n",
      "Epoch 107: val_loss improved from 2.04269 to 2.01201, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 1.0123 - val_loss: 2.0120 - lr: 4.3862e-05\n",
      "Epoch 108/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0034\n",
      "Epoch 108: val_loss did not improve from 2.01201\n",
      "\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 4.342311796790454e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 1.0034 - val_loss: 2.1277 - lr: 4.3862e-05\n",
      "Epoch 109/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0177\n",
      "Epoch 109: val_loss did not improve from 2.01201\n",
      "\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 4.298888758057728e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 1.0177 - val_loss: 2.0326 - lr: 4.3423e-05\n",
      "Epoch 110/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0159\n",
      "Epoch 110: val_loss improved from 2.01201 to 1.97307, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_2_CCMP_t+1_0.19.hdf5\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 1.0159 - val_loss: 1.9731 - lr: 4.2989e-05\n",
      "Epoch 111/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0120\n",
      "Epoch 111: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 4.2558997120067946e-05.\n",
      "66/66 [==============================] - 7s 108ms/step - loss: 1.0120 - val_loss: 2.0279 - lr: 4.2989e-05\n",
      "Epoch 112/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0151\n",
      "Epoch 112: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 4.2133406968787315e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 1.0167 - val_loss: 2.0296 - lr: 4.2559e-05\n",
      "Epoch 113/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0014\n",
      "Epoch 113: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 4.1712073907547166e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 1.0032 - val_loss: 2.1064 - lr: 4.2133e-05\n",
      "Epoch 114/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0044\n",
      "Epoch 114: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 4.1294954717159274e-05.\n",
      "66/66 [==============================] - 6s 84ms/step - loss: 1.0019 - val_loss: 2.0773 - lr: 4.1712e-05\n",
      "Epoch 115/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0165\n",
      "Epoch 115: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 4.08820061784354e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 1.0170 - val_loss: 2.0604 - lr: 4.1295e-05\n",
      "Epoch 116/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0097\n",
      "Epoch 116: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 4.0473185072187336e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 1.0097 - val_loss: 2.0880 - lr: 4.0882e-05\n",
      "Epoch 117/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0062\n",
      "Epoch 117: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 4.0068451780825854e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 1.0054 - val_loss: 2.1113 - lr: 4.0473e-05\n",
      "Epoch 118/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0096\n",
      "Epoch 118: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 3.966776668676175e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 1.0088 - val_loss: 2.0611 - lr: 4.0068e-05\n",
      "Epoch 119/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0071\n",
      "Epoch 119: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 3.927109017240582e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 1.0079 - val_loss: 2.0412 - lr: 3.9668e-05\n",
      "Epoch 120/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0076\n",
      "Epoch 120: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 3.887837901856983e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 1.0066 - val_loss: 2.0729 - lr: 3.9271e-05\n",
      "Epoch 121/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0082\n",
      "Epoch 121: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 3.848959360766457e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 1.0083 - val_loss: 2.0725 - lr: 3.8878e-05\n",
      "Epoch 122/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0037\n",
      "Epoch 122: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 3.810469792369986e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 1.0081 - val_loss: 2.0393 - lr: 3.8490e-05\n",
      "Epoch 123/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0100\n",
      "Epoch 123: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 123: ReduceLROnPlateau reducing learning rate to 3.772365234908648e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 1.0103 - val_loss: 2.0994 - lr: 3.8105e-05\n",
      "Epoch 124/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0084\n",
      "Epoch 124: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 124: ReduceLROnPlateau reducing learning rate to 3.734641726623522e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 1.0080 - val_loss: 2.0555 - lr: 3.7724e-05\n",
      "Epoch 125/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0022\n",
      "Epoch 125: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 3.6972953057556876e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 1.0038 - val_loss: 2.0351 - lr: 3.7346e-05\n",
      "Epoch 126/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9954\n",
      "Epoch 126: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 3.660322370706126e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 0.9961 - val_loss: 2.0408 - lr: 3.6973e-05\n",
      "Epoch 127/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9992\n",
      "Epoch 127: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 127: ReduceLROnPlateau reducing learning rate to 3.623719319875818e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 0.9985 - val_loss: 2.0594 - lr: 3.6603e-05\n",
      "Epoch 128/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0057\n",
      "Epoch 128: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 128: ReduceLROnPlateau reducing learning rate to 3.5874821915058416e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 1.0054 - val_loss: 2.0405 - lr: 3.6237e-05\n",
      "Epoch 129/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0013\n",
      "Epoch 129: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 129: ReduceLROnPlateau reducing learning rate to 3.551607383997179e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 1.0018 - val_loss: 2.0523 - lr: 3.5875e-05\n",
      "Epoch 130/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0022\n",
      "Epoch 130: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 130: ReduceLROnPlateau reducing learning rate to 3.516091295750812e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 1.0005 - val_loss: 2.0733 - lr: 3.5516e-05\n",
      "Epoch 131/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0014\n",
      "Epoch 131: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 131: ReduceLROnPlateau reducing learning rate to 3.480930325167719e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 1.0018 - val_loss: 2.1002 - lr: 3.5161e-05\n",
      "Epoch 132/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9978\n",
      "Epoch 132: val_loss did not improve from 1.97307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 132: ReduceLROnPlateau reducing learning rate to 3.446120870648883e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9972 - val_loss: 2.0715 - lr: 3.4809e-05\n",
      "Epoch 133/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0019\n",
      "Epoch 133: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 133: ReduceLROnPlateau reducing learning rate to 3.4116596907551866e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 1.0016 - val_loss: 2.0392 - lr: 3.4461e-05\n",
      "Epoch 134/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0025\n",
      "Epoch 134: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 134: ReduceLROnPlateau reducing learning rate to 3.37754318388761e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 1.0003 - val_loss: 2.0719 - lr: 3.4117e-05\n",
      "Epoch 135/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9896\n",
      "Epoch 135: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 135: ReduceLROnPlateau reducing learning rate to 3.3437677484471354e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9904 - val_loss: 2.0843 - lr: 3.3775e-05\n",
      "Epoch 136/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9957\n",
      "Epoch 136: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 3.310330142994644e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 0.9954 - val_loss: 2.0958 - lr: 3.3438e-05\n",
      "Epoch 137/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0035\n",
      "Epoch 137: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 137: ReduceLROnPlateau reducing learning rate to 3.277226765931118e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 1.0014 - val_loss: 2.0585 - lr: 3.3103e-05\n",
      "Epoch 138/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0054\n",
      "Epoch 138: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 138: ReduceLROnPlateau reducing learning rate to 3.24445437581744e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 1.0049 - val_loss: 2.0390 - lr: 3.2772e-05\n",
      "Epoch 139/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0012\n",
      "Epoch 139: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 139: ReduceLROnPlateau reducing learning rate to 3.212009731214493e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 1.0014 - val_loss: 2.0669 - lr: 3.2445e-05\n",
      "Epoch 140/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9927\n",
      "Epoch 140: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 140: ReduceLROnPlateau reducing learning rate to 3.17988959068316e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 0.9927 - val_loss: 2.0595 - lr: 3.2120e-05\n",
      "Epoch 141/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9945\n",
      "Epoch 141: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 141: ReduceLROnPlateau reducing learning rate to 3.1480907127843236e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 0.9938 - val_loss: 2.0454 - lr: 3.1799e-05\n",
      "Epoch 142/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0008\n",
      "Epoch 142: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 142: ReduceLROnPlateau reducing learning rate to 3.116609856078867e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 1.0015 - val_loss: 2.0455 - lr: 3.1481e-05\n",
      "Epoch 143/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0041\n",
      "Epoch 143: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 143: ReduceLROnPlateau reducing learning rate to 3.085443779127672e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 1.0027 - val_loss: 2.0728 - lr: 3.1166e-05\n",
      "Epoch 144/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9983\n",
      "Epoch 144: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 144: ReduceLROnPlateau reducing learning rate to 3.054589240491623e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 0.9951 - val_loss: 2.0680 - lr: 3.0854e-05\n",
      "Epoch 145/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9938\n",
      "Epoch 145: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 145: ReduceLROnPlateau reducing learning rate to 3.024043358891504e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 0.9925 - val_loss: 2.0485 - lr: 3.0546e-05\n",
      "Epoch 146/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9979\n",
      "Epoch 146: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 146: ReduceLROnPlateau reducing learning rate to 2.9938028928881975e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 0.9974 - val_loss: 2.0620 - lr: 3.0240e-05\n",
      "Epoch 147/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9966\n",
      "Epoch 147: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 147: ReduceLROnPlateau reducing learning rate to 2.963864781122538e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 0.9960 - val_loss: 2.0107 - lr: 2.9938e-05\n",
      "Epoch 148/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9961\n",
      "Epoch 148: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 148: ReduceLROnPlateau reducing learning rate to 2.9342261423153105e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9956 - val_loss: 2.0048 - lr: 2.9639e-05\n",
      "Epoch 149/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9882\n",
      "Epoch 149: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 149: ReduceLROnPlateau reducing learning rate to 2.9048839151073478e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 0.9884 - val_loss: 2.0182 - lr: 2.9342e-05\n",
      "Epoch 150/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9861\n",
      "Epoch 150: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 150: ReduceLROnPlateau reducing learning rate to 2.875835038139485e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 0.9864 - val_loss: 2.0162 - lr: 2.9049e-05\n",
      "Epoch 151/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0000\n",
      "Epoch 151: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 151: ReduceLROnPlateau reducing learning rate to 2.8470766301325058e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 1.0018 - val_loss: 1.9981 - lr: 2.8758e-05\n",
      "Epoch 152/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9908\n",
      "Epoch 152: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 152: ReduceLROnPlateau reducing learning rate to 2.8186058098071954e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.9925 - val_loss: 2.0552 - lr: 2.8471e-05\n",
      "Epoch 153/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9978\n",
      "Epoch 153: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 153: ReduceLROnPlateau reducing learning rate to 2.7904196958843385e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9980 - val_loss: 2.0386 - lr: 2.8186e-05\n",
      "Epoch 154/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9993\n",
      "Epoch 154: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 154: ReduceLROnPlateau reducing learning rate to 2.7625155871646713e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9994 - val_loss: 2.0556 - lr: 2.7904e-05\n",
      "Epoch 155/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9973\n",
      "Epoch 155: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 155: ReduceLROnPlateau reducing learning rate to 2.734890422289027e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9978 - val_loss: 2.0089 - lr: 2.7625e-05\n",
      "Epoch 156/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9808\n",
      "Epoch 156: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 156: ReduceLROnPlateau reducing learning rate to 2.7075415000581416e-05.\n",
      "66/66 [==============================] - 6s 85ms/step - loss: 0.9808 - val_loss: 2.0624 - lr: 2.7349e-05\n",
      "Epoch 157/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.9981\n",
      "Epoch 157: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 157: ReduceLROnPlateau reducing learning rate to 2.6804661192727508e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9988 - val_loss: 2.0401 - lr: 2.7075e-05\n",
      "Epoch 158/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9881\n",
      "Epoch 158: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 158: ReduceLROnPlateau reducing learning rate to 2.6536613986536395e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 7s 101ms/step - loss: 0.9881 - val_loss: 2.0708 - lr: 2.6805e-05\n",
      "Epoch 159/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9963\n",
      "Epoch 159: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 159: ReduceLROnPlateau reducing learning rate to 2.6271248170814942e-05.\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 0.9963 - val_loss: 2.0722 - lr: 2.6537e-05\n",
      "Epoch 160/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9827\n",
      "Epoch 160: val_loss did not improve from 1.97307\n",
      "\n",
      "Epoch 160: ReduceLROnPlateau reducing learning rate to 2.6008534932770998e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 0.9827 - val_loss: 2.0742 - lr: 2.6271e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_30\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_31 (InputLayer)          [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_90 (LSTM)                 (None, 45, 24)       3744        ['input_31[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_60 (Dropout)           (None, 45, 24)       0           ['lstm_90[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_91 (LSTM)                 (None, 45, 16)       2624        ['dropout_60[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_61 (Dropout)           (None, 45, 16)       0           ['lstm_91[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_92 (LSTM)                 (None, 32)           6272        ['dropout_61[0][0]']             \n",
      "                                                                                                  \n",
      " dense_60 (Dense)               (None, 40)           1320        ['lstm_92[0][0]']                \n",
      "                                                                                                  \n",
      " dense_61 (Dense)               (None, 5)            205         ['dense_60[0][0]']               \n",
      "                                                                                                  \n",
      " tf.unstack_30 (TFOpLambda)     [(None,),            0           ['dense_61[0][0]']               \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_150 (TFOpLambda  (None, 1)           0           ['tf.unstack_30[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_60 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_150[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_154 (TFOpLambda  (None, 1)           0           ['tf.unstack_30[0][4]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_90 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_60[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_61 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_154[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_91 (TFOpLambd  (None, 1)           0           ['tf.math.multiply_90[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_151 (TFOpLambda  (None, 1)           0           ['tf.unstack_30[0][1]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_153 (TFOpLambda  (None, 1)           0           ['tf.unstack_30[0][3]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_92 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_61[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_60 (TFOpL  (None, 1)           0           ['tf.math.multiply_91[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.softplus_60 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_151[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_152 (TFOpLambda  (None, 1)           0           ['tf.unstack_30[0][2]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.softplus_61 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_153[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_61 (TFOpL  (None, 1)           0           ['tf.math.multiply_92[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack_30 (TFOpLambda)       (None, 5, 1)         0           ['tf.__operators__.add_60[0][0]',\n",
      "                                                                  'tf.math.softplus_60[0][0]',    \n",
      "                                                                  'tf.expand_dims_152[0][0]',     \n",
      "                                                                  'tf.math.softplus_61[0][0]',    \n",
      "                                                                  'tf.__operators__.add_61[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _3_CCMP_t+1_0.05\n",
      "Epoch 1/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 3.3800\n",
      "Epoch 1: val_loss improved from inf to 3.92722, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.05.hdf5\n",
      "66/66 [==============================] - 11s 104ms/step - loss: 3.3800 - val_loss: 3.9272 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 2.9033\n",
      "Epoch 2: val_loss improved from 3.92722 to 2.86856, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.05.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 6s 96ms/step - loss: 2.9033 - val_loss: 2.8686 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.8050\n",
      "Epoch 3: val_loss improved from 2.86856 to 2.72369, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.05.hdf5\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 1.8020 - val_loss: 2.7237 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1411\n",
      "Epoch 4: val_loss did not improve from 2.72369\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 1.1411 - val_loss: 3.3256 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8822\n",
      "Epoch 5: val_loss did not improve from 2.72369\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.8822 - val_loss: 3.4856 - lr: 9.9000e-05\n",
      "Epoch 6/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7368\n",
      "Epoch 6: val_loss did not improve from 2.72369\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.7368 - val_loss: 3.5378 - lr: 9.8010e-05\n",
      "Epoch 7/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6475\n",
      "Epoch 7: val_loss did not improve from 2.72369\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.6475 - val_loss: 3.4019 - lr: 9.7030e-05\n",
      "Epoch 8/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6002\n",
      "Epoch 8: val_loss did not improve from 2.72369\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.6002 - val_loss: 3.4453 - lr: 9.6060e-05\n",
      "Epoch 9/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5725\n",
      "Epoch 9: val_loss did not improve from 2.72369\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.5725 - val_loss: 3.4050 - lr: 9.5099e-05\n",
      "Epoch 10/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5376\n",
      "Epoch 10: val_loss did not improve from 2.72369\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.5376 - val_loss: 3.5108 - lr: 9.4148e-05\n",
      "Epoch 11/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5158\n",
      "Epoch 11: val_loss did not improve from 2.72369\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.5158 - val_loss: 3.3290 - lr: 9.3207e-05\n",
      "Epoch 12/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5061\n",
      "Epoch 12: val_loss did not improve from 2.72369\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.5049 - val_loss: 3.2951 - lr: 9.2274e-05\n",
      "Epoch 13/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4845\n",
      "Epoch 13: val_loss did not improve from 2.72369\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.4845 - val_loss: 3.4218 - lr: 9.1352e-05\n",
      "Epoch 14/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4743\n",
      "Epoch 14: val_loss improved from 2.72369 to 2.61851, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.05.hdf5\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.4743 - val_loss: 2.6185 - lr: 9.0438e-05\n",
      "Epoch 15/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4526\n",
      "Epoch 15: val_loss did not improve from 2.61851\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.4526 - val_loss: 3.0265 - lr: 9.0438e-05\n",
      "Epoch 16/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4459\n",
      "Epoch 16: val_loss did not improve from 2.61851\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.4459 - val_loss: 3.5721 - lr: 8.9534e-05\n",
      "Epoch 17/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4336\n",
      "Epoch 17: val_loss did not improve from 2.61851\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.4336 - val_loss: 2.9515 - lr: 8.8638e-05\n",
      "Epoch 18/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4294\n",
      "Epoch 18: val_loss did not improve from 2.61851\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.4294 - val_loss: 2.9376 - lr: 8.7752e-05\n",
      "Epoch 19/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4007\n",
      "Epoch 19: val_loss did not improve from 2.61851\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.4007 - val_loss: 2.9763 - lr: 8.6875e-05\n",
      "Epoch 20/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4076\n",
      "Epoch 20: val_loss did not improve from 2.61851\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.4076 - val_loss: 2.7231 - lr: 8.6006e-05\n",
      "Epoch 21/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3950\n",
      "Epoch 21: val_loss did not improve from 2.61851\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.3951 - val_loss: 3.0100 - lr: 8.5146e-05\n",
      "Epoch 22/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3929\n",
      "Epoch 22: val_loss improved from 2.61851 to 2.35350, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.05.hdf5\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.3929 - val_loss: 2.3535 - lr: 8.4294e-05\n",
      "Epoch 23/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3772\n",
      "Epoch 23: val_loss did not improve from 2.35350\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.3852 - val_loss: 2.7965 - lr: 8.4294e-05\n",
      "Epoch 24/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3885\n",
      "Epoch 24: val_loss did not improve from 2.35350\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.3885 - val_loss: 2.8898 - lr: 8.3451e-05\n",
      "Epoch 25/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3616\n",
      "Epoch 25: val_loss did not improve from 2.35350\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.3616 - val_loss: 2.6851 - lr: 8.2617e-05\n",
      "Epoch 26/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3719\n",
      "Epoch 26: val_loss did not improve from 2.35350\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.3719 - val_loss: 2.5439 - lr: 8.1791e-05\n",
      "Epoch 27/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3779\n",
      "Epoch 27: val_loss did not improve from 2.35350\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.3779 - val_loss: 2.7226 - lr: 8.0973e-05\n",
      "Epoch 28/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3702\n",
      "Epoch 28: val_loss did not improve from 2.35350\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.3702 - val_loss: 2.6675 - lr: 8.0163e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3523\n",
      "Epoch 29: val_loss did not improve from 2.35350\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.3524 - val_loss: 2.7117 - lr: 7.9361e-05\n",
      "Epoch 30/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3534\n",
      "Epoch 30: val_loss did not improve from 2.35350\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.3534 - val_loss: 2.5364 - lr: 7.8568e-05\n",
      "Epoch 31/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3408\n",
      "Epoch 31: val_loss did not improve from 2.35350\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.3408 - val_loss: 2.6113 - lr: 7.7782e-05\n",
      "Epoch 32/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3527\n",
      "Epoch 32: val_loss did not improve from 2.35350\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.3527 - val_loss: 2.4722 - lr: 7.7004e-05\n",
      "Epoch 33/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3348\n",
      "Epoch 33: val_loss did not improve from 2.35350\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.3348 - val_loss: 2.8276 - lr: 7.6234e-05\n",
      "Epoch 34/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3376\n",
      "Epoch 34: val_loss did not improve from 2.35350\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.3376 - val_loss: 2.4529 - lr: 7.5472e-05\n",
      "Epoch 35/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3356\n",
      "Epoch 35: val_loss improved from 2.35350 to 2.32419, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.05.hdf5\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.3351 - val_loss: 2.3242 - lr: 7.4717e-05\n",
      "Epoch 36/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3353\n",
      "Epoch 36: val_loss did not improve from 2.32419\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.3353 - val_loss: 2.9740 - lr: 7.4717e-05\n",
      "Epoch 37/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3332\n",
      "Epoch 37: val_loss did not improve from 2.32419\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.3332 - val_loss: 2.4098 - lr: 7.3970e-05\n",
      "Epoch 38/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3295\n",
      "Epoch 38: val_loss did not improve from 2.32419\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.3295 - val_loss: 2.7048 - lr: 7.3230e-05\n",
      "Epoch 39/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3169\n",
      "Epoch 39: val_loss did not improve from 2.32419\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.3169 - val_loss: 2.4868 - lr: 7.2498e-05\n",
      "Epoch 40/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3131\n",
      "Epoch 40: val_loss did not improve from 2.32419\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.3131 - val_loss: 2.6136 - lr: 7.1773e-05\n",
      "Epoch 41/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3162\n",
      "Epoch 41: val_loss did not improve from 2.32419\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.3141 - val_loss: 2.5760 - lr: 7.1055e-05\n",
      "Epoch 42/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3238\n",
      "Epoch 42: val_loss did not improve from 2.32419\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.3238 - val_loss: 2.3649 - lr: 7.0345e-05\n",
      "Epoch 43/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3129\n",
      "Epoch 43: val_loss did not improve from 2.32419\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.3129 - val_loss: 2.5679 - lr: 6.9641e-05\n",
      "Epoch 44/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3099\n",
      "Epoch 44: val_loss did not improve from 2.32419\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.3099 - val_loss: 2.4010 - lr: 6.8945e-05\n",
      "Epoch 45/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3108\n",
      "Epoch 45: val_loss did not improve from 2.32419\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.3108 - val_loss: 2.9421 - lr: 6.8255e-05\n",
      "Epoch 46/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3153\n",
      "Epoch 46: val_loss did not improve from 2.32419\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.3153 - val_loss: 2.8241 - lr: 6.7573e-05\n",
      "Epoch 47/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3104\n",
      "Epoch 47: val_loss did not improve from 2.32419\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.3129 - val_loss: 2.5671 - lr: 6.6897e-05\n",
      "Epoch 48/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3123\n",
      "Epoch 48: val_loss improved from 2.32419 to 2.30203, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.05.hdf5\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.3123 - val_loss: 2.3020 - lr: 6.6228e-05\n",
      "Epoch 49/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3002\n",
      "Epoch 49: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.3002 - val_loss: 2.3118 - lr: 6.6228e-05\n",
      "Epoch 50/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2947\n",
      "Epoch 50: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.2947 - val_loss: 2.7342 - lr: 6.5566e-05\n",
      "Epoch 51/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3070\n",
      "Epoch 51: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.3070 - val_loss: 2.5447 - lr: 6.4910e-05\n",
      "Epoch 52/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3006\n",
      "Epoch 52: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.3006 - val_loss: 2.6874 - lr: 6.4261e-05\n",
      "Epoch 53/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3044\n",
      "Epoch 53: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.3044 - val_loss: 2.7680 - lr: 6.3619e-05\n",
      "Epoch 54/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2967\n",
      "Epoch 54: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.2967 - val_loss: 2.5494 - lr: 6.2982e-05\n",
      "Epoch 55/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3091\n",
      "Epoch 55: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.3091 - val_loss: 2.5115 - lr: 6.2353e-05\n",
      "Epoch 56/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2878\n",
      "Epoch 56: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.2878 - val_loss: 2.6905 - lr: 6.1729e-05\n",
      "Epoch 57/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2877\n",
      "Epoch 57: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.2877 - val_loss: 2.6279 - lr: 6.1112e-05\n",
      "Epoch 58/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2839\n",
      "Epoch 58: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.2840 - val_loss: 2.3505 - lr: 6.0501e-05\n",
      "Epoch 59/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2877\n",
      "Epoch 59: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.2877 - val_loss: 2.5235 - lr: 5.9896e-05\n",
      "Epoch 60/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2872\n",
      "Epoch 60: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 5.8703650938696225e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.2872 - val_loss: 2.7133 - lr: 5.9297e-05\n",
      "Epoch 61/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2803\n",
      "Epoch 61: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 5.811661550978897e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.2803 - val_loss: 2.5034 - lr: 5.8704e-05\n",
      "Epoch 62/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2894\n",
      "Epoch 62: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 5.753545068728272e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.2881 - val_loss: 2.4584 - lr: 5.8117e-05\n",
      "Epoch 63/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2844\n",
      "Epoch 63: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 5.6960095243994146e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.2844 - val_loss: 2.5279 - lr: 5.7535e-05\n",
      "Epoch 64/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2844\n",
      "Epoch 64: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 5.639049515593797e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.2833 - val_loss: 2.3888 - lr: 5.6960e-05\n",
      "Epoch 65/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2926\n",
      "Epoch 65: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 5.5826589195930865e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.2926 - val_loss: 2.3795 - lr: 5.6390e-05\n",
      "Epoch 66/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2804\n",
      "Epoch 66: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 5.526832333998754e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.2804 - val_loss: 2.5814 - lr: 5.5827e-05\n",
      "Epoch 67/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2745\n",
      "Epoch 67: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 5.471563996252371e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.2745 - val_loss: 2.5870 - lr: 5.5268e-05\n",
      "Epoch 68/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2827\n",
      "Epoch 68: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 5.416848503955407e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.2827 - val_loss: 2.4698 - lr: 5.4716e-05\n",
      "Epoch 69/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2770\n",
      "Epoch 69: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 5.362680094549433e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.2770 - val_loss: 2.6147 - lr: 5.4168e-05\n",
      "Epoch 70/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2769\n",
      "Epoch 70: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 5.309053365635918e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.2761 - val_loss: 2.9046 - lr: 5.3627e-05\n",
      "Epoch 71/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2665\n",
      "Epoch 71: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 5.2559629148163366e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.2665 - val_loss: 2.4975 - lr: 5.3091e-05\n",
      "Epoch 72/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2732\n",
      "Epoch 72: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 5.2034033396921584e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.2732 - val_loss: 2.4342 - lr: 5.2560e-05\n",
      "Epoch 73/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2794\n",
      "Epoch 73: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 5.1513692378648555e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.2794 - val_loss: 2.4238 - lr: 5.2034e-05\n",
      "Epoch 74/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2707\n",
      "Epoch 74: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 5.099855567095801e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.2707 - val_loss: 2.4953 - lr: 5.1514e-05\n",
      "Epoch 75/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2684\n",
      "Epoch 75: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 5.048856924986467e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.2664 - val_loss: 2.6168 - lr: 5.0999e-05\n",
      "Epoch 76/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2674\n",
      "Epoch 76: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 4.998368269298226e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.2708 - val_loss: 2.5554 - lr: 5.0489e-05\n",
      "Epoch 77/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2632\n",
      "Epoch 77: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 4.9483845577924514e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.2632 - val_loss: 2.6330 - lr: 4.9984e-05\n",
      "Epoch 78/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2657\n",
      "Epoch 78: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 4.898900748230517e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.2657 - val_loss: 2.5471 - lr: 4.9484e-05\n",
      "Epoch 79/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2705\n",
      "Epoch 79: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 4.849911798373796e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.2705 - val_loss: 2.7665 - lr: 4.8989e-05\n",
      "Epoch 80/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2634\n",
      "Epoch 80: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 4.801412665983662e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.2634 - val_loss: 2.5504 - lr: 4.8499e-05\n",
      "Epoch 81/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2664\n",
      "Epoch 81: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 4.75339866898139e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 6s 88ms/step - loss: 0.2644 - val_loss: 2.6938 - lr: 4.8014e-05\n",
      "Epoch 82/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2689\n",
      "Epoch 82: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 4.705864765128354e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.2686 - val_loss: 2.3932 - lr: 4.7534e-05\n",
      "Epoch 83/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2628\n",
      "Epoch 83: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 4.658806272345828e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.2628 - val_loss: 2.4562 - lr: 4.7059e-05\n",
      "Epoch 84/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2572\n",
      "Epoch 84: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 4.612218148395186e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.2572 - val_loss: 2.5621 - lr: 4.6588e-05\n",
      "Epoch 85/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2669\n",
      "Epoch 85: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 4.566096071357606e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.2663 - val_loss: 2.6852 - lr: 4.6122e-05\n",
      "Epoch 86/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2664\n",
      "Epoch 86: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 4.52043499899446e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.2664 - val_loss: 2.7727 - lr: 4.5661e-05\n",
      "Epoch 87/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2579\n",
      "Epoch 87: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 4.475230609386927e-05.\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 0.2587 - val_loss: 2.5733 - lr: 4.5204e-05\n",
      "Epoch 88/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2524\n",
      "Epoch 88: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 4.43047822045628e-05.\n",
      "66/66 [==============================] - 6s 87ms/step - loss: 0.2520 - val_loss: 2.6501 - lr: 4.4752e-05\n",
      "Epoch 89/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2575\n",
      "Epoch 89: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 4.386173510283697e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.2575 - val_loss: 2.4327 - lr: 4.4305e-05\n",
      "Epoch 90/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2685\n",
      "Epoch 90: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 4.342311796790454e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.2685 - val_loss: 2.4695 - lr: 4.3862e-05\n",
      "Epoch 91/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2497\n",
      "Epoch 91: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 4.298888758057728e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.2497 - val_loss: 2.7526 - lr: 4.3423e-05\n",
      "Epoch 92/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2610\n",
      "Epoch 92: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 4.2558997120067946e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.2610 - val_loss: 2.4137 - lr: 4.2989e-05\n",
      "Epoch 93/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2526\n",
      "Epoch 93: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 4.2133406968787315e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.2526 - val_loss: 2.6066 - lr: 4.2559e-05\n",
      "Epoch 94/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2568\n",
      "Epoch 94: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 4.1712073907547166e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.2568 - val_loss: 2.5827 - lr: 4.2133e-05\n",
      "Epoch 95/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2617\n",
      "Epoch 95: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 4.1294954717159274e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.2617 - val_loss: 2.6080 - lr: 4.1712e-05\n",
      "Epoch 96/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2536\n",
      "Epoch 96: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 4.08820061784354e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.2536 - val_loss: 2.4852 - lr: 4.1295e-05\n",
      "Epoch 97/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.2573\n",
      "Epoch 97: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 4.0473185072187336e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.2573 - val_loss: 2.6411 - lr: 4.0882e-05\n",
      "Epoch 98/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.2578\n",
      "Epoch 98: val_loss did not improve from 2.30203\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 4.0068451780825854e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.2575 - val_loss: 2.3443 - lr: 4.0473e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_31\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_32 (InputLayer)          [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_93 (LSTM)                 (None, 45, 24)       3744        ['input_32[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_62 (Dropout)           (None, 45, 24)       0           ['lstm_93[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_94 (LSTM)                 (None, 45, 16)       2624        ['dropout_62[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_63 (Dropout)           (None, 45, 16)       0           ['lstm_94[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_95 (LSTM)                 (None, 32)           6272        ['dropout_63[0][0]']             \n",
      "                                                                                                  \n",
      " dense_62 (Dense)               (None, 40)           1320        ['lstm_95[0][0]']                \n",
      "                                                                                                  \n",
      " dense_63 (Dense)               (None, 5)            205         ['dense_62[0][0]']               \n",
      "                                                                                                  \n",
      " tf.unstack_31 (TFOpLambda)     [(None,),            0           ['dense_63[0][0]']               \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_155 (TFOpLambda  (None, 1)           0           ['tf.unstack_31[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_62 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_155[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_159 (TFOpLambda  (None, 1)           0           ['tf.unstack_31[0][4]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_93 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_62[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_63 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_159[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_94 (TFOpLambd  (None, 1)           0           ['tf.math.multiply_93[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_156 (TFOpLambda  (None, 1)           0           ['tf.unstack_31[0][1]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_158 (TFOpLambda  (None, 1)           0           ['tf.unstack_31[0][3]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_95 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_63[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_62 (TFOpL  (None, 1)           0           ['tf.math.multiply_94[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.softplus_62 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_156[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_157 (TFOpLambda  (None, 1)           0           ['tf.unstack_31[0][2]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.softplus_63 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_158[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_63 (TFOpL  (None, 1)           0           ['tf.math.multiply_95[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack_31 (TFOpLambda)       (None, 5, 1)         0           ['tf.__operators__.add_62[0][0]',\n",
      "                                                                  'tf.math.softplus_62[0][0]',    \n",
      "                                                                  'tf.expand_dims_157[0][0]',     \n",
      "                                                                  'tf.math.softplus_63[0][0]',    \n",
      "                                                                  'tf.__operators__.add_63[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _3_CCMP_t+1_0.06\n",
      "Epoch 1/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 3.3942\n",
      "Epoch 1: val_loss improved from inf to 4.12367, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 11s 101ms/step - loss: 3.3919 - val_loss: 4.1237 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 2.7095\n",
      "Epoch 2: val_loss did not improve from 4.12367\n",
      "\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 2.7061 - val_loss: 4.2735 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.5415\n",
      "Epoch 3: val_loss improved from 4.12367 to 3.66398, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 1.5415 - val_loss: 3.6640 - lr: 9.9000e-05\n",
      "Epoch 4/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1218\n",
      "Epoch 4: val_loss improved from 3.66398 to 3.57529, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 1.1218 - val_loss: 3.5753 - lr: 9.9000e-05\n",
      "Epoch 5/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9294\n",
      "Epoch 5: val_loss improved from 3.57529 to 3.39952, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.9294 - val_loss: 3.3995 - lr: 9.9000e-05\n",
      "Epoch 6/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8210\n",
      "Epoch 6: val_loss did not improve from 3.39952\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.8210 - val_loss: 3.6755 - lr: 9.9000e-05\n",
      "Epoch 7/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7474\n",
      "Epoch 7: val_loss did not improve from 3.39952\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.7474 - val_loss: 3.6756 - lr: 9.8010e-05\n",
      "Epoch 8/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6947\n",
      "Epoch 8: val_loss did not improve from 3.39952\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.6945 - val_loss: 4.0873 - lr: 9.7030e-05\n",
      "Epoch 9/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6599\n",
      "Epoch 9: val_loss did not improve from 3.39952\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.6599 - val_loss: 3.6450 - lr: 9.6060e-05\n",
      "Epoch 10/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6438\n",
      "Epoch 10: val_loss did not improve from 3.39952\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.6438 - val_loss: 4.4562 - lr: 9.5099e-05\n",
      "Epoch 11/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6155\n",
      "Epoch 11: val_loss did not improve from 3.39952\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.6155 - val_loss: 3.9171 - lr: 9.4148e-05\n",
      "Epoch 12/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6058\n",
      "Epoch 12: val_loss did not improve from 3.39952\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.6058 - val_loss: 4.0916 - lr: 9.3207e-05\n",
      "Epoch 13/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5749\n",
      "Epoch 13: val_loss did not improve from 3.39952\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.5730 - val_loss: 4.1876 - lr: 9.2274e-05\n",
      "Epoch 14/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5592\n",
      "Epoch 14: val_loss did not improve from 3.39952\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.5592 - val_loss: 3.9532 - lr: 9.1352e-05\n",
      "Epoch 15/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5360\n",
      "Epoch 15: val_loss did not improve from 3.39952\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.5360 - val_loss: 4.2446 - lr: 9.0438e-05\n",
      "Epoch 16/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5310\n",
      "Epoch 16: val_loss improved from 3.39952 to 3.09597, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.5310 - val_loss: 3.0960 - lr: 8.9534e-05\n",
      "Epoch 17/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5163\n",
      "Epoch 17: val_loss improved from 3.09597 to 2.98319, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.5167 - val_loss: 2.9832 - lr: 8.9534e-05\n",
      "Epoch 18/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5068\n",
      "Epoch 18: val_loss did not improve from 2.98319\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.5068 - val_loss: 3.4485 - lr: 8.9534e-05\n",
      "Epoch 19/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4969\n",
      "Epoch 19: val_loss improved from 2.98319 to 2.84932, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.4969 - val_loss: 2.8493 - lr: 8.8638e-05\n",
      "Epoch 20/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4925\n",
      "Epoch 20: val_loss improved from 2.84932 to 2.70142, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.4925 - val_loss: 2.7014 - lr: 8.8638e-05\n",
      "Epoch 21/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4899\n",
      "Epoch 21: val_loss improved from 2.70142 to 2.48644, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.4904 - val_loss: 2.4864 - lr: 8.8638e-05\n",
      "Epoch 22/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4879\n",
      "Epoch 22: val_loss did not improve from 2.48644\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.4879 - val_loss: 2.7528 - lr: 8.8638e-05\n",
      "Epoch 23/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4796\n",
      "Epoch 23: val_loss did not improve from 2.48644\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.4803 - val_loss: 2.9588 - lr: 8.7752e-05\n",
      "Epoch 24/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4698\n",
      "Epoch 24: val_loss did not improve from 2.48644\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.4698 - val_loss: 2.6095 - lr: 8.6875e-05\n",
      "Epoch 25/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4568\n",
      "Epoch 25: val_loss did not improve from 2.48644\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.4568 - val_loss: 2.5752 - lr: 8.6006e-05\n",
      "Epoch 26/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4597\n",
      "Epoch 26: val_loss improved from 2.48644 to 2.31012, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.4597 - val_loss: 2.3101 - lr: 8.5146e-05\n",
      "Epoch 27/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4606\n",
      "Epoch 27: val_loss did not improve from 2.31012\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.4606 - val_loss: 2.3944 - lr: 8.5146e-05\n",
      "Epoch 28/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4483\n",
      "Epoch 28: val_loss improved from 2.31012 to 2.28849, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.4483 - val_loss: 2.2885 - lr: 8.4294e-05\n",
      "Epoch 29/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4650\n",
      "Epoch 29: val_loss improved from 2.28849 to 2.24854, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.4650 - val_loss: 2.2485 - lr: 8.4294e-05\n",
      "Epoch 30/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4580\n",
      "Epoch 30: val_loss improved from 2.24854 to 2.18509, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.4586 - val_loss: 2.1851 - lr: 8.4294e-05\n",
      "Epoch 31/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4453\n",
      "Epoch 31: val_loss did not improve from 2.18509\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.4453 - val_loss: 2.2008 - lr: 8.4294e-05\n",
      "Epoch 32/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4366\n",
      "Epoch 32: val_loss did not improve from 2.18509\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.4366 - val_loss: 2.2872 - lr: 8.3451e-05\n",
      "Epoch 33/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4443\n",
      "Epoch 33: val_loss improved from 2.18509 to 2.17443, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.4443 - val_loss: 2.1744 - lr: 8.2617e-05\n",
      "Epoch 34/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4365\n",
      "Epoch 34: val_loss did not improve from 2.17443\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.4353 - val_loss: 2.1856 - lr: 8.2617e-05\n",
      "Epoch 35/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4311\n",
      "Epoch 35: val_loss did not improve from 2.17443\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.4311 - val_loss: 2.2633 - lr: 8.1791e-05\n",
      "Epoch 36/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4287\n",
      "Epoch 36: val_loss did not improve from 2.17443\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.4287 - val_loss: 2.3166 - lr: 8.0973e-05\n",
      "Epoch 37/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4364\n",
      "Epoch 37: val_loss improved from 2.17443 to 1.94968, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.4364 - val_loss: 1.9497 - lr: 8.0163e-05\n",
      "Epoch 38/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4254\n",
      "Epoch 38: val_loss did not improve from 1.94968\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.4254 - val_loss: 2.2627 - lr: 8.0163e-05\n",
      "Epoch 39/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4296\n",
      "Epoch 39: val_loss did not improve from 1.94968\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.4296 - val_loss: 2.0676 - lr: 7.9361e-05\n",
      "Epoch 40/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4207\n",
      "Epoch 40: val_loss did not improve from 1.94968\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.4207 - val_loss: 2.0148 - lr: 7.8568e-05\n",
      "Epoch 41/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4205\n",
      "Epoch 41: val_loss did not improve from 1.94968\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.4205 - val_loss: 1.9738 - lr: 7.7782e-05\n",
      "Epoch 42/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4136\n",
      "Epoch 42: val_loss did not improve from 1.94968\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.4136 - val_loss: 2.2593 - lr: 7.7004e-05\n",
      "Epoch 43/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4131\n",
      "Epoch 43: val_loss did not improve from 1.94968\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.4127 - val_loss: 2.0079 - lr: 7.6234e-05\n",
      "Epoch 44/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4195\n",
      "Epoch 44: val_loss did not improve from 1.94968\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.4195 - val_loss: 2.0875 - lr: 7.5472e-05\n",
      "Epoch 45/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4125\n",
      "Epoch 45: val_loss improved from 1.94968 to 1.91655, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.4125 - val_loss: 1.9165 - lr: 7.4717e-05\n",
      "Epoch 46/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3993\n",
      "Epoch 46: val_loss improved from 1.91655 to 1.91358, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.3993 - val_loss: 1.9136 - lr: 7.4717e-05\n",
      "Epoch 47/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4109\n",
      "Epoch 47: val_loss did not improve from 1.91358\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.4109 - val_loss: 1.9933 - lr: 7.4717e-05\n",
      "Epoch 48/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4081\n",
      "Epoch 48: val_loss did not improve from 1.91358\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.4087 - val_loss: 1.9897 - lr: 7.3970e-05\n",
      "Epoch 49/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4148\n",
      "Epoch 49: val_loss did not improve from 1.91358\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.4148 - val_loss: 1.9729 - lr: 7.3230e-05\n",
      "Epoch 50/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4091\n",
      "Epoch 50: val_loss improved from 1.91358 to 1.86085, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.4091 - val_loss: 1.8608 - lr: 7.2498e-05\n",
      "Epoch 51/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3994\n",
      "Epoch 51: val_loss did not improve from 1.86085\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.3994 - val_loss: 1.8814 - lr: 7.2498e-05\n",
      "Epoch 52/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4093\n",
      "Epoch 52: val_loss did not improve from 1.86085\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.4079 - val_loss: 1.9914 - lr: 7.1773e-05\n",
      "Epoch 53/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4007\n",
      "Epoch 53: val_loss improved from 1.86085 to 1.81613, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.06.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 6s 95ms/step - loss: 0.4007 - val_loss: 1.8161 - lr: 7.1055e-05\n",
      "Epoch 54/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3958\n",
      "Epoch 54: val_loss did not improve from 1.81613\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.3958 - val_loss: 1.8713 - lr: 7.1055e-05\n",
      "Epoch 55/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3981\n",
      "Epoch 55: val_loss did not improve from 1.81613\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.3981 - val_loss: 1.9513 - lr: 7.0345e-05\n",
      "Epoch 56/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4035\n",
      "Epoch 56: val_loss improved from 1.81613 to 1.77595, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.4015 - val_loss: 1.7759 - lr: 6.9641e-05\n",
      "Epoch 57/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3893\n",
      "Epoch 57: val_loss did not improve from 1.77595\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.3893 - val_loss: 1.8596 - lr: 6.9641e-05\n",
      "Epoch 58/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3832\n",
      "Epoch 58: val_loss improved from 1.77595 to 1.65798, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.3832 - val_loss: 1.6580 - lr: 6.8945e-05\n",
      "Epoch 59/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3989\n",
      "Epoch 59: val_loss did not improve from 1.65798\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.3989 - val_loss: 1.8749 - lr: 6.8945e-05\n",
      "Epoch 60/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3966\n",
      "Epoch 60: val_loss did not improve from 1.65798\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.3981 - val_loss: 1.7459 - lr: 6.8255e-05\n",
      "Epoch 61/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3841\n",
      "Epoch 61: val_loss did not improve from 1.65798\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.3841 - val_loss: 1.9294 - lr: 6.7573e-05\n",
      "Epoch 62/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3876\n",
      "Epoch 62: val_loss did not improve from 1.65798\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.3876 - val_loss: 1.7119 - lr: 6.6897e-05\n",
      "Epoch 63/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3936\n",
      "Epoch 63: val_loss did not improve from 1.65798\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.3936 - val_loss: 1.8977 - lr: 6.6228e-05\n",
      "Epoch 64/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3902\n",
      "Epoch 64: val_loss did not improve from 1.65798\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.3893 - val_loss: 1.8409 - lr: 6.5566e-05\n",
      "Epoch 65/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3784\n",
      "Epoch 65: val_loss did not improve from 1.65798\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.3784 - val_loss: 1.7352 - lr: 6.4910e-05\n",
      "Epoch 66/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3736\n",
      "Epoch 66: val_loss did not improve from 1.65798\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.3736 - val_loss: 1.7901 - lr: 6.4261e-05\n",
      "Epoch 67/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3804\n",
      "Epoch 67: val_loss did not improve from 1.65798\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.3804 - val_loss: 1.8000 - lr: 6.3619e-05\n",
      "Epoch 68/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3706\n",
      "Epoch 68: val_loss did not improve from 1.65798\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.3694 - val_loss: 1.7103 - lr: 6.2982e-05\n",
      "Epoch 69/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3771\n",
      "Epoch 69: val_loss did not improve from 1.65798\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.3771 - val_loss: 1.6686 - lr: 6.2353e-05\n",
      "Epoch 70/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3662\n",
      "Epoch 70: val_loss did not improve from 1.65798\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.3662 - val_loss: 1.7477 - lr: 6.1729e-05\n",
      "Epoch 71/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3755\n",
      "Epoch 71: val_loss improved from 1.65798 to 1.60783, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.3755 - val_loss: 1.6078 - lr: 6.1112e-05\n",
      "Epoch 72/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3787\n",
      "Epoch 72: val_loss did not improve from 1.60783\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.3787 - val_loss: 1.7836 - lr: 6.1112e-05\n",
      "Epoch 73/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3531\n",
      "Epoch 73: val_loss did not improve from 1.60783\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.3568 - val_loss: 1.8233 - lr: 6.0501e-05\n",
      "Epoch 74/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3726\n",
      "Epoch 74: val_loss did not improve from 1.60783\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.3726 - val_loss: 1.7812 - lr: 5.9896e-05\n",
      "Epoch 75/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3670\n",
      "Epoch 75: val_loss did not improve from 1.60783\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 5.8703650938696225e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.3670 - val_loss: 1.7723 - lr: 5.9297e-05\n",
      "Epoch 76/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3616\n",
      "Epoch 76: val_loss did not improve from 1.60783\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 5.811661550978897e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.3616 - val_loss: 1.6724 - lr: 5.8704e-05\n",
      "Epoch 77/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3667\n",
      "Epoch 77: val_loss improved from 1.60783 to 1.55587, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.3668 - val_loss: 1.5559 - lr: 5.8117e-05\n",
      "Epoch 78/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3678\n",
      "Epoch 78: val_loss did not improve from 1.55587\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 5.753545068728272e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.3678 - val_loss: 1.6364 - lr: 5.8117e-05\n",
      "Epoch 79/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3638\n",
      "Epoch 79: val_loss did not improve from 1.55587\n",
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 5.6960095243994146e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 6s 94ms/step - loss: 0.3638 - val_loss: 1.7837 - lr: 5.7535e-05\n",
      "Epoch 80/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3679\n",
      "Epoch 80: val_loss did not improve from 1.55587\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 5.639049515593797e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.3679 - val_loss: 1.5716 - lr: 5.6960e-05\n",
      "Epoch 81/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3586\n",
      "Epoch 81: val_loss did not improve from 1.55587\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 5.5826589195930865e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.3586 - val_loss: 1.8201 - lr: 5.6390e-05\n",
      "Epoch 82/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3563\n",
      "Epoch 82: val_loss did not improve from 1.55587\n",
      "\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 5.526832333998754e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.3550 - val_loss: 1.7207 - lr: 5.5827e-05\n",
      "Epoch 83/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3600\n",
      "Epoch 83: val_loss did not improve from 1.55587\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 5.471563996252371e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.3600 - val_loss: 1.8133 - lr: 5.5268e-05\n",
      "Epoch 84/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3665\n",
      "Epoch 84: val_loss did not improve from 1.55587\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 5.416848503955407e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.3665 - val_loss: 1.6095 - lr: 5.4716e-05\n",
      "Epoch 85/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3614\n",
      "Epoch 85: val_loss did not improve from 1.55587\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 5.362680094549433e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.3614 - val_loss: 1.6744 - lr: 5.4168e-05\n",
      "Epoch 86/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3629\n",
      "Epoch 86: val_loss improved from 1.55587 to 1.49921, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.06.hdf5\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.3629 - val_loss: 1.4992 - lr: 5.3627e-05\n",
      "Epoch 87/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3616\n",
      "Epoch 87: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 5.309053365635918e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.3638 - val_loss: 1.6137 - lr: 5.3627e-05\n",
      "Epoch 88/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3598\n",
      "Epoch 88: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 5.2559629148163366e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.3598 - val_loss: 1.7027 - lr: 5.3091e-05\n",
      "Epoch 89/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3554\n",
      "Epoch 89: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 5.2034033396921584e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.3554 - val_loss: 1.6349 - lr: 5.2560e-05\n",
      "Epoch 90/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3617\n",
      "Epoch 90: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 5.1513692378648555e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.3617 - val_loss: 1.7031 - lr: 5.2034e-05\n",
      "Epoch 91/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3564\n",
      "Epoch 91: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 5.099855567095801e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.3564 - val_loss: 1.6509 - lr: 5.1514e-05\n",
      "Epoch 92/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3481\n",
      "Epoch 92: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 5.048856924986467e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 0.3481 - val_loss: 1.7022 - lr: 5.0999e-05\n",
      "Epoch 93/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3613\n",
      "Epoch 93: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 4.998368269298226e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.3613 - val_loss: 1.6324 - lr: 5.0489e-05\n",
      "Epoch 94/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3568\n",
      "Epoch 94: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 4.9483845577924514e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.3568 - val_loss: 1.5607 - lr: 4.9984e-05\n",
      "Epoch 95/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3475\n",
      "Epoch 95: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 4.898900748230517e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.3475 - val_loss: 1.6282 - lr: 4.9484e-05\n",
      "Epoch 96/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3484\n",
      "Epoch 96: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 4.849911798373796e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.3484 - val_loss: 1.5796 - lr: 4.8989e-05\n",
      "Epoch 97/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3393\n",
      "Epoch 97: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 4.801412665983662e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.3393 - val_loss: 1.6364 - lr: 4.8499e-05\n",
      "Epoch 98/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3482\n",
      "Epoch 98: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 4.75339866898139e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.3482 - val_loss: 1.6151 - lr: 4.8014e-05\n",
      "Epoch 99/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3496\n",
      "Epoch 99: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 4.705864765128354e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.3496 - val_loss: 1.6503 - lr: 4.7534e-05\n",
      "Epoch 100/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3463\n",
      "Epoch 100: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 4.658806272345828e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.3463 - val_loss: 1.7382 - lr: 4.7059e-05\n",
      "Epoch 101/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3445\n",
      "Epoch 101: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 4.612218148395186e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.3445 - val_loss: 1.6635 - lr: 4.6588e-05\n",
      "Epoch 102/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3429\n",
      "Epoch 102: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 4.566096071357606e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.3429 - val_loss: 1.6282 - lr: 4.6122e-05\n",
      "Epoch 103/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3505\n",
      "Epoch 103: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 4.52043499899446e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.3496 - val_loss: 1.5997 - lr: 4.5661e-05\n",
      "Epoch 104/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3457\n",
      "Epoch 104: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 4.475230609386927e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.3457 - val_loss: 1.5499 - lr: 4.5204e-05\n",
      "Epoch 105/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3440\n",
      "Epoch 105: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 4.43047822045628e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.3440 - val_loss: 1.5730 - lr: 4.4752e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3621\n",
      "Epoch 106: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 4.386173510283697e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.3621 - val_loss: 1.6151 - lr: 4.4305e-05\n",
      "Epoch 107/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3477\n",
      "Epoch 107: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 4.342311796790454e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.3477 - val_loss: 1.6947 - lr: 4.3862e-05\n",
      "Epoch 108/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3463\n",
      "Epoch 108: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 4.298888758057728e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.3463 - val_loss: 1.5880 - lr: 4.3423e-05\n",
      "Epoch 109/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3314\n",
      "Epoch 109: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 4.2558997120067946e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.3314 - val_loss: 1.5552 - lr: 4.2989e-05\n",
      "Epoch 110/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3344\n",
      "Epoch 110: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 4.2133406968787315e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.3344 - val_loss: 1.6505 - lr: 4.2559e-05\n",
      "Epoch 111/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3632\n",
      "Epoch 111: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 4.1712073907547166e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.3624 - val_loss: 1.6531 - lr: 4.2133e-05\n",
      "Epoch 112/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3444\n",
      "Epoch 112: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 4.1294954717159274e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.3444 - val_loss: 1.5760 - lr: 4.1712e-05\n",
      "Epoch 113/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3388\n",
      "Epoch 113: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 4.08820061784354e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.3388 - val_loss: 1.5031 - lr: 4.1295e-05\n",
      "Epoch 114/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3338\n",
      "Epoch 114: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 4.0473185072187336e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.3338 - val_loss: 1.6028 - lr: 4.0882e-05\n",
      "Epoch 115/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3365\n",
      "Epoch 115: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 4.0068451780825854e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.3365 - val_loss: 1.7237 - lr: 4.0473e-05\n",
      "Epoch 116/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3488\n",
      "Epoch 116: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 3.966776668676175e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.3488 - val_loss: 1.5699 - lr: 4.0068e-05\n",
      "Epoch 117/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3412\n",
      "Epoch 117: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 3.927109017240582e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.3412 - val_loss: 1.5552 - lr: 3.9668e-05\n",
      "Epoch 118/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3536\n",
      "Epoch 118: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 3.887837901856983e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.3536 - val_loss: 1.6122 - lr: 3.9271e-05\n",
      "Epoch 119/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3330\n",
      "Epoch 119: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 3.848959360766457e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.3330 - val_loss: 1.5634 - lr: 3.8878e-05\n",
      "Epoch 120/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3291\n",
      "Epoch 120: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 3.810469792369986e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.3296 - val_loss: 1.5866 - lr: 3.8490e-05\n",
      "Epoch 121/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3380\n",
      "Epoch 121: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 3.772365234908648e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.3404 - val_loss: 1.6260 - lr: 3.8105e-05\n",
      "Epoch 122/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3271\n",
      "Epoch 122: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 3.734641726623522e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.3271 - val_loss: 1.5647 - lr: 3.7724e-05\n",
      "Epoch 123/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3417\n",
      "Epoch 123: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 123: ReduceLROnPlateau reducing learning rate to 3.6972953057556876e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.3417 - val_loss: 1.5129 - lr: 3.7346e-05\n",
      "Epoch 124/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3374\n",
      "Epoch 124: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 124: ReduceLROnPlateau reducing learning rate to 3.660322370706126e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.3374 - val_loss: 1.5880 - lr: 3.6973e-05\n",
      "Epoch 125/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3344\n",
      "Epoch 125: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 3.623719319875818e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.3375 - val_loss: 1.5956 - lr: 3.6603e-05\n",
      "Epoch 126/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3262\n",
      "Epoch 126: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 3.5874821915058416e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.3262 - val_loss: 1.5414 - lr: 3.6237e-05\n",
      "Epoch 127/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3297\n",
      "Epoch 127: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 127: ReduceLROnPlateau reducing learning rate to 3.551607383997179e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.3297 - val_loss: 1.6091 - lr: 3.5875e-05\n",
      "Epoch 128/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3305\n",
      "Epoch 128: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 128: ReduceLROnPlateau reducing learning rate to 3.516091295750812e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.3305 - val_loss: 1.6602 - lr: 3.5516e-05\n",
      "Epoch 129/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3339\n",
      "Epoch 129: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 129: ReduceLROnPlateau reducing learning rate to 3.480930325167719e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.3348 - val_loss: 1.6353 - lr: 3.5161e-05\n",
      "Epoch 130/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3310\n",
      "Epoch 130: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 130: ReduceLROnPlateau reducing learning rate to 3.446120870648883e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.3310 - val_loss: 1.6720 - lr: 3.4809e-05\n",
      "Epoch 131/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3511\n",
      "Epoch 131: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 131: ReduceLROnPlateau reducing learning rate to 3.4116596907551866e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.3511 - val_loss: 1.5897 - lr: 3.4461e-05\n",
      "Epoch 132/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3334\n",
      "Epoch 132: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 132: ReduceLROnPlateau reducing learning rate to 3.37754318388761e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.3334 - val_loss: 1.5023 - lr: 3.4117e-05\n",
      "Epoch 133/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.3385\n",
      "Epoch 133: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 133: ReduceLROnPlateau reducing learning rate to 3.3437677484471354e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.3369 - val_loss: 1.5741 - lr: 3.3775e-05\n",
      "Epoch 134/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3283\n",
      "Epoch 134: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 134: ReduceLROnPlateau reducing learning rate to 3.310330142994644e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.3283 - val_loss: 1.5989 - lr: 3.3438e-05\n",
      "Epoch 135/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3380\n",
      "Epoch 135: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 135: ReduceLROnPlateau reducing learning rate to 3.277226765931118e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.3380 - val_loss: 1.5505 - lr: 3.3103e-05\n",
      "Epoch 136/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3272\n",
      "Epoch 136: val_loss did not improve from 1.49921\n",
      "\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 3.24445437581744e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.3272 - val_loss: 1.5247 - lr: 3.2772e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_32\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_33 (InputLayer)          [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_96 (LSTM)                 (None, 45, 24)       3744        ['input_33[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_64 (Dropout)           (None, 45, 24)       0           ['lstm_96[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_97 (LSTM)                 (None, 45, 16)       2624        ['dropout_64[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_65 (Dropout)           (None, 45, 16)       0           ['lstm_97[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_98 (LSTM)                 (None, 32)           6272        ['dropout_65[0][0]']             \n",
      "                                                                                                  \n",
      " dense_64 (Dense)               (None, 40)           1320        ['lstm_98[0][0]']                \n",
      "                                                                                                  \n",
      " dense_65 (Dense)               (None, 5)            205         ['dense_64[0][0]']               \n",
      "                                                                                                  \n",
      " tf.unstack_32 (TFOpLambda)     [(None,),            0           ['dense_65[0][0]']               \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_160 (TFOpLambda  (None, 1)           0           ['tf.unstack_32[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_64 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_160[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_164 (TFOpLambda  (None, 1)           0           ['tf.unstack_32[0][4]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_96 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_64[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_65 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_164[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_97 (TFOpLambd  (None, 1)           0           ['tf.math.multiply_96[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_161 (TFOpLambda  (None, 1)           0           ['tf.unstack_32[0][1]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_163 (TFOpLambda  (None, 1)           0           ['tf.unstack_32[0][3]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_98 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_65[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_64 (TFOpL  (None, 1)           0           ['tf.math.multiply_97[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.softplus_64 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_161[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_162 (TFOpLambda  (None, 1)           0           ['tf.unstack_32[0][2]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.softplus_65 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_163[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_65 (TFOpL  (None, 1)           0           ['tf.math.multiply_98[0][0]']    \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack_32 (TFOpLambda)       (None, 5, 1)         0           ['tf.__operators__.add_64[0][0]',\n",
      "                                                                  'tf.math.softplus_64[0][0]',    \n",
      "                                                                  'tf.expand_dims_162[0][0]',     \n",
      "                                                                  'tf.math.softplus_65[0][0]',    \n",
      "                                                                  'tf.__operators__.add_65[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _3_CCMP_t+1_0.07\n",
      "Epoch 1/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 3.4421\n",
      "Epoch 1: val_loss improved from inf to 4.17107, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 10s 93ms/step - loss: 3.4413 - val_loss: 4.1711 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 2.7544\n",
      "Epoch 2: val_loss improved from 4.17107 to 3.25426, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.07.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 6s 84ms/step - loss: 2.7515 - val_loss: 3.2543 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.6381\n",
      "Epoch 3: val_loss improved from 3.25426 to 3.18615, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 1.6381 - val_loss: 3.1861 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.2274\n",
      "Epoch 4: val_loss did not improve from 3.18615\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 1.2274 - val_loss: 3.2037 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0318\n",
      "Epoch 5: val_loss did not improve from 3.18615\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 1.0318 - val_loss: 3.1999 - lr: 9.9000e-05\n",
      "Epoch 6/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9156\n",
      "Epoch 6: val_loss improved from 3.18615 to 3.04863, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.07.hdf5\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.9156 - val_loss: 3.0486 - lr: 9.8010e-05\n",
      "Epoch 7/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8557\n",
      "Epoch 7: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 6s 89ms/step - loss: 0.8567 - val_loss: 3.3665 - lr: 9.8010e-05\n",
      "Epoch 8/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8113\n",
      "Epoch 8: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.8113 - val_loss: 3.1766 - lr: 9.7030e-05\n",
      "Epoch 9/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7875\n",
      "Epoch 9: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.7875 - val_loss: 3.2000 - lr: 9.6060e-05\n",
      "Epoch 10/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7640\n",
      "Epoch 10: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.7640 - val_loss: 3.5433 - lr: 9.5099e-05\n",
      "Epoch 11/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7514\n",
      "Epoch 11: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.7514 - val_loss: 3.7224 - lr: 9.4148e-05\n",
      "Epoch 12/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7414\n",
      "Epoch 12: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.7424 - val_loss: 3.1473 - lr: 9.3207e-05\n",
      "Epoch 13/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7228\n",
      "Epoch 13: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.7228 - val_loss: 3.1186 - lr: 9.2274e-05\n",
      "Epoch 14/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6967\n",
      "Epoch 14: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.6967 - val_loss: 3.5993 - lr: 9.1352e-05\n",
      "Epoch 15/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6811\n",
      "Epoch 15: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.6811 - val_loss: 3.4211 - lr: 9.0438e-05\n",
      "Epoch 16/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6538\n",
      "Epoch 16: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.6538 - val_loss: 3.7401 - lr: 8.9534e-05\n",
      "Epoch 17/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6381\n",
      "Epoch 17: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.6381 - val_loss: 3.5203 - lr: 8.8638e-05\n",
      "Epoch 18/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6297\n",
      "Epoch 18: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.6297 - val_loss: 3.7402 - lr: 8.7752e-05\n",
      "Epoch 19/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6287\n",
      "Epoch 19: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.6287 - val_loss: 3.8481 - lr: 8.6875e-05\n",
      "Epoch 20/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6086\n",
      "Epoch 20: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.6086 - val_loss: 3.4766 - lr: 8.6006e-05\n",
      "Epoch 21/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6016\n",
      "Epoch 21: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.6012 - val_loss: 3.7413 - lr: 8.5146e-05\n",
      "Epoch 22/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5895\n",
      "Epoch 22: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.5895 - val_loss: 3.8183 - lr: 8.4294e-05\n",
      "Epoch 23/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5847\n",
      "Epoch 23: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.5847 - val_loss: 3.7239 - lr: 8.3451e-05\n",
      "Epoch 24/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5778\n",
      "Epoch 24: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.5778 - val_loss: 3.4066 - lr: 8.2617e-05\n",
      "Epoch 25/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5738\n",
      "Epoch 25: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.5738 - val_loss: 3.8074 - lr: 8.1791e-05\n",
      "Epoch 26/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5651\n",
      "Epoch 26: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.5657 - val_loss: 3.7291 - lr: 8.0973e-05\n",
      "Epoch 27/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5565\n",
      "Epoch 27: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.5565 - val_loss: 3.6802 - lr: 8.0163e-05\n",
      "Epoch 28/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5475\n",
      "Epoch 28: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.5475 - val_loss: 3.6100 - lr: 7.9361e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5551\n",
      "Epoch 29: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.5551 - val_loss: 3.3737 - lr: 7.8568e-05\n",
      "Epoch 30/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5400\n",
      "Epoch 30: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.5400 - val_loss: 3.7144 - lr: 7.7782e-05\n",
      "Epoch 31/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5444\n",
      "Epoch 31: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.5443 - val_loss: 3.8447 - lr: 7.7004e-05\n",
      "Epoch 32/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5352\n",
      "Epoch 32: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.5352 - val_loss: 3.5327 - lr: 7.6234e-05\n",
      "Epoch 33/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5274\n",
      "Epoch 33: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.5274 - val_loss: 3.8868 - lr: 7.5472e-05\n",
      "Epoch 34/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5297\n",
      "Epoch 34: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.5297 - val_loss: 3.4981 - lr: 7.4717e-05\n",
      "Epoch 35/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5356\n",
      "Epoch 35: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.5335 - val_loss: 3.4711 - lr: 7.3970e-05\n",
      "Epoch 36/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5191\n",
      "Epoch 36: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.5191 - val_loss: 3.7665 - lr: 7.3230e-05\n",
      "Epoch 37/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5233\n",
      "Epoch 37: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.5233 - val_loss: 3.7701 - lr: 7.2498e-05\n",
      "Epoch 38/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5086\n",
      "Epoch 38: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.5086 - val_loss: 3.6621 - lr: 7.1773e-05\n",
      "Epoch 39/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5183\n",
      "Epoch 39: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.5183 - val_loss: 3.7926 - lr: 7.1055e-05\n",
      "Epoch 40/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5109\n",
      "Epoch 40: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.5152 - val_loss: 3.2841 - lr: 7.0345e-05\n",
      "Epoch 41/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5163\n",
      "Epoch 41: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.5163 - val_loss: 3.5859 - lr: 6.9641e-05\n",
      "Epoch 42/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5195\n",
      "Epoch 42: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.5195 - val_loss: 3.7251 - lr: 6.8945e-05\n",
      "Epoch 43/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5041\n",
      "Epoch 43: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.5041 - val_loss: 3.7481 - lr: 6.8255e-05\n",
      "Epoch 44/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5029\n",
      "Epoch 44: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.5029 - val_loss: 3.6155 - lr: 6.7573e-05\n",
      "Epoch 45/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5022\n",
      "Epoch 45: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.5010 - val_loss: 3.5276 - lr: 6.6897e-05\n",
      "Epoch 46/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4992\n",
      "Epoch 46: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.4992 - val_loss: 3.7016 - lr: 6.6228e-05\n",
      "Epoch 47/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4935\n",
      "Epoch 47: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.4935 - val_loss: 3.5487 - lr: 6.5566e-05\n",
      "Epoch 48/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4983\n",
      "Epoch 48: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.4983 - val_loss: 3.9553 - lr: 6.4910e-05\n",
      "Epoch 49/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5021\n",
      "Epoch 49: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.5021 - val_loss: 3.8328 - lr: 6.4261e-05\n",
      "Epoch 50/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4924\n",
      "Epoch 50: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.4945 - val_loss: 3.6621 - lr: 6.3619e-05\n",
      "Epoch 51/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4868\n",
      "Epoch 51: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.4868 - val_loss: 3.4185 - lr: 6.2982e-05\n",
      "Epoch 52/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4885\n",
      "Epoch 52: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.4885 - val_loss: 3.5071 - lr: 6.2353e-05\n",
      "Epoch 53/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4939\n",
      "Epoch 53: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.4939 - val_loss: 3.4999 - lr: 6.1729e-05\n",
      "Epoch 54/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4854\n",
      "Epoch 54: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.4854 - val_loss: 3.6170 - lr: 6.1112e-05\n",
      "Epoch 55/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4851\n",
      "Epoch 55: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 6s 89ms/step - loss: 0.4863 - val_loss: 3.7156 - lr: 6.0501e-05\n",
      "Epoch 56/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4864\n",
      "Epoch 56: val_loss did not improve from 3.04863\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 0.4864 - val_loss: 3.7073 - lr: 5.9896e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_33\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_34 (InputLayer)          [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_99 (LSTM)                 (None, 45, 24)       3744        ['input_34[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_66 (Dropout)           (None, 45, 24)       0           ['lstm_99[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_100 (LSTM)                (None, 45, 16)       2624        ['dropout_66[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_67 (Dropout)           (None, 45, 16)       0           ['lstm_100[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_101 (LSTM)                (None, 32)           6272        ['dropout_67[0][0]']             \n",
      "                                                                                                  \n",
      " dense_66 (Dense)               (None, 40)           1320        ['lstm_101[0][0]']               \n",
      "                                                                                                  \n",
      " dense_67 (Dense)               (None, 5)            205         ['dense_66[0][0]']               \n",
      "                                                                                                  \n",
      " tf.unstack_33 (TFOpLambda)     [(None,),            0           ['dense_67[0][0]']               \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_165 (TFOpLambda  (None, 1)           0           ['tf.unstack_33[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_66 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_165[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_169 (TFOpLambda  (None, 1)           0           ['tf.unstack_33[0][4]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_99 (TFOpLambd  (None, 1)           0           ['tf.math.sigmoid_66[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_67 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_169[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_100 (TFOpLamb  (None, 1)           0           ['tf.math.multiply_99[0][0]']    \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.expand_dims_166 (TFOpLambda  (None, 1)           0           ['tf.unstack_33[0][1]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_168 (TFOpLambda  (None, 1)           0           ['tf.unstack_33[0][3]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_101 (TFOpLamb  (None, 1)           0           ['tf.math.sigmoid_67[0][0]']     \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_66 (TFOpL  (None, 1)           0           ['tf.math.multiply_100[0][0]']   \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.softplus_66 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_166[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_167 (TFOpLambda  (None, 1)           0           ['tf.unstack_33[0][2]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.softplus_67 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_168[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_67 (TFOpL  (None, 1)           0           ['tf.math.multiply_101[0][0]']   \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack_33 (TFOpLambda)       (None, 5, 1)         0           ['tf.__operators__.add_66[0][0]',\n",
      "                                                                  'tf.math.softplus_66[0][0]',    \n",
      "                                                                  'tf.expand_dims_167[0][0]',     \n",
      "                                                                  'tf.math.softplus_67[0][0]',    \n",
      "                                                                  'tf.__operators__.add_67[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _3_CCMP_t+1_0.08\n",
      "Epoch 1/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 3.4478\n",
      "Epoch 1: val_loss improved from inf to 4.22984, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 11s 103ms/step - loss: 3.4566 - val_loss: 4.2298 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 3.0662\n",
      "Epoch 2: val_loss improved from 4.22984 to 3.60521, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.08.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 6s 89ms/step - loss: 3.0646 - val_loss: 3.6052 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 2.0644\n",
      "Epoch 3: val_loss improved from 3.60521 to 2.72671, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 2.0644 - val_loss: 2.7267 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.3546\n",
      "Epoch 4: val_loss improved from 2.72671 to 2.24117, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 1.3544 - val_loss: 2.2412 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.0773\n",
      "Epoch 5: val_loss improved from 2.24117 to 2.18056, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 6s 88ms/step - loss: 1.0752 - val_loss: 2.1806 - lr: 1.0000e-04\n",
      "Epoch 6/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9499\n",
      "Epoch 6: val_loss did not improve from 2.18056\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.9499 - val_loss: 2.2136 - lr: 1.0000e-04\n",
      "Epoch 7/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.8589\n",
      "Epoch 7: val_loss improved from 2.18056 to 1.97868, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 7s 103ms/step - loss: 0.8569 - val_loss: 1.9787 - lr: 9.9000e-05\n",
      "Epoch 8/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8215\n",
      "Epoch 8: val_loss did not improve from 1.97868\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.8215 - val_loss: 2.4468 - lr: 9.9000e-05\n",
      "Epoch 9/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7615\n",
      "Epoch 9: val_loss did not improve from 1.97868\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.7615 - val_loss: 2.4688 - lr: 9.8010e-05\n",
      "Epoch 10/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7416\n",
      "Epoch 10: val_loss improved from 1.97868 to 1.95913, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 6s 98ms/step - loss: 0.7416 - val_loss: 1.9591 - lr: 9.7030e-05\n",
      "Epoch 11/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7211\n",
      "Epoch 11: val_loss improved from 1.95913 to 1.84132, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 0.7211 - val_loss: 1.8413 - lr: 9.7030e-05\n",
      "Epoch 12/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7063\n",
      "Epoch 12: val_loss did not improve from 1.84132\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 6s 98ms/step - loss: 0.7063 - val_loss: 2.4315 - lr: 9.7030e-05\n",
      "Epoch 13/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6958\n",
      "Epoch 13: val_loss did not improve from 1.84132\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 6s 98ms/step - loss: 0.6958 - val_loss: 1.9802 - lr: 9.6060e-05\n",
      "Epoch 14/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6709\n",
      "Epoch 14: val_loss did not improve from 1.84132\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.6709 - val_loss: 2.1157 - lr: 9.5099e-05\n",
      "Epoch 15/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6587\n",
      "Epoch 15: val_loss did not improve from 1.84132\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 6s 98ms/step - loss: 0.6587 - val_loss: 2.0404 - lr: 9.4148e-05\n",
      "Epoch 16/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6533\n",
      "Epoch 16: val_loss did not improve from 1.84132\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 7s 100ms/step - loss: 0.6533 - val_loss: 1.9133 - lr: 9.3207e-05\n",
      "Epoch 17/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6621\n",
      "Epoch 17: val_loss improved from 1.84132 to 1.80439, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 0.6621 - val_loss: 1.8044 - lr: 9.2274e-05\n",
      "Epoch 18/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6416\n",
      "Epoch 18: val_loss did not improve from 1.80439\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.6432 - val_loss: 1.9030 - lr: 9.2274e-05\n",
      "Epoch 19/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6342\n",
      "Epoch 19: val_loss did not improve from 1.80439\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.6342 - val_loss: 1.8060 - lr: 9.1352e-05\n",
      "Epoch 20/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6279\n",
      "Epoch 20: val_loss did not improve from 1.80439\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.6279 - val_loss: 1.9038 - lr: 9.0438e-05\n",
      "Epoch 21/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6242\n",
      "Epoch 21: val_loss improved from 1.80439 to 1.71490, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.6250 - val_loss: 1.7149 - lr: 8.9534e-05\n",
      "Epoch 22/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6221\n",
      "Epoch 22: val_loss did not improve from 1.71490\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.6221 - val_loss: 1.7668 - lr: 8.9534e-05\n",
      "Epoch 23/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6167\n",
      "Epoch 23: val_loss did not improve from 1.71490\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.6167 - val_loss: 1.8145 - lr: 8.8638e-05\n",
      "Epoch 24/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6077\n",
      "Epoch 24: val_loss did not improve from 1.71490\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.6077 - val_loss: 1.7987 - lr: 8.7752e-05\n",
      "Epoch 25/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6001\n",
      "Epoch 25: val_loss improved from 1.71490 to 1.62322, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.6001 - val_loss: 1.6232 - lr: 8.6875e-05\n",
      "Epoch 26/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5998\n",
      "Epoch 26: val_loss did not improve from 1.62322\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.6008 - val_loss: 1.8522 - lr: 8.6875e-05\n",
      "Epoch 27/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6096\n",
      "Epoch 27: val_loss improved from 1.62322 to 1.56391, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 6s 98ms/step - loss: 0.6096 - val_loss: 1.5639 - lr: 8.6006e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5947\n",
      "Epoch 28: val_loss did not improve from 1.56391\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.5947 - val_loss: 1.7328 - lr: 8.6006e-05\n",
      "Epoch 29/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5971\n",
      "Epoch 29: val_loss did not improve from 1.56391\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.5971 - val_loss: 1.7330 - lr: 8.5146e-05\n",
      "Epoch 30/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5905\n",
      "Epoch 30: val_loss did not improve from 1.56391\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.5905 - val_loss: 1.7134 - lr: 8.4294e-05\n",
      "Epoch 31/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5841\n",
      "Epoch 31: val_loss did not improve from 1.56391\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.5841 - val_loss: 1.6611 - lr: 8.3451e-05\n",
      "Epoch 32/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5830\n",
      "Epoch 32: val_loss did not improve from 1.56391\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.5830 - val_loss: 1.7099 - lr: 8.2617e-05\n",
      "Epoch 33/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5850\n",
      "Epoch 33: val_loss did not improve from 1.56391\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.5846 - val_loss: 1.8045 - lr: 8.1791e-05\n",
      "Epoch 34/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5761\n",
      "Epoch 34: val_loss did not improve from 1.56391\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.5761 - val_loss: 1.6382 - lr: 8.0973e-05\n",
      "Epoch 35/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5688\n",
      "Epoch 35: val_loss did not improve from 1.56391\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.5688 - val_loss: 1.6497 - lr: 8.0163e-05\n",
      "Epoch 36/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5660\n",
      "Epoch 36: val_loss did not improve from 1.56391\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.5660 - val_loss: 1.7042 - lr: 7.9361e-05\n",
      "Epoch 37/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5666\n",
      "Epoch 37: val_loss did not improve from 1.56391\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.5666 - val_loss: 1.7054 - lr: 7.8568e-05\n",
      "Epoch 38/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5621\n",
      "Epoch 38: val_loss did not improve from 1.56391\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.5621 - val_loss: 1.6567 - lr: 7.7782e-05\n",
      "Epoch 39/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5698\n",
      "Epoch 39: val_loss did not improve from 1.56391\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.5698 - val_loss: 1.6020 - lr: 7.7004e-05\n",
      "Epoch 40/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5652\n",
      "Epoch 40: val_loss did not improve from 1.56391\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.5652 - val_loss: 1.7016 - lr: 7.6234e-05\n",
      "Epoch 41/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5610\n",
      "Epoch 41: val_loss did not improve from 1.56391\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.5615 - val_loss: 1.6396 - lr: 7.5472e-05\n",
      "Epoch 42/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5520\n",
      "Epoch 42: val_loss did not improve from 1.56391\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.5520 - val_loss: 1.6248 - lr: 7.4717e-05\n",
      "Epoch 43/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5475\n",
      "Epoch 43: val_loss did not improve from 1.56391\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.5475 - val_loss: 1.6850 - lr: 7.3970e-05\n",
      "Epoch 44/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5487\n",
      "Epoch 44: val_loss did not improve from 1.56391\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.5487 - val_loss: 1.6261 - lr: 7.3230e-05\n",
      "Epoch 45/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5415\n",
      "Epoch 45: val_loss did not improve from 1.56391\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 0.5404 - val_loss: 1.6121 - lr: 7.2498e-05\n",
      "Epoch 46/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5459\n",
      "Epoch 46: val_loss did not improve from 1.56391\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.5459 - val_loss: 1.5640 - lr: 7.1773e-05\n",
      "Epoch 47/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5476\n",
      "Epoch 47: val_loss did not improve from 1.56391\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.5476 - val_loss: 1.6018 - lr: 7.1055e-05\n",
      "Epoch 48/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5483\n",
      "Epoch 48: val_loss did not improve from 1.56391\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.5483 - val_loss: 1.7002 - lr: 7.0345e-05\n",
      "Epoch 49/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5435\n",
      "Epoch 49: val_loss improved from 1.56391 to 1.53150, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.5435 - val_loss: 1.5315 - lr: 6.9641e-05\n",
      "Epoch 50/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5375\n",
      "Epoch 50: val_loss did not improve from 1.53150\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.5375 - val_loss: 1.7262 - lr: 6.9641e-05\n",
      "Epoch 51/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5430\n",
      "Epoch 51: val_loss improved from 1.53150 to 1.50054, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.08.hdf5\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.5430 - val_loss: 1.5005 - lr: 6.8945e-05\n",
      "Epoch 52/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5409\n",
      "Epoch 52: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.5409 - val_loss: 1.5981 - lr: 6.8945e-05\n",
      "Epoch 53/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5334\n",
      "Epoch 53: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.5334 - val_loss: 1.5849 - lr: 6.8255e-05\n",
      "Epoch 54/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5316\n",
      "Epoch 54: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 7s 100ms/step - loss: 0.5316 - val_loss: 1.6660 - lr: 6.7573e-05\n",
      "Epoch 55/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5356\n",
      "Epoch 55: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.5356 - val_loss: 1.5632 - lr: 6.6897e-05\n",
      "Epoch 56/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5363\n",
      "Epoch 56: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.5363 - val_loss: 1.5324 - lr: 6.6228e-05\n",
      "Epoch 57/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5243\n",
      "Epoch 57: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.5243 - val_loss: 1.5953 - lr: 6.5566e-05\n",
      "Epoch 58/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5271\n",
      "Epoch 58: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.5271 - val_loss: 1.5261 - lr: 6.4910e-05\n",
      "Epoch 59/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5431\n",
      "Epoch 59: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.5431 - val_loss: 1.5353 - lr: 6.4261e-05\n",
      "Epoch 60/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5285\n",
      "Epoch 60: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.5287 - val_loss: 1.6504 - lr: 6.3619e-05\n",
      "Epoch 61/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5278\n",
      "Epoch 61: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.5278 - val_loss: 1.6169 - lr: 6.2982e-05\n",
      "Epoch 62/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5278\n",
      "Epoch 62: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.5278 - val_loss: 1.5451 - lr: 6.2353e-05\n",
      "Epoch 63/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5190\n",
      "Epoch 63: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.5190 - val_loss: 1.6532 - lr: 6.1729e-05\n",
      "Epoch 64/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5252\n",
      "Epoch 64: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.5252 - val_loss: 1.7268 - lr: 6.1112e-05\n",
      "Epoch 65/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5246\n",
      "Epoch 65: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.5246 - val_loss: 1.6180 - lr: 6.0501e-05\n",
      "Epoch 66/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5266\n",
      "Epoch 66: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.5266 - val_loss: 1.5263 - lr: 5.9896e-05\n",
      "Epoch 67/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5250\n",
      "Epoch 67: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 5.8703650938696225e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.5250 - val_loss: 1.5950 - lr: 5.9297e-05\n",
      "Epoch 68/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5161\n",
      "Epoch 68: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 5.811661550978897e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.5161 - val_loss: 1.5883 - lr: 5.8704e-05\n",
      "Epoch 69/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5069\n",
      "Epoch 69: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 5.753545068728272e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.5069 - val_loss: 1.5505 - lr: 5.8117e-05\n",
      "Epoch 70/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5137\n",
      "Epoch 70: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 5.6960095243994146e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.5137 - val_loss: 1.6457 - lr: 5.7535e-05\n",
      "Epoch 71/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5135\n",
      "Epoch 71: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 5.639049515593797e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.5135 - val_loss: 1.6352 - lr: 5.6960e-05\n",
      "Epoch 72/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5113\n",
      "Epoch 72: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 5.5826589195930865e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.5113 - val_loss: 1.5805 - lr: 5.6390e-05\n",
      "Epoch 73/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5007\n",
      "Epoch 73: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 5.526832333998754e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 0.5007 - val_loss: 1.6551 - lr: 5.5827e-05\n",
      "Epoch 74/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5103\n",
      "Epoch 74: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 5.471563996252371e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.5103 - val_loss: 1.7041 - lr: 5.5268e-05\n",
      "Epoch 75/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5147\n",
      "Epoch 75: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 5.416848503955407e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.5147 - val_loss: 1.5594 - lr: 5.4716e-05\n",
      "Epoch 76/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4958\n",
      "Epoch 76: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 5.362680094549433e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.4958 - val_loss: 1.6534 - lr: 5.4168e-05\n",
      "Epoch 77/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5202\n",
      "Epoch 77: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 5.309053365635918e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.5202 - val_loss: 1.5512 - lr: 5.3627e-05\n",
      "Epoch 78/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5195\n",
      "Epoch 78: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 5.2559629148163366e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.5195 - val_loss: 1.6481 - lr: 5.3091e-05\n",
      "Epoch 79/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5058\n",
      "Epoch 79: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 5.2034033396921584e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.5042 - val_loss: 1.6714 - lr: 5.2560e-05\n",
      "Epoch 80/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4988\n",
      "Epoch 80: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 5.1513692378648555e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 6s 96ms/step - loss: 0.4988 - val_loss: 1.6211 - lr: 5.2034e-05\n",
      "Epoch 81/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4940\n",
      "Epoch 81: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 5.099855567095801e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.4940 - val_loss: 1.6625 - lr: 5.1514e-05\n",
      "Epoch 82/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5139\n",
      "Epoch 82: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 5.048856924986467e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.5139 - val_loss: 1.6263 - lr: 5.0999e-05\n",
      "Epoch 83/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5013\n",
      "Epoch 83: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 4.998368269298226e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.5062 - val_loss: 1.5787 - lr: 5.0489e-05\n",
      "Epoch 84/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4984\n",
      "Epoch 84: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 4.9483845577924514e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.4984 - val_loss: 1.6085 - lr: 4.9984e-05\n",
      "Epoch 85/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4972\n",
      "Epoch 85: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 4.898900748230517e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.4972 - val_loss: 1.5791 - lr: 4.9484e-05\n",
      "Epoch 86/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5024\n",
      "Epoch 86: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 4.849911798373796e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.5024 - val_loss: 1.6286 - lr: 4.8989e-05\n",
      "Epoch 87/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4896\n",
      "Epoch 87: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 4.801412665983662e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.4902 - val_loss: 1.5256 - lr: 4.8499e-05\n",
      "Epoch 88/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5001\n",
      "Epoch 88: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 4.75339866898139e-05.\n",
      "66/66 [==============================] - 6s 98ms/step - loss: 0.5001 - val_loss: 1.5711 - lr: 4.8014e-05\n",
      "Epoch 89/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4979\n",
      "Epoch 89: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 4.705864765128354e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.4979 - val_loss: 1.5313 - lr: 4.7534e-05\n",
      "Epoch 90/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4965\n",
      "Epoch 90: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 4.658806272345828e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.4965 - val_loss: 1.5170 - lr: 4.7059e-05\n",
      "Epoch 91/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4987\n",
      "Epoch 91: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 4.612218148395186e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.4987 - val_loss: 1.6200 - lr: 4.6588e-05\n",
      "Epoch 92/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5042\n",
      "Epoch 92: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 4.566096071357606e-05.\n",
      "66/66 [==============================] - 7s 102ms/step - loss: 0.5042 - val_loss: 1.5611 - lr: 4.6122e-05\n",
      "Epoch 93/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4937\n",
      "Epoch 93: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 4.52043499899446e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.4937 - val_loss: 1.5651 - lr: 4.5661e-05\n",
      "Epoch 94/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.5014\n",
      "Epoch 94: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 4.475230609386927e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.5013 - val_loss: 1.6475 - lr: 4.5204e-05\n",
      "Epoch 95/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4942\n",
      "Epoch 95: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 4.43047822045628e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.4942 - val_loss: 1.6321 - lr: 4.4752e-05\n",
      "Epoch 96/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4891\n",
      "Epoch 96: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 4.386173510283697e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.4891 - val_loss: 1.5318 - lr: 4.4305e-05\n",
      "Epoch 97/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4890\n",
      "Epoch 97: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 4.342311796790454e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.4890 - val_loss: 1.6216 - lr: 4.3862e-05\n",
      "Epoch 98/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4917\n",
      "Epoch 98: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 4.298888758057728e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.4929 - val_loss: 1.5926 - lr: 4.3423e-05\n",
      "Epoch 99/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4915\n",
      "Epoch 99: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 4.2558997120067946e-05.\n",
      "66/66 [==============================] - 6s 98ms/step - loss: 0.4915 - val_loss: 1.6044 - lr: 4.2989e-05\n",
      "Epoch 100/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.4905\n",
      "Epoch 100: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 4.2133406968787315e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.4905 - val_loss: 1.6134 - lr: 4.2559e-05\n",
      "Epoch 101/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.4796\n",
      "Epoch 101: val_loss did not improve from 1.50054\n",
      "\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 4.1712073907547166e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.4789 - val_loss: 1.6305 - lr: 4.2133e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_34\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_35 (InputLayer)          [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_102 (LSTM)                (None, 45, 24)       3744        ['input_35[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_68 (Dropout)           (None, 45, 24)       0           ['lstm_102[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_103 (LSTM)                (None, 45, 16)       2624        ['dropout_68[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_69 (Dropout)           (None, 45, 16)       0           ['lstm_103[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_104 (LSTM)                (None, 32)           6272        ['dropout_69[0][0]']             \n",
      "                                                                                                  \n",
      " dense_68 (Dense)               (None, 40)           1320        ['lstm_104[0][0]']               \n",
      "                                                                                                  \n",
      " dense_69 (Dense)               (None, 5)            205         ['dense_68[0][0]']               \n",
      "                                                                                                  \n",
      " tf.unstack_34 (TFOpLambda)     [(None,),            0           ['dense_69[0][0]']               \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_170 (TFOpLambda  (None, 1)           0           ['tf.unstack_34[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_68 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_170[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_174 (TFOpLambda  (None, 1)           0           ['tf.unstack_34[0][4]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_102 (TFOpLamb  (None, 1)           0           ['tf.math.sigmoid_68[0][0]']     \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_69 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_174[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_103 (TFOpLamb  (None, 1)           0           ['tf.math.multiply_102[0][0]']   \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.expand_dims_171 (TFOpLambda  (None, 1)           0           ['tf.unstack_34[0][1]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_173 (TFOpLambda  (None, 1)           0           ['tf.unstack_34[0][3]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_104 (TFOpLamb  (None, 1)           0           ['tf.math.sigmoid_69[0][0]']     \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_68 (TFOpL  (None, 1)           0           ['tf.math.multiply_103[0][0]']   \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.softplus_68 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_171[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_172 (TFOpLambda  (None, 1)           0           ['tf.unstack_34[0][2]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.softplus_69 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_173[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_69 (TFOpL  (None, 1)           0           ['tf.math.multiply_104[0][0]']   \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack_34 (TFOpLambda)       (None, 5, 1)         0           ['tf.__operators__.add_68[0][0]',\n",
      "                                                                  'tf.math.softplus_68[0][0]',    \n",
      "                                                                  'tf.expand_dims_172[0][0]',     \n",
      "                                                                  'tf.math.softplus_69[0][0]',    \n",
      "                                                                  'tf.__operators__.add_69[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _3_CCMP_t+1_0.09\n",
      "Epoch 1/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 3.4517\n",
      "Epoch 1: val_loss improved from inf to 4.25499, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 11s 109ms/step - loss: 3.4517 - val_loss: 4.2550 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 2.7788\n",
      "Epoch 2: val_loss improved from 4.25499 to 3.62864, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.09.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 6s 91ms/step - loss: 2.7788 - val_loss: 3.6286 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.8476\n",
      "Epoch 3: val_loss improved from 3.62864 to 3.25360, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 1.8476 - val_loss: 3.2536 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 1.4487\n",
      "Epoch 4: val_loss improved from 3.25360 to 2.78886, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 6s 90ms/step - loss: 1.4465 - val_loss: 2.7889 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.2261\n",
      "Epoch 5: val_loss improved from 2.78886 to 2.57352, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 6s 92ms/step - loss: 1.2261 - val_loss: 2.5735 - lr: 1.0000e-04\n",
      "Epoch 6/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1016\n",
      "Epoch 6: val_loss improved from 2.57352 to 2.44666, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 7s 101ms/step - loss: 1.1016 - val_loss: 2.4467 - lr: 1.0000e-04\n",
      "Epoch 7/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0069\n",
      "Epoch 7: val_loss did not improve from 2.44666\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 7s 100ms/step - loss: 1.0069 - val_loss: 2.6738 - lr: 1.0000e-04\n",
      "Epoch 8/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9537\n",
      "Epoch 8: val_loss improved from 2.44666 to 2.32876, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 6s 99ms/step - loss: 0.9537 - val_loss: 2.3288 - lr: 9.9000e-05\n",
      "Epoch 9/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9057\n",
      "Epoch 9: val_loss did not improve from 2.32876\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 7s 101ms/step - loss: 0.9057 - val_loss: 2.3621 - lr: 9.9000e-05\n",
      "Epoch 10/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8692\n",
      "Epoch 10: val_loss improved from 2.32876 to 2.25430, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 7s 112ms/step - loss: 0.8692 - val_loss: 2.2543 - lr: 9.8010e-05\n",
      "Epoch 11/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8406\n",
      "Epoch 11: val_loss did not improve from 2.25430\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 0.8406 - val_loss: 2.2979 - lr: 9.8010e-05\n",
      "Epoch 12/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8143\n",
      "Epoch 12: val_loss did not improve from 2.25430\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 7s 100ms/step - loss: 0.8143 - val_loss: 2.4459 - lr: 9.7030e-05\n",
      "Epoch 13/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7895\n",
      "Epoch 13: val_loss did not improve from 2.25430\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 7s 100ms/step - loss: 0.7895 - val_loss: 2.4425 - lr: 9.6060e-05\n",
      "Epoch 14/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7832\n",
      "Epoch 14: val_loss did not improve from 2.25430\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 7s 102ms/step - loss: 0.7832 - val_loss: 2.3209 - lr: 9.5099e-05\n",
      "Epoch 15/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7586\n",
      "Epoch 15: val_loss did not improve from 2.25430\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 7s 101ms/step - loss: 0.7586 - val_loss: 2.5041 - lr: 9.4148e-05\n",
      "Epoch 16/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7479\n",
      "Epoch 16: val_loss did not improve from 2.25430\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 7s 100ms/step - loss: 0.7468 - val_loss: 2.6215 - lr: 9.3207e-05\n",
      "Epoch 17/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7281\n",
      "Epoch 17: val_loss improved from 2.25430 to 2.24856, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.09.hdf5\n",
      "66/66 [==============================] - 7s 101ms/step - loss: 0.7281 - val_loss: 2.2486 - lr: 9.2274e-05\n",
      "Epoch 18/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7248\n",
      "Epoch 18: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 7s 101ms/step - loss: 0.7248 - val_loss: 2.5738 - lr: 9.2274e-05\n",
      "Epoch 19/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.7226\n",
      "Epoch 19: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 7s 100ms/step - loss: 0.7211 - val_loss: 2.5473 - lr: 9.1352e-05\n",
      "Epoch 20/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7250\n",
      "Epoch 20: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 0.7250 - val_loss: 2.7467 - lr: 9.0438e-05\n",
      "Epoch 21/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7213\n",
      "Epoch 21: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 7s 100ms/step - loss: 0.7213 - val_loss: 2.3203 - lr: 8.9534e-05\n",
      "Epoch 22/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6987\n",
      "Epoch 22: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 7s 100ms/step - loss: 0.6987 - val_loss: 2.3788 - lr: 8.8638e-05\n",
      "Epoch 23/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7034\n",
      "Epoch 23: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 6s 98ms/step - loss: 0.7034 - val_loss: 2.3784 - lr: 8.7752e-05\n",
      "Epoch 24/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6955\n",
      "Epoch 24: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 7s 100ms/step - loss: 0.6955 - val_loss: 2.6337 - lr: 8.6875e-05\n",
      "Epoch 25/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6941\n",
      "Epoch 25: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 0.6941 - val_loss: 2.2549 - lr: 8.6006e-05\n",
      "Epoch 26/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6883\n",
      "Epoch 26: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.6883 - val_loss: 2.5786 - lr: 8.5146e-05\n",
      "Epoch 27/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6855\n",
      "Epoch 27: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 7s 104ms/step - loss: 0.6855 - val_loss: 2.2984 - lr: 8.4294e-05\n",
      "Epoch 28/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6819\n",
      "Epoch 28: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 0.6819 - val_loss: 2.4941 - lr: 8.3451e-05\n",
      "Epoch 29/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6873\n",
      "Epoch 29: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.6854 - val_loss: 2.5495 - lr: 8.2617e-05\n",
      "Epoch 30/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6726\n",
      "Epoch 30: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 0.6726 - val_loss: 2.5821 - lr: 8.1791e-05\n",
      "Epoch 31/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6640\n",
      "Epoch 31: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 6s 98ms/step - loss: 0.6640 - val_loss: 2.3770 - lr: 8.0973e-05\n",
      "Epoch 32/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6703\n",
      "Epoch 32: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 0.6703 - val_loss: 2.4106 - lr: 8.0163e-05\n",
      "Epoch 33/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6684\n",
      "Epoch 33: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 6s 98ms/step - loss: 0.6684 - val_loss: 2.6517 - lr: 7.9361e-05\n",
      "Epoch 34/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6596\n",
      "Epoch 34: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 6s 98ms/step - loss: 0.6596 - val_loss: 2.3474 - lr: 7.8568e-05\n",
      "Epoch 35/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6569\n",
      "Epoch 35: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.6569 - val_loss: 2.4996 - lr: 7.7782e-05\n",
      "Epoch 36/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6593\n",
      "Epoch 36: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 0.6593 - val_loss: 2.4193 - lr: 7.7004e-05\n",
      "Epoch 37/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6603\n",
      "Epoch 37: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 6s 98ms/step - loss: 0.6603 - val_loss: 2.4030 - lr: 7.6234e-05\n",
      "Epoch 38/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6478\n",
      "Epoch 38: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.6478 - val_loss: 2.5375 - lr: 7.5472e-05\n",
      "Epoch 39/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6422\n",
      "Epoch 39: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.6422 - val_loss: 2.4402 - lr: 7.4717e-05\n",
      "Epoch 40/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6444\n",
      "Epoch 40: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.6444 - val_loss: 2.4611 - lr: 7.3970e-05\n",
      "Epoch 41/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6340\n",
      "Epoch 41: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.6340 - val_loss: 2.3499 - lr: 7.3230e-05\n",
      "Epoch 42/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6444\n",
      "Epoch 42: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.6444 - val_loss: 2.4007 - lr: 7.2498e-05\n",
      "Epoch 43/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6354\n",
      "Epoch 43: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.6350 - val_loss: 2.4654 - lr: 7.1773e-05\n",
      "Epoch 44/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6442\n",
      "Epoch 44: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.6442 - val_loss: 2.3389 - lr: 7.1055e-05\n",
      "Epoch 45/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6340\n",
      "Epoch 45: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.6340 - val_loss: 2.4778 - lr: 7.0345e-05\n",
      "Epoch 46/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6314\n",
      "Epoch 46: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 6s 98ms/step - loss: 0.6314 - val_loss: 2.4282 - lr: 6.9641e-05\n",
      "Epoch 47/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6352\n",
      "Epoch 47: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.6352 - val_loss: 2.3074 - lr: 6.8945e-05\n",
      "Epoch 48/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6329\n",
      "Epoch 48: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.6329 - val_loss: 2.6639 - lr: 6.8255e-05\n",
      "Epoch 49/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6240\n",
      "Epoch 49: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.6240 - val_loss: 2.5423 - lr: 6.7573e-05\n",
      "Epoch 50/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6305\n",
      "Epoch 50: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.6305 - val_loss: 2.5868 - lr: 6.6897e-05\n",
      "Epoch 51/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6316\n",
      "Epoch 51: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.6316 - val_loss: 2.4986 - lr: 6.6228e-05\n",
      "Epoch 52/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6213\n",
      "Epoch 52: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 6s 98ms/step - loss: 0.6213 - val_loss: 2.3848 - lr: 6.5566e-05\n",
      "Epoch 53/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6185\n",
      "Epoch 53: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 6s 98ms/step - loss: 0.6185 - val_loss: 2.5365 - lr: 6.4910e-05\n",
      "Epoch 54/1000\n",
      "65/66 [============================>.] - ETA: 0s - loss: 0.6251\n",
      "Epoch 54: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 6s 97ms/step - loss: 0.6231 - val_loss: 2.4641 - lr: 6.4261e-05\n",
      "Epoch 55/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6203\n",
      "Epoch 55: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 6s 98ms/step - loss: 0.6203 - val_loss: 2.2761 - lr: 6.3619e-05\n",
      "Epoch 56/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6139\n",
      "Epoch 56: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 7s 100ms/step - loss: 0.6139 - val_loss: 2.2884 - lr: 6.2982e-05\n",
      "Epoch 57/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6127\n",
      "Epoch 57: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 0.6127 - val_loss: 2.4764 - lr: 6.2353e-05\n",
      "Epoch 58/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6096\n",
      "Epoch 58: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.6096 - val_loss: 2.3880 - lr: 6.1729e-05\n",
      "Epoch 59/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6143\n",
      "Epoch 59: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 6s 98ms/step - loss: 0.6143 - val_loss: 2.6235 - lr: 6.1112e-05\n",
      "Epoch 60/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6087\n",
      "Epoch 60: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 0.6087 - val_loss: 2.4716 - lr: 6.0501e-05\n",
      "Epoch 61/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6036\n",
      "Epoch 61: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.6036 - val_loss: 2.4243 - lr: 5.9896e-05\n",
      "Epoch 62/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6106\n",
      "Epoch 62: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 5.8703650938696225e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 0.6106 - val_loss: 2.4164 - lr: 5.9297e-05\n",
      "Epoch 63/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6140\n",
      "Epoch 63: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 5.811661550978897e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 0.6140 - val_loss: 2.4560 - lr: 5.8704e-05\n",
      "Epoch 64/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6013\n",
      "Epoch 64: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 5.753545068728272e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 0.6013 - val_loss: 2.2622 - lr: 5.8117e-05\n",
      "Epoch 65/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.5999\n",
      "Epoch 65: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 5.6960095243994146e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 0.5999 - val_loss: 2.4758 - lr: 5.7535e-05\n",
      "Epoch 66/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6063\n",
      "Epoch 66: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 5.639049515593797e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.6063 - val_loss: 2.3655 - lr: 5.6960e-05\n",
      "Epoch 67/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6003\n",
      "Epoch 67: val_loss did not improve from 2.24856\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 5.5826589195930865e-05.\n",
      "66/66 [==============================] - 6s 98ms/step - loss: 0.6003 - val_loss: 2.2984 - lr: 5.6390e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_35\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_36 (InputLayer)          [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_105 (LSTM)                (None, 45, 24)       3744        ['input_36[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_70 (Dropout)           (None, 45, 24)       0           ['lstm_105[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_106 (LSTM)                (None, 45, 16)       2624        ['dropout_70[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_71 (Dropout)           (None, 45, 16)       0           ['lstm_106[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_107 (LSTM)                (None, 32)           6272        ['dropout_71[0][0]']             \n",
      "                                                                                                  \n",
      " dense_70 (Dense)               (None, 40)           1320        ['lstm_107[0][0]']               \n",
      "                                                                                                  \n",
      " dense_71 (Dense)               (None, 5)            205         ['dense_70[0][0]']               \n",
      "                                                                                                  \n",
      " tf.unstack_35 (TFOpLambda)     [(None,),            0           ['dense_71[0][0]']               \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_175 (TFOpLambda  (None, 1)           0           ['tf.unstack_35[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_70 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_175[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_179 (TFOpLambda  (None, 1)           0           ['tf.unstack_35[0][4]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_105 (TFOpLamb  (None, 1)           0           ['tf.math.sigmoid_70[0][0]']     \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_71 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_179[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_106 (TFOpLamb  (None, 1)           0           ['tf.math.multiply_105[0][0]']   \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.expand_dims_176 (TFOpLambda  (None, 1)           0           ['tf.unstack_35[0][1]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_178 (TFOpLambda  (None, 1)           0           ['tf.unstack_35[0][3]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_107 (TFOpLamb  (None, 1)           0           ['tf.math.sigmoid_71[0][0]']     \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_70 (TFOpL  (None, 1)           0           ['tf.math.multiply_106[0][0]']   \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.softplus_70 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_176[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_177 (TFOpLambda  (None, 1)           0           ['tf.unstack_35[0][2]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.softplus_71 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_178[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_71 (TFOpL  (None, 1)           0           ['tf.math.multiply_107[0][0]']   \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack_35 (TFOpLambda)       (None, 5, 1)         0           ['tf.__operators__.add_70[0][0]',\n",
      "                                                                  'tf.math.softplus_70[0][0]',    \n",
      "                                                                  'tf.expand_dims_177[0][0]',     \n",
      "                                                                  'tf.math.softplus_71[0][0]',    \n",
      "                                                                  'tf.__operators__.add_71[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _3_CCMP_t+1_0.1\n",
      "Epoch 1/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 3.4184\n",
      "Epoch 1: val_loss improved from inf to 4.05631, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 11s 109ms/step - loss: 3.4184 - val_loss: 4.0563 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 2.8462\n",
      "Epoch 2: val_loss improved from 4.05631 to 3.46674, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.1.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 7s 106ms/step - loss: 2.8462 - val_loss: 3.4667 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.8603\n",
      "Epoch 3: val_loss improved from 3.46674 to 2.87634, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 7s 101ms/step - loss: 1.8603 - val_loss: 2.8763 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.3536\n",
      "Epoch 4: val_loss improved from 2.87634 to 2.46185, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 7s 100ms/step - loss: 1.3536 - val_loss: 2.4618 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1445\n",
      "Epoch 5: val_loss improved from 2.46185 to 2.23587, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 7s 101ms/step - loss: 1.1445 - val_loss: 2.2359 - lr: 1.0000e-04\n",
      "Epoch 6/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0675\n",
      "Epoch 6: val_loss improved from 2.23587 to 2.20903, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 7s 110ms/step - loss: 1.0675 - val_loss: 2.2090 - lr: 1.0000e-04\n",
      "Epoch 7/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0064\n",
      "Epoch 7: val_loss improved from 2.20903 to 2.05318, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 7s 109ms/step - loss: 1.0064 - val_loss: 2.0532 - lr: 1.0000e-04\n",
      "Epoch 8/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9711\n",
      "Epoch 8: val_loss improved from 2.05318 to 2.00212, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 7s 107ms/step - loss: 0.9711 - val_loss: 2.0021 - lr: 1.0000e-04\n",
      "Epoch 9/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9543\n",
      "Epoch 9: val_loss did not improve from 2.00212\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 0.9543 - val_loss: 2.0889 - lr: 1.0000e-04\n",
      "Epoch 10/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9441\n",
      "Epoch 10: val_loss did not improve from 2.00212\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 7s 107ms/step - loss: 0.9441 - val_loss: 2.1328 - lr: 9.9000e-05\n",
      "Epoch 11/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9333\n",
      "Epoch 11: val_loss did not improve from 2.00212\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 0.9333 - val_loss: 2.0569 - lr: 9.8010e-05\n",
      "Epoch 12/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9273\n",
      "Epoch 12: val_loss improved from 2.00212 to 1.97243, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 7s 108ms/step - loss: 0.9273 - val_loss: 1.9724 - lr: 9.7030e-05\n",
      "Epoch 13/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9117\n",
      "Epoch 13: val_loss did not improve from 1.97243\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 0.9117 - val_loss: 2.1611 - lr: 9.7030e-05\n",
      "Epoch 14/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9108\n",
      "Epoch 14: val_loss did not improve from 1.97243\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 7s 111ms/step - loss: 0.9108 - val_loss: 2.1348 - lr: 9.6060e-05\n",
      "Epoch 15/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8907\n",
      "Epoch 15: val_loss did not improve from 1.97243\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 7s 108ms/step - loss: 0.8907 - val_loss: 2.0182 - lr: 9.5099e-05\n",
      "Epoch 16/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8915\n",
      "Epoch 16: val_loss did not improve from 1.97243\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 0.8915 - val_loss: 2.0057 - lr: 9.4148e-05\n",
      "Epoch 17/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8887\n",
      "Epoch 17: val_loss did not improve from 1.97243\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 7s 109ms/step - loss: 0.8887 - val_loss: 2.0771 - lr: 9.3207e-05\n",
      "Epoch 18/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8584\n",
      "Epoch 18: val_loss did not improve from 1.97243\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 7s 107ms/step - loss: 0.8584 - val_loss: 2.1633 - lr: 9.2274e-05\n",
      "Epoch 19/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8437\n",
      "Epoch 19: val_loss did not improve from 1.97243\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 7s 108ms/step - loss: 0.8437 - val_loss: 2.1593 - lr: 9.1352e-05\n",
      "Epoch 20/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8334\n",
      "Epoch 20: val_loss did not improve from 1.97243\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 7s 109ms/step - loss: 0.8334 - val_loss: 2.0166 - lr: 9.0438e-05\n",
      "Epoch 21/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8249\n",
      "Epoch 21: val_loss did not improve from 1.97243\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 7s 108ms/step - loss: 0.8249 - val_loss: 2.1662 - lr: 8.9534e-05\n",
      "Epoch 22/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8192\n",
      "Epoch 22: val_loss did not improve from 1.97243\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 7s 108ms/step - loss: 0.8192 - val_loss: 2.0669 - lr: 8.8638e-05\n",
      "Epoch 23/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8241\n",
      "Epoch 23: val_loss did not improve from 1.97243\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 0.8241 - val_loss: 2.1256 - lr: 8.7752e-05\n",
      "Epoch 24/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7992\n",
      "Epoch 24: val_loss improved from 1.97243 to 1.94327, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.1.hdf5\n",
      "66/66 [==============================] - 7s 107ms/step - loss: 0.7992 - val_loss: 1.9433 - lr: 8.6875e-05\n",
      "Epoch 25/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7905\n",
      "Epoch 25: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.7905 - val_loss: 2.0964 - lr: 8.6875e-05\n",
      "Epoch 26/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7894\n",
      "Epoch 26: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 0.7894 - val_loss: 2.1138 - lr: 8.6006e-05\n",
      "Epoch 27/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7845\n",
      "Epoch 27: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 7s 107ms/step - loss: 0.7845 - val_loss: 2.1695 - lr: 8.5146e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7778\n",
      "Epoch 28: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.7778 - val_loss: 2.1469 - lr: 8.4294e-05\n",
      "Epoch 29/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7716\n",
      "Epoch 29: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 0.7716 - val_loss: 2.1127 - lr: 8.3451e-05\n",
      "Epoch 30/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7657\n",
      "Epoch 30: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.7657 - val_loss: 2.0954 - lr: 8.2617e-05\n",
      "Epoch 31/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7768\n",
      "Epoch 31: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 7s 109ms/step - loss: 0.7768 - val_loss: 2.1639 - lr: 8.1791e-05\n",
      "Epoch 32/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7497\n",
      "Epoch 32: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 0.7497 - val_loss: 2.4196 - lr: 8.0973e-05\n",
      "Epoch 33/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7611\n",
      "Epoch 33: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.7611 - val_loss: 2.3319 - lr: 8.0163e-05\n",
      "Epoch 34/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7586\n",
      "Epoch 34: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.7586 - val_loss: 2.1952 - lr: 7.9361e-05\n",
      "Epoch 35/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7481\n",
      "Epoch 35: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.7481 - val_loss: 2.2805 - lr: 7.8568e-05\n",
      "Epoch 36/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7424\n",
      "Epoch 36: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 0.7424 - val_loss: 2.2254 - lr: 7.7782e-05\n",
      "Epoch 37/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7407\n",
      "Epoch 37: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.7407 - val_loss: 2.1646 - lr: 7.7004e-05\n",
      "Epoch 38/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7415\n",
      "Epoch 38: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.7415 - val_loss: 2.2538 - lr: 7.6234e-05\n",
      "Epoch 39/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7385\n",
      "Epoch 39: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.7385 - val_loss: 2.3891 - lr: 7.5472e-05\n",
      "Epoch 40/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7298\n",
      "Epoch 40: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 7s 110ms/step - loss: 0.7298 - val_loss: 2.3464 - lr: 7.4717e-05\n",
      "Epoch 41/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7298\n",
      "Epoch 41: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 0.7298 - val_loss: 2.1800 - lr: 7.3970e-05\n",
      "Epoch 42/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7267\n",
      "Epoch 42: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 0.7267 - val_loss: 2.2636 - lr: 7.3230e-05\n",
      "Epoch 43/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7253\n",
      "Epoch 43: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 7s 107ms/step - loss: 0.7253 - val_loss: 2.3413 - lr: 7.2498e-05\n",
      "Epoch 44/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7252\n",
      "Epoch 44: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 7s 108ms/step - loss: 0.7252 - val_loss: 2.2801 - lr: 7.1773e-05\n",
      "Epoch 45/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7224\n",
      "Epoch 45: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 7s 111ms/step - loss: 0.7224 - val_loss: 2.0544 - lr: 7.1055e-05\n",
      "Epoch 46/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7232\n",
      "Epoch 46: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 7s 110ms/step - loss: 0.7232 - val_loss: 2.3078 - lr: 7.0345e-05\n",
      "Epoch 47/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7053\n",
      "Epoch 47: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 7s 107ms/step - loss: 0.7053 - val_loss: 2.2749 - lr: 6.9641e-05\n",
      "Epoch 48/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7138\n",
      "Epoch 48: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 7s 113ms/step - loss: 0.7138 - val_loss: 2.1908 - lr: 6.8945e-05\n",
      "Epoch 49/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7208\n",
      "Epoch 49: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 7s 109ms/step - loss: 0.7208 - val_loss: 2.0897 - lr: 6.8255e-05\n",
      "Epoch 50/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7082\n",
      "Epoch 50: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 7s 109ms/step - loss: 0.7082 - val_loss: 2.1249 - lr: 6.7573e-05\n",
      "Epoch 51/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7180\n",
      "Epoch 51: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 7s 110ms/step - loss: 0.7180 - val_loss: 2.1985 - lr: 6.6897e-05\n",
      "Epoch 52/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6974\n",
      "Epoch 52: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 7s 108ms/step - loss: 0.6974 - val_loss: 2.2921 - lr: 6.6228e-05\n",
      "Epoch 53/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7159\n",
      "Epoch 53: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 7s 111ms/step - loss: 0.7159 - val_loss: 2.4148 - lr: 6.5566e-05\n",
      "Epoch 54/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7014\n",
      "Epoch 54: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 7s 106ms/step - loss: 0.7014 - val_loss: 2.0984 - lr: 6.4910e-05\n",
      "Epoch 55/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6957\n",
      "Epoch 55: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 7s 109ms/step - loss: 0.6957 - val_loss: 2.3004 - lr: 6.4261e-05\n",
      "Epoch 56/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7035\n",
      "Epoch 56: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 7s 108ms/step - loss: 0.7035 - val_loss: 2.1078 - lr: 6.3619e-05\n",
      "Epoch 57/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7012\n",
      "Epoch 57: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 7s 107ms/step - loss: 0.7012 - val_loss: 2.1826 - lr: 6.2982e-05\n",
      "Epoch 58/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6997\n",
      "Epoch 58: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 0.6997 - val_loss: 2.0425 - lr: 6.2353e-05\n",
      "Epoch 59/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6842\n",
      "Epoch 59: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 0.6842 - val_loss: 2.1920 - lr: 6.1729e-05\n",
      "Epoch 60/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6932\n",
      "Epoch 60: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 0.6932 - val_loss: 2.2342 - lr: 6.1112e-05\n",
      "Epoch 61/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6886\n",
      "Epoch 61: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.6886 - val_loss: 2.1199 - lr: 6.0501e-05\n",
      "Epoch 62/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6920\n",
      "Epoch 62: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.6920 - val_loss: 2.2438 - lr: 5.9896e-05\n",
      "Epoch 63/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6917\n",
      "Epoch 63: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 5.8703650938696225e-05.\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.6917 - val_loss: 2.1602 - lr: 5.9297e-05\n",
      "Epoch 64/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6892\n",
      "Epoch 64: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 5.811661550978897e-05.\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.6892 - val_loss: 2.1275 - lr: 5.8704e-05\n",
      "Epoch 65/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6905\n",
      "Epoch 65: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 5.753545068728272e-05.\n",
      "66/66 [==============================] - 7s 108ms/step - loss: 0.6905 - val_loss: 2.1423 - lr: 5.8117e-05\n",
      "Epoch 66/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6969\n",
      "Epoch 66: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 5.6960095243994146e-05.\n",
      "66/66 [==============================] - 7s 104ms/step - loss: 0.6969 - val_loss: 2.3079 - lr: 5.7535e-05\n",
      "Epoch 67/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6972\n",
      "Epoch 67: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 5.639049515593797e-05.\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.6972 - val_loss: 1.9934 - lr: 5.6960e-05\n",
      "Epoch 68/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6854\n",
      "Epoch 68: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 5.5826589195930865e-05.\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.6854 - val_loss: 2.2691 - lr: 5.6390e-05\n",
      "Epoch 69/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6775\n",
      "Epoch 69: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 5.526832333998754e-05.\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 0.6775 - val_loss: 2.0540 - lr: 5.5827e-05\n",
      "Epoch 70/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6755\n",
      "Epoch 70: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 5.471563996252371e-05.\n",
      "66/66 [==============================] - 7s 107ms/step - loss: 0.6755 - val_loss: 1.9658 - lr: 5.5268e-05\n",
      "Epoch 71/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6807\n",
      "Epoch 71: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 5.416848503955407e-05.\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 0.6807 - val_loss: 1.9955 - lr: 5.4716e-05\n",
      "Epoch 72/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6780\n",
      "Epoch 72: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 5.362680094549433e-05.\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 0.6780 - val_loss: 2.0543 - lr: 5.4168e-05\n",
      "Epoch 73/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6754\n",
      "Epoch 73: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 5.309053365635918e-05.\n",
      "66/66 [==============================] - 7s 107ms/step - loss: 0.6754 - val_loss: 2.0658 - lr: 5.3627e-05\n",
      "Epoch 74/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.6687\n",
      "Epoch 74: val_loss did not improve from 1.94327\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 5.2559629148163366e-05.\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.6687 - val_loss: 1.9880 - lr: 5.3091e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_36\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_37 (InputLayer)          [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_108 (LSTM)                (None, 45, 24)       3744        ['input_37[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_72 (Dropout)           (None, 45, 24)       0           ['lstm_108[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_109 (LSTM)                (None, 45, 16)       2624        ['dropout_72[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_73 (Dropout)           (None, 45, 16)       0           ['lstm_109[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_110 (LSTM)                (None, 32)           6272        ['dropout_73[0][0]']             \n",
      "                                                                                                  \n",
      " dense_72 (Dense)               (None, 40)           1320        ['lstm_110[0][0]']               \n",
      "                                                                                                  \n",
      " dense_73 (Dense)               (None, 5)            205         ['dense_72[0][0]']               \n",
      "                                                                                                  \n",
      " tf.unstack_36 (TFOpLambda)     [(None,),            0           ['dense_73[0][0]']               \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_180 (TFOpLambda  (None, 1)           0           ['tf.unstack_36[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_72 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_180[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_184 (TFOpLambda  (None, 1)           0           ['tf.unstack_36[0][4]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_108 (TFOpLamb  (None, 1)           0           ['tf.math.sigmoid_72[0][0]']     \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_73 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_184[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_109 (TFOpLamb  (None, 1)           0           ['tf.math.multiply_108[0][0]']   \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.expand_dims_181 (TFOpLambda  (None, 1)           0           ['tf.unstack_36[0][1]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_183 (TFOpLambda  (None, 1)           0           ['tf.unstack_36[0][3]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_110 (TFOpLamb  (None, 1)           0           ['tf.math.sigmoid_73[0][0]']     \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_72 (TFOpL  (None, 1)           0           ['tf.math.multiply_109[0][0]']   \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.softplus_72 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_181[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_182 (TFOpLambda  (None, 1)           0           ['tf.unstack_36[0][2]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.softplus_73 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_183[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_73 (TFOpL  (None, 1)           0           ['tf.math.multiply_110[0][0]']   \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack_36 (TFOpLambda)       (None, 5, 1)         0           ['tf.__operators__.add_72[0][0]',\n",
      "                                                                  'tf.math.softplus_72[0][0]',    \n",
      "                                                                  'tf.expand_dims_182[0][0]',     \n",
      "                                                                  'tf.math.softplus_73[0][0]',    \n",
      "                                                                  'tf.__operators__.add_73[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _3_CCMP_t+1_0.11\n",
      "Epoch 1/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 3.4565\n",
      "Epoch 1: val_loss improved from inf to 4.49159, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.11.hdf5\n",
      "66/66 [==============================] - 12s 112ms/step - loss: 3.4565 - val_loss: 4.4916 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 2.8559\n",
      "Epoch 2: val_loss improved from 4.49159 to 3.60067, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.11.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 7s 103ms/step - loss: 2.8559 - val_loss: 3.6007 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.6675\n",
      "Epoch 3: val_loss improved from 3.60067 to 3.37359, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.11.hdf5\n",
      "66/66 [==============================] - 7s 101ms/step - loss: 1.6675 - val_loss: 3.3736 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.3376\n",
      "Epoch 4: val_loss did not improve from 3.37359\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 7s 100ms/step - loss: 1.3376 - val_loss: 3.3815 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.2166\n",
      "Epoch 5: val_loss improved from 3.37359 to 3.33960, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.11.hdf5\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 1.2166 - val_loss: 3.3396 - lr: 9.9000e-05\n",
      "Epoch 6/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1435\n",
      "Epoch 6: val_loss improved from 3.33960 to 3.32769, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.11.hdf5\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 1.1435 - val_loss: 3.3277 - lr: 9.9000e-05\n",
      "Epoch 7/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1036\n",
      "Epoch 7: val_loss improved from 3.32769 to 3.27694, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.11.hdf5\n",
      "66/66 [==============================] - 7s 102ms/step - loss: 1.1036 - val_loss: 3.2769 - lr: 9.9000e-05\n",
      "Epoch 8/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0815\n",
      "Epoch 8: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 7s 103ms/step - loss: 1.0815 - val_loss: 3.5255 - lr: 9.9000e-05\n",
      "Epoch 9/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0454\n",
      "Epoch 9: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 8s 114ms/step - loss: 1.0454 - val_loss: 3.6276 - lr: 9.8010e-05\n",
      "Epoch 10/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0307\n",
      "Epoch 10: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 7s 108ms/step - loss: 1.0307 - val_loss: 3.5008 - lr: 9.7030e-05\n",
      "Epoch 11/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0143\n",
      "Epoch 11: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 7s 110ms/step - loss: 1.0143 - val_loss: 4.0566 - lr: 9.6060e-05\n",
      "Epoch 12/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9918\n",
      "Epoch 12: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 7s 108ms/step - loss: 0.9918 - val_loss: 4.1511 - lr: 9.5099e-05\n",
      "Epoch 13/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9775\n",
      "Epoch 13: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 7s 107ms/step - loss: 0.9775 - val_loss: 4.1490 - lr: 9.4148e-05\n",
      "Epoch 14/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9622\n",
      "Epoch 14: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 7s 108ms/step - loss: 0.9622 - val_loss: 4.4498 - lr: 9.3207e-05\n",
      "Epoch 15/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9582\n",
      "Epoch 15: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.9582 - val_loss: 3.7707 - lr: 9.2274e-05\n",
      "Epoch 16/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9426\n",
      "Epoch 16: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 0.9426 - val_loss: 4.1505 - lr: 9.1352e-05\n",
      "Epoch 17/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9476\n",
      "Epoch 17: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 7s 107ms/step - loss: 0.9476 - val_loss: 4.5373 - lr: 9.0438e-05\n",
      "Epoch 18/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9035\n",
      "Epoch 18: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 7s 103ms/step - loss: 0.9035 - val_loss: 4.5080 - lr: 8.9534e-05\n",
      "Epoch 19/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9041\n",
      "Epoch 19: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 7s 107ms/step - loss: 0.9041 - val_loss: 4.9324 - lr: 8.8638e-05\n",
      "Epoch 20/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8836\n",
      "Epoch 20: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 7s 104ms/step - loss: 0.8836 - val_loss: 4.7334 - lr: 8.7752e-05\n",
      "Epoch 21/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8777\n",
      "Epoch 21: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 0.8777 - val_loss: 4.7526 - lr: 8.6875e-05\n",
      "Epoch 22/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8592\n",
      "Epoch 22: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.8592 - val_loss: 4.1087 - lr: 8.6006e-05\n",
      "Epoch 23/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8834\n",
      "Epoch 23: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 7s 104ms/step - loss: 0.8834 - val_loss: 4.7890 - lr: 8.5146e-05\n",
      "Epoch 24/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8422\n",
      "Epoch 24: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.8422 - val_loss: 4.5544 - lr: 8.4294e-05\n",
      "Epoch 25/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8334\n",
      "Epoch 25: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 7s 108ms/step - loss: 0.8334 - val_loss: 4.5924 - lr: 8.3451e-05\n",
      "Epoch 26/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8257\n",
      "Epoch 26: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 7s 103ms/step - loss: 0.8257 - val_loss: 4.7173 - lr: 8.2617e-05\n",
      "Epoch 27/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8069\n",
      "Epoch 27: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 7s 102ms/step - loss: 0.8069 - val_loss: 4.7319 - lr: 8.1791e-05\n",
      "Epoch 28/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8150\n",
      "Epoch 28: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 7s 102ms/step - loss: 0.8150 - val_loss: 4.3700 - lr: 8.0973e-05\n",
      "Epoch 29/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8082\n",
      "Epoch 29: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 7s 103ms/step - loss: 0.8082 - val_loss: 4.5195 - lr: 8.0163e-05\n",
      "Epoch 30/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8010\n",
      "Epoch 30: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 7s 102ms/step - loss: 0.8010 - val_loss: 4.4077 - lr: 7.9361e-05\n",
      "Epoch 31/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8047\n",
      "Epoch 31: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 7s 103ms/step - loss: 0.8047 - val_loss: 4.4866 - lr: 7.8568e-05\n",
      "Epoch 32/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7761\n",
      "Epoch 32: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 7s 102ms/step - loss: 0.7761 - val_loss: 4.9028 - lr: 7.7782e-05\n",
      "Epoch 33/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7909\n",
      "Epoch 33: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 7s 102ms/step - loss: 0.7909 - val_loss: 4.3916 - lr: 7.7004e-05\n",
      "Epoch 34/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7694\n",
      "Epoch 34: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 7s 103ms/step - loss: 0.7694 - val_loss: 4.2142 - lr: 7.6234e-05\n",
      "Epoch 35/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7738\n",
      "Epoch 35: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 7s 102ms/step - loss: 0.7738 - val_loss: 4.2354 - lr: 7.5472e-05\n",
      "Epoch 36/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7700\n",
      "Epoch 36: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 7s 101ms/step - loss: 0.7700 - val_loss: 4.1062 - lr: 7.4717e-05\n",
      "Epoch 37/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7699\n",
      "Epoch 37: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 7s 102ms/step - loss: 0.7699 - val_loss: 4.0071 - lr: 7.3970e-05\n",
      "Epoch 38/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7638\n",
      "Epoch 38: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.7638 - val_loss: 4.3730 - lr: 7.3230e-05\n",
      "Epoch 39/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7645\n",
      "Epoch 39: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 7s 107ms/step - loss: 0.7645 - val_loss: 4.4553 - lr: 7.2498e-05\n",
      "Epoch 40/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7570\n",
      "Epoch 40: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.7570 - val_loss: 4.2940 - lr: 7.1773e-05\n",
      "Epoch 41/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7597\n",
      "Epoch 41: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.7597 - val_loss: 4.3342 - lr: 7.1055e-05\n",
      "Epoch 42/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7476\n",
      "Epoch 42: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 7s 108ms/step - loss: 0.7476 - val_loss: 4.1041 - lr: 7.0345e-05\n",
      "Epoch 43/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7537\n",
      "Epoch 43: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 7s 108ms/step - loss: 0.7537 - val_loss: 3.9999 - lr: 6.9641e-05\n",
      "Epoch 44/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7443\n",
      "Epoch 44: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 7s 104ms/step - loss: 0.7443 - val_loss: 3.8639 - lr: 6.8945e-05\n",
      "Epoch 45/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7466\n",
      "Epoch 45: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.7466 - val_loss: 3.9292 - lr: 6.8255e-05\n",
      "Epoch 46/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7382\n",
      "Epoch 46: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.7382 - val_loss: 4.1769 - lr: 6.7573e-05\n",
      "Epoch 47/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7436\n",
      "Epoch 47: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 7s 103ms/step - loss: 0.7436 - val_loss: 3.9673 - lr: 6.6897e-05\n",
      "Epoch 48/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7369\n",
      "Epoch 48: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 7s 104ms/step - loss: 0.7369 - val_loss: 4.1798 - lr: 6.6228e-05\n",
      "Epoch 49/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7367\n",
      "Epoch 49: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 7s 104ms/step - loss: 0.7367 - val_loss: 3.9986 - lr: 6.5566e-05\n",
      "Epoch 50/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7312\n",
      "Epoch 50: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 7s 104ms/step - loss: 0.7312 - val_loss: 4.1167 - lr: 6.4910e-05\n",
      "Epoch 51/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7369\n",
      "Epoch 51: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 7s 103ms/step - loss: 0.7369 - val_loss: 3.8949 - lr: 6.4261e-05\n",
      "Epoch 52/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7336\n",
      "Epoch 52: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 7s 103ms/step - loss: 0.7336 - val_loss: 4.1649 - lr: 6.3619e-05\n",
      "Epoch 53/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7265\n",
      "Epoch 53: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 7s 102ms/step - loss: 0.7265 - val_loss: 3.8187 - lr: 6.2982e-05\n",
      "Epoch 54/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7287\n",
      "Epoch 54: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 7s 104ms/step - loss: 0.7287 - val_loss: 3.8621 - lr: 6.2353e-05\n",
      "Epoch 55/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7345\n",
      "Epoch 55: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 7s 102ms/step - loss: 0.7345 - val_loss: 3.6897 - lr: 6.1729e-05\n",
      "Epoch 56/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7128\n",
      "Epoch 56: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 7s 103ms/step - loss: 0.7128 - val_loss: 3.9343 - lr: 6.1112e-05\n",
      "Epoch 57/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7297\n",
      "Epoch 57: val_loss did not improve from 3.27694\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 7s 103ms/step - loss: 0.7297 - val_loss: 3.7823 - lr: 6.0501e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_37\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_38 (InputLayer)          [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_111 (LSTM)                (None, 45, 24)       3744        ['input_38[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_74 (Dropout)           (None, 45, 24)       0           ['lstm_111[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_112 (LSTM)                (None, 45, 16)       2624        ['dropout_74[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_75 (Dropout)           (None, 45, 16)       0           ['lstm_112[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_113 (LSTM)                (None, 32)           6272        ['dropout_75[0][0]']             \n",
      "                                                                                                  \n",
      " dense_74 (Dense)               (None, 40)           1320        ['lstm_113[0][0]']               \n",
      "                                                                                                  \n",
      " dense_75 (Dense)               (None, 5)            205         ['dense_74[0][0]']               \n",
      "                                                                                                  \n",
      " tf.unstack_37 (TFOpLambda)     [(None,),            0           ['dense_75[0][0]']               \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_185 (TFOpLambda  (None, 1)           0           ['tf.unstack_37[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_74 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_185[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_189 (TFOpLambda  (None, 1)           0           ['tf.unstack_37[0][4]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_111 (TFOpLamb  (None, 1)           0           ['tf.math.sigmoid_74[0][0]']     \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_75 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_189[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_112 (TFOpLamb  (None, 1)           0           ['tf.math.multiply_111[0][0]']   \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.expand_dims_186 (TFOpLambda  (None, 1)           0           ['tf.unstack_37[0][1]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_188 (TFOpLambda  (None, 1)           0           ['tf.unstack_37[0][3]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_113 (TFOpLamb  (None, 1)           0           ['tf.math.sigmoid_75[0][0]']     \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_74 (TFOpL  (None, 1)           0           ['tf.math.multiply_112[0][0]']   \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.softplus_74 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_186[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_187 (TFOpLambda  (None, 1)           0           ['tf.unstack_37[0][2]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.softplus_75 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_188[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_75 (TFOpL  (None, 1)           0           ['tf.math.multiply_113[0][0]']   \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack_37 (TFOpLambda)       (None, 5, 1)         0           ['tf.__operators__.add_74[0][0]',\n",
      "                                                                  'tf.math.softplus_74[0][0]',    \n",
      "                                                                  'tf.expand_dims_187[0][0]',     \n",
      "                                                                  'tf.math.softplus_75[0][0]',    \n",
      "                                                                  'tf.__operators__.add_75[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _3_CCMP_t+1_0.12\n",
      "Epoch 1/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 3.4424\n",
      "Epoch 1: val_loss improved from inf to 4.01403, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 12s 117ms/step - loss: 3.4424 - val_loss: 4.0140 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 2.9616\n",
      "Epoch 2: val_loss improved from 4.01403 to 3.67117, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.12.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 7s 102ms/step - loss: 2.9616 - val_loss: 3.6712 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 2.2253\n",
      "Epoch 3: val_loss improved from 3.67117 to 3.18113, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 7s 100ms/step - loss: 2.2253 - val_loss: 3.1811 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.6218\n",
      "Epoch 4: val_loss improved from 3.18113 to 2.65723, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 7s 100ms/step - loss: 1.6218 - val_loss: 2.6572 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.3361\n",
      "Epoch 5: val_loss improved from 2.65723 to 2.41503, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 7s 100ms/step - loss: 1.3361 - val_loss: 2.4150 - lr: 1.0000e-04\n",
      "Epoch 6/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.2072\n",
      "Epoch 6: val_loss improved from 2.41503 to 2.38992, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 7s 100ms/step - loss: 1.2072 - val_loss: 2.3899 - lr: 1.0000e-04\n",
      "Epoch 7/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1392\n",
      "Epoch 7: val_loss did not improve from 2.38992\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 7s 100ms/step - loss: 1.1392 - val_loss: 2.5564 - lr: 1.0000e-04\n",
      "Epoch 8/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0796\n",
      "Epoch 8: val_loss improved from 2.38992 to 2.37385, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.12.hdf5\n",
      "66/66 [==============================] - 7s 100ms/step - loss: 1.0796 - val_loss: 2.3738 - lr: 9.9000e-05\n",
      "Epoch 9/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0539\n",
      "Epoch 9: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 1.0539 - val_loss: 2.8153 - lr: 9.9000e-05\n",
      "Epoch 10/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0142\n",
      "Epoch 10: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 7s 109ms/step - loss: 1.0142 - val_loss: 2.5577 - lr: 9.8010e-05\n",
      "Epoch 11/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9925\n",
      "Epoch 11: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 8s 114ms/step - loss: 0.9925 - val_loss: 2.5016 - lr: 9.7030e-05\n",
      "Epoch 12/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9632\n",
      "Epoch 12: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 8s 116ms/step - loss: 0.9632 - val_loss: 2.6591 - lr: 9.6060e-05\n",
      "Epoch 13/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9427\n",
      "Epoch 13: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 7s 109ms/step - loss: 0.9427 - val_loss: 2.5874 - lr: 9.5099e-05\n",
      "Epoch 14/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9335\n",
      "Epoch 14: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 7s 110ms/step - loss: 0.9335 - val_loss: 2.4448 - lr: 9.4148e-05\n",
      "Epoch 15/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9256\n",
      "Epoch 15: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 7s 111ms/step - loss: 0.9256 - val_loss: 2.8233 - lr: 9.3207e-05\n",
      "Epoch 16/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9096\n",
      "Epoch 16: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 7s 108ms/step - loss: 0.9096 - val_loss: 2.9915 - lr: 9.2274e-05\n",
      "Epoch 17/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8868\n",
      "Epoch 17: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 7s 110ms/step - loss: 0.8868 - val_loss: 2.6490 - lr: 9.1352e-05\n",
      "Epoch 18/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8888\n",
      "Epoch 18: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 7s 108ms/step - loss: 0.8888 - val_loss: 2.5659 - lr: 9.0438e-05\n",
      "Epoch 19/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8828\n",
      "Epoch 19: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 7s 111ms/step - loss: 0.8828 - val_loss: 2.9753 - lr: 8.9534e-05\n",
      "Epoch 20/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8781\n",
      "Epoch 20: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 7s 108ms/step - loss: 0.8781 - val_loss: 2.6757 - lr: 8.8638e-05\n",
      "Epoch 21/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8692\n",
      "Epoch 21: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 7s 108ms/step - loss: 0.8692 - val_loss: 2.9241 - lr: 8.7752e-05\n",
      "Epoch 22/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8738\n",
      "Epoch 22: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 7s 107ms/step - loss: 0.8738 - val_loss: 2.8963 - lr: 8.6875e-05\n",
      "Epoch 23/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8557\n",
      "Epoch 23: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 7s 107ms/step - loss: 0.8557 - val_loss: 2.7385 - lr: 8.6006e-05\n",
      "Epoch 24/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8520\n",
      "Epoch 24: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 0.8520 - val_loss: 2.5044 - lr: 8.5146e-05\n",
      "Epoch 25/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8593\n",
      "Epoch 25: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 0.8593 - val_loss: 2.8368 - lr: 8.4294e-05\n",
      "Epoch 26/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8579\n",
      "Epoch 26: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 0.8579 - val_loss: 2.4820 - lr: 8.3451e-05\n",
      "Epoch 27/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8399\n",
      "Epoch 27: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 0.8399 - val_loss: 2.8183 - lr: 8.2617e-05\n",
      "Epoch 28/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8381\n",
      "Epoch 28: val_loss did not improve from 2.37385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 7s 104ms/step - loss: 0.8381 - val_loss: 2.7073 - lr: 8.1791e-05\n",
      "Epoch 29/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8323\n",
      "Epoch 29: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 0.8323 - val_loss: 2.6730 - lr: 8.0973e-05\n",
      "Epoch 30/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8317\n",
      "Epoch 30: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.8317 - val_loss: 2.6718 - lr: 8.0163e-05\n",
      "Epoch 31/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8306\n",
      "Epoch 31: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 7s 104ms/step - loss: 0.8306 - val_loss: 2.5849 - lr: 7.9361e-05\n",
      "Epoch 32/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8362\n",
      "Epoch 32: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 7s 104ms/step - loss: 0.8362 - val_loss: 2.7208 - lr: 7.8568e-05\n",
      "Epoch 33/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8265\n",
      "Epoch 33: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 7s 103ms/step - loss: 0.8265 - val_loss: 2.5879 - lr: 7.7782e-05\n",
      "Epoch 34/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8321\n",
      "Epoch 34: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 7s 104ms/step - loss: 0.8321 - val_loss: 3.0084 - lr: 7.7004e-05\n",
      "Epoch 35/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8278\n",
      "Epoch 35: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 7s 103ms/step - loss: 0.8278 - val_loss: 2.6830 - lr: 7.6234e-05\n",
      "Epoch 36/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8224\n",
      "Epoch 36: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.8224 - val_loss: 2.6229 - lr: 7.5472e-05\n",
      "Epoch 37/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8190\n",
      "Epoch 37: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.8190 - val_loss: 2.8457 - lr: 7.4717e-05\n",
      "Epoch 38/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8119\n",
      "Epoch 38: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.8119 - val_loss: 2.9791 - lr: 7.3970e-05\n",
      "Epoch 39/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8068\n",
      "Epoch 39: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.8068 - val_loss: 2.9647 - lr: 7.3230e-05\n",
      "Epoch 40/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8180\n",
      "Epoch 40: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 7s 107ms/step - loss: 0.8180 - val_loss: 2.8390 - lr: 7.2498e-05\n",
      "Epoch 41/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8089\n",
      "Epoch 41: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 7s 107ms/step - loss: 0.8089 - val_loss: 2.9623 - lr: 7.1773e-05\n",
      "Epoch 42/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8051\n",
      "Epoch 42: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 7s 108ms/step - loss: 0.8051 - val_loss: 2.7854 - lr: 7.1055e-05\n",
      "Epoch 43/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8010\n",
      "Epoch 43: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 0.8010 - val_loss: 3.1336 - lr: 7.0345e-05\n",
      "Epoch 44/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8010\n",
      "Epoch 44: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 7s 108ms/step - loss: 0.8010 - val_loss: 2.9058 - lr: 6.9641e-05\n",
      "Epoch 45/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7980\n",
      "Epoch 45: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 7s 108ms/step - loss: 0.7980 - val_loss: 2.7718 - lr: 6.8945e-05\n",
      "Epoch 46/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8025\n",
      "Epoch 46: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 0.8025 - val_loss: 2.8412 - lr: 6.8255e-05\n",
      "Epoch 47/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8007\n",
      "Epoch 47: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 7s 107ms/step - loss: 0.8007 - val_loss: 3.1613 - lr: 6.7573e-05\n",
      "Epoch 48/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7987\n",
      "Epoch 48: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 0.7987 - val_loss: 3.1862 - lr: 6.6897e-05\n",
      "Epoch 49/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7838\n",
      "Epoch 49: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 0.7838 - val_loss: 2.8772 - lr: 6.6228e-05\n",
      "Epoch 50/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7958\n",
      "Epoch 50: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 0.7958 - val_loss: 3.0836 - lr: 6.5566e-05\n",
      "Epoch 51/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7909\n",
      "Epoch 51: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 7s 104ms/step - loss: 0.7909 - val_loss: 3.2695 - lr: 6.4910e-05\n",
      "Epoch 52/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7774\n",
      "Epoch 52: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.7774 - val_loss: 3.1632 - lr: 6.4261e-05\n",
      "Epoch 53/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7892\n",
      "Epoch 53: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 7s 110ms/step - loss: 0.7892 - val_loss: 3.1522 - lr: 6.3619e-05\n",
      "Epoch 54/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7804\n",
      "Epoch 54: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 7s 108ms/step - loss: 0.7804 - val_loss: 3.0118 - lr: 6.2982e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7776\n",
      "Epoch 55: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 7s 104ms/step - loss: 0.7776 - val_loss: 3.4992 - lr: 6.2353e-05\n",
      "Epoch 56/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7843\n",
      "Epoch 56: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 7s 104ms/step - loss: 0.7843 - val_loss: 3.2436 - lr: 6.1729e-05\n",
      "Epoch 57/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7740\n",
      "Epoch 57: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.7740 - val_loss: 3.2058 - lr: 6.1112e-05\n",
      "Epoch 58/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7762\n",
      "Epoch 58: val_loss did not improve from 2.37385\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 7s 104ms/step - loss: 0.7762 - val_loss: 3.0873 - lr: 6.0501e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_38\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_39 (InputLayer)          [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_114 (LSTM)                (None, 45, 24)       3744        ['input_39[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_76 (Dropout)           (None, 45, 24)       0           ['lstm_114[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_115 (LSTM)                (None, 45, 16)       2624        ['dropout_76[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_77 (Dropout)           (None, 45, 16)       0           ['lstm_115[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_116 (LSTM)                (None, 32)           6272        ['dropout_77[0][0]']             \n",
      "                                                                                                  \n",
      " dense_76 (Dense)               (None, 40)           1320        ['lstm_116[0][0]']               \n",
      "                                                                                                  \n",
      " dense_77 (Dense)               (None, 5)            205         ['dense_76[0][0]']               \n",
      "                                                                                                  \n",
      " tf.unstack_38 (TFOpLambda)     [(None,),            0           ['dense_77[0][0]']               \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_190 (TFOpLambda  (None, 1)           0           ['tf.unstack_38[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_76 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_190[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_194 (TFOpLambda  (None, 1)           0           ['tf.unstack_38[0][4]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_114 (TFOpLamb  (None, 1)           0           ['tf.math.sigmoid_76[0][0]']     \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_77 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_194[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_115 (TFOpLamb  (None, 1)           0           ['tf.math.multiply_114[0][0]']   \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.expand_dims_191 (TFOpLambda  (None, 1)           0           ['tf.unstack_38[0][1]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_193 (TFOpLambda  (None, 1)           0           ['tf.unstack_38[0][3]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_116 (TFOpLamb  (None, 1)           0           ['tf.math.sigmoid_77[0][0]']     \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_76 (TFOpL  (None, 1)           0           ['tf.math.multiply_115[0][0]']   \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.softplus_76 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_191[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_192 (TFOpLambda  (None, 1)           0           ['tf.unstack_38[0][2]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.softplus_77 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_193[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_77 (TFOpL  (None, 1)           0           ['tf.math.multiply_116[0][0]']   \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack_38 (TFOpLambda)       (None, 5, 1)         0           ['tf.__operators__.add_76[0][0]',\n",
      "                                                                  'tf.math.softplus_76[0][0]',    \n",
      "                                                                  'tf.expand_dims_192[0][0]',     \n",
      "                                                                  'tf.math.softplus_77[0][0]',    \n",
      "                                                                  'tf.__operators__.add_77[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _3_CCMP_t+1_0.13\n",
      "Epoch 1/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 3.4784\n",
      "Epoch 1: val_loss improved from inf to 4.02253, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 12s 119ms/step - loss: 3.4784 - val_loss: 4.0225 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 3.0285\n",
      "Epoch 2: val_loss improved from 4.02253 to 3.27051, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.13.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 7s 100ms/step - loss: 3.0285 - val_loss: 3.2705 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 2.0670\n",
      "Epoch 3: val_loss improved from 3.27051 to 3.20483, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 7s 100ms/step - loss: 2.0670 - val_loss: 3.2048 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.4943\n",
      "Epoch 4: val_loss did not improve from 3.20483\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 1.4943 - val_loss: 3.2654 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.3127\n",
      "Epoch 5: val_loss improved from 3.20483 to 3.08984, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 1.3127 - val_loss: 3.0898 - lr: 9.9000e-05\n",
      "Epoch 6/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.2051\n",
      "Epoch 6: val_loss did not improve from 3.08984\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 1.2051 - val_loss: 3.5021 - lr: 9.9000e-05\n",
      "Epoch 7/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1471\n",
      "Epoch 7: val_loss did not improve from 3.08984\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 6s 98ms/step - loss: 1.1471 - val_loss: 3.4613 - lr: 9.8010e-05\n",
      "Epoch 8/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0922\n",
      "Epoch 8: val_loss did not improve from 3.08984\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 1.0922 - val_loss: 3.7146 - lr: 9.7030e-05\n",
      "Epoch 9/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0720\n",
      "Epoch 9: val_loss did not improve from 3.08984\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 1.0720 - val_loss: 3.8852 - lr: 9.6060e-05\n",
      "Epoch 10/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0559\n",
      "Epoch 10: val_loss did not improve from 3.08984\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 1.0559 - val_loss: 3.7301 - lr: 9.5099e-05\n",
      "Epoch 11/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0182\n",
      "Epoch 11: val_loss did not improve from 3.08984\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 7s 100ms/step - loss: 1.0182 - val_loss: 3.5568 - lr: 9.4148e-05\n",
      "Epoch 12/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0375\n",
      "Epoch 12: val_loss did not improve from 3.08984\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 8s 119ms/step - loss: 1.0375 - val_loss: 3.6972 - lr: 9.3207e-05\n",
      "Epoch 13/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9989\n",
      "Epoch 13: val_loss did not improve from 3.08984\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 8s 114ms/step - loss: 0.9989 - val_loss: 3.5682 - lr: 9.2274e-05\n",
      "Epoch 14/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9977\n",
      "Epoch 14: val_loss did not improve from 3.08984\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 7s 109ms/step - loss: 0.9977 - val_loss: 3.2946 - lr: 9.1352e-05\n",
      "Epoch 15/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9911\n",
      "Epoch 15: val_loss did not improve from 3.08984\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 7s 110ms/step - loss: 0.9911 - val_loss: 3.5232 - lr: 9.0438e-05\n",
      "Epoch 16/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9717\n",
      "Epoch 16: val_loss did not improve from 3.08984\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 7s 108ms/step - loss: 0.9717 - val_loss: 3.5923 - lr: 8.9534e-05\n",
      "Epoch 17/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9557\n",
      "Epoch 17: val_loss did not improve from 3.08984\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 7s 110ms/step - loss: 0.9557 - val_loss: 3.3124 - lr: 8.8638e-05\n",
      "Epoch 18/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9650\n",
      "Epoch 18: val_loss did not improve from 3.08984\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 7s 108ms/step - loss: 0.9650 - val_loss: 3.6410 - lr: 8.7752e-05\n",
      "Epoch 19/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9510\n",
      "Epoch 19: val_loss did not improve from 3.08984\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 7s 109ms/step - loss: 0.9510 - val_loss: 3.7210 - lr: 8.6875e-05\n",
      "Epoch 20/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9515\n",
      "Epoch 20: val_loss improved from 3.08984 to 3.00779, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 7s 109ms/step - loss: 0.9515 - val_loss: 3.0078 - lr: 8.6006e-05\n",
      "Epoch 21/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9746\n",
      "Epoch 21: val_loss improved from 3.00779 to 2.75188, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 7s 109ms/step - loss: 0.9746 - val_loss: 2.7519 - lr: 8.6006e-05\n",
      "Epoch 22/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9354\n",
      "Epoch 22: val_loss did not improve from 2.75188\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 7s 107ms/step - loss: 0.9354 - val_loss: 3.0280 - lr: 8.6006e-05\n",
      "Epoch 23/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9205\n",
      "Epoch 23: val_loss did not improve from 2.75188\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 7s 107ms/step - loss: 0.9205 - val_loss: 2.9185 - lr: 8.5146e-05\n",
      "Epoch 24/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9160\n",
      "Epoch 24: val_loss did not improve from 2.75188\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 7s 107ms/step - loss: 0.9160 - val_loss: 3.0450 - lr: 8.4294e-05\n",
      "Epoch 25/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9208\n",
      "Epoch 25: val_loss did not improve from 2.75188\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 0.9208 - val_loss: 3.0351 - lr: 8.3451e-05\n",
      "Epoch 26/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8961\n",
      "Epoch 26: val_loss did not improve from 2.75188\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 7s 107ms/step - loss: 0.8961 - val_loss: 3.0646 - lr: 8.2617e-05\n",
      "Epoch 27/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9096\n",
      "Epoch 27: val_loss did not improve from 2.75188\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 7s 110ms/step - loss: 0.9096 - val_loss: 2.7798 - lr: 8.1791e-05\n",
      "Epoch 28/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8869\n",
      "Epoch 28: val_loss improved from 2.75188 to 2.64600, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.13.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 8s 122ms/step - loss: 0.8869 - val_loss: 2.6460 - lr: 8.0973e-05\n",
      "Epoch 29/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9009\n",
      "Epoch 29: val_loss did not improve from 2.64600\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 8s 114ms/step - loss: 0.9009 - val_loss: 2.8862 - lr: 8.0973e-05\n",
      "Epoch 30/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8848\n",
      "Epoch 30: val_loss improved from 2.64600 to 2.55834, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 8s 116ms/step - loss: 0.8848 - val_loss: 2.5583 - lr: 8.0163e-05\n",
      "Epoch 31/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8918\n",
      "Epoch 31: val_loss did not improve from 2.55834\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 8s 117ms/step - loss: 0.8918 - val_loss: 2.5864 - lr: 8.0163e-05\n",
      "Epoch 32/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8872\n",
      "Epoch 32: val_loss did not improve from 2.55834\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 8s 117ms/step - loss: 0.8872 - val_loss: 2.6462 - lr: 7.9361e-05\n",
      "Epoch 33/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8759\n",
      "Epoch 33: val_loss did not improve from 2.55834\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 8s 115ms/step - loss: 0.8759 - val_loss: 2.6534 - lr: 7.8568e-05\n",
      "Epoch 34/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8783\n",
      "Epoch 34: val_loss did not improve from 2.55834\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 8s 116ms/step - loss: 0.8783 - val_loss: 2.6823 - lr: 7.7782e-05\n",
      "Epoch 35/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8727\n",
      "Epoch 35: val_loss did not improve from 2.55834\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 8s 119ms/step - loss: 0.8727 - val_loss: 2.8306 - lr: 7.7004e-05\n",
      "Epoch 36/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8724\n",
      "Epoch 36: val_loss did not improve from 2.55834\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 8s 120ms/step - loss: 0.8724 - val_loss: 2.6521 - lr: 7.6234e-05\n",
      "Epoch 37/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8714\n",
      "Epoch 37: val_loss did not improve from 2.55834\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 8s 121ms/step - loss: 0.8714 - val_loss: 2.6995 - lr: 7.5472e-05\n",
      "Epoch 38/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8632\n",
      "Epoch 38: val_loss did not improve from 2.55834\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 7s 110ms/step - loss: 0.8632 - val_loss: 2.6578 - lr: 7.4717e-05\n",
      "Epoch 39/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8404\n",
      "Epoch 39: val_loss did not improve from 2.55834\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 7s 103ms/step - loss: 0.8404 - val_loss: 2.5750 - lr: 7.3970e-05\n",
      "Epoch 40/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8576\n",
      "Epoch 40: val_loss did not improve from 2.55834\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 7s 103ms/step - loss: 0.8576 - val_loss: 2.5795 - lr: 7.3230e-05\n",
      "Epoch 41/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8476\n",
      "Epoch 41: val_loss improved from 2.55834 to 2.43764, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 7s 102ms/step - loss: 0.8476 - val_loss: 2.4376 - lr: 7.2498e-05\n",
      "Epoch 42/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8468\n",
      "Epoch 42: val_loss did not improve from 2.43764\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 0.8468 - val_loss: 2.4890 - lr: 7.2498e-05\n",
      "Epoch 43/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8470\n",
      "Epoch 43: val_loss did not improve from 2.43764\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 7s 107ms/step - loss: 0.8470 - val_loss: 2.6584 - lr: 7.1773e-05\n",
      "Epoch 44/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8403\n",
      "Epoch 44: val_loss improved from 2.43764 to 2.28368, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 8s 122ms/step - loss: 0.8403 - val_loss: 2.2837 - lr: 7.1055e-05\n",
      "Epoch 45/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8606\n",
      "Epoch 45: val_loss did not improve from 2.28368\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 11s 163ms/step - loss: 0.8606 - val_loss: 2.4170 - lr: 7.1055e-05\n",
      "Epoch 46/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8406\n",
      "Epoch 46: val_loss did not improve from 2.28368\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 10s 148ms/step - loss: 0.8406 - val_loss: 2.4667 - lr: 7.0345e-05\n",
      "Epoch 47/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8487\n",
      "Epoch 47: val_loss did not improve from 2.28368\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 8s 125ms/step - loss: 0.8487 - val_loss: 2.6172 - lr: 6.9641e-05\n",
      "Epoch 48/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8318\n",
      "Epoch 48: val_loss did not improve from 2.28368\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 8s 129ms/step - loss: 0.8318 - val_loss: 2.4485 - lr: 6.8945e-05\n",
      "Epoch 49/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8347\n",
      "Epoch 49: val_loss did not improve from 2.28368\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 8s 128ms/step - loss: 0.8347 - val_loss: 2.3128 - lr: 6.8255e-05\n",
      "Epoch 50/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8342\n",
      "Epoch 50: val_loss did not improve from 2.28368\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 8s 129ms/step - loss: 0.8342 - val_loss: 2.2928 - lr: 6.7573e-05\n",
      "Epoch 51/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8258\n",
      "Epoch 51: val_loss improved from 2.28368 to 2.24135, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 9s 134ms/step - loss: 0.8258 - val_loss: 2.2413 - lr: 6.6897e-05\n",
      "Epoch 52/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8174\n",
      "Epoch 52: val_loss did not improve from 2.24135\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 8s 126ms/step - loss: 0.8174 - val_loss: 2.2882 - lr: 6.6897e-05\n",
      "Epoch 53/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8216\n",
      "Epoch 53: val_loss improved from 2.24135 to 2.16372, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 8s 126ms/step - loss: 0.8216 - val_loss: 2.1637 - lr: 6.6228e-05\n",
      "Epoch 54/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8247\n",
      "Epoch 54: val_loss did not improve from 2.16372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 8s 126ms/step - loss: 0.8247 - val_loss: 2.4589 - lr: 6.6228e-05\n",
      "Epoch 55/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8218\n",
      "Epoch 55: val_loss did not improve from 2.16372\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 8s 125ms/step - loss: 0.8218 - val_loss: 2.3864 - lr: 6.5566e-05\n",
      "Epoch 56/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8193\n",
      "Epoch 56: val_loss did not improve from 2.16372\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 8s 124ms/step - loss: 0.8193 - val_loss: 2.3447 - lr: 6.4910e-05\n",
      "Epoch 57/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8239\n",
      "Epoch 57: val_loss did not improve from 2.16372\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 8s 124ms/step - loss: 0.8239 - val_loss: 2.2229 - lr: 6.4261e-05\n",
      "Epoch 58/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8125\n",
      "Epoch 58: val_loss did not improve from 2.16372\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 8s 122ms/step - loss: 0.8125 - val_loss: 2.1961 - lr: 6.3619e-05\n",
      "Epoch 59/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8120\n",
      "Epoch 59: val_loss did not improve from 2.16372\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 8s 122ms/step - loss: 0.8120 - val_loss: 2.3353 - lr: 6.2982e-05\n",
      "Epoch 60/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8155\n",
      "Epoch 60: val_loss did not improve from 2.16372\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 8s 121ms/step - loss: 0.8155 - val_loss: 2.3327 - lr: 6.2353e-05\n",
      "Epoch 61/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8217\n",
      "Epoch 61: val_loss did not improve from 2.16372\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 8s 122ms/step - loss: 0.8217 - val_loss: 2.3493 - lr: 6.1729e-05\n",
      "Epoch 62/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8129\n",
      "Epoch 62: val_loss did not improve from 2.16372\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 8s 121ms/step - loss: 0.8129 - val_loss: 2.2257 - lr: 6.1112e-05\n",
      "Epoch 63/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8047\n",
      "Epoch 63: val_loss did not improve from 2.16372\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 8s 121ms/step - loss: 0.8047 - val_loss: 2.2635 - lr: 6.0501e-05\n",
      "Epoch 64/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8113\n",
      "Epoch 64: val_loss did not improve from 2.16372\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 8s 122ms/step - loss: 0.8113 - val_loss: 2.3549 - lr: 5.9896e-05\n",
      "Epoch 65/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8148\n",
      "Epoch 65: val_loss did not improve from 2.16372\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 5.8703650938696225e-05.\n",
      "66/66 [==============================] - 9s 136ms/step - loss: 0.8148 - val_loss: 2.2503 - lr: 5.9297e-05\n",
      "Epoch 66/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8123\n",
      "Epoch 66: val_loss did not improve from 2.16372\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 5.811661550978897e-05.\n",
      "66/66 [==============================] - 8s 125ms/step - loss: 0.8123 - val_loss: 2.3611 - lr: 5.8704e-05\n",
      "Epoch 67/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8024\n",
      "Epoch 67: val_loss did not improve from 2.16372\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 5.753545068728272e-05.\n",
      "66/66 [==============================] - 8s 119ms/step - loss: 0.8024 - val_loss: 2.2438 - lr: 5.8117e-05\n",
      "Epoch 68/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8150\n",
      "Epoch 68: val_loss improved from 2.16372 to 2.16297, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 8s 120ms/step - loss: 0.8150 - val_loss: 2.1630 - lr: 5.7535e-05\n",
      "Epoch 69/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8101\n",
      "Epoch 69: val_loss improved from 2.16297 to 2.08386, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.13.hdf5\n",
      "66/66 [==============================] - 8s 120ms/step - loss: 0.8101 - val_loss: 2.0839 - lr: 5.7535e-05\n",
      "Epoch 70/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8071\n",
      "Epoch 70: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 5.6960095243994146e-05.\n",
      "66/66 [==============================] - 8s 120ms/step - loss: 0.8071 - val_loss: 2.2766 - lr: 5.7535e-05\n",
      "Epoch 71/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7946\n",
      "Epoch 71: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 5.639049515593797e-05.\n",
      "66/66 [==============================] - 8s 121ms/step - loss: 0.7946 - val_loss: 2.3597 - lr: 5.6960e-05\n",
      "Epoch 72/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8049\n",
      "Epoch 72: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 5.5826589195930865e-05.\n",
      "66/66 [==============================] - 8s 118ms/step - loss: 0.8049 - val_loss: 2.2155 - lr: 5.6390e-05\n",
      "Epoch 73/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7946\n",
      "Epoch 73: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 5.526832333998754e-05.\n",
      "66/66 [==============================] - 8s 118ms/step - loss: 0.7946 - val_loss: 2.2126 - lr: 5.5827e-05\n",
      "Epoch 74/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7945\n",
      "Epoch 74: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 5.471563996252371e-05.\n",
      "66/66 [==============================] - 8s 118ms/step - loss: 0.7945 - val_loss: 2.2592 - lr: 5.5268e-05\n",
      "Epoch 75/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7908\n",
      "Epoch 75: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 5.416848503955407e-05.\n",
      "66/66 [==============================] - 8s 119ms/step - loss: 0.7908 - val_loss: 2.3706 - lr: 5.4716e-05\n",
      "Epoch 76/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7924\n",
      "Epoch 76: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 5.362680094549433e-05.\n",
      "66/66 [==============================] - 8s 118ms/step - loss: 0.7924 - val_loss: 2.1756 - lr: 5.4168e-05\n",
      "Epoch 77/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7843\n",
      "Epoch 77: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 5.309053365635918e-05.\n",
      "66/66 [==============================] - 8s 119ms/step - loss: 0.7843 - val_loss: 2.2793 - lr: 5.3627e-05\n",
      "Epoch 78/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7802\n",
      "Epoch 78: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 5.2559629148163366e-05.\n",
      "66/66 [==============================] - 8s 118ms/step - loss: 0.7802 - val_loss: 2.2372 - lr: 5.3091e-05\n",
      "Epoch 79/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7939\n",
      "Epoch 79: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 5.2034033396921584e-05.\n",
      "66/66 [==============================] - 8s 118ms/step - loss: 0.7939 - val_loss: 2.3054 - lr: 5.2560e-05\n",
      "Epoch 80/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7945\n",
      "Epoch 80: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 5.1513692378648555e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 8s 118ms/step - loss: 0.7945 - val_loss: 2.3370 - lr: 5.2034e-05\n",
      "Epoch 81/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7888\n",
      "Epoch 81: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 5.099855567095801e-05.\n",
      "66/66 [==============================] - 8s 119ms/step - loss: 0.7888 - val_loss: 2.2461 - lr: 5.1514e-05\n",
      "Epoch 82/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7842\n",
      "Epoch 82: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 5.048856924986467e-05.\n",
      "66/66 [==============================] - 8s 117ms/step - loss: 0.7842 - val_loss: 2.3717 - lr: 5.0999e-05\n",
      "Epoch 83/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7878\n",
      "Epoch 83: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 4.998368269298226e-05.\n",
      "66/66 [==============================] - 8s 115ms/step - loss: 0.7878 - val_loss: 2.2179 - lr: 5.0489e-05\n",
      "Epoch 84/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7831\n",
      "Epoch 84: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 4.9483845577924514e-05.\n",
      "66/66 [==============================] - 8s 115ms/step - loss: 0.7831 - val_loss: 2.2412 - lr: 4.9984e-05\n",
      "Epoch 85/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7928\n",
      "Epoch 85: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 4.898900748230517e-05.\n",
      "66/66 [==============================] - 8s 115ms/step - loss: 0.7928 - val_loss: 2.2593 - lr: 4.9484e-05\n",
      "Epoch 86/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7952\n",
      "Epoch 86: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 4.849911798373796e-05.\n",
      "66/66 [==============================] - 8s 116ms/step - loss: 0.7952 - val_loss: 2.1728 - lr: 4.8989e-05\n",
      "Epoch 87/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7885\n",
      "Epoch 87: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 4.801412665983662e-05.\n",
      "66/66 [==============================] - 8s 116ms/step - loss: 0.7885 - val_loss: 2.2957 - lr: 4.8499e-05\n",
      "Epoch 88/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7808\n",
      "Epoch 88: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 4.75339866898139e-05.\n",
      "66/66 [==============================] - 8s 115ms/step - loss: 0.7808 - val_loss: 2.1562 - lr: 4.8014e-05\n",
      "Epoch 89/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7852\n",
      "Epoch 89: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 4.705864765128354e-05.\n",
      "66/66 [==============================] - 8s 115ms/step - loss: 0.7852 - val_loss: 2.3288 - lr: 4.7534e-05\n",
      "Epoch 90/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7894\n",
      "Epoch 90: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 4.658806272345828e-05.\n",
      "66/66 [==============================] - 8s 115ms/step - loss: 0.7894 - val_loss: 2.3204 - lr: 4.7059e-05\n",
      "Epoch 91/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7827\n",
      "Epoch 91: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 4.612218148395186e-05.\n",
      "66/66 [==============================] - 8s 115ms/step - loss: 0.7827 - val_loss: 2.3787 - lr: 4.6588e-05\n",
      "Epoch 92/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7781\n",
      "Epoch 92: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 4.566096071357606e-05.\n",
      "66/66 [==============================] - 8s 114ms/step - loss: 0.7781 - val_loss: 2.2993 - lr: 4.6122e-05\n",
      "Epoch 93/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7881\n",
      "Epoch 93: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 4.52043499899446e-05.\n",
      "66/66 [==============================] - 8s 114ms/step - loss: 0.7881 - val_loss: 2.3557 - lr: 4.5661e-05\n",
      "Epoch 94/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7747\n",
      "Epoch 94: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 4.475230609386927e-05.\n",
      "66/66 [==============================] - 8s 115ms/step - loss: 0.7747 - val_loss: 2.3252 - lr: 4.5204e-05\n",
      "Epoch 95/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7842\n",
      "Epoch 95: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 4.43047822045628e-05.\n",
      "66/66 [==============================] - 8s 114ms/step - loss: 0.7842 - val_loss: 2.3714 - lr: 4.4752e-05\n",
      "Epoch 96/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7795\n",
      "Epoch 96: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 4.386173510283697e-05.\n",
      "66/66 [==============================] - 8s 114ms/step - loss: 0.7795 - val_loss: 2.2209 - lr: 4.4305e-05\n",
      "Epoch 97/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7847\n",
      "Epoch 97: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 4.342311796790454e-05.\n",
      "66/66 [==============================] - 8s 114ms/step - loss: 0.7847 - val_loss: 2.2846 - lr: 4.3862e-05\n",
      "Epoch 98/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7904\n",
      "Epoch 98: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 4.298888758057728e-05.\n",
      "66/66 [==============================] - 7s 114ms/step - loss: 0.7904 - val_loss: 2.3153 - lr: 4.3423e-05\n",
      "Epoch 99/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7754\n",
      "Epoch 99: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 4.2558997120067946e-05.\n",
      "66/66 [==============================] - 7s 114ms/step - loss: 0.7754 - val_loss: 2.2928 - lr: 4.2989e-05\n",
      "Epoch 100/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7733\n",
      "Epoch 100: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 4.2133406968787315e-05.\n",
      "66/66 [==============================] - 7s 113ms/step - loss: 0.7733 - val_loss: 2.1916 - lr: 4.2559e-05\n",
      "Epoch 101/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7763\n",
      "Epoch 101: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 4.1712073907547166e-05.\n",
      "66/66 [==============================] - 7s 113ms/step - loss: 0.7763 - val_loss: 2.2878 - lr: 4.2133e-05\n",
      "Epoch 102/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7765\n",
      "Epoch 102: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 4.1294954717159274e-05.\n",
      "66/66 [==============================] - 7s 113ms/step - loss: 0.7765 - val_loss: 2.2462 - lr: 4.1712e-05\n",
      "Epoch 103/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7802\n",
      "Epoch 103: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 4.08820061784354e-05.\n",
      "66/66 [==============================] - 8s 116ms/step - loss: 0.7802 - val_loss: 2.1634 - lr: 4.1295e-05\n",
      "Epoch 104/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7757\n",
      "Epoch 104: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 4.0473185072187336e-05.\n",
      "66/66 [==============================] - 7s 111ms/step - loss: 0.7757 - val_loss: 2.2464 - lr: 4.0882e-05\n",
      "Epoch 105/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7659\n",
      "Epoch 105: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 4.0068451780825854e-05.\n",
      "66/66 [==============================] - 9s 142ms/step - loss: 0.7659 - val_loss: 2.2674 - lr: 4.0473e-05\n",
      "Epoch 106/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7685\n",
      "Epoch 106: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 3.966776668676175e-05.\n",
      "66/66 [==============================] - 18s 270ms/step - loss: 0.7685 - val_loss: 2.3057 - lr: 4.0068e-05\n",
      "Epoch 107/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - ETA: 0s - loss: 0.7674\n",
      "Epoch 107: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 3.927109017240582e-05.\n",
      "66/66 [==============================] - 11s 166ms/step - loss: 0.7674 - val_loss: 2.2380 - lr: 3.9668e-05\n",
      "Epoch 108/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7588\n",
      "Epoch 108: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 3.887837901856983e-05.\n",
      "66/66 [==============================] - 11s 167ms/step - loss: 0.7588 - val_loss: 2.1810 - lr: 3.9271e-05\n",
      "Epoch 109/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7728\n",
      "Epoch 109: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 3.848959360766457e-05.\n",
      "66/66 [==============================] - 10s 147ms/step - loss: 0.7728 - val_loss: 2.2431 - lr: 3.8878e-05\n",
      "Epoch 110/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7654\n",
      "Epoch 110: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 3.810469792369986e-05.\n",
      "66/66 [==============================] - 10s 158ms/step - loss: 0.7654 - val_loss: 2.2659 - lr: 3.8490e-05\n",
      "Epoch 111/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7686\n",
      "Epoch 111: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 3.772365234908648e-05.\n",
      "66/66 [==============================] - 7s 108ms/step - loss: 0.7686 - val_loss: 2.3594 - lr: 3.8105e-05\n",
      "Epoch 112/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7591\n",
      "Epoch 112: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 3.734641726623522e-05.\n",
      "66/66 [==============================] - 7s 107ms/step - loss: 0.7591 - val_loss: 2.3290 - lr: 3.7724e-05\n",
      "Epoch 113/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7598\n",
      "Epoch 113: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 3.6972953057556876e-05.\n",
      "66/66 [==============================] - 7s 110ms/step - loss: 0.7598 - val_loss: 2.2430 - lr: 3.7346e-05\n",
      "Epoch 114/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7722\n",
      "Epoch 114: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 3.660322370706126e-05.\n",
      "66/66 [==============================] - 8s 115ms/step - loss: 0.7722 - val_loss: 2.2047 - lr: 3.6973e-05\n",
      "Epoch 115/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7601\n",
      "Epoch 115: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 3.623719319875818e-05.\n",
      "66/66 [==============================] - 7s 108ms/step - loss: 0.7601 - val_loss: 2.2090 - lr: 3.6603e-05\n",
      "Epoch 116/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7556\n",
      "Epoch 116: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 3.5874821915058416e-05.\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.7556 - val_loss: 2.3160 - lr: 3.6237e-05\n",
      "Epoch 117/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7573\n",
      "Epoch 117: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 3.551607383997179e-05.\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.7573 - val_loss: 2.2908 - lr: 3.5875e-05\n",
      "Epoch 118/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7580\n",
      "Epoch 118: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 3.516091295750812e-05.\n",
      "66/66 [==============================] - 8s 118ms/step - loss: 0.7580 - val_loss: 2.2167 - lr: 3.5516e-05\n",
      "Epoch 119/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.7611\n",
      "Epoch 119: val_loss did not improve from 2.08386\n",
      "\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 3.480930325167719e-05.\n",
      "66/66 [==============================] - 8s 122ms/step - loss: 0.7611 - val_loss: 2.1808 - lr: 3.5161e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_39\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_40 (InputLayer)          [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_117 (LSTM)                (None, 45, 24)       3744        ['input_40[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_78 (Dropout)           (None, 45, 24)       0           ['lstm_117[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_118 (LSTM)                (None, 45, 16)       2624        ['dropout_78[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_79 (Dropout)           (None, 45, 16)       0           ['lstm_118[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_119 (LSTM)                (None, 32)           6272        ['dropout_79[0][0]']             \n",
      "                                                                                                  \n",
      " dense_78 (Dense)               (None, 40)           1320        ['lstm_119[0][0]']               \n",
      "                                                                                                  \n",
      " dense_79 (Dense)               (None, 5)            205         ['dense_78[0][0]']               \n",
      "                                                                                                  \n",
      " tf.unstack_39 (TFOpLambda)     [(None,),            0           ['dense_79[0][0]']               \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_195 (TFOpLambda  (None, 1)           0           ['tf.unstack_39[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_78 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_195[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_199 (TFOpLambda  (None, 1)           0           ['tf.unstack_39[0][4]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_117 (TFOpLamb  (None, 1)           0           ['tf.math.sigmoid_78[0][0]']     \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_79 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_199[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_118 (TFOpLamb  (None, 1)           0           ['tf.math.multiply_117[0][0]']   \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.expand_dims_196 (TFOpLambda  (None, 1)           0           ['tf.unstack_39[0][1]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_198 (TFOpLambda  (None, 1)           0           ['tf.unstack_39[0][3]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_119 (TFOpLamb  (None, 1)           0           ['tf.math.sigmoid_79[0][0]']     \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_78 (TFOpL  (None, 1)           0           ['tf.math.multiply_118[0][0]']   \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.softplus_78 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_196[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_197 (TFOpLambda  (None, 1)           0           ['tf.unstack_39[0][2]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.softplus_79 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_198[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_79 (TFOpL  (None, 1)           0           ['tf.math.multiply_119[0][0]']   \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack_39 (TFOpLambda)       (None, 5, 1)         0           ['tf.__operators__.add_78[0][0]',\n",
      "                                                                  'tf.math.softplus_78[0][0]',    \n",
      "                                                                  'tf.expand_dims_197[0][0]',     \n",
      "                                                                  'tf.math.softplus_79[0][0]',    \n",
      "                                                                  'tf.__operators__.add_79[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _3_CCMP_t+1_0.14\n",
      "Epoch 1/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 3.5186\n",
      "Epoch 1: val_loss improved from inf to 4.47249, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 13s 121ms/step - loss: 3.5186 - val_loss: 4.4725 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 3.0061\n",
      "Epoch 2: val_loss improved from 4.47249 to 3.96958, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.14.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 7s 110ms/step - loss: 3.0061 - val_loss: 3.9696 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.9627\n",
      "Epoch 3: val_loss improved from 3.96958 to 2.82365, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 7s 101ms/step - loss: 1.9627 - val_loss: 2.8237 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.5171\n",
      "Epoch 4: val_loss did not improve from 2.82365\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 7s 104ms/step - loss: 1.5171 - val_loss: 2.8286 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.3883\n",
      "Epoch 5: val_loss improved from 2.82365 to 2.80369, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 7s 102ms/step - loss: 1.3883 - val_loss: 2.8037 - lr: 9.9000e-05\n",
      "Epoch 6/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.3004\n",
      "Epoch 6: val_loss improved from 2.80369 to 2.72242, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 7s 114ms/step - loss: 1.3004 - val_loss: 2.7224 - lr: 9.9000e-05\n",
      "Epoch 7/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.2360\n",
      "Epoch 7: val_loss did not improve from 2.72242\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 1.2360 - val_loss: 2.9173 - lr: 9.9000e-05\n",
      "Epoch 8/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1982\n",
      "Epoch 8: val_loss did not improve from 2.72242\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 7s 103ms/step - loss: 1.1982 - val_loss: 2.9303 - lr: 9.8010e-05\n",
      "Epoch 9/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1777\n",
      "Epoch 9: val_loss improved from 2.72242 to 2.69237, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 7s 104ms/step - loss: 1.1777 - val_loss: 2.6924 - lr: 9.7030e-05\n",
      "Epoch 10/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1550\n",
      "Epoch 10: val_loss did not improve from 2.69237\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 7s 100ms/step - loss: 1.1550 - val_loss: 2.8748 - lr: 9.7030e-05\n",
      "Epoch 11/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1344\n",
      "Epoch 11: val_loss improved from 2.69237 to 2.51487, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 7s 101ms/step - loss: 1.1344 - val_loss: 2.5149 - lr: 9.6060e-05\n",
      "Epoch 12/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1160\n",
      "Epoch 12: val_loss did not improve from 2.51487\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 8s 114ms/step - loss: 1.1160 - val_loss: 2.6245 - lr: 9.6060e-05\n",
      "Epoch 13/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1155\n",
      "Epoch 13: val_loss did not improve from 2.51487\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 7s 103ms/step - loss: 1.1155 - val_loss: 2.6361 - lr: 9.5099e-05\n",
      "Epoch 14/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0811\n",
      "Epoch 14: val_loss improved from 2.51487 to 2.45226, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 6s 98ms/step - loss: 1.0811 - val_loss: 2.4523 - lr: 9.4148e-05\n",
      "Epoch 15/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0633\n",
      "Epoch 15: val_loss did not improve from 2.45226\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 7s 107ms/step - loss: 1.0633 - val_loss: 2.5231 - lr: 9.4148e-05\n",
      "Epoch 16/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0587\n",
      "Epoch 16: val_loss improved from 2.45226 to 2.31255, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 7s 113ms/step - loss: 1.0587 - val_loss: 2.3126 - lr: 9.3207e-05\n",
      "Epoch 17/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0592\n",
      "Epoch 17: val_loss did not improve from 2.31255\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 7s 102ms/step - loss: 1.0592 - val_loss: 2.3598 - lr: 9.3207e-05\n",
      "Epoch 18/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0155\n",
      "Epoch 18: val_loss did not improve from 2.31255\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 8s 119ms/step - loss: 1.0155 - val_loss: 2.3319 - lr: 9.2274e-05\n",
      "Epoch 19/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0100\n",
      "Epoch 19: val_loss improved from 2.31255 to 2.20890, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 8s 116ms/step - loss: 1.0100 - val_loss: 2.2089 - lr: 9.1352e-05\n",
      "Epoch 20/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0084\n",
      "Epoch 20: val_loss improved from 2.20890 to 2.08079, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 8s 114ms/step - loss: 1.0084 - val_loss: 2.0808 - lr: 9.1352e-05\n",
      "Epoch 21/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9920\n",
      "Epoch 21: val_loss improved from 2.08079 to 2.07419, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 7s 111ms/step - loss: 0.9920 - val_loss: 2.0742 - lr: 9.1352e-05\n",
      "Epoch 22/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9825\n",
      "Epoch 22: val_loss did not improve from 2.07419\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 7s 112ms/step - loss: 0.9825 - val_loss: 2.1183 - lr: 9.1352e-05\n",
      "Epoch 23/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9571\n",
      "Epoch 23: val_loss did not improve from 2.07419\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 7s 112ms/step - loss: 0.9571 - val_loss: 2.3488 - lr: 9.0438e-05\n",
      "Epoch 24/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9658\n",
      "Epoch 24: val_loss did not improve from 2.07419\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 7s 110ms/step - loss: 0.9658 - val_loss: 2.1619 - lr: 8.9534e-05\n",
      "Epoch 25/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9557\n",
      "Epoch 25: val_loss did not improve from 2.07419\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 8s 125ms/step - loss: 0.9557 - val_loss: 2.1922 - lr: 8.8638e-05\n",
      "Epoch 26/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9357\n",
      "Epoch 26: val_loss improved from 2.07419 to 2.04167, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 8s 117ms/step - loss: 0.9357 - val_loss: 2.0417 - lr: 8.7752e-05\n",
      "Epoch 27/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9348\n",
      "Epoch 27: val_loss did not improve from 2.04167\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 8s 126ms/step - loss: 0.9348 - val_loss: 2.0462 - lr: 8.7752e-05\n",
      "Epoch 28/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9358\n",
      "Epoch 28: val_loss did not improve from 2.04167\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 8s 119ms/step - loss: 0.9358 - val_loss: 2.1725 - lr: 8.6875e-05\n",
      "Epoch 29/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9389\n",
      "Epoch 29: val_loss improved from 2.04167 to 1.96651, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 8s 120ms/step - loss: 0.9389 - val_loss: 1.9665 - lr: 8.6006e-05\n",
      "Epoch 30/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9285\n",
      "Epoch 30: val_loss did not improve from 1.96651\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 8s 123ms/step - loss: 0.9285 - val_loss: 2.0717 - lr: 8.6006e-05\n",
      "Epoch 31/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9171\n",
      "Epoch 31: val_loss did not improve from 1.96651\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 9s 137ms/step - loss: 0.9171 - val_loss: 2.2753 - lr: 8.5146e-05\n",
      "Epoch 32/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9197\n",
      "Epoch 32: val_loss did not improve from 1.96651\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 9s 136ms/step - loss: 0.9197 - val_loss: 2.2569 - lr: 8.4294e-05\n",
      "Epoch 33/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9076\n",
      "Epoch 33: val_loss did not improve from 1.96651\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.9076 - val_loss: 2.1744 - lr: 8.3451e-05\n",
      "Epoch 34/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9092\n",
      "Epoch 34: val_loss did not improve from 1.96651\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.9092 - val_loss: 2.1339 - lr: 8.2617e-05\n",
      "Epoch 35/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8976\n",
      "Epoch 35: val_loss did not improve from 1.96651\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.8976 - val_loss: 2.2235 - lr: 8.1791e-05\n",
      "Epoch 36/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9051\n",
      "Epoch 36: val_loss did not improve from 1.96651\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.9051 - val_loss: 2.2151 - lr: 8.0973e-05\n",
      "Epoch 37/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8971\n",
      "Epoch 37: val_loss did not improve from 1.96651\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 6s 98ms/step - loss: 0.8971 - val_loss: 1.9733 - lr: 8.0163e-05\n",
      "Epoch 38/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8982\n",
      "Epoch 38: val_loss did not improve from 1.96651\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.8982 - val_loss: 2.1407 - lr: 7.9361e-05\n",
      "Epoch 39/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8815\n",
      "Epoch 39: val_loss did not improve from 1.96651\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.8815 - val_loss: 2.1840 - lr: 7.8568e-05\n",
      "Epoch 40/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8909\n",
      "Epoch 40: val_loss improved from 1.96651 to 1.92598, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.14.hdf5\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.8909 - val_loss: 1.9260 - lr: 7.7782e-05\n",
      "Epoch 41/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8775\n",
      "Epoch 41: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.8775 - val_loss: 2.1844 - lr: 7.7782e-05\n",
      "Epoch 42/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8890\n",
      "Epoch 42: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 7s 111ms/step - loss: 0.8890 - val_loss: 2.0139 - lr: 7.7004e-05\n",
      "Epoch 43/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8721\n",
      "Epoch 43: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 7s 113ms/step - loss: 0.8721 - val_loss: 2.0743 - lr: 7.6234e-05\n",
      "Epoch 44/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8774\n",
      "Epoch 44: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 7s 100ms/step - loss: 0.8774 - val_loss: 2.2968 - lr: 7.5472e-05\n",
      "Epoch 45/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8698\n",
      "Epoch 45: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.8698 - val_loss: 2.1593 - lr: 7.4717e-05\n",
      "Epoch 46/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8684\n",
      "Epoch 46: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.8684 - val_loss: 2.1032 - lr: 7.3970e-05\n",
      "Epoch 47/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8778\n",
      "Epoch 47: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.8778 - val_loss: 2.0945 - lr: 7.3230e-05\n",
      "Epoch 48/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8596\n",
      "Epoch 48: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 7s 101ms/step - loss: 0.8596 - val_loss: 2.0806 - lr: 7.2498e-05\n",
      "Epoch 49/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8818\n",
      "Epoch 49: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.8818 - val_loss: 2.1523 - lr: 7.1773e-05\n",
      "Epoch 50/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8707\n",
      "Epoch 50: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.8707 - val_loss: 2.0721 - lr: 7.1055e-05\n",
      "Epoch 51/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8753\n",
      "Epoch 51: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.8753 - val_loss: 2.1150 - lr: 7.0345e-05\n",
      "Epoch 52/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8679\n",
      "Epoch 52: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.8679 - val_loss: 1.9552 - lr: 6.9641e-05\n",
      "Epoch 53/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8635\n",
      "Epoch 53: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.8635 - val_loss: 2.2339 - lr: 6.8945e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8627\n",
      "Epoch 54: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.8627 - val_loss: 2.3977 - lr: 6.8255e-05\n",
      "Epoch 55/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8658\n",
      "Epoch 55: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.8658 - val_loss: 2.2144 - lr: 6.7573e-05\n",
      "Epoch 56/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8523\n",
      "Epoch 56: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.8523 - val_loss: 2.1583 - lr: 6.6897e-05\n",
      "Epoch 57/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8578\n",
      "Epoch 57: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.8578 - val_loss: 2.2051 - lr: 6.6228e-05\n",
      "Epoch 58/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8694\n",
      "Epoch 58: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.8694 - val_loss: 2.2225 - lr: 6.5566e-05\n",
      "Epoch 59/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8586\n",
      "Epoch 59: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 7s 111ms/step - loss: 0.8586 - val_loss: 2.0404 - lr: 6.4910e-05\n",
      "Epoch 60/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8450\n",
      "Epoch 60: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 0.8450 - val_loss: 2.2178 - lr: 6.4261e-05\n",
      "Epoch 61/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8572\n",
      "Epoch 61: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.8572 - val_loss: 2.1888 - lr: 6.3619e-05\n",
      "Epoch 62/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8570\n",
      "Epoch 62: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.8570 - val_loss: 2.2788 - lr: 6.2982e-05\n",
      "Epoch 63/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8541\n",
      "Epoch 63: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 7s 101ms/step - loss: 0.8541 - val_loss: 2.1852 - lr: 6.2353e-05\n",
      "Epoch 64/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8543\n",
      "Epoch 64: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 7s 100ms/step - loss: 0.8543 - val_loss: 2.1301 - lr: 6.1729e-05\n",
      "Epoch 65/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8488\n",
      "Epoch 65: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 6s 98ms/step - loss: 0.8488 - val_loss: 2.1532 - lr: 6.1112e-05\n",
      "Epoch 66/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8502\n",
      "Epoch 66: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.8502 - val_loss: 2.1456 - lr: 6.0501e-05\n",
      "Epoch 67/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8508\n",
      "Epoch 67: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.8508 - val_loss: 2.1639 - lr: 5.9896e-05\n",
      "Epoch 68/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8526\n",
      "Epoch 68: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 5.8703650938696225e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.8526 - val_loss: 2.1078 - lr: 5.9297e-05\n",
      "Epoch 69/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8393\n",
      "Epoch 69: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 5.811661550978897e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.8393 - val_loss: 2.2427 - lr: 5.8704e-05\n",
      "Epoch 70/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8439\n",
      "Epoch 70: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 5.753545068728272e-05.\n",
      "66/66 [==============================] - 9s 139ms/step - loss: 0.8439 - val_loss: 2.2048 - lr: 5.8117e-05\n",
      "Epoch 71/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8550\n",
      "Epoch 71: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 5.6960095243994146e-05.\n",
      "66/66 [==============================] - 11s 163ms/step - loss: 0.8550 - val_loss: 2.0979 - lr: 5.7535e-05\n",
      "Epoch 72/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8448\n",
      "Epoch 72: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 5.639049515593797e-05.\n",
      "66/66 [==============================] - 8s 126ms/step - loss: 0.8448 - val_loss: 2.2379 - lr: 5.6960e-05\n",
      "Epoch 73/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8415\n",
      "Epoch 73: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 5.5826589195930865e-05.\n",
      "66/66 [==============================] - 8s 115ms/step - loss: 0.8415 - val_loss: 2.0964 - lr: 5.6390e-05\n",
      "Epoch 74/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8399\n",
      "Epoch 74: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 5.526832333998754e-05.\n",
      "66/66 [==============================] - 7s 112ms/step - loss: 0.8399 - val_loss: 2.2109 - lr: 5.5827e-05\n",
      "Epoch 75/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8459\n",
      "Epoch 75: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 5.471563996252371e-05.\n",
      "66/66 [==============================] - 9s 140ms/step - loss: 0.8459 - val_loss: 2.1907 - lr: 5.5268e-05\n",
      "Epoch 76/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8331\n",
      "Epoch 76: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 5.416848503955407e-05.\n",
      "66/66 [==============================] - 9s 141ms/step - loss: 0.8331 - val_loss: 2.1940 - lr: 5.4716e-05\n",
      "Epoch 77/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8458\n",
      "Epoch 77: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 5.362680094549433e-05.\n",
      "66/66 [==============================] - 8s 119ms/step - loss: 0.8458 - val_loss: 2.1042 - lr: 5.4168e-05\n",
      "Epoch 78/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8322\n",
      "Epoch 78: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 5.309053365635918e-05.\n",
      "66/66 [==============================] - 7s 112ms/step - loss: 0.8322 - val_loss: 2.1270 - lr: 5.3627e-05\n",
      "Epoch 79/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8316\n",
      "Epoch 79: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 5.2559629148163366e-05.\n",
      "66/66 [==============================] - 7s 113ms/step - loss: 0.8316 - val_loss: 2.2685 - lr: 5.3091e-05\n",
      "Epoch 80/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8300\n",
      "Epoch 80: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 5.2034033396921584e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 7s 113ms/step - loss: 0.8300 - val_loss: 2.0056 - lr: 5.2560e-05\n",
      "Epoch 81/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8384\n",
      "Epoch 81: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 5.1513692378648555e-05.\n",
      "66/66 [==============================] - 8s 125ms/step - loss: 0.8384 - val_loss: 2.0324 - lr: 5.2034e-05\n",
      "Epoch 82/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8453\n",
      "Epoch 82: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 5.099855567095801e-05.\n",
      "66/66 [==============================] - 8s 126ms/step - loss: 0.8453 - val_loss: 2.0931 - lr: 5.1514e-05\n",
      "Epoch 83/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8348\n",
      "Epoch 83: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 5.048856924986467e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.8348 - val_loss: 2.1499 - lr: 5.0999e-05\n",
      "Epoch 84/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8295\n",
      "Epoch 84: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 4.998368269298226e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.8295 - val_loss: 2.1204 - lr: 5.0489e-05\n",
      "Epoch 85/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8343\n",
      "Epoch 85: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 4.9483845577924514e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.8343 - val_loss: 2.1579 - lr: 4.9984e-05\n",
      "Epoch 86/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8312\n",
      "Epoch 86: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 4.898900748230517e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 0.8312 - val_loss: 2.1419 - lr: 4.9484e-05\n",
      "Epoch 87/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8294\n",
      "Epoch 87: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 4.849911798373796e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.8294 - val_loss: 2.1442 - lr: 4.8989e-05\n",
      "Epoch 88/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8332\n",
      "Epoch 88: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 4.801412665983662e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.8332 - val_loss: 2.1263 - lr: 4.8499e-05\n",
      "Epoch 89/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8289\n",
      "Epoch 89: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 4.75339866898139e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.8289 - val_loss: 2.2628 - lr: 4.8014e-05\n",
      "Epoch 90/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8378\n",
      "Epoch 90: val_loss did not improve from 1.92598\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 4.705864765128354e-05.\n",
      "66/66 [==============================] - 6s 98ms/step - loss: 0.8378 - val_loss: 2.2732 - lr: 4.7534e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_40\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_41 (InputLayer)          [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_120 (LSTM)                (None, 45, 24)       3744        ['input_41[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_80 (Dropout)           (None, 45, 24)       0           ['lstm_120[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_121 (LSTM)                (None, 45, 16)       2624        ['dropout_80[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_81 (Dropout)           (None, 45, 16)       0           ['lstm_121[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_122 (LSTM)                (None, 32)           6272        ['dropout_81[0][0]']             \n",
      "                                                                                                  \n",
      " dense_80 (Dense)               (None, 40)           1320        ['lstm_122[0][0]']               \n",
      "                                                                                                  \n",
      " dense_81 (Dense)               (None, 5)            205         ['dense_80[0][0]']               \n",
      "                                                                                                  \n",
      " tf.unstack_40 (TFOpLambda)     [(None,),            0           ['dense_81[0][0]']               \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_200 (TFOpLambda  (None, 1)           0           ['tf.unstack_40[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_80 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_200[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_204 (TFOpLambda  (None, 1)           0           ['tf.unstack_40[0][4]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_120 (TFOpLamb  (None, 1)           0           ['tf.math.sigmoid_80[0][0]']     \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_81 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_204[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_121 (TFOpLamb  (None, 1)           0           ['tf.math.multiply_120[0][0]']   \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.expand_dims_201 (TFOpLambda  (None, 1)           0           ['tf.unstack_40[0][1]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_203 (TFOpLambda  (None, 1)           0           ['tf.unstack_40[0][3]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_122 (TFOpLamb  (None, 1)           0           ['tf.math.sigmoid_81[0][0]']     \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_80 (TFOpL  (None, 1)           0           ['tf.math.multiply_121[0][0]']   \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.softplus_80 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_201[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_202 (TFOpLambda  (None, 1)           0           ['tf.unstack_40[0][2]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.softplus_81 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_203[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_81 (TFOpL  (None, 1)           0           ['tf.math.multiply_122[0][0]']   \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack_40 (TFOpLambda)       (None, 5, 1)         0           ['tf.__operators__.add_80[0][0]',\n",
      "                                                                  'tf.math.softplus_80[0][0]',    \n",
      "                                                                  'tf.expand_dims_202[0][0]',     \n",
      "                                                                  'tf.math.softplus_81[0][0]',    \n",
      "                                                                  'tf.__operators__.add_81[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _3_CCMP_t+1_0.15\n",
      "Epoch 1/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 3.4508\n",
      "Epoch 1: val_loss improved from inf to 4.28202, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 14s 129ms/step - loss: 3.4508 - val_loss: 4.2820 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 2.7241\n",
      "Epoch 2: val_loss improved from 4.28202 to 3.52666, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.15.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 7s 112ms/step - loss: 2.7241 - val_loss: 3.5267 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.8719\n",
      "Epoch 3: val_loss improved from 3.52666 to 3.04121, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 7s 104ms/step - loss: 1.8719 - val_loss: 3.0412 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.5835\n",
      "Epoch 4: val_loss improved from 3.04121 to 2.88486, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 7s 104ms/step - loss: 1.5835 - val_loss: 2.8849 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.4191\n",
      "Epoch 5: val_loss improved from 2.88486 to 2.65536, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 7s 104ms/step - loss: 1.4191 - val_loss: 2.6554 - lr: 1.0000e-04\n",
      "Epoch 6/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.3206\n",
      "Epoch 6: val_loss improved from 2.65536 to 2.64844, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 7s 104ms/step - loss: 1.3206 - val_loss: 2.6484 - lr: 1.0000e-04\n",
      "Epoch 7/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.2459\n",
      "Epoch 7: val_loss improved from 2.64844 to 2.31036, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 7s 104ms/step - loss: 1.2459 - val_loss: 2.3104 - lr: 1.0000e-04\n",
      "Epoch 8/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.2146\n",
      "Epoch 8: val_loss did not improve from 2.31036\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 7s 103ms/step - loss: 1.2146 - val_loss: 2.4253 - lr: 1.0000e-04\n",
      "Epoch 9/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1896\n",
      "Epoch 9: val_loss did not improve from 2.31036\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 1.1896 - val_loss: 2.4549 - lr: 9.9000e-05\n",
      "Epoch 10/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1623\n",
      "Epoch 10: val_loss did not improve from 2.31036\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 1.1623 - val_loss: 2.5304 - lr: 9.8010e-05\n",
      "Epoch 11/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1467\n",
      "Epoch 11: val_loss improved from 2.31036 to 2.28230, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 1.1467 - val_loss: 2.2823 - lr: 9.7030e-05\n",
      "Epoch 12/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1347\n",
      "Epoch 12: val_loss did not improve from 2.28230\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 7s 103ms/step - loss: 1.1347 - val_loss: 2.3525 - lr: 9.7030e-05\n",
      "Epoch 13/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1119\n",
      "Epoch 13: val_loss did not improve from 2.28230\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 1.1119 - val_loss: 2.4838 - lr: 9.6060e-05\n",
      "Epoch 14/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0914\n",
      "Epoch 14: val_loss did not improve from 2.28230\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 1.0914 - val_loss: 2.3281 - lr: 9.5099e-05\n",
      "Epoch 15/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0952\n",
      "Epoch 15: val_loss did not improve from 2.28230\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 1.0952 - val_loss: 2.3495 - lr: 9.4148e-05\n",
      "Epoch 16/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0707\n",
      "Epoch 16: val_loss improved from 2.28230 to 2.18339, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 1.0707 - val_loss: 2.1834 - lr: 9.3207e-05\n",
      "Epoch 17/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0606\n",
      "Epoch 17: val_loss did not improve from 2.18339\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 7s 104ms/step - loss: 1.0606 - val_loss: 2.4317 - lr: 9.3207e-05\n",
      "Epoch 18/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0686\n",
      "Epoch 18: val_loss did not improve from 2.18339\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 8s 129ms/step - loss: 1.0686 - val_loss: 2.1873 - lr: 9.2274e-05\n",
      "Epoch 19/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0463\n",
      "Epoch 19: val_loss did not improve from 2.18339\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 10s 158ms/step - loss: 1.0463 - val_loss: 2.3115 - lr: 9.1352e-05\n",
      "Epoch 20/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0340\n",
      "Epoch 20: val_loss did not improve from 2.18339\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 8s 122ms/step - loss: 1.0340 - val_loss: 2.3724 - lr: 9.0438e-05\n",
      "Epoch 21/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0428\n",
      "Epoch 21: val_loss did not improve from 2.18339\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 8s 116ms/step - loss: 1.0428 - val_loss: 2.3657 - lr: 8.9534e-05\n",
      "Epoch 22/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0205\n",
      "Epoch 22: val_loss did not improve from 2.18339\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 8s 118ms/step - loss: 1.0205 - val_loss: 2.2304 - lr: 8.8638e-05\n",
      "Epoch 23/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0110\n",
      "Epoch 23: val_loss did not improve from 2.18339\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 8s 116ms/step - loss: 1.0110 - val_loss: 2.3114 - lr: 8.7752e-05\n",
      "Epoch 24/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0097\n",
      "Epoch 24: val_loss did not improve from 2.18339\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 8s 115ms/step - loss: 1.0097 - val_loss: 2.3052 - lr: 8.6875e-05\n",
      "Epoch 25/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0087\n",
      "Epoch 25: val_loss did not improve from 2.18339\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 8s 117ms/step - loss: 1.0087 - val_loss: 2.3307 - lr: 8.6006e-05\n",
      "Epoch 26/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0133\n",
      "Epoch 26: val_loss did not improve from 2.18339\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 8s 114ms/step - loss: 1.0133 - val_loss: 2.1861 - lr: 8.5146e-05\n",
      "Epoch 27/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0013\n",
      "Epoch 27: val_loss improved from 2.18339 to 2.16547, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 8s 117ms/step - loss: 1.0013 - val_loss: 2.1655 - lr: 8.4294e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9998\n",
      "Epoch 28: val_loss did not improve from 2.16547\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 8s 123ms/step - loss: 0.9998 - val_loss: 2.2402 - lr: 8.4294e-05\n",
      "Epoch 29/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9923\n",
      "Epoch 29: val_loss did not improve from 2.16547\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 7s 109ms/step - loss: 0.9923 - val_loss: 2.2071 - lr: 8.3451e-05\n",
      "Epoch 30/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9834\n",
      "Epoch 30: val_loss improved from 2.16547 to 2.10965, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 6s 98ms/step - loss: 0.9834 - val_loss: 2.1096 - lr: 8.2617e-05\n",
      "Epoch 31/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9929\n",
      "Epoch 31: val_loss improved from 2.10965 to 2.08140, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 7s 112ms/step - loss: 0.9929 - val_loss: 2.0814 - lr: 8.2617e-05\n",
      "Epoch 32/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9820\n",
      "Epoch 32: val_loss did not improve from 2.08140\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 8s 127ms/step - loss: 0.9820 - val_loss: 2.2952 - lr: 8.2617e-05\n",
      "Epoch 33/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9789\n",
      "Epoch 33: val_loss did not improve from 2.08140\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 9s 136ms/step - loss: 0.9789 - val_loss: 2.1174 - lr: 8.1791e-05\n",
      "Epoch 34/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9710\n",
      "Epoch 34: val_loss did not improve from 2.08140\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 8s 124ms/step - loss: 0.9710 - val_loss: 2.1966 - lr: 8.0973e-05\n",
      "Epoch 35/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9637\n",
      "Epoch 35: val_loss improved from 2.08140 to 2.05777, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 8s 123ms/step - loss: 0.9637 - val_loss: 2.0578 - lr: 8.0163e-05\n",
      "Epoch 36/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9633\n",
      "Epoch 36: val_loss did not improve from 2.05777\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 9s 133ms/step - loss: 0.9633 - val_loss: 2.0823 - lr: 8.0163e-05\n",
      "Epoch 37/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9662\n",
      "Epoch 37: val_loss did not improve from 2.05777\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 8s 118ms/step - loss: 0.9662 - val_loss: 2.1294 - lr: 7.9361e-05\n",
      "Epoch 38/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9628\n",
      "Epoch 38: val_loss did not improve from 2.05777\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 9s 135ms/step - loss: 0.9628 - val_loss: 2.1947 - lr: 7.8568e-05\n",
      "Epoch 39/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9543\n",
      "Epoch 39: val_loss did not improve from 2.05777\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 8s 127ms/step - loss: 0.9543 - val_loss: 2.1854 - lr: 7.7782e-05\n",
      "Epoch 40/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9572\n",
      "Epoch 40: val_loss did not improve from 2.05777\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 7s 101ms/step - loss: 0.9572 - val_loss: 2.0632 - lr: 7.7004e-05\n",
      "Epoch 41/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9548\n",
      "Epoch 41: val_loss improved from 2.05777 to 2.00561, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 7s 101ms/step - loss: 0.9548 - val_loss: 2.0056 - lr: 7.6234e-05\n",
      "Epoch 42/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9576\n",
      "Epoch 42: val_loss did not improve from 2.00561\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 7s 104ms/step - loss: 0.9576 - val_loss: 2.1327 - lr: 7.6234e-05\n",
      "Epoch 43/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9504\n",
      "Epoch 43: val_loss improved from 2.00561 to 1.99910, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 7s 100ms/step - loss: 0.9504 - val_loss: 1.9991 - lr: 7.5472e-05\n",
      "Epoch 44/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9467\n",
      "Epoch 44: val_loss improved from 1.99910 to 1.93930, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.9467 - val_loss: 1.9393 - lr: 7.5472e-05\n",
      "Epoch 45/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9525\n",
      "Epoch 45: val_loss did not improve from 1.93930\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.9525 - val_loss: 2.0856 - lr: 7.5472e-05\n",
      "Epoch 46/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9428\n",
      "Epoch 46: val_loss did not improve from 1.93930\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 0.9428 - val_loss: 2.1167 - lr: 7.4717e-05\n",
      "Epoch 47/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9405\n",
      "Epoch 47: val_loss did not improve from 1.93930\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 7s 100ms/step - loss: 0.9405 - val_loss: 2.0422 - lr: 7.3970e-05\n",
      "Epoch 48/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9475\n",
      "Epoch 48: val_loss did not improve from 1.93930\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 0.9475 - val_loss: 2.0725 - lr: 7.3230e-05\n",
      "Epoch 49/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9427\n",
      "Epoch 49: val_loss did not improve from 1.93930\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 7s 112ms/step - loss: 0.9427 - val_loss: 2.0361 - lr: 7.2498e-05\n",
      "Epoch 50/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9393\n",
      "Epoch 50: val_loss did not improve from 1.93930\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 12s 177ms/step - loss: 0.9393 - val_loss: 2.0028 - lr: 7.1773e-05\n",
      "Epoch 51/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9388\n",
      "Epoch 51: val_loss did not improve from 1.93930\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 9s 136ms/step - loss: 0.9388 - val_loss: 2.0038 - lr: 7.1055e-05\n",
      "Epoch 52/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9420\n",
      "Epoch 52: val_loss did not improve from 1.93930\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 8s 117ms/step - loss: 0.9420 - val_loss: 1.9769 - lr: 7.0345e-05\n",
      "Epoch 53/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9374\n",
      "Epoch 53: val_loss did not improve from 1.93930\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 8s 119ms/step - loss: 0.9374 - val_loss: 2.0869 - lr: 6.9641e-05\n",
      "Epoch 54/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9218\n",
      "Epoch 54: val_loss did not improve from 1.93930\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 8s 118ms/step - loss: 0.9218 - val_loss: 2.0051 - lr: 6.8945e-05\n",
      "Epoch 55/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9262\n",
      "Epoch 55: val_loss improved from 1.93930 to 1.92910, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 8s 119ms/step - loss: 0.9262 - val_loss: 1.9291 - lr: 6.8255e-05\n",
      "Epoch 56/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9268\n",
      "Epoch 56: val_loss did not improve from 1.92910\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 8s 124ms/step - loss: 0.9268 - val_loss: 1.9397 - lr: 6.8255e-05\n",
      "Epoch 57/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9319\n",
      "Epoch 57: val_loss did not improve from 1.92910\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 8s 117ms/step - loss: 0.9319 - val_loss: 1.9325 - lr: 6.7573e-05\n",
      "Epoch 58/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9254\n",
      "Epoch 58: val_loss did not improve from 1.92910\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 8s 117ms/step - loss: 0.9254 - val_loss: 1.9569 - lr: 6.6897e-05\n",
      "Epoch 59/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9289\n",
      "Epoch 59: val_loss did not improve from 1.92910\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 8s 116ms/step - loss: 0.9289 - val_loss: 1.9489 - lr: 6.6228e-05\n",
      "Epoch 60/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9190\n",
      "Epoch 60: val_loss did not improve from 1.92910\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 9s 131ms/step - loss: 0.9190 - val_loss: 1.9537 - lr: 6.5566e-05\n",
      "Epoch 61/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9269\n",
      "Epoch 61: val_loss improved from 1.92910 to 1.84550, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 9s 134ms/step - loss: 0.9269 - val_loss: 1.8455 - lr: 6.4910e-05\n",
      "Epoch 62/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9221\n",
      "Epoch 62: val_loss did not improve from 1.84550\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 8s 121ms/step - loss: 0.9221 - val_loss: 1.9459 - lr: 6.4910e-05\n",
      "Epoch 63/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9227\n",
      "Epoch 63: val_loss did not improve from 1.84550\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 8s 116ms/step - loss: 0.9227 - val_loss: 1.9487 - lr: 6.4261e-05\n",
      "Epoch 64/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9200\n",
      "Epoch 64: val_loss did not improve from 1.84550\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 9s 136ms/step - loss: 0.9200 - val_loss: 1.9430 - lr: 6.3619e-05\n",
      "Epoch 65/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9108\n",
      "Epoch 65: val_loss did not improve from 1.84550\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 8s 119ms/step - loss: 0.9108 - val_loss: 2.0126 - lr: 6.2982e-05\n",
      "Epoch 66/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9259\n",
      "Epoch 66: val_loss did not improve from 1.84550\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 0.9259 - val_loss: 1.9232 - lr: 6.2353e-05\n",
      "Epoch 67/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9119\n",
      "Epoch 67: val_loss did not improve from 1.84550\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.9119 - val_loss: 1.9788 - lr: 6.1729e-05\n",
      "Epoch 68/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9123\n",
      "Epoch 68: val_loss did not improve from 1.84550\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.9123 - val_loss: 1.8766 - lr: 6.1112e-05\n",
      "Epoch 69/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9176\n",
      "Epoch 69: val_loss did not improve from 1.84550\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 7s 102ms/step - loss: 0.9176 - val_loss: 1.9355 - lr: 6.0501e-05\n",
      "Epoch 70/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9107\n",
      "Epoch 70: val_loss did not improve from 1.84550\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 6s 98ms/step - loss: 0.9107 - val_loss: 1.9597 - lr: 5.9896e-05\n",
      "Epoch 71/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9139\n",
      "Epoch 71: val_loss did not improve from 1.84550\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 5.8703650938696225e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.9139 - val_loss: 1.8989 - lr: 5.9297e-05\n",
      "Epoch 72/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9056\n",
      "Epoch 72: val_loss did not improve from 1.84550\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 5.811661550978897e-05.\n",
      "66/66 [==============================] - 6s 98ms/step - loss: 0.9056 - val_loss: 1.9293 - lr: 5.8704e-05\n",
      "Epoch 73/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9016\n",
      "Epoch 73: val_loss improved from 1.84550 to 1.82913, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 0.9016 - val_loss: 1.8291 - lr: 5.8117e-05\n",
      "Epoch 74/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9072\n",
      "Epoch 74: val_loss did not improve from 1.82913\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 5.753545068728272e-05.\n",
      "66/66 [==============================] - 7s 103ms/step - loss: 0.9072 - val_loss: 1.9029 - lr: 5.8117e-05\n",
      "Epoch 75/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9019\n",
      "Epoch 75: val_loss did not improve from 1.82913\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 5.6960095243994146e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 0.9019 - val_loss: 1.9529 - lr: 5.7535e-05\n",
      "Epoch 76/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9089\n",
      "Epoch 76: val_loss did not improve from 1.82913\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 5.639049515593797e-05.\n",
      "66/66 [==============================] - 6s 98ms/step - loss: 0.9089 - val_loss: 1.9369 - lr: 5.6960e-05\n",
      "Epoch 77/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9016\n",
      "Epoch 77: val_loss did not improve from 1.82913\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 5.5826589195930865e-05.\n",
      "66/66 [==============================] - 6s 98ms/step - loss: 0.9016 - val_loss: 1.9193 - lr: 5.6390e-05\n",
      "Epoch 78/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9039\n",
      "Epoch 78: val_loss did not improve from 1.82913\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 5.526832333998754e-05.\n",
      "66/66 [==============================] - 7s 98ms/step - loss: 0.9039 - val_loss: 1.9036 - lr: 5.5827e-05\n",
      "Epoch 79/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9098\n",
      "Epoch 79: val_loss did not improve from 1.82913\n",
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 5.471563996252371e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 7s 99ms/step - loss: 0.9098 - val_loss: 1.8941 - lr: 5.5268e-05\n",
      "Epoch 80/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8991\n",
      "Epoch 80: val_loss did not improve from 1.82913\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 5.416848503955407e-05.\n",
      "66/66 [==============================] - 7s 100ms/step - loss: 0.8991 - val_loss: 1.8995 - lr: 5.4716e-05\n",
      "Epoch 81/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9015\n",
      "Epoch 81: val_loss did not improve from 1.82913\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 5.362680094549433e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 0.9015 - val_loss: 1.8474 - lr: 5.4168e-05\n",
      "Epoch 82/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8946\n",
      "Epoch 82: val_loss improved from 1.82913 to 1.80252, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.15.hdf5\n",
      "66/66 [==============================] - 8s 114ms/step - loss: 0.8946 - val_loss: 1.8025 - lr: 5.3627e-05\n",
      "Epoch 83/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9014\n",
      "Epoch 83: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 5.309053365635918e-05.\n",
      "66/66 [==============================] - 7s 111ms/step - loss: 0.9014 - val_loss: 1.8980 - lr: 5.3627e-05\n",
      "Epoch 84/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8988\n",
      "Epoch 84: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 5.2559629148163366e-05.\n",
      "66/66 [==============================] - 6s 98ms/step - loss: 0.8988 - val_loss: 1.8703 - lr: 5.3091e-05\n",
      "Epoch 85/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9050\n",
      "Epoch 85: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 5.2034033396921584e-05.\n",
      "66/66 [==============================] - 7s 100ms/step - loss: 0.9050 - val_loss: 1.8897 - lr: 5.2560e-05\n",
      "Epoch 86/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9031\n",
      "Epoch 86: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 5.1513692378648555e-05.\n",
      "66/66 [==============================] - 7s 108ms/step - loss: 0.9031 - val_loss: 1.8665 - lr: 5.2034e-05\n",
      "Epoch 87/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8863\n",
      "Epoch 87: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 5.099855567095801e-05.\n",
      "66/66 [==============================] - 8s 117ms/step - loss: 0.8863 - val_loss: 1.9018 - lr: 5.1514e-05\n",
      "Epoch 88/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8987\n",
      "Epoch 88: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 5.048856924986467e-05.\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.8987 - val_loss: 1.8771 - lr: 5.0999e-05\n",
      "Epoch 89/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9001\n",
      "Epoch 89: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 4.998368269298226e-05.\n",
      "66/66 [==============================] - 7s 108ms/step - loss: 0.9001 - val_loss: 1.8486 - lr: 5.0489e-05\n",
      "Epoch 90/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8893\n",
      "Epoch 90: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 4.9483845577924514e-05.\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 0.8893 - val_loss: 1.9446 - lr: 4.9984e-05\n",
      "Epoch 91/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8900\n",
      "Epoch 91: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 4.898900748230517e-05.\n",
      "66/66 [==============================] - 7s 105ms/step - loss: 0.8900 - val_loss: 1.8970 - lr: 4.9484e-05\n",
      "Epoch 92/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8893\n",
      "Epoch 92: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 4.849911798373796e-05.\n",
      "66/66 [==============================] - 7s 106ms/step - loss: 0.8893 - val_loss: 1.8809 - lr: 4.8989e-05\n",
      "Epoch 93/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8916\n",
      "Epoch 93: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 4.801412665983662e-05.\n",
      "66/66 [==============================] - 6s 99ms/step - loss: 0.8916 - val_loss: 1.9145 - lr: 4.8499e-05\n",
      "Epoch 94/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8893\n",
      "Epoch 94: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 4.75339866898139e-05.\n",
      "66/66 [==============================] - 7s 103ms/step - loss: 0.8893 - val_loss: 1.8514 - lr: 4.8014e-05\n",
      "Epoch 95/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8956\n",
      "Epoch 95: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 4.705864765128354e-05.\n",
      "66/66 [==============================] - 7s 113ms/step - loss: 0.8956 - val_loss: 1.8553 - lr: 4.7534e-05\n",
      "Epoch 96/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8803\n",
      "Epoch 96: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 4.658806272345828e-05.\n",
      "66/66 [==============================] - 7s 103ms/step - loss: 0.8803 - val_loss: 1.9239 - lr: 4.7059e-05\n",
      "Epoch 97/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8830\n",
      "Epoch 97: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 4.612218148395186e-05.\n",
      "66/66 [==============================] - 9s 133ms/step - loss: 0.8830 - val_loss: 1.8941 - lr: 4.6588e-05\n",
      "Epoch 98/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8820\n",
      "Epoch 98: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 4.566096071357606e-05.\n",
      "66/66 [==============================] - 8s 122ms/step - loss: 0.8820 - val_loss: 1.9383 - lr: 4.6122e-05\n",
      "Epoch 99/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8838\n",
      "Epoch 99: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 4.52043499899446e-05.\n",
      "66/66 [==============================] - 12s 176ms/step - loss: 0.8838 - val_loss: 1.9255 - lr: 4.5661e-05\n",
      "Epoch 100/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8790\n",
      "Epoch 100: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 100: ReduceLROnPlateau reducing learning rate to 4.475230609386927e-05.\n",
      "66/66 [==============================] - 11s 163ms/step - loss: 0.8790 - val_loss: 1.9726 - lr: 4.5204e-05\n",
      "Epoch 101/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8838\n",
      "Epoch 101: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 4.43047822045628e-05.\n",
      "66/66 [==============================] - 9s 140ms/step - loss: 0.8838 - val_loss: 1.9502 - lr: 4.4752e-05\n",
      "Epoch 102/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8771\n",
      "Epoch 102: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 4.386173510283697e-05.\n",
      "66/66 [==============================] - 9s 143ms/step - loss: 0.8771 - val_loss: 1.9028 - lr: 4.4305e-05\n",
      "Epoch 103/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8863\n",
      "Epoch 103: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 4.342311796790454e-05.\n",
      "66/66 [==============================] - 8s 120ms/step - loss: 0.8863 - val_loss: 1.9868 - lr: 4.3862e-05\n",
      "Epoch 104/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8821\n",
      "Epoch 104: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 4.298888758057728e-05.\n",
      "66/66 [==============================] - 8s 126ms/step - loss: 0.8821 - val_loss: 2.0165 - lr: 4.3423e-05\n",
      "Epoch 105/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8759\n",
      "Epoch 105: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 4.2558997120067946e-05.\n",
      "66/66 [==============================] - 8s 118ms/step - loss: 0.8759 - val_loss: 1.8711 - lr: 4.2989e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8822\n",
      "Epoch 106: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 4.2133406968787315e-05.\n",
      "66/66 [==============================] - 8s 115ms/step - loss: 0.8822 - val_loss: 1.9251 - lr: 4.2559e-05\n",
      "Epoch 107/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8866\n",
      "Epoch 107: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 4.1712073907547166e-05.\n",
      "66/66 [==============================] - 8s 116ms/step - loss: 0.8866 - val_loss: 1.8909 - lr: 4.2133e-05\n",
      "Epoch 108/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8778\n",
      "Epoch 108: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 108: ReduceLROnPlateau reducing learning rate to 4.1294954717159274e-05.\n",
      "66/66 [==============================] - 10s 159ms/step - loss: 0.8778 - val_loss: 1.9311 - lr: 4.1712e-05\n",
      "Epoch 109/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8709\n",
      "Epoch 109: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 4.08820061784354e-05.\n",
      "66/66 [==============================] - 12s 175ms/step - loss: 0.8709 - val_loss: 1.9397 - lr: 4.1295e-05\n",
      "Epoch 110/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8830\n",
      "Epoch 110: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 110: ReduceLROnPlateau reducing learning rate to 4.0473185072187336e-05.\n",
      "66/66 [==============================] - 10s 152ms/step - loss: 0.8830 - val_loss: 1.9032 - lr: 4.0882e-05\n",
      "Epoch 111/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8705\n",
      "Epoch 111: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 4.0068451780825854e-05.\n",
      "66/66 [==============================] - 9s 137ms/step - loss: 0.8705 - val_loss: 1.8437 - lr: 4.0473e-05\n",
      "Epoch 112/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8780\n",
      "Epoch 112: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 112: ReduceLROnPlateau reducing learning rate to 3.966776668676175e-05.\n",
      "66/66 [==============================] - 9s 129ms/step - loss: 0.8780 - val_loss: 1.8638 - lr: 4.0068e-05\n",
      "Epoch 113/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8715\n",
      "Epoch 113: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 3.927109017240582e-05.\n",
      "66/66 [==============================] - 8s 120ms/step - loss: 0.8715 - val_loss: 1.9114 - lr: 3.9668e-05\n",
      "Epoch 114/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8758\n",
      "Epoch 114: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 114: ReduceLROnPlateau reducing learning rate to 3.887837901856983e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.8758 - val_loss: 1.9017 - lr: 3.9271e-05\n",
      "Epoch 115/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8722\n",
      "Epoch 115: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 115: ReduceLROnPlateau reducing learning rate to 3.848959360766457e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.8722 - val_loss: 1.9027 - lr: 3.8878e-05\n",
      "Epoch 116/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8804\n",
      "Epoch 116: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 3.810469792369986e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 0.8804 - val_loss: 1.9011 - lr: 3.8490e-05\n",
      "Epoch 117/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8627\n",
      "Epoch 117: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 3.772365234908648e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.8627 - val_loss: 1.9416 - lr: 3.8105e-05\n",
      "Epoch 118/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8732\n",
      "Epoch 118: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 118: ReduceLROnPlateau reducing learning rate to 3.734641726623522e-05.\n",
      "66/66 [==============================] - 7s 107ms/step - loss: 0.8732 - val_loss: 1.9233 - lr: 3.7724e-05\n",
      "Epoch 119/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8695\n",
      "Epoch 119: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 3.6972953057556876e-05.\n",
      "66/66 [==============================] - 6s 96ms/step - loss: 0.8695 - val_loss: 1.9326 - lr: 3.7346e-05\n",
      "Epoch 120/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8725\n",
      "Epoch 120: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 120: ReduceLROnPlateau reducing learning rate to 3.660322370706126e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 0.8725 - val_loss: 1.9071 - lr: 3.6973e-05\n",
      "Epoch 121/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8704\n",
      "Epoch 121: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 3.623719319875818e-05.\n",
      "66/66 [==============================] - 8s 117ms/step - loss: 0.8704 - val_loss: 1.9824 - lr: 3.6603e-05\n",
      "Epoch 122/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8708\n",
      "Epoch 122: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 3.5874821915058416e-05.\n",
      "66/66 [==============================] - 7s 103ms/step - loss: 0.8708 - val_loss: 1.9456 - lr: 3.6237e-05\n",
      "Epoch 123/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8782\n",
      "Epoch 123: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 123: ReduceLROnPlateau reducing learning rate to 3.551607383997179e-05.\n",
      "66/66 [==============================] - 7s 103ms/step - loss: 0.8782 - val_loss: 1.8884 - lr: 3.5875e-05\n",
      "Epoch 124/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8652\n",
      "Epoch 124: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 124: ReduceLROnPlateau reducing learning rate to 3.516091295750812e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.8652 - val_loss: 1.9255 - lr: 3.5516e-05\n",
      "Epoch 125/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8620\n",
      "Epoch 125: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 3.480930325167719e-05.\n",
      "66/66 [==============================] - 7s 101ms/step - loss: 0.8620 - val_loss: 1.9757 - lr: 3.5161e-05\n",
      "Epoch 126/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8750\n",
      "Epoch 126: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 3.446120870648883e-05.\n",
      "66/66 [==============================] - 7s 99ms/step - loss: 0.8750 - val_loss: 1.9117 - lr: 3.4809e-05\n",
      "Epoch 127/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8694\n",
      "Epoch 127: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 127: ReduceLROnPlateau reducing learning rate to 3.4116596907551866e-05.\n",
      "66/66 [==============================] - 6s 98ms/step - loss: 0.8694 - val_loss: 1.9297 - lr: 3.4461e-05\n",
      "Epoch 128/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8677\n",
      "Epoch 128: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 128: ReduceLROnPlateau reducing learning rate to 3.37754318388761e-05.\n",
      "66/66 [==============================] - 7s 102ms/step - loss: 0.8677 - val_loss: 2.0108 - lr: 3.4117e-05\n",
      "Epoch 129/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8613\n",
      "Epoch 129: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 129: ReduceLROnPlateau reducing learning rate to 3.3437677484471354e-05.\n",
      "66/66 [==============================] - 8s 115ms/step - loss: 0.8613 - val_loss: 1.9651 - lr: 3.3775e-05\n",
      "Epoch 130/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8699\n",
      "Epoch 130: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 130: ReduceLROnPlateau reducing learning rate to 3.310330142994644e-05.\n",
      "66/66 [==============================] - 8s 122ms/step - loss: 0.8699 - val_loss: 1.9432 - lr: 3.3438e-05\n",
      "Epoch 131/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8634\n",
      "Epoch 131: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 131: ReduceLROnPlateau reducing learning rate to 3.277226765931118e-05.\n",
      "66/66 [==============================] - 7s 104ms/step - loss: 0.8634 - val_loss: 1.9691 - lr: 3.3103e-05\n",
      "Epoch 132/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.8644\n",
      "Epoch 132: val_loss did not improve from 1.80252\n",
      "\n",
      "Epoch 132: ReduceLROnPlateau reducing learning rate to 3.24445437581744e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.8644 - val_loss: 1.9714 - lr: 3.2772e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: \"model_41\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_42 (InputLayer)          [(None, 45, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_123 (LSTM)                (None, 45, 24)       3744        ['input_42[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_82 (Dropout)           (None, 45, 24)       0           ['lstm_123[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_124 (LSTM)                (None, 45, 16)       2624        ['dropout_82[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_83 (Dropout)           (None, 45, 16)       0           ['lstm_124[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_125 (LSTM)                (None, 32)           6272        ['dropout_83[0][0]']             \n",
      "                                                                                                  \n",
      " dense_82 (Dense)               (None, 40)           1320        ['lstm_125[0][0]']               \n",
      "                                                                                                  \n",
      " dense_83 (Dense)               (None, 5)            205         ['dense_82[0][0]']               \n",
      "                                                                                                  \n",
      " tf.unstack_41 (TFOpLambda)     [(None,),            0           ['dense_83[0][0]']               \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,),                                                         \n",
      "                                 (None,)]                                                         \n",
      "                                                                                                  \n",
      " tf.expand_dims_205 (TFOpLambda  (None, 1)           0           ['tf.unstack_41[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_82 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_205[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_209 (TFOpLambda  (None, 1)           0           ['tf.unstack_41[0][4]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_123 (TFOpLamb  (None, 1)           0           ['tf.math.sigmoid_82[0][0]']     \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_83 (TFOpLambda  (None, 1)           0           ['tf.expand_dims_209[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_124 (TFOpLamb  (None, 1)           0           ['tf.math.multiply_123[0][0]']   \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.expand_dims_206 (TFOpLambda  (None, 1)           0           ['tf.unstack_41[0][1]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_208 (TFOpLambda  (None, 1)           0           ['tf.unstack_41[0][3]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_125 (TFOpLamb  (None, 1)           0           ['tf.math.sigmoid_83[0][0]']     \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_82 (TFOpL  (None, 1)           0           ['tf.math.multiply_124[0][0]']   \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.softplus_82 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_206[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_207 (TFOpLambda  (None, 1)           0           ['tf.unstack_41[0][2]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.softplus_83 (TFOpLambd  (None, 1)           0           ['tf.expand_dims_208[0][0]']     \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_83 (TFOpL  (None, 1)           0           ['tf.math.multiply_125[0][0]']   \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.stack_41 (TFOpLambda)       (None, 5, 1)         0           ['tf.__operators__.add_82[0][0]',\n",
      "                                                                  'tf.math.softplus_82[0][0]',    \n",
      "                                                                  'tf.expand_dims_207[0][0]',     \n",
      "                                                                  'tf.math.softplus_83[0][0]',    \n",
      "                                                                  'tf.__operators__.add_83[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,165\n",
      "Trainable params: 14,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "This is the start of the tunning of the model with weight of the positive component = _3_CCMP_t+1_0.16\n",
      "Epoch 1/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 3.4871\n",
      "Epoch 1: val_loss improved from inf to 4.14851, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 16s 153ms/step - loss: 3.4871 - val_loss: 4.1485 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 3.0005\n",
      "Epoch 2: val_loss improved from 4.14851 to 3.43988, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.16.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 9s 131ms/step - loss: 3.0005 - val_loss: 3.4399 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 2.0275\n",
      "Epoch 3: val_loss improved from 3.43988 to 3.17204, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 10s 144ms/step - loss: 2.0275 - val_loss: 3.1720 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.5525\n",
      "Epoch 4: val_loss improved from 3.17204 to 3.04085, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 9s 140ms/step - loss: 1.5525 - val_loss: 3.0408 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.4006\n",
      "Epoch 5: val_loss improved from 3.04085 to 2.84676, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 9s 140ms/step - loss: 1.4006 - val_loss: 2.8468 - lr: 1.0000e-04\n",
      "Epoch 6/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.3292\n",
      "Epoch 6: val_loss improved from 2.84676 to 2.68552, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 9s 140ms/step - loss: 1.3292 - val_loss: 2.6855 - lr: 1.0000e-04\n",
      "Epoch 7/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.2826\n",
      "Epoch 7: val_loss did not improve from 2.68552\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 9.899999749904965e-05.\n",
      "66/66 [==============================] - 10s 146ms/step - loss: 1.2826 - val_loss: 2.7372 - lr: 1.0000e-04\n",
      "Epoch 8/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.2484\n",
      "Epoch 8: val_loss did not improve from 2.68552\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 9.800999716389924e-05.\n",
      "66/66 [==============================] - 7s 113ms/step - loss: 1.2484 - val_loss: 2.7206 - lr: 9.9000e-05\n",
      "Epoch 9/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.2226\n",
      "Epoch 9: val_loss did not improve from 2.68552\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.702989402285311e-05.\n",
      "66/66 [==============================] - 8s 116ms/step - loss: 1.2226 - val_loss: 2.7815 - lr: 9.8010e-05\n",
      "Epoch 10/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1879\n",
      "Epoch 10: val_loss improved from 2.68552 to 2.44720, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 10s 147ms/step - loss: 1.1879 - val_loss: 2.4472 - lr: 9.7030e-05\n",
      "Epoch 11/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1764\n",
      "Epoch 11: val_loss did not improve from 2.44720\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 9.605959443433675e-05.\n",
      "66/66 [==============================] - 10s 152ms/step - loss: 1.1764 - val_loss: 2.8261 - lr: 9.7030e-05\n",
      "Epoch 12/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1540\n",
      "Epoch 12: val_loss did not improve from 2.44720\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 9.509899755357764e-05.\n",
      "66/66 [==============================] - 9s 139ms/step - loss: 1.1540 - val_loss: 2.4849 - lr: 9.6060e-05\n",
      "Epoch 13/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1250\n",
      "Epoch 13: val_loss improved from 2.44720 to 2.32656, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 8s 128ms/step - loss: 1.1250 - val_loss: 2.3266 - lr: 9.5099e-05\n",
      "Epoch 14/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1087\n",
      "Epoch 14: val_loss improved from 2.32656 to 2.31903, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 10s 148ms/step - loss: 1.1087 - val_loss: 2.3190 - lr: 9.5099e-05\n",
      "Epoch 15/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.1022\n",
      "Epoch 15: val_loss improved from 2.31903 to 2.08106, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 9s 140ms/step - loss: 1.1022 - val_loss: 2.0811 - lr: 9.5099e-05\n",
      "Epoch 16/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0744\n",
      "Epoch 16: val_loss improved from 2.08106 to 2.06627, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 9s 139ms/step - loss: 1.0744 - val_loss: 2.0663 - lr: 9.5099e-05\n",
      "Epoch 17/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0861\n",
      "Epoch 17: val_loss did not improve from 2.06627\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 9.414800973900128e-05.\n",
      "66/66 [==============================] - 9s 140ms/step - loss: 1.0861 - val_loss: 2.0888 - lr: 9.5099e-05\n",
      "Epoch 18/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0610\n",
      "Epoch 18: val_loss improved from 2.06627 to 2.04766, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 7s 111ms/step - loss: 1.0610 - val_loss: 2.0477 - lr: 9.4148e-05\n",
      "Epoch 19/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0646\n",
      "Epoch 19: val_loss did not improve from 2.04766\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 9.320653014583512e-05.\n",
      "66/66 [==============================] - 8s 120ms/step - loss: 1.0646 - val_loss: 2.0775 - lr: 9.4148e-05\n",
      "Epoch 20/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0435\n",
      "Epoch 20: val_loss did not improve from 2.04766\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 9.22744651325047e-05.\n",
      "66/66 [==============================] - 8s 117ms/step - loss: 1.0435 - val_loss: 2.1246 - lr: 9.3207e-05\n",
      "Epoch 21/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0397\n",
      "Epoch 21: val_loss improved from 2.04766 to 1.94465, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 8s 126ms/step - loss: 1.0397 - val_loss: 1.9446 - lr: 9.2274e-05\n",
      "Epoch 22/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0449\n",
      "Epoch 22: val_loss did not improve from 1.94465\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 9.13517210574355e-05.\n",
      "66/66 [==============================] - 9s 131ms/step - loss: 1.0449 - val_loss: 2.1152 - lr: 9.2274e-05\n",
      "Epoch 23/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0328\n",
      "Epoch 23: val_loss did not improve from 1.94465\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 9.043820427905303e-05.\n",
      "66/66 [==============================] - 9s 130ms/step - loss: 1.0328 - val_loss: 1.9545 - lr: 9.1352e-05\n",
      "Epoch 24/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0435\n",
      "Epoch 24: val_loss did not improve from 1.94465\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 8.953382115578279e-05.\n",
      "66/66 [==============================] - 11s 162ms/step - loss: 1.0435 - val_loss: 2.0137 - lr: 9.0438e-05\n",
      "Epoch 25/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0137\n",
      "Epoch 25: val_loss improved from 1.94465 to 1.89513, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 9s 140ms/step - loss: 1.0137 - val_loss: 1.8951 - lr: 8.9534e-05\n",
      "Epoch 26/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0119\n",
      "Epoch 26: val_loss did not improve from 1.89513\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 8.863848524924834e-05.\n",
      "66/66 [==============================] - 8s 119ms/step - loss: 1.0119 - val_loss: 2.0515 - lr: 8.9534e-05\n",
      "Epoch 27/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0150\n",
      "Epoch 27: val_loss improved from 1.89513 to 1.85215, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.16.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 8s 115ms/step - loss: 1.0150 - val_loss: 1.8522 - lr: 8.8638e-05\n",
      "Epoch 28/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0154\n",
      "Epoch 28: val_loss did not improve from 1.85215\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 8.775210291787516e-05.\n",
      "66/66 [==============================] - 9s 130ms/step - loss: 1.0154 - val_loss: 1.8573 - lr: 8.8638e-05\n",
      "Epoch 29/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0112\n",
      "Epoch 29: val_loss did not improve from 1.85215\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 8.687458052008878e-05.\n",
      "66/66 [==============================] - 8s 129ms/step - loss: 1.0112 - val_loss: 2.0106 - lr: 8.7752e-05\n",
      "Epoch 30/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0036\n",
      "Epoch 30: val_loss did not improve from 1.85215\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 8.600583161751274e-05.\n",
      "66/66 [==============================] - 9s 133ms/step - loss: 1.0036 - val_loss: 1.9257 - lr: 8.6875e-05\n",
      "Epoch 31/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9976\n",
      "Epoch 31: val_loss did not improve from 1.85215\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 8.514576977177058e-05.\n",
      "66/66 [==============================] - 9s 137ms/step - loss: 0.9976 - val_loss: 1.8742 - lr: 8.6006e-05\n",
      "Epoch 32/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 1.0073\n",
      "Epoch 32: val_loss improved from 1.85215 to 1.83854, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 8s 128ms/step - loss: 1.0073 - val_loss: 1.8385 - lr: 8.5146e-05\n",
      "Epoch 33/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9991\n",
      "Epoch 33: val_loss did not improve from 1.83854\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 8.429430854448584e-05.\n",
      "66/66 [==============================] - 9s 140ms/step - loss: 0.9991 - val_loss: 1.8521 - lr: 8.5146e-05\n",
      "Epoch 34/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9897\n",
      "Epoch 34: val_loss did not improve from 1.83854\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 8.345136870048008e-05.\n",
      "66/66 [==============================] - 8s 125ms/step - loss: 0.9897 - val_loss: 1.9223 - lr: 8.4294e-05\n",
      "Epoch 35/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9791\n",
      "Epoch 35: val_loss did not improve from 1.83854\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 8.261685659817886e-05.\n",
      "66/66 [==============================] - 8s 127ms/step - loss: 0.9791 - val_loss: 1.9242 - lr: 8.3451e-05\n",
      "Epoch 36/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9925\n",
      "Epoch 36: val_loss did not improve from 1.83854\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 8.179068579920567e-05.\n",
      "66/66 [==============================] - 8s 120ms/step - loss: 0.9925 - val_loss: 1.8566 - lr: 8.2617e-05\n",
      "Epoch 37/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9866\n",
      "Epoch 37: val_loss did not improve from 1.83854\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 8.097277706838213e-05.\n",
      "66/66 [==============================] - 8s 118ms/step - loss: 0.9866 - val_loss: 1.9001 - lr: 8.1791e-05\n",
      "Epoch 38/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9870\n",
      "Epoch 38: val_loss did not improve from 1.83854\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 8.01630511705298e-05.\n",
      "66/66 [==============================] - 9s 135ms/step - loss: 0.9870 - val_loss: 1.9220 - lr: 8.0973e-05\n",
      "Epoch 39/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9788\n",
      "Epoch 39: val_loss did not improve from 1.83854\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 7.936142166727223e-05.\n",
      "66/66 [==============================] - 8s 120ms/step - loss: 0.9788 - val_loss: 1.9290 - lr: 8.0163e-05\n",
      "Epoch 40/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9758\n",
      "Epoch 40: val_loss did not improve from 1.83854\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 7.8567809323431e-05.\n",
      "66/66 [==============================] - 8s 116ms/step - loss: 0.9758 - val_loss: 1.9277 - lr: 7.9361e-05\n",
      "Epoch 41/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9728\n",
      "Epoch 41: val_loss did not improve from 1.83854\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 7.778212770062965e-05.\n",
      "66/66 [==============================] - 8s 118ms/step - loss: 0.9728 - val_loss: 1.9539 - lr: 7.8568e-05\n",
      "Epoch 42/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9778\n",
      "Epoch 42: val_loss did not improve from 1.83854\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 7.70043047668878e-05.\n",
      "66/66 [==============================] - 8s 120ms/step - loss: 0.9778 - val_loss: 1.8840 - lr: 7.7782e-05\n",
      "Epoch 43/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9637\n",
      "Epoch 43: val_loss improved from 1.83854 to 1.83808, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 8s 125ms/step - loss: 0.9637 - val_loss: 1.8381 - lr: 7.7004e-05\n",
      "Epoch 44/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9713\n",
      "Epoch 44: val_loss did not improve from 1.83808\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 7.623426128702704e-05.\n",
      "66/66 [==============================] - 9s 136ms/step - loss: 0.9713 - val_loss: 1.9668 - lr: 7.7004e-05\n",
      "Epoch 45/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9761\n",
      "Epoch 45: val_loss did not improve from 1.83808\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 7.547191802586894e-05.\n",
      "66/66 [==============================] - 9s 132ms/step - loss: 0.9761 - val_loss: 1.9748 - lr: 7.6234e-05\n",
      "Epoch 46/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9589\n",
      "Epoch 46: val_loss did not improve from 1.83808\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 7.47171957482351e-05.\n",
      "66/66 [==============================] - 9s 133ms/step - loss: 0.9589 - val_loss: 1.8715 - lr: 7.5472e-05\n",
      "Epoch 47/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9577\n",
      "Epoch 47: val_loss did not improve from 1.83808\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 7.397002242214513e-05.\n",
      "66/66 [==============================] - 8s 128ms/step - loss: 0.9577 - val_loss: 1.9730 - lr: 7.4717e-05\n",
      "Epoch 48/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9570\n",
      "Epoch 48: val_loss did not improve from 1.83808\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 7.32303188124206e-05.\n",
      "66/66 [==============================] - 8s 126ms/step - loss: 0.9570 - val_loss: 1.8619 - lr: 7.3970e-05\n",
      "Epoch 49/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9540\n",
      "Epoch 49: val_loss did not improve from 1.83808\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 7.249801288708113e-05.\n",
      "66/66 [==============================] - 8s 125ms/step - loss: 0.9540 - val_loss: 1.8416 - lr: 7.3230e-05\n",
      "Epoch 50/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9530\n",
      "Epoch 50: val_loss did not improve from 1.83808\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 7.177303261414636e-05.\n",
      "66/66 [==============================] - 8s 117ms/step - loss: 0.9530 - val_loss: 1.8818 - lr: 7.2498e-05\n",
      "Epoch 51/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9521\n",
      "Epoch 51: val_loss did not improve from 1.83808\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 7.105529875843786e-05.\n",
      "66/66 [==============================] - 8s 113ms/step - loss: 0.9521 - val_loss: 1.9500 - lr: 7.1773e-05\n",
      "Epoch 52/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9524\n",
      "Epoch 52: val_loss did not improve from 1.83808\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 7.034474649117328e-05.\n",
      "66/66 [==============================] - 8s 127ms/step - loss: 0.9524 - val_loss: 1.9694 - lr: 7.1055e-05\n",
      "Epoch 53/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9626\n",
      "Epoch 53: val_loss did not improve from 1.83808\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 6.964129657717422e-05.\n",
      "66/66 [==============================] - 9s 133ms/step - loss: 0.9626 - val_loss: 1.9744 - lr: 7.0345e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9594\n",
      "Epoch 54: val_loss did not improve from 1.83808\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 6.894488418765832e-05.\n",
      "66/66 [==============================] - 8s 117ms/step - loss: 0.9594 - val_loss: 1.8867 - lr: 6.9641e-05\n",
      "Epoch 55/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9491\n",
      "Epoch 55: val_loss did not improve from 1.83808\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 6.82554372906452e-05.\n",
      "66/66 [==============================] - 8s 121ms/step - loss: 0.9491 - val_loss: 1.8819 - lr: 6.8945e-05\n",
      "Epoch 56/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9493\n",
      "Epoch 56: val_loss improved from 1.83808 to 1.81260, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 8s 123ms/step - loss: 0.9493 - val_loss: 1.8126 - lr: 6.8255e-05\n",
      "Epoch 57/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9434\n",
      "Epoch 57: val_loss did not improve from 1.81260\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 6.75728838541545e-05.\n",
      "66/66 [==============================] - 9s 132ms/step - loss: 0.9434 - val_loss: 1.9011 - lr: 6.8255e-05\n",
      "Epoch 58/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9517\n",
      "Epoch 58: val_loss did not improve from 1.81260\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 6.689715184620581e-05.\n",
      "66/66 [==============================] - 9s 135ms/step - loss: 0.9517 - val_loss: 1.8264 - lr: 6.7573e-05\n",
      "Epoch 59/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9415\n",
      "Epoch 59: val_loss did not improve from 1.81260\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 6.622818364121486e-05.\n",
      "66/66 [==============================] - 9s 140ms/step - loss: 0.9415 - val_loss: 1.8880 - lr: 6.6897e-05\n",
      "Epoch 60/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9369\n",
      "Epoch 60: val_loss did not improve from 1.81260\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 6.55659000040032e-05.\n",
      "66/66 [==============================] - 8s 126ms/step - loss: 0.9369 - val_loss: 1.9340 - lr: 6.6228e-05\n",
      "Epoch 61/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9335\n",
      "Epoch 61: val_loss did not improve from 1.81260\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 6.491024330898654e-05.\n",
      "66/66 [==============================] - 8s 120ms/step - loss: 0.9335 - val_loss: 1.9584 - lr: 6.5566e-05\n",
      "Epoch 62/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9445\n",
      "Epoch 62: val_loss did not improve from 1.81260\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 6.426114152418449e-05.\n",
      "66/66 [==============================] - 8s 119ms/step - loss: 0.9445 - val_loss: 1.8860 - lr: 6.4910e-05\n",
      "Epoch 63/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9324\n",
      "Epoch 63: val_loss did not improve from 1.81260\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 6.361852982081473e-05.\n",
      "66/66 [==============================] - 8s 121ms/step - loss: 0.9324 - val_loss: 1.8725 - lr: 6.4261e-05\n",
      "Epoch 64/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9524\n",
      "Epoch 64: val_loss did not improve from 1.81260\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 6.298234337009489e-05.\n",
      "66/66 [==============================] - 8s 117ms/step - loss: 0.9524 - val_loss: 1.8554 - lr: 6.3619e-05\n",
      "Epoch 65/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9372\n",
      "Epoch 65: val_loss did not improve from 1.81260\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 6.235251734324266e-05.\n",
      "66/66 [==============================] - 8s 115ms/step - loss: 0.9372 - val_loss: 1.9379 - lr: 6.2982e-05\n",
      "Epoch 66/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9357\n",
      "Epoch 66: val_loss did not improve from 1.81260\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 6.17289941146737e-05.\n",
      "66/66 [==============================] - 8s 117ms/step - loss: 0.9357 - val_loss: 1.9139 - lr: 6.2353e-05\n",
      "Epoch 67/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9415\n",
      "Epoch 67: val_loss did not improve from 1.81260\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 6.111170165240765e-05.\n",
      "66/66 [==============================] - 9s 138ms/step - loss: 0.9415 - val_loss: 1.8325 - lr: 6.1729e-05\n",
      "Epoch 68/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9335\n",
      "Epoch 68: val_loss did not improve from 1.81260\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 6.0500582330860196e-05.\n",
      "66/66 [==============================] - 8s 128ms/step - loss: 0.9335 - val_loss: 1.8222 - lr: 6.1112e-05\n",
      "Epoch 69/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9405\n",
      "Epoch 69: val_loss did not improve from 1.81260\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 5.9895574922848026e-05.\n",
      "66/66 [==============================] - 9s 130ms/step - loss: 0.9405 - val_loss: 1.9221 - lr: 6.0501e-05\n",
      "Epoch 70/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9270\n",
      "Epoch 70: val_loss did not improve from 1.81260\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 5.9296618201187815e-05.\n",
      "66/66 [==============================] - 8s 120ms/step - loss: 0.9270 - val_loss: 1.9382 - lr: 5.9896e-05\n",
      "Epoch 71/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9402\n",
      "Epoch 71: val_loss did not improve from 1.81260\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 5.8703650938696225e-05.\n",
      "66/66 [==============================] - 10s 146ms/step - loss: 0.9402 - val_loss: 1.8317 - lr: 5.9297e-05\n",
      "Epoch 72/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9402\n",
      "Epoch 72: val_loss did not improve from 1.81260\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 5.811661550978897e-05.\n",
      "66/66 [==============================] - 9s 134ms/step - loss: 0.9402 - val_loss: 1.8466 - lr: 5.8704e-05\n",
      "Epoch 73/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9362\n",
      "Epoch 73: val_loss did not improve from 1.81260\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 5.753545068728272e-05.\n",
      "66/66 [==============================] - 8s 125ms/step - loss: 0.9362 - val_loss: 1.8640 - lr: 5.8117e-05\n",
      "Epoch 74/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9287\n",
      "Epoch 74: val_loss did not improve from 1.81260\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 5.6960095243994146e-05.\n",
      "66/66 [==============================] - 8s 126ms/step - loss: 0.9287 - val_loss: 2.0019 - lr: 5.7535e-05\n",
      "Epoch 75/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9249\n",
      "Epoch 75: val_loss did not improve from 1.81260\n",
      "\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 5.639049515593797e-05.\n",
      "66/66 [==============================] - 10s 146ms/step - loss: 0.9249 - val_loss: 1.9512 - lr: 5.6960e-05\n",
      "Epoch 76/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9296\n",
      "Epoch 76: val_loss improved from 1.81260 to 1.81090, saving model to C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_3_CCMP_t+1_0.16.hdf5\n",
      "66/66 [==============================] - 8s 119ms/step - loss: 0.9296 - val_loss: 1.8109 - lr: 5.6390e-05\n",
      "Epoch 77/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9325\n",
      "Epoch 77: val_loss did not improve from 1.81090\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 5.5826589195930865e-05.\n",
      "66/66 [==============================] - 8s 114ms/step - loss: 0.9325 - val_loss: 1.8377 - lr: 5.6390e-05\n",
      "Epoch 78/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9314\n",
      "Epoch 78: val_loss did not improve from 1.81090\n",
      "\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 5.526832333998754e-05.\n",
      "66/66 [==============================] - 8s 117ms/step - loss: 0.9314 - val_loss: 1.8778 - lr: 5.5827e-05\n",
      "Epoch 79/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9187\n",
      "Epoch 79: val_loss did not improve from 1.81090\n",
      "\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 5.471563996252371e-05.\n",
      "66/66 [==============================] - 8s 117ms/step - loss: 0.9187 - val_loss: 1.9097 - lr: 5.5268e-05\n",
      "Epoch 80/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9189\n",
      "Epoch 80: val_loss did not improve from 1.81090\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 5.416848503955407e-05.\n",
      "66/66 [==============================] - 8s 122ms/step - loss: 0.9189 - val_loss: 1.9537 - lr: 5.4716e-05\n",
      "Epoch 81/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9209\n",
      "Epoch 81: val_loss did not improve from 1.81090\n",
      "\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 5.362680094549433e-05.\n",
      "66/66 [==============================] - 9s 132ms/step - loss: 0.9209 - val_loss: 1.9419 - lr: 5.4168e-05\n",
      "Epoch 82/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9290\n",
      "Epoch 82: val_loss did not improve from 1.81090\n",
      "\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 5.309053365635918e-05.\n",
      "66/66 [==============================] - 9s 134ms/step - loss: 0.9290 - val_loss: 1.9249 - lr: 5.3627e-05\n",
      "Epoch 83/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9231\n",
      "Epoch 83: val_loss did not improve from 1.81090\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 5.2559629148163366e-05.\n",
      "66/66 [==============================] - 9s 130ms/step - loss: 0.9231 - val_loss: 1.9828 - lr: 5.3091e-05\n",
      "Epoch 84/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9111\n",
      "Epoch 84: val_loss did not improve from 1.81090\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 5.2034033396921584e-05.\n",
      "66/66 [==============================] - 8s 127ms/step - loss: 0.9111 - val_loss: 1.9547 - lr: 5.2560e-05\n",
      "Epoch 85/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9294\n",
      "Epoch 85: val_loss did not improve from 1.81090\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 5.1513692378648555e-05.\n",
      "66/66 [==============================] - 9s 129ms/step - loss: 0.9294 - val_loss: 1.9443 - lr: 5.2034e-05\n",
      "Epoch 86/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9191\n",
      "Epoch 86: val_loss did not improve from 1.81090\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 5.099855567095801e-05.\n",
      "66/66 [==============================] - 9s 134ms/step - loss: 0.9191 - val_loss: 2.0300 - lr: 5.1514e-05\n",
      "Epoch 87/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9196\n",
      "Epoch 87: val_loss did not improve from 1.81090\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 5.048856924986467e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.9196 - val_loss: 1.8712 - lr: 5.0999e-05\n",
      "Epoch 88/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9244\n",
      "Epoch 88: val_loss did not improve from 1.81090\n",
      "\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 4.998368269298226e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.9244 - val_loss: 1.9119 - lr: 5.0489e-05\n",
      "Epoch 89/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9220\n",
      "Epoch 89: val_loss did not improve from 1.81090\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 4.9483845577924514e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.9220 - val_loss: 1.9522 - lr: 4.9984e-05\n",
      "Epoch 90/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9165\n",
      "Epoch 90: val_loss did not improve from 1.81090\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 4.898900748230517e-05.\n",
      "66/66 [==============================] - 6s 97ms/step - loss: 0.9165 - val_loss: 1.9526 - lr: 4.9484e-05\n",
      "Epoch 91/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9096\n",
      "Epoch 91: val_loss did not improve from 1.81090\n",
      "\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 4.849911798373796e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.9096 - val_loss: 1.9555 - lr: 4.8989e-05\n",
      "Epoch 92/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9147\n",
      "Epoch 92: val_loss did not improve from 1.81090\n",
      "\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 4.801412665983662e-05.\n",
      "66/66 [==============================] - 7s 111ms/step - loss: 0.9147 - val_loss: 1.9404 - lr: 4.8499e-05\n",
      "Epoch 93/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9202\n",
      "Epoch 93: val_loss did not improve from 1.81090\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 4.75339866898139e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.9202 - val_loss: 1.8999 - lr: 4.8014e-05\n",
      "Epoch 94/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9220\n",
      "Epoch 94: val_loss did not improve from 1.81090\n",
      "\n",
      "Epoch 94: ReduceLROnPlateau reducing learning rate to 4.705864765128354e-05.\n",
      "66/66 [==============================] - 6s 94ms/step - loss: 0.9220 - val_loss: 1.9598 - lr: 4.7534e-05\n",
      "Epoch 95/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9189\n",
      "Epoch 95: val_loss did not improve from 1.81090\n",
      "\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 4.658806272345828e-05.\n",
      "66/66 [==============================] - 6s 91ms/step - loss: 0.9189 - val_loss: 1.9040 - lr: 4.7059e-05\n",
      "Epoch 96/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9190\n",
      "Epoch 96: val_loss did not improve from 1.81090\n",
      "\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 4.612218148395186e-05.\n",
      "66/66 [==============================] - 6s 95ms/step - loss: 0.9190 - val_loss: 1.9846 - lr: 4.6588e-05\n",
      "Epoch 97/1000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.9153\n",
      "Epoch 97: val_loss did not improve from 1.81090\n",
      "\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 4.566096071357606e-05.\n",
      "66/66 [==============================] - 6s 93ms/step - loss: 0.9153 - val_loss: 1.9453 - lr: 4.6122e-05\n",
      "Epoch 98/1000\n",
      "16/66 [======>.......................] - ETA: 4s - loss: 0.9019"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "import os\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "for loop in range(1,10):\n",
    "    os.chdir(r\"C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\")\n",
    "    for i in range(5,20):\n",
    "        global k \n",
    "        k = i/100\n",
    "        j = 'CCMP_t+1'\n",
    "        import os\n",
    "        # Check whether the specified path exists or not\n",
    "        path = fr\"C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\"\n",
    "        isExist = os.path.exists(path)\n",
    "        if not isExist:\n",
    "            # Create a new directory because it does not exist\n",
    "            os.makedirs(path)\n",
    "            print(\"The new directory is created!\")\n",
    "\n",
    "        ###MODEL INFRASTRUCTURE###\n",
    "            ###MODEL INFRASTRUCTURE###\n",
    "                ###MODEL INFRASTRUCTURE###\n",
    "                    ###MODEL INFRASTRUCTURE###\n",
    "                        ###MODEL INFRASTRUCTURE###\n",
    "                            ###MODEL INFRASTRUCTURE###\n",
    "                                ###MODEL INFRASTRUCTURE###\n",
    "        # Define inputs with predefined shape\n",
    "        inputs = Input(shape=(X_train.shape[1],X_train.shape[2]))\n",
    "        # Build network with some predefined architecture\n",
    "        output1 = LSTM(24,recurrent_initializer =glorot_uniform(),return_sequences = True)(inputs)\n",
    "        do1 = Dropout(0.4)(output1)\n",
    "        output2 = LSTM(16,return_sequences = True)(do1)\n",
    "        do2 = Dropout(0.4)(output2)\n",
    "        output3 = LSTM(32)(do2)\n",
    "        output4 = Dense(40)(output3)\n",
    "        outputs = Dense(5)(output4)\n",
    "        distribution_outputs = layer.Normal_Dist_Layer_2(outputs)\n",
    "\n",
    "        # Construct model\n",
    "        model4 = Model(inputs=inputs, outputs=distribution_outputs)\n",
    "        model4.summary()\n",
    "        print('This is the start of the tunning of the model with weight of the positive component =',f'_{loop}_{j}_{k}')\n",
    "        model4.compile(loss = layer.negative_log_likelihood_loss_2, optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001))\n",
    "\n",
    "        # checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='max')\n",
    "        # callbacks_list = [checkpoint]\n",
    "        # filepath=\"weights-improvement-{epoch:02d}-{val_binary_accuracy:.2f}.hdf5\"\n",
    "        # history = model3.fit(X_train, Y_train, batch_size=256, epochs = 1000,\n",
    "        #                     validation_data = [X_val, Y_val], callbacks = callback_list)\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50)\n",
    "        filepath = fr\"C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_{loop}_{j}_{k}.hdf5\"\n",
    "        reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\",factor=0.99, patience = 1, verbose=1)\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "        history = model4.fit(X_train, Y_train, \n",
    "                                 batch_size=128, \n",
    "                                 epochs = 1000, \n",
    "                                 validation_data = [X_val, Y_val], \n",
    "                                 callbacks = [checkpoint, early_stop, reduce_lr])\n",
    "\n",
    "        import os \n",
    "        os.chmod(filepath, 0o777)\n",
    "        minimum = pd.Series(history.history['val_loss']).min()\n",
    "        historydf = pd.DataFrame(history.history)\n",
    "        historydf.to_csv(f\"_{j}exp1-{k}_tloss&vloss_{minimum}.csv\") # Save training loss vs validation loss\n",
    "\n",
    "        %matplotlib inline\n",
    "        plt.plot(history.history['loss'],label = 'Training loss')\n",
    "        plt.plot(history.history['val_loss'], label = 'Validation loss')\n",
    "        plt.legend()\n",
    "        plt.figure()\n",
    "        plt.savefig(f'_{j}exp1-{k}_tlossvloss_{minimum}.jpg')\n",
    "        plt.show()\n",
    "        print('\\n')\n",
    "        print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aecb522",
   "metadata": {},
   "source": [
    "# LOAD MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e53755e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261/261 [==============================] - 7s 20ms/step\n",
      "The negative log-likelihood of the model in the training period is: 3.6925797046719357\n",
      "The Deviation Measure the training period is: 0.01\n",
      "Pearson corr coef of expected return of the Gaussian Mean with Y_train during Training Period: -0.03\n",
      "Pearson corr coef of mu2 with Y_train during Training Period: -0.03\n",
      "Pearson corr coef of var of Y_train vs weighted_sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Training Period: 0.67\n",
      "var of Y_train is calculated based on the var of previous 59 days\n",
      "TRAINING PERIOD Spearman rank correlation coefficient: -0.02\n",
      "TRAINING PERIOD p-val:0.04\n",
      "15/15 [==============================] - 0s 22ms/step\n",
      "The negative log-likelihood of the model in the testing period is: 6.203037294994178\n",
      "The Deviation Measure the Test period is: 0.0026\n",
      "Pearson corr coef of expected return of the Gaussian Mixture with Y_Test during Test Period: 0.04\n",
      "Pearson corr coef of mu2 with Y_Test during Test Period: 0.05\n",
      "Pearson corr coef of var of Y_Test vs weighted sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Test Period: -0.21\n",
      "var of Y_test is calculated based on the var of previous 59 days\n",
      "TESTING PERIOD Spearman rank correlation coefficient: 0.03\n",
      "TESTING PERIOD p-val:0.53\n",
      "261/261 [==============================] - 7s 20ms/step\n",
      "The negative log-likelihood of the model in the training period is: 3.6206377471079167\n",
      "The Deviation Measure the training period is: 0.01\n",
      "Pearson corr coef of expected return of the Gaussian Mean with Y_train during Training Period: 0.03\n",
      "Pearson corr coef of mu2 with Y_train during Training Period: 0.03\n",
      "Pearson corr coef of var of Y_train vs weighted_sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Training Period: 0.71\n",
      "var of Y_train is calculated based on the var of previous 59 days\n",
      "TRAINING PERIOD Spearman rank correlation coefficient: 0.02\n",
      "TRAINING PERIOD p-val:0.09\n",
      "15/15 [==============================] - 0s 22ms/step\n",
      "The negative log-likelihood of the model in the testing period is: 7.879184254732211\n",
      "The Deviation Measure the Test period is: 0.0286\n",
      "Pearson corr coef of expected return of the Gaussian Mixture with Y_Test during Test Period: 0.06\n",
      "Pearson corr coef of mu2 with Y_Test during Test Period: 0.06\n",
      "Pearson corr coef of var of Y_Test vs weighted sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Test Period: 0.01\n",
      "var of Y_test is calculated based on the var of previous 59 days\n",
      "TESTING PERIOD Spearman rank correlation coefficient: 0.10\n",
      "TESTING PERIOD p-val:0.03\n",
      "261/261 [==============================] - 8s 20ms/step\n",
      "The negative log-likelihood of the model in the training period is: 3.540012928249142\n",
      "The Deviation Measure the training period is: 0.01\n",
      "Pearson corr coef of expected return of the Gaussian Mean with Y_train during Training Period: 0.03\n",
      "Pearson corr coef of mu2 with Y_train during Training Period: 0.03\n",
      "Pearson corr coef of var of Y_train vs weighted_sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Training Period: 0.71\n",
      "var of Y_train is calculated based on the var of previous 59 days\n",
      "TRAINING PERIOD Spearman rank correlation coefficient: 0.00\n",
      "TRAINING PERIOD p-val:0.65\n",
      "15/15 [==============================] - 0s 22ms/step\n",
      "The negative log-likelihood of the model in the testing period is: 6.270192709785114\n",
      "The Deviation Measure the Test period is: 0.0151\n",
      "Pearson corr coef of expected return of the Gaussian Mixture with Y_Test during Test Period: 0.00\n",
      "Pearson corr coef of mu2 with Y_Test during Test Period: 0.00\n",
      "Pearson corr coef of var of Y_Test vs weighted sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Test Period: 0.06\n",
      "var of Y_test is calculated based on the var of previous 59 days\n",
      "TESTING PERIOD Spearman rank correlation coefficient: 0.04\n",
      "TESTING PERIOD p-val:0.43\n",
      "261/261 [==============================] - 7s 20ms/step\n",
      "The negative log-likelihood of the model in the training period is: 3.5063970161074547\n",
      "The Deviation Measure the training period is: 0.01\n",
      "Pearson corr coef of expected return of the Gaussian Mean with Y_train during Training Period: 0.00\n",
      "Pearson corr coef of mu2 with Y_train during Training Period: 0.00\n",
      "Pearson corr coef of var of Y_train vs weighted_sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Training Period: 0.58\n",
      "var of Y_train is calculated based on the var of previous 59 days\n",
      "TRAINING PERIOD Spearman rank correlation coefficient: 0.01\n",
      "TRAINING PERIOD p-val:0.57\n",
      "15/15 [==============================] - 0s 22ms/step\n",
      "The negative log-likelihood of the model in the testing period is: 4.032386743940005\n",
      "The Deviation Measure the Test period is: 0.0167\n",
      "Pearson corr coef of expected return of the Gaussian Mixture with Y_Test during Test Period: 0.07\n",
      "Pearson corr coef of mu2 with Y_Test during Test Period: 0.07\n",
      "Pearson corr coef of var of Y_Test vs weighted sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Test Period: 0.14\n",
      "var of Y_test is calculated based on the var of previous 59 days\n",
      "TESTING PERIOD Spearman rank correlation coefficient: 0.04\n",
      "TESTING PERIOD p-val:0.40\n",
      "261/261 [==============================] - 8s 22ms/step\n",
      "The negative log-likelihood of the model in the training period is: 3.4551802245425263\n",
      "The Deviation Measure the training period is: 0.01\n",
      "Pearson corr coef of expected return of the Gaussian Mean with Y_train during Training Period: 0.00\n",
      "Pearson corr coef of mu2 with Y_train during Training Period: 0.00\n",
      "Pearson corr coef of var of Y_train vs weighted_sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Training Period: 0.76\n",
      "var of Y_train is calculated based on the var of previous 59 days\n",
      "TRAINING PERIOD Spearman rank correlation coefficient: 0.01\n",
      "TRAINING PERIOD p-val:0.34\n",
      "15/15 [==============================] - 0s 24ms/step\n",
      "The negative log-likelihood of the model in the testing period is: 6.004143257609232\n",
      "The Deviation Measure the Test period is: 0.0203\n",
      "Pearson corr coef of expected return of the Gaussian Mixture with Y_Test during Test Period: -0.07\n",
      "Pearson corr coef of mu2 with Y_Test during Test Period: -0.07\n",
      "Pearson corr coef of var of Y_Test vs weighted sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Test Period: 0.30\n",
      "var of Y_test is calculated based on the var of previous 59 days\n",
      "TESTING PERIOD Spearman rank correlation coefficient: -0.06\n",
      "TESTING PERIOD p-val:0.21\n",
      "261/261 [==============================] - 9s 24ms/step\n",
      "The negative log-likelihood of the model in the training period is: 3.448141871819069\n",
      "The Deviation Measure the training period is: 0.02\n",
      "Pearson corr coef of expected return of the Gaussian Mean with Y_train during Training Period: 0.02\n",
      "Pearson corr coef of mu2 with Y_train during Training Period: 0.02\n",
      "Pearson corr coef of var of Y_train vs weighted_sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Training Period: 0.68\n",
      "var of Y_train is calculated based on the var of previous 59 days\n",
      "TRAINING PERIOD Spearman rank correlation coefficient: 0.01\n",
      "TRAINING PERIOD p-val:0.29\n",
      "15/15 [==============================] - 0s 25ms/step\n",
      "The negative log-likelihood of the model in the testing period is: 6.33915809384635\n",
      "The Deviation Measure the Test period is: 0.0059\n",
      "Pearson corr coef of expected return of the Gaussian Mixture with Y_Test during Test Period: -0.05\n",
      "Pearson corr coef of mu2 with Y_Test during Test Period: -0.05\n",
      "Pearson corr coef of var of Y_Test vs weighted sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Test Period: -0.21\n",
      "var of Y_test is calculated based on the var of previous 59 days\n",
      "TESTING PERIOD Spearman rank correlation coefficient: -0.08\n",
      "TESTING PERIOD p-val:0.08\n",
      "261/261 [==============================] - 8s 22ms/step\n",
      "The negative log-likelihood of the model in the training period is: 3.561446847486156\n",
      "The Deviation Measure the training period is: 0.02\n",
      "Pearson corr coef of expected return of the Gaussian Mean with Y_train during Training Period: -0.01\n",
      "Pearson corr coef of mu2 with Y_train during Training Period: -0.01\n",
      "Pearson corr coef of var of Y_train vs weighted_sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Training Period: 0.57\n",
      "var of Y_train is calculated based on the var of previous 59 days\n",
      "TRAINING PERIOD Spearman rank correlation coefficient: 0.01\n",
      "TRAINING PERIOD p-val:0.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 21ms/step\n",
      "The negative log-likelihood of the model in the testing period is: 3.7130504678402287\n",
      "The Deviation Measure the Test period is: 0.0123\n",
      "Pearson corr coef of expected return of the Gaussian Mixture with Y_Test during Test Period: -0.10\n",
      "Pearson corr coef of mu2 with Y_Test during Test Period: -0.10\n",
      "Pearson corr coef of var of Y_Test vs weighted sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Test Period: -0.12\n",
      "var of Y_test is calculated based on the var of previous 59 days\n",
      "TESTING PERIOD Spearman rank correlation coefficient: -0.13\n",
      "TESTING PERIOD p-val:0.01\n",
      "261/261 [==============================] - 7s 20ms/step\n",
      "The negative log-likelihood of the model in the training period is: 3.3357941149692403\n",
      "The Deviation Measure the training period is: 0.02\n",
      "Pearson corr coef of expected return of the Gaussian Mean with Y_train during Training Period: 0.05\n",
      "Pearson corr coef of mu2 with Y_train during Training Period: 0.05\n",
      "Pearson corr coef of var of Y_train vs weighted_sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Training Period: 0.71\n",
      "var of Y_train is calculated based on the var of previous 59 days\n",
      "TRAINING PERIOD Spearman rank correlation coefficient: 0.01\n",
      "TRAINING PERIOD p-val:0.28\n",
      "15/15 [==============================] - 0s 21ms/step\n",
      "The negative log-likelihood of the model in the testing period is: 9.162853025734302\n",
      "The Deviation Measure the Test period is: 0.0116\n",
      "Pearson corr coef of expected return of the Gaussian Mixture with Y_Test during Test Period: 0.01\n",
      "Pearson corr coef of mu2 with Y_Test during Test Period: 0.01\n",
      "Pearson corr coef of var of Y_Test vs weighted sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Test Period: 0.16\n",
      "var of Y_test is calculated based on the var of previous 59 days\n",
      "TESTING PERIOD Spearman rank correlation coefficient: 0.06\n",
      "TESTING PERIOD p-val:0.17\n",
      "261/261 [==============================] - 8s 21ms/step\n",
      "The negative log-likelihood of the model in the training period is: 3.3473008593801246\n",
      "The Deviation Measure the training period is: 0.02\n",
      "Pearson corr coef of expected return of the Gaussian Mean with Y_train during Training Period: 0.05\n",
      "Pearson corr coef of mu2 with Y_train during Training Period: 0.05\n",
      "Pearson corr coef of var of Y_train vs weighted_sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Training Period: 0.74\n",
      "var of Y_train is calculated based on the var of previous 59 days\n",
      "TRAINING PERIOD Spearman rank correlation coefficient: 0.01\n",
      "TRAINING PERIOD p-val:0.42\n",
      "15/15 [==============================] - 0s 22ms/step\n",
      "The negative log-likelihood of the model in the testing period is: 4.0906944770152505\n",
      "The Deviation Measure the Test period is: 0.0137\n",
      "Pearson corr coef of expected return of the Gaussian Mixture with Y_Test during Test Period: 0.02\n",
      "Pearson corr coef of mu2 with Y_Test during Test Period: 0.02\n",
      "Pearson corr coef of var of Y_Test vs weighted sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Test Period: -0.03\n",
      "var of Y_test is calculated based on the var of previous 59 days\n",
      "TESTING PERIOD Spearman rank correlation coefficient: 0.05\n",
      "TESTING PERIOD p-val:0.27\n",
      "261/261 [==============================] - 8s 21ms/step\n",
      "The negative log-likelihood of the model in the training period is: 3.2661227097896535\n",
      "The Deviation Measure the training period is: 0.02\n",
      "Pearson corr coef of expected return of the Gaussian Mean with Y_train during Training Period: 0.03\n",
      "Pearson corr coef of mu2 with Y_train during Training Period: 0.03\n",
      "Pearson corr coef of var of Y_train vs weighted_sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Training Period: 0.71\n",
      "var of Y_train is calculated based on the var of previous 59 days\n",
      "TRAINING PERIOD Spearman rank correlation coefficient: 0.01\n",
      "TRAINING PERIOD p-val:0.18\n",
      "15/15 [==============================] - 0s 22ms/step\n",
      "The negative log-likelihood of the model in the testing period is: 4.115786939192172\n",
      "The Deviation Measure the Test period is: 0.1839\n",
      "Pearson corr coef of expected return of the Gaussian Mixture with Y_Test during Test Period: -0.06\n",
      "Pearson corr coef of mu2 with Y_Test during Test Period: -0.06\n",
      "Pearson corr coef of var of Y_Test vs weighted sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Test Period: 0.37\n",
      "var of Y_test is calculated based on the var of previous 59 days\n",
      "TESTING PERIOD Spearman rank correlation coefficient: -0.02\n",
      "TESTING PERIOD p-val:0.71\n",
      "261/261 [==============================] - 8s 22ms/step\n",
      "The negative log-likelihood of the model in the training period is: 3.291489726386735\n",
      "The Deviation Measure the training period is: 0.03\n",
      "Pearson corr coef of expected return of the Gaussian Mean with Y_train during Training Period: 0.04\n",
      "Pearson corr coef of mu2 with Y_train during Training Period: 0.04\n",
      "Pearson corr coef of var of Y_train vs weighted_sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Training Period: 0.73\n",
      "var of Y_train is calculated based on the var of previous 59 days\n",
      "TRAINING PERIOD Spearman rank correlation coefficient: 0.02\n",
      "TRAINING PERIOD p-val:0.03\n",
      "15/15 [==============================] - 0s 23ms/step\n",
      "The negative log-likelihood of the model in the testing period is: 7.022606115613976\n",
      "The Deviation Measure the Test period is: 0.0221\n",
      "Pearson corr coef of expected return of the Gaussian Mixture with Y_Test during Test Period: 0.03\n",
      "Pearson corr coef of mu2 with Y_Test during Test Period: 0.03\n",
      "Pearson corr coef of var of Y_Test vs weighted sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Test Period: 0.52\n",
      "var of Y_test is calculated based on the var of previous 59 days\n",
      "TESTING PERIOD Spearman rank correlation coefficient: 0.02\n",
      "TESTING PERIOD p-val:0.60\n",
      "261/261 [==============================] - 8s 21ms/step\n",
      "The negative log-likelihood of the model in the training period is: 3.2129303082036365\n",
      "The Deviation Measure the training period is: 0.02\n",
      "Pearson corr coef of expected return of the Gaussian Mean with Y_train during Training Period: 0.03\n",
      "Pearson corr coef of mu2 with Y_train during Training Period: 0.03\n",
      "Pearson corr coef of var of Y_train vs weighted_sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Training Period: 0.72\n",
      "var of Y_train is calculated based on the var of previous 59 days\n",
      "TRAINING PERIOD Spearman rank correlation coefficient: 0.01\n",
      "TRAINING PERIOD p-val:0.35\n",
      "15/15 [==============================] - 0s 22ms/step\n",
      "The negative log-likelihood of the model in the testing period is: 3.7312414325481593\n",
      "The Deviation Measure the Test period is: -0.0078\n",
      "Pearson corr coef of expected return of the Gaussian Mixture with Y_Test during Test Period: -0.03\n",
      "Pearson corr coef of mu2 with Y_Test during Test Period: -0.02\n",
      "Pearson corr coef of var of Y_Test vs weighted sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Test Period: 0.25\n",
      "var of Y_test is calculated based on the var of previous 59 days\n",
      "TESTING PERIOD Spearman rank correlation coefficient: -0.04\n",
      "TESTING PERIOD p-val:0.38\n",
      "261/261 [==============================] - 8s 22ms/step\n",
      "The negative log-likelihood of the model in the training period is: 3.218908947734829\n",
      "The Deviation Measure the training period is: 0.03\n",
      "Pearson corr coef of expected return of the Gaussian Mean with Y_train during Training Period: 0.03\n",
      "Pearson corr coef of mu2 with Y_train during Training Period: 0.03\n",
      "Pearson corr coef of var of Y_train vs weighted_sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Training Period: 0.78\n",
      "var of Y_train is calculated based on the var of previous 59 days\n",
      "TRAINING PERIOD Spearman rank correlation coefficient: 0.01\n",
      "TRAINING PERIOD p-val:0.17\n",
      "15/15 [==============================] - 0s 23ms/step\n",
      "The negative log-likelihood of the model in the testing period is: 6.643969206713659\n",
      "The Deviation Measure the Test period is: 0.0180\n",
      "Pearson corr coef of expected return of the Gaussian Mixture with Y_Test during Test Period: 0.09\n",
      "Pearson corr coef of mu2 with Y_Test during Test Period: 0.09\n",
      "Pearson corr coef of var of Y_Test vs weighted sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Test Period: 0.43\n",
      "var of Y_test is calculated based on the var of previous 59 days\n",
      "TESTING PERIOD Spearman rank correlation coefficient: 0.12\n",
      "TESTING PERIOD p-val:0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261/261 [==============================] - 8s 20ms/step\n",
      "The negative log-likelihood of the model in the training period is: 3.1815199217535612\n",
      "The Deviation Measure the training period is: 0.03\n",
      "Pearson corr coef of expected return of the Gaussian Mean with Y_train during Training Period: 0.03\n",
      "Pearson corr coef of mu2 with Y_train during Training Period: 0.03\n",
      "Pearson corr coef of var of Y_train vs weighted_sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Training Period: 0.70\n",
      "var of Y_train is calculated based on the var of previous 59 days\n",
      "TRAINING PERIOD Spearman rank correlation coefficient: 0.01\n",
      "TRAINING PERIOD p-val:0.41\n",
      "15/15 [==============================] - 0s 22ms/step\n",
      "The negative log-likelihood of the model in the testing period is: 5.141128658288086\n",
      "The Deviation Measure the Test period is: 0.0168\n",
      "Pearson corr coef of expected return of the Gaussian Mixture with Y_Test during Test Period: -0.08\n",
      "Pearson corr coef of mu2 with Y_Test during Test Period: -0.08\n",
      "Pearson corr coef of var of Y_Test vs weighted sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Test Period: 0.60\n",
      "var of Y_test is calculated based on the var of previous 59 days\n",
      "TESTING PERIOD Spearman rank correlation coefficient: -0.11\n",
      "TESTING PERIOD p-val:0.02\n",
      "261/261 [==============================] - 7s 20ms/step\n",
      "The negative log-likelihood of the model in the training period is: 3.154859442550658\n",
      "The Deviation Measure the training period is: 0.03\n",
      "Pearson corr coef of expected return of the Gaussian Mean with Y_train during Training Period: 0.04\n",
      "Pearson corr coef of mu2 with Y_train during Training Period: 0.04\n",
      "Pearson corr coef of var of Y_train vs weighted_sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Training Period: 0.72\n",
      "var of Y_train is calculated based on the var of previous 59 days\n",
      "TRAINING PERIOD Spearman rank correlation coefficient: 0.03\n",
      "TRAINING PERIOD p-val:0.01\n",
      "15/15 [==============================] - 0s 20ms/step\n",
      "The negative log-likelihood of the model in the testing period is: 3.785406844190049\n",
      "The Deviation Measure the Test period is: 0.0288\n",
      "Pearson corr coef of expected return of the Gaussian Mixture with Y_Test during Test Period: 0.07\n",
      "Pearson corr coef of mu2 with Y_Test during Test Period: 0.07\n",
      "Pearson corr coef of var of Y_Test vs weighted sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Test Period: 0.36\n",
      "var of Y_test is calculated based on the var of previous 59 days\n",
      "TESTING PERIOD Spearman rank correlation coefficient: 0.11\n",
      "TESTING PERIOD p-val:0.02\n",
      "261/261 [==============================] - 8s 21ms/step\n",
      "The negative log-likelihood of the model in the training period is: 3.686777261320608\n",
      "The Deviation Measure the training period is: 0.01\n",
      "Pearson corr coef of expected return of the Gaussian Mean with Y_train during Training Period: 0.02\n",
      "Pearson corr coef of mu2 with Y_train during Training Period: 0.02\n",
      "Pearson corr coef of var of Y_train vs weighted_sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Training Period: 0.77\n",
      "var of Y_train is calculated based on the var of previous 59 days\n",
      "TRAINING PERIOD Spearman rank correlation coefficient: 0.01\n",
      "TRAINING PERIOD p-val:0.56\n",
      "15/15 [==============================] - 0s 22ms/step\n",
      "The negative log-likelihood of the model in the testing period is: 4.940415613380326\n",
      "The Deviation Measure the Test period is: -0.0019\n",
      "Pearson corr coef of expected return of the Gaussian Mixture with Y_Test during Test Period: 0.02\n",
      "Pearson corr coef of mu2 with Y_Test during Test Period: 0.02\n",
      "Pearson corr coef of var of Y_Test vs weighted sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Test Period: 0.08\n",
      "var of Y_test is calculated based on the var of previous 59 days\n",
      "TESTING PERIOD Spearman rank correlation coefficient: 0.07\n",
      "TESTING PERIOD p-val:0.14\n",
      "261/261 [==============================] - 9s 22ms/step\n",
      "The negative log-likelihood of the model in the training period is: 3.641188993874841\n",
      "The Deviation Measure the training period is: 0.01\n",
      "Pearson corr coef of expected return of the Gaussian Mean with Y_train during Training Period: 0.01\n",
      "Pearson corr coef of mu2 with Y_train during Training Period: 0.01\n",
      "Pearson corr coef of var of Y_train vs weighted_sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Training Period: 0.69\n",
      "var of Y_train is calculated based on the var of previous 59 days\n",
      "TRAINING PERIOD Spearman rank correlation coefficient: 0.01\n",
      "TRAINING PERIOD p-val:0.30\n",
      "15/15 [==============================] - 0s 22ms/step\n",
      "The negative log-likelihood of the model in the testing period is: 8.976517060294384\n",
      "The Deviation Measure the Test period is: 0.0069\n",
      "Pearson corr coef of expected return of the Gaussian Mixture with Y_Test during Test Period: 0.01\n",
      "Pearson corr coef of mu2 with Y_Test during Test Period: 0.01\n",
      "Pearson corr coef of var of Y_Test vs weighted sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Test Period: -0.23\n",
      "var of Y_test is calculated based on the var of previous 59 days\n",
      "TESTING PERIOD Spearman rank correlation coefficient: -0.06\n",
      "TESTING PERIOD p-val:0.22\n",
      "261/261 [==============================] - 8s 22ms/step\n",
      "The negative log-likelihood of the model in the training period is: 3.583550051846872\n",
      "The Deviation Measure the training period is: 0.01\n",
      "Pearson corr coef of expected return of the Gaussian Mean with Y_train during Training Period: 0.03\n",
      "Pearson corr coef of mu2 with Y_train during Training Period: 0.03\n",
      "Pearson corr coef of var of Y_train vs weighted_sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Training Period: 0.73\n",
      "var of Y_train is calculated based on the var of previous 59 days\n",
      "TRAINING PERIOD Spearman rank correlation coefficient: 0.01\n",
      "TRAINING PERIOD p-val:0.21\n",
      "15/15 [==============================] - 0s 22ms/step\n",
      "The negative log-likelihood of the model in the testing period is: 5.8909620576306\n",
      "The Deviation Measure the Test period is: 0.0157\n",
      "Pearson corr coef of expected return of the Gaussian Mixture with Y_Test during Test Period: -0.05\n",
      "Pearson corr coef of mu2 with Y_Test during Test Period: -0.05\n",
      "Pearson corr coef of var of Y_Test vs weighted sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Test Period: -0.22\n",
      "var of Y_test is calculated based on the var of previous 59 days\n",
      "TESTING PERIOD Spearman rank correlation coefficient: -0.08\n",
      "TESTING PERIOD p-val:0.07\n",
      "261/261 [==============================] - 8s 20ms/step\n",
      "The negative log-likelihood of the model in the training period is: 3.4928542368401847\n",
      "The Deviation Measure the training period is: 0.01\n",
      "Pearson corr coef of expected return of the Gaussian Mean with Y_train during Training Period: 0.03\n",
      "Pearson corr coef of mu2 with Y_train during Training Period: 0.03\n",
      "Pearson corr coef of var of Y_train vs weighted_sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Training Period: 0.72\n",
      "var of Y_train is calculated based on the var of previous 59 days\n",
      "TRAINING PERIOD Spearman rank correlation coefficient: 0.01\n",
      "TRAINING PERIOD p-val:0.28\n",
      "15/15 [==============================] - 0s 21ms/step\n",
      "The negative log-likelihood of the model in the testing period is: 6.452513214104477\n",
      "The Deviation Measure the Test period is: 0.0067\n",
      "Pearson corr coef of expected return of the Gaussian Mixture with Y_Test during Test Period: 0.06\n",
      "Pearson corr coef of mu2 with Y_Test during Test Period: 0.06\n",
      "Pearson corr coef of var of Y_Test vs weighted sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Test Period: 0.18\n",
      "var of Y_test is calculated based on the var of previous 59 days\n",
      "TESTING PERIOD Spearman rank correlation coefficient: 0.08\n",
      "TESTING PERIOD p-val:0.08\n",
      "261/261 [==============================] - 8s 20ms/step\n",
      "The negative log-likelihood of the model in the training period is: 3.4966947482594155\n",
      "The Deviation Measure the training period is: 0.02\n",
      "Pearson corr coef of expected return of the Gaussian Mean with Y_train during Training Period: 0.06\n",
      "Pearson corr coef of mu2 with Y_train during Training Period: 0.06\n",
      "Pearson corr coef of var of Y_train vs weighted_sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Training Period: 0.74\n",
      "var of Y_train is calculated based on the var of previous 59 days\n",
      "TRAINING PERIOD Spearman rank correlation coefficient: 0.02\n",
      "TRAINING PERIOD p-val:0.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 20ms/step\n",
      "The negative log-likelihood of the model in the testing period is: 4.970096006038697\n",
      "The Deviation Measure the Test period is: 0.0207\n",
      "Pearson corr coef of expected return of the Gaussian Mixture with Y_Test during Test Period: -0.01\n",
      "Pearson corr coef of mu2 with Y_Test during Test Period: -0.01\n",
      "Pearson corr coef of var of Y_Test vs weighted sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Test Period: 0.54\n",
      "var of Y_test is calculated based on the var of previous 59 days\n",
      "TESTING PERIOD Spearman rank correlation coefficient: 0.02\n",
      "TESTING PERIOD p-val:0.73\n",
      "261/261 [==============================] - 7s 19ms/step\n",
      "The negative log-likelihood of the model in the training period is: 3.4272484337036815\n",
      "The Deviation Measure the training period is: 0.02\n",
      "Pearson corr coef of expected return of the Gaussian Mean with Y_train during Training Period: 0.03\n",
      "Pearson corr coef of mu2 with Y_train during Training Period: 0.03\n",
      "Pearson corr coef of var of Y_train vs weighted_sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Training Period: 0.69\n",
      "var of Y_train is calculated based on the var of previous 59 days\n",
      "TRAINING PERIOD Spearman rank correlation coefficient: 0.01\n",
      "TRAINING PERIOD p-val:0.25\n",
      "15/15 [==============================] - 0s 20ms/step\n",
      "The negative log-likelihood of the model in the testing period is: 3.3411809055141233\n",
      "The Deviation Measure the Test period is: 0.0191\n",
      "Pearson corr coef of expected return of the Gaussian Mixture with Y_Test during Test Period: -0.05\n",
      "Pearson corr coef of mu2 with Y_Test during Test Period: -0.05\n",
      "Pearson corr coef of var of Y_Test vs weighted sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Test Period: 0.16\n",
      "var of Y_test is calculated based on the var of previous 59 days\n",
      "TESTING PERIOD Spearman rank correlation coefficient: -0.04\n",
      "TESTING PERIOD p-val:0.37\n",
      "261/261 [==============================] - 7s 20ms/step\n",
      "The negative log-likelihood of the model in the training period is: 3.4030453102746274\n",
      "The Deviation Measure the training period is: 0.02\n",
      "Pearson corr coef of expected return of the Gaussian Mean with Y_train during Training Period: 0.01\n",
      "Pearson corr coef of mu2 with Y_train during Training Period: 0.01\n",
      "Pearson corr coef of var of Y_train vs weighted_sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Training Period: 0.69\n",
      "var of Y_train is calculated based on the var of previous 59 days\n",
      "TRAINING PERIOD Spearman rank correlation coefficient: -0.00\n",
      "TRAINING PERIOD p-val:0.68\n",
      "15/15 [==============================] - 0s 19ms/step\n",
      "The negative log-likelihood of the model in the testing period is: 7.670776650888294\n",
      "The Deviation Measure the Test period is: 0.0238\n",
      "Pearson corr coef of expected return of the Gaussian Mixture with Y_Test during Test Period: 0.02\n",
      "Pearson corr coef of mu2 with Y_Test during Test Period: 0.02\n",
      "Pearson corr coef of var of Y_Test vs weighted sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Test Period: 0.32\n",
      "var of Y_test is calculated based on the var of previous 59 days\n",
      "TESTING PERIOD Spearman rank correlation coefficient: 0.04\n",
      "TESTING PERIOD p-val:0.43\n",
      "261/261 [==============================] - 8s 20ms/step\n",
      "The negative log-likelihood of the model in the training period is: 3.398268070711019\n",
      "The Deviation Measure the training period is: 0.02\n",
      "Pearson corr coef of expected return of the Gaussian Mean with Y_train during Training Period: 0.06\n",
      "Pearson corr coef of mu2 with Y_train during Training Period: 0.06\n",
      "Pearson corr coef of var of Y_train vs weighted_sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Training Period: 0.72\n",
      "var of Y_train is calculated based on the var of previous 59 days\n",
      "TRAINING PERIOD Spearman rank correlation coefficient: 0.03\n",
      "TRAINING PERIOD p-val:0.01\n",
      "15/15 [==============================] - 0s 21ms/step\n",
      "The negative log-likelihood of the model in the testing period is: 5.08810787448651\n",
      "The Deviation Measure the Test period is: 0.0152\n",
      "Pearson corr coef of expected return of the Gaussian Mixture with Y_Test during Test Period: 0.04\n",
      "Pearson corr coef of mu2 with Y_Test during Test Period: 0.04\n",
      "Pearson corr coef of var of Y_Test vs weighted sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Test Period: 0.47\n",
      "var of Y_test is calculated based on the var of previous 59 days\n",
      "TESTING PERIOD Spearman rank correlation coefficient: 0.05\n",
      "TESTING PERIOD p-val:0.32\n",
      "261/261 [==============================] - 8s 20ms/step\n",
      "The negative log-likelihood of the model in the training period is: 3.3044794342779236\n",
      "The Deviation Measure the training period is: 0.02\n",
      "Pearson corr coef of expected return of the Gaussian Mean with Y_train during Training Period: 0.03\n",
      "Pearson corr coef of mu2 with Y_train during Training Period: 0.03\n",
      "Pearson corr coef of var of Y_train vs weighted_sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Training Period: 0.72\n",
      "var of Y_train is calculated based on the var of previous 59 days\n",
      "TRAINING PERIOD Spearman rank correlation coefficient: 0.01\n",
      "TRAINING PERIOD p-val:0.20\n",
      "15/15 [==============================] - 0s 21ms/step\n",
      "The negative log-likelihood of the model in the testing period is: 8.849213401816325\n",
      "The Deviation Measure the Test period is: 0.0112\n",
      "Pearson corr coef of expected return of the Gaussian Mixture with Y_Test during Test Period: 0.08\n",
      "Pearson corr coef of mu2 with Y_Test during Test Period: 0.08\n",
      "Pearson corr coef of var of Y_Test vs weighted sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Test Period: 0.43\n",
      "var of Y_test is calculated based on the var of previous 59 days\n",
      "TESTING PERIOD Spearman rank correlation coefficient: 0.10\n",
      "TESTING PERIOD p-val:0.03\n",
      "261/261 [==============================] - 8s 21ms/step\n",
      "The negative log-likelihood of the model in the training period is: 3.323668132306701\n",
      "The Deviation Measure the training period is: 0.02\n",
      "Pearson corr coef of expected return of the Gaussian Mean with Y_train during Training Period: 0.03\n",
      "Pearson corr coef of mu2 with Y_train during Training Period: 0.03\n",
      "Pearson corr coef of var of Y_train vs weighted_sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Training Period: 0.76\n",
      "var of Y_train is calculated based on the var of previous 59 days\n",
      "TRAINING PERIOD Spearman rank correlation coefficient: 0.02\n",
      "TRAINING PERIOD p-val:0.10\n",
      "15/15 [==============================] - 0s 21ms/step\n",
      "The negative log-likelihood of the model in the testing period is: 4.8711194204752575\n",
      "The Deviation Measure the Test period is: 0.0212\n",
      "Pearson corr coef of expected return of the Gaussian Mixture with Y_Test during Test Period: -0.05\n",
      "Pearson corr coef of mu2 with Y_Test during Test Period: -0.05\n",
      "Pearson corr coef of var of Y_Test vs weighted sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Test Period: -0.36\n",
      "var of Y_test is calculated based on the var of previous 59 days\n",
      "TESTING PERIOD Spearman rank correlation coefficient: -0.08\n",
      "TESTING PERIOD p-val:0.07\n",
      "261/261 [==============================] - 8s 20ms/step\n",
      "The negative log-likelihood of the model in the training period is: 3.262609188060169\n",
      "The Deviation Measure the training period is: 0.03\n",
      "Pearson corr coef of expected return of the Gaussian Mean with Y_train during Training Period: 0.02\n",
      "Pearson corr coef of mu2 with Y_train during Training Period: 0.02\n",
      "Pearson corr coef of var of Y_train vs weighted_sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Training Period: 0.75\n",
      "var of Y_train is calculated based on the var of previous 59 days\n",
      "TRAINING PERIOD Spearman rank correlation coefficient: -0.01\n",
      "TRAINING PERIOD p-val:0.36\n",
      "15/15 [==============================] - 0s 19ms/step\n",
      "The negative log-likelihood of the model in the testing period is: 6.11727061883225\n",
      "The Deviation Measure the Test period is: 0.0195\n",
      "Pearson corr coef of expected return of the Gaussian Mixture with Y_Test during Test Period: -0.07\n",
      "Pearson corr coef of mu2 with Y_Test during Test Period: -0.07\n",
      "Pearson corr coef of var of Y_Test vs weighted sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Test Period: 0.29\n",
      "var of Y_test is calculated based on the var of previous 59 days\n",
      "TESTING PERIOD Spearman rank correlation coefficient: -0.08\n",
      "TESTING PERIOD p-val:0.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261/261 [==============================] - 8s 20ms/step\n",
      "The negative log-likelihood of the model in the training period is: 3.1980434155452544\n",
      "The Deviation Measure the training period is: 0.02\n",
      "Pearson corr coef of expected return of the Gaussian Mean with Y_train during Training Period: 0.04\n",
      "Pearson corr coef of mu2 with Y_train during Training Period: 0.04\n",
      "Pearson corr coef of var of Y_train vs weighted_sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Training Period: 0.74\n",
      "var of Y_train is calculated based on the var of previous 59 days\n",
      "TRAINING PERIOD Spearman rank correlation coefficient: 0.02\n",
      "TRAINING PERIOD p-val:0.11\n",
      "15/15 [==============================] - 0s 20ms/step\n",
      "The negative log-likelihood of the model in the testing period is: 4.180378113982963\n",
      "The Deviation Measure the Test period is: 0.0286\n",
      "Pearson corr coef of expected return of the Gaussian Mixture with Y_Test during Test Period: 0.02\n",
      "Pearson corr coef of mu2 with Y_Test during Test Period: 0.02\n",
      "Pearson corr coef of var of Y_Test vs weighted sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Test Period: 0.63\n",
      "var of Y_test is calculated based on the var of previous 59 days\n",
      "TESTING PERIOD Spearman rank correlation coefficient: 0.06\n",
      "TESTING PERIOD p-val:0.19\n",
      "261/261 [==============================] - 8s 22ms/step\n",
      "The negative log-likelihood of the model in the training period is: 3.2183397429654677\n",
      "The Deviation Measure the training period is: 0.02\n",
      "Pearson corr coef of expected return of the Gaussian Mean with Y_train during Training Period: 0.03\n",
      "Pearson corr coef of mu2 with Y_train during Training Period: 0.03\n",
      "Pearson corr coef of var of Y_train vs weighted_sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Training Period: 0.71\n",
      "var of Y_train is calculated based on the var of previous 59 days\n",
      "TRAINING PERIOD Spearman rank correlation coefficient: 0.02\n",
      "TRAINING PERIOD p-val:0.05\n",
      "15/15 [==============================] - 0s 21ms/step\n",
      "The negative log-likelihood of the model in the testing period is: 5.32179928311638\n",
      "The Deviation Measure the Test period is: 0.0312\n",
      "Pearson corr coef of expected return of the Gaussian Mixture with Y_Test during Test Period: 0.03\n",
      "Pearson corr coef of mu2 with Y_Test during Test Period: 0.03\n",
      "Pearson corr coef of var of Y_Test vs weighted sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Test Period: 0.04\n",
      "var of Y_test is calculated based on the var of previous 59 days\n",
      "TESTING PERIOD Spearman rank correlation coefficient: 0.07\n",
      "TESTING PERIOD p-val:0.15\n",
      "261/261 [==============================] - 10s 22ms/step\n",
      "The negative log-likelihood of the model in the training period is: 3.1735998262110727\n",
      "The Deviation Measure the training period is: 0.03\n",
      "Pearson corr coef of expected return of the Gaussian Mean with Y_train during Training Period: 0.05\n",
      "Pearson corr coef of mu2 with Y_train during Training Period: 0.05\n",
      "Pearson corr coef of var of Y_train vs weighted_sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Training Period: 0.74\n",
      "var of Y_train is calculated based on the var of previous 59 days\n",
      "TRAINING PERIOD Spearman rank correlation coefficient: 0.01\n",
      "TRAINING PERIOD p-val:0.24\n",
      "15/15 [==============================] - 0s 21ms/step\n",
      "The negative log-likelihood of the model in the testing period is: 3.792104561438585\n",
      "The Deviation Measure the Test period is: -0.0196\n",
      "Pearson corr coef of expected return of the Gaussian Mixture with Y_Test during Test Period: 0.01\n",
      "Pearson corr coef of mu2 with Y_Test during Test Period: 0.01\n",
      "Pearson corr coef of var of Y_Test vs weighted sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Test Period: 0.35\n",
      "var of Y_test is calculated based on the var of previous 59 days\n",
      "TESTING PERIOD Spearman rank correlation coefficient: -0.04\n",
      "TESTING PERIOD p-val:0.36\n",
      "261/261 [==============================] - 8s 22ms/step\n",
      "The negative log-likelihood of the model in the training period is: 3.1693819204515212\n",
      "The Deviation Measure the training period is: 0.03\n",
      "Pearson corr coef of expected return of the Gaussian Mean with Y_train during Training Period: 0.05\n",
      "Pearson corr coef of mu2 with Y_train during Training Period: 0.05\n",
      "Pearson corr coef of var of Y_train vs weighted_sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Training Period: 0.73\n",
      "var of Y_train is calculated based on the var of previous 59 days\n",
      "TRAINING PERIOD Spearman rank correlation coefficient: -0.00\n",
      "TRAINING PERIOD p-val:0.96\n",
      "15/15 [==============================] - 0s 21ms/step\n",
      "The negative log-likelihood of the model in the testing period is: 6.398879146145317\n",
      "The Deviation Measure the Test period is: 0.0310\n",
      "Pearson corr coef of expected return of the Gaussian Mixture with Y_Test during Test Period: -0.09\n",
      "Pearson corr coef of mu2 with Y_Test during Test Period: -0.09\n",
      "Pearson corr coef of var of Y_Test vs weighted sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Test Period: 0.32\n",
      "var of Y_test is calculated based on the var of previous 59 days\n",
      "TESTING PERIOD Spearman rank correlation coefficient: -0.09\n",
      "TESTING PERIOD p-val:0.05\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1_Train_nll</th>\n",
       "      <th>1_Mean Train_Pearson Corr Coeff</th>\n",
       "      <th>1_Mean Train_Spearman rank Corr Coeff</th>\n",
       "      <th>1_Variance Train Pear Corr Coeff</th>\n",
       "      <th>1_Deviation Measure Train</th>\n",
       "      <th>1_Test_nll</th>\n",
       "      <th>1_Mean Test_Pearson Corr Coeff</th>\n",
       "      <th>1_Mean Test_Spearman rank Corr Coeff</th>\n",
       "      <th>1_Variance Test Pear Corr Coeff</th>\n",
       "      <th>1_Deviation Measure Test</th>\n",
       "      <th>2_Train_nll</th>\n",
       "      <th>2_Mean Train_Pearson Corr Coeff</th>\n",
       "      <th>2_Mean Train_Spearman rank Corr Coeff</th>\n",
       "      <th>2_Variance Train Pear Corr Coeff</th>\n",
       "      <th>2_Deviation Measure Train</th>\n",
       "      <th>2_Test_nll</th>\n",
       "      <th>2_Mean Test_Pearson Corr Coeff</th>\n",
       "      <th>2_Mean Test_Spearman rank Corr Coeff</th>\n",
       "      <th>2_Variance Test Pear Corr Coeff</th>\n",
       "      <th>2_Deviation Measure Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.05</th>\n",
       "      <td>3.69</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.0091</td>\n",
       "      <td>6.20</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>3.69</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>4.94</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>-0.0019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.06</th>\n",
       "      <td>3.62</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>7.88</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0286</td>\n",
       "      <td>3.64</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>8.98</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>0.0069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.07</th>\n",
       "      <td>3.54</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.0127</td>\n",
       "      <td>6.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>3.58</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.0139</td>\n",
       "      <td>5.89</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.0157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.08</th>\n",
       "      <td>3.51</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>4.03</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.0147</td>\n",
       "      <td>6.45</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.0067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.09</th>\n",
       "      <td>3.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>6.00</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.0203</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.0170</td>\n",
       "      <td>4.97</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.0207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.10</th>\n",
       "      <td>3.45</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0175</td>\n",
       "      <td>6.34</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>3.43</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>3.34</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.0191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.11</th>\n",
       "      <td>3.56</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>3.71</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.0123</td>\n",
       "      <td>3.40</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.0182</td>\n",
       "      <td>7.67</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.0238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.12</th>\n",
       "      <td>3.34</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.0216</td>\n",
       "      <td>9.16</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>3.40</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0163</td>\n",
       "      <td>5.09</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.0152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.13</th>\n",
       "      <td>3.35</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.0234</td>\n",
       "      <td>4.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0137</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.0219</td>\n",
       "      <td>8.85</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.0112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.14</th>\n",
       "      <td>3.27</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.0216</td>\n",
       "      <td>4.12</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.1839</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.0238</td>\n",
       "      <td>4.87</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>0.0212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.15</th>\n",
       "      <td>3.29</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.0251</td>\n",
       "      <td>7.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.0221</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0287</td>\n",
       "      <td>6.12</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.0195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.16</th>\n",
       "      <td>3.21</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>3.73</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-0.0078</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0247</td>\n",
       "      <td>4.18</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.17</th>\n",
       "      <td>3.22</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>6.64</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>3.22</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0247</td>\n",
       "      <td>5.32</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.18</th>\n",
       "      <td>3.18</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.0281</td>\n",
       "      <td>5.14</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.0297</td>\n",
       "      <td>3.79</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-0.0196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.19</th>\n",
       "      <td>3.15</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>3.79</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.0288</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.0349</td>\n",
       "      <td>6.40</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.0310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     1_Train_nll 1_Mean Train_Pearson Corr Coeff  \\\n",
       "0.05        3.69                           -0.03   \n",
       "0.06        3.62                            0.03   \n",
       "0.07        3.54                            0.03   \n",
       "0.08        3.51                            0.00   \n",
       "0.09        3.46                            0.00   \n",
       "0.10        3.45                            0.02   \n",
       "0.11        3.56                           -0.01   \n",
       "0.12        3.34                            0.05   \n",
       "0.13        3.35                            0.05   \n",
       "0.14        3.27                            0.03   \n",
       "0.15        3.29                            0.04   \n",
       "0.16        3.21                            0.03   \n",
       "0.17        3.22                            0.03   \n",
       "0.18        3.18                            0.03   \n",
       "0.19        3.15                            0.04   \n",
       "\n",
       "     1_Mean Train_Spearman rank Corr Coeff 1_Variance Train Pear Corr Coeff  \\\n",
       "0.05                                 -0.02                             0.66   \n",
       "0.06                                  0.02                             0.72   \n",
       "0.07                                  0.00                             0.73   \n",
       "0.08                                  0.01                             0.56   \n",
       "0.09                                  0.01                             0.76   \n",
       "0.10                                  0.01                             0.65   \n",
       "0.11                                  0.01                             0.48   \n",
       "0.12                                  0.01                             0.72   \n",
       "0.13                                  0.01                             0.74   \n",
       "0.14                                  0.01                             0.72   \n",
       "0.15                                  0.02                             0.74   \n",
       "0.16                                  0.01                             0.72   \n",
       "0.17                                  0.01                             0.78   \n",
       "0.18                                  0.01                             0.70   \n",
       "0.19                                  0.03                             0.73   \n",
       "\n",
       "     1_Deviation Measure Train 1_Test_nll 1_Mean Test_Pearson Corr Coeff  \\\n",
       "0.05                    0.0091       6.20                           0.04   \n",
       "0.06                    0.0111       7.88                           0.06   \n",
       "0.07                    0.0127       6.27                           0.00   \n",
       "0.08                    0.0135       4.03                           0.07   \n",
       "0.09                    0.0129       6.00                          -0.07   \n",
       "0.10                    0.0175       6.34                          -0.05   \n",
       "0.11                    0.0191       3.71                          -0.10   \n",
       "0.12                    0.0216       9.16                           0.01   \n",
       "0.13                    0.0234       4.09                           0.02   \n",
       "0.14                    0.0216       4.12                          -0.06   \n",
       "0.15                    0.0251       7.02                           0.03   \n",
       "0.16                    0.0250       3.73                          -0.03   \n",
       "0.17                    0.0250       6.64                           0.09   \n",
       "0.18                    0.0281       5.14                          -0.08   \n",
       "0.19                    0.0310       3.79                           0.07   \n",
       "\n",
       "     1_Mean Test_Spearman rank Corr Coeff 1_Variance Test Pear Corr Coeff  \\\n",
       "0.05                                 0.03                           -0.18   \n",
       "0.06                                 0.10                            0.02   \n",
       "0.07                                 0.04                            0.04   \n",
       "0.08                                 0.04                            0.14   \n",
       "0.09                                -0.06                            0.28   \n",
       "0.10                                -0.08                           -0.25   \n",
       "0.11                                -0.13                           -0.16   \n",
       "0.12                                 0.06                            0.18   \n",
       "0.13                                 0.05                            0.02   \n",
       "0.14                                -0.02                            0.22   \n",
       "0.15                                 0.02                            0.53   \n",
       "0.16                                -0.04                            0.31   \n",
       "0.17                                 0.12                            0.43   \n",
       "0.18                                -0.11                            0.59   \n",
       "0.19                                 0.11                            0.34   \n",
       "\n",
       "     1_Deviation Measure Test 2_Train_nll 2_Mean Train_Pearson Corr Coeff  \\\n",
       "0.05                   0.0026        3.69                            0.02   \n",
       "0.06                   0.0286        3.64                            0.01   \n",
       "0.07                   0.0151        3.58                            0.03   \n",
       "0.08                   0.0167        3.49                            0.03   \n",
       "0.09                   0.0203        3.50                            0.06   \n",
       "0.10                   0.0059        3.43                            0.03   \n",
       "0.11                   0.0123        3.40                            0.01   \n",
       "0.12                   0.0116        3.40                            0.06   \n",
       "0.13                   0.0137        3.30                            0.03   \n",
       "0.14                   0.1839        3.32                            0.03   \n",
       "0.15                   0.0221        3.26                            0.02   \n",
       "0.16                  -0.0078        3.20                            0.04   \n",
       "0.17                   0.0180        3.22                            0.03   \n",
       "0.18                   0.0168        3.17                            0.05   \n",
       "0.19                   0.0288        3.17                            0.05   \n",
       "\n",
       "     2_Mean Train_Spearman rank Corr Coeff 2_Variance Train Pear Corr Coeff  \\\n",
       "0.05                                  0.01                             0.74   \n",
       "0.06                                  0.01                             0.65   \n",
       "0.07                                  0.01                             0.72   \n",
       "0.08                                  0.01                             0.73   \n",
       "0.09                                  0.02                             0.73   \n",
       "0.10                                  0.01                             0.70   \n",
       "0.11                                 -0.00                             0.67   \n",
       "0.12                                  0.03                             0.71   \n",
       "0.13                                  0.01                             0.72   \n",
       "0.14                                  0.02                             0.76   \n",
       "0.15                                 -0.01                             0.75   \n",
       "0.16                                  0.02                             0.75   \n",
       "0.17                                  0.02                             0.71   \n",
       "0.18                                  0.01                             0.74   \n",
       "0.19                                 -0.00                             0.74   \n",
       "\n",
       "     2_Deviation Measure Train 2_Test_nll 2_Mean Test_Pearson Corr Coeff  \\\n",
       "0.05                    0.0103       4.94                           0.02   \n",
       "0.06                    0.0115       8.98                           0.01   \n",
       "0.07                    0.0139       5.89                          -0.05   \n",
       "0.08                    0.0147       6.45                           0.06   \n",
       "0.09                    0.0170       4.97                          -0.01   \n",
       "0.10                    0.0185       3.34                          -0.05   \n",
       "0.11                    0.0182       7.67                           0.02   \n",
       "0.12                    0.0163       5.09                           0.04   \n",
       "0.13                    0.0219       8.85                           0.08   \n",
       "0.14                    0.0238       4.87                          -0.05   \n",
       "0.15                    0.0287       6.12                          -0.07   \n",
       "0.16                    0.0247       4.18                           0.02   \n",
       "0.17                    0.0247       5.32                           0.03   \n",
       "0.18                    0.0297       3.79                           0.01   \n",
       "0.19                    0.0349       6.40                          -0.09   \n",
       "\n",
       "     2_Mean Test_Spearman rank Corr Coeff 2_Variance Test Pear Corr Coeff  \\\n",
       "0.05                                 0.07                           -0.63   \n",
       "0.06                                -0.06                           -0.31   \n",
       "0.07                                -0.08                           -0.22   \n",
       "0.08                                 0.08                            0.17   \n",
       "0.09                                 0.02                            0.54   \n",
       "0.10                                -0.04                            0.17   \n",
       "0.11                                 0.04                            0.27   \n",
       "0.12                                 0.05                            0.41   \n",
       "0.13                                 0.10                            0.39   \n",
       "0.14                                -0.08                           -0.31   \n",
       "0.15                                -0.08                            0.32   \n",
       "0.16                                 0.06                            0.64   \n",
       "0.17                                 0.07                            0.04   \n",
       "0.18                                -0.04                            0.31   \n",
       "0.19                                -0.09                            0.31   \n",
       "\n",
       "     2_Deviation Measure Test  \n",
       "0.05                  -0.0019  \n",
       "0.06                   0.0069  \n",
       "0.07                   0.0157  \n",
       "0.08                   0.0067  \n",
       "0.09                   0.0207  \n",
       "0.10                   0.0191  \n",
       "0.11                   0.0238  \n",
       "0.12                   0.0152  \n",
       "0.13                   0.0112  \n",
       "0.14                   0.0212  \n",
       "0.15                   0.0195  \n",
       "0.16                   0.0286  \n",
       "0.17                   0.0312  \n",
       "0.18                  -0.0196  \n",
       "0.19                   0.0310  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load models according to the respective coefficient of the term sigma_2^2 of the loss function (after NLL).\n",
    "d_report = pd.DataFrame()\n",
    "d_report.index = range(5,20)\n",
    "d_report.index = d_report.index/100 \n",
    "\n",
    "for loop in range(1,3):    \n",
    "    Train_NLL = []\n",
    "    Train_Pearson_Corr_Coeff = []\n",
    "    Train_Spearman_rank_Corr_Coeff = []\n",
    "    Train_Pearson_Corr_Coeff_Var = []\n",
    "    Deviation_Measure_Train = []\n",
    "\n",
    "    Test_NLL = []\n",
    "    Test_Pearson_Corr_Coeff = []\n",
    "    Test_Spearman_rank_Corr_Coeff = []\n",
    "    Test_Pearson_Corr_Coeff_Var = []\n",
    "    Deviation_Measure_Test = []\n",
    "    \n",
    "    for i in range(5,20):\n",
    "        k = i/100\n",
    "        j = 'CCMP_t+1'\n",
    "\n",
    "        tf.keras.utils.get_custom_objects().update({'negative_log_likelihood_loss_2':layer.negative_log_likelihood_loss_2})\n",
    "        model = tf.keras.models.load_model(fr\"C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\_CCMP_t+1_Mar23\\_{loop}_{j}_{k}.hdf5\")\n",
    "\n",
    "    #     model = tf.keras.models.load_model(fr\"C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\CCMP_t+1\\exp1-{k}.hdf5\")\n",
    "\n",
    "        #Predict the parameters in training period after loading the model\n",
    "        pred_params = model.predict(X_train)\n",
    "        mu1_train = pred_params[:,0]\n",
    "        sigma2_1_train = pred_params[:,1]\n",
    "        mu2_train = pred_params[:,2]\n",
    "        sigma2_2_train = pred_params[:,3] \n",
    "        w1_train = pred_params[:,4]\n",
    "\n",
    "        # PLOT THE TRATINING PERIOD PERFORMANCE\n",
    "        mu1_train = mu1_train.reshape(-1,1)\n",
    "        sigma2_1_train = sigma2_1_train.reshape(-1,1)\n",
    "        mu2_train = mu2_train.reshape(-1,1)\n",
    "        sigma2_2_train = sigma2_2_train.reshape(-1,1)\n",
    "        w1_train = w1_train.reshape(-1,1)\n",
    "\n",
    "        # Inverse transform the scaled target (Y_train), and the estimated mu1 and mu2.\n",
    "        Y_train_plot = SC2.inverse_transform(Y_train)\n",
    "        mu1_train_plot = SC2.inverse_transform(mu1_train)\n",
    "        mu2_train_plot = SC2.inverse_transform(mu2_train)\n",
    "\n",
    "        # PLOT THE TRATINING PERIOD PERFORMANCE\n",
    "        d_plot = pd.DataFrame()\n",
    "        d_plot.index = d1.index[n_past-1:-n_val-1]\n",
    "        d_plot['Y_train'] = Y_train\n",
    "        d_plot['Y_train_plot'] = pd.DataFrame(Y_train_plot)\n",
    "        d_plot['mu1_train'] = mu1_train\n",
    "        d_plot['mu1_train_plot'] = mu1_train_plot\n",
    "        d_plot['mu2_train'] = mu2_train\n",
    "        d_plot['mu2_train_plot'] = mu2_train_plot\n",
    "        d_plot['sigma2_1_train'] = sigma2_1_train\n",
    "        d_plot['sigma2_2_train'] = sigma2_2_train\n",
    "        d_plot['w1_train'] = w1_train\n",
    "        d_plot['varY_train'] = varY_train\n",
    "\n",
    "        x = d_plot.index\n",
    "        y2 = d_plot['Y_train']\n",
    "        y1 = d_plot['Y_train_plot'] \n",
    "        y4 = d_plot['mu1_train']\n",
    "        y3 = d_plot['mu1_train_plot']\n",
    "        y6 = d_plot['mu2_train']\n",
    "        y5 = d_plot['mu2_train_plot']\n",
    "        y7 = d_plot['sigma2_1_train']\n",
    "        y8 = d_plot['sigma2_2_train']\n",
    "        y9 = d_plot['w1_train']\n",
    "        y10 = d_plot['varY_train']\n",
    "\n",
    "        # Negative log - likelihood function at training period\n",
    "        nll_train = np.mean(\n",
    "            (-np.log(w1_train)+0.5*(np.log(sigma2_1_train)+np.square(Y_train-mu1_train)/sigma2_1_train)\n",
    "                   -np.log(1-w1_train)+0.5*(np.log(sigma2_2_train)+np.square(Y_train-mu2_train)/sigma2_2_train)))\n",
    "\n",
    "#         # create figure and axis objects with subplots()\n",
    "#         fig,ax = plt.subplots(figsize = (23,8))\n",
    "#         # make a plot\n",
    "#         ax.plot(x,y2, label='$\\Y_Train$', color = 'g')\n",
    "#         # set x-axis label\n",
    "#         ax.set_xlabel(f'_{j}exp1-{k} Training Period', fontsize = 14)\n",
    "#         # set y-axis label\n",
    "#         ax.set_ylabel(\"CCMP INDEX LOG DIFF\", fontsize=14)\n",
    "#         # twin object for two different y-axis on the sample plot\n",
    "#         ax2=ax.twinx()\n",
    "#         # make a plot with different y-axis using second axis object\n",
    "#         ax2.plot(x, y4, label='$\\mu1_t$', color = 'b', linestyle='-')\n",
    "#         ax2.plot(x, y6, label='$\\mu2_t$', color = 'r', linestyle='-')\n",
    "#         ax2.plot(x, y7, label='$\\sigma1^2_t$', color = 'b', linestyle='--')\n",
    "#         ax2.plot(x, y8, label='$\\sigma2^2_t$', color = 'r', linestyle='--')\n",
    "#         ax2.set_ylabel(f'_{j}exp1-{k}Estimated Value',fontsize=14)\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "        print(\"The negative log-likelihood of the model in the training period is:\",nll_train)\n",
    "        print(\"The Deviation Measure the training period is:\",'{:.2f}'.format(np.mean((y2-y9*y4-(1-y9)*y6)/(y9*y7+(1-y9)*y8))))\n",
    "        print(f'Pearson corr coef of expected return of the Gaussian Mean with Y_train during Training Period:', '{:.2f}'.format(np.corrcoef(y2, y4*y9+(1-y9)*y6)[0,1]))\n",
    "        print(f'Pearson corr coef of mu2 with Y_train during Training Period:', '{:.2f}'.format(np.corrcoef(y2, y6)[0,1]))\n",
    "        print(f'Pearson corr coef of var of Y_train vs weighted_sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Training Period:', '{:.2f}'.format(np.corrcoef(y10, np.square(y9)*y7+np.square(1-y9)*y8)[0,1]))\n",
    "        print(f'var of Y_train is calculated based on the var of previous 59 days')\n",
    "\n",
    "\n",
    "        corr,pval = spearmanr(y2, y4*y9+(1-y9)*y6)\n",
    "        print(f\"TRAINING PERIOD Spearman rank correlation coefficient: {corr:.2f}\")\n",
    "        print(f\"TRAINING PERIOD p-val:{pval:.2f}\")\n",
    "        Train_NLL.append('{:.2f}'.format(nll_train))\n",
    "        Deviation_Measure_Train.append('{:.4f}'.format(np.mean((y2-y9*y4-(1-y9)*y6)/(y9*y7+(1-y9)*y8))))\n",
    "        Train_Pearson_Corr_Coeff.append('{:.2f}'.format(np.corrcoef(y2, y4*y9+(1-y9)*y6)[0,1]))\n",
    "        Train_Spearman_rank_Corr_Coeff.append('{:.2f}'.format(corr))\n",
    "        Train_Pearson_Corr_Coeff_Var.append('{:.2f}'.format(np.corrcoef(y10, y7*y9+(1-y9)*y8)[0,1]))\n",
    "\n",
    "\n",
    "        #Predict the parameters in testing period after loading the model\n",
    "        pred_params = model.predict(X_rtest)\n",
    "        mu1_test = pred_params[:,0]\n",
    "        sigma2_1_test = pred_params[:,1]\n",
    "        mu2_test = pred_params[:,2]\n",
    "        sigma2_2_test = pred_params[:,3] \n",
    "        w1_test = pred_params[:,4] \n",
    "\n",
    "        mu1_test = mu1_test.reshape(-1,1)\n",
    "        sigma2_1_test = sigma2_1_test.reshape(-1,1)\n",
    "        mu2_test = mu2_test.reshape(-1,1)\n",
    "        sigma2_2_test = sigma2_2_test.reshape(-1,1)\n",
    "        w1_test = w1_test.reshape(-1,1)\n",
    "\n",
    "        # Inverse transform the scaled target (Y_test), and the estimated mu1 and mu2.\n",
    "        Y_test_plot = np.exp(SC2.inverse_transform(Y_rtest))-1\n",
    "        mu1_test_plot = np.exp(SC2.inverse_transform(mu1_test))-1\n",
    "        mu2_test_plot = np.exp(SC2.inverse_transform(mu2_test))-1\n",
    "\n",
    "        # PLOT THE TESTING PERIOD PERFORMANCE        \n",
    "        d_test = pd.DataFrame()\n",
    "        d_test.index = d1.index[-n_rtest:]\n",
    "        d_test['Y_test'] = Y_rtest\n",
    "        d_test['Y_test_plot'] = Y_test_plot\n",
    "        d_test['mu1_test'] = mu1_test\n",
    "        d_test['mu1_test_plot'] = mu1_test_plot    \n",
    "        d_test['mu2_test'] = mu2_test\n",
    "        d_test['mu2_test_plot'] = mu2_test_plot\n",
    "        d_test['sigma2_1_test'] = sigma2_1_test\n",
    "        d_test['sigma2_2_test'] = sigma2_2_test\n",
    "        d_test['sigma_1_test'] = np.sqrt(sigma2_1_test)\n",
    "        d_test['sigma_2_test'] = np.sqrt(sigma2_2_test)\n",
    "        d_test['w1_test'] = w1_test\n",
    "        d_test['varY_test'] = varY_rtest\n",
    "\n",
    "        x = d_test.index\n",
    "        y1 = d_test['Y_test'] \n",
    "        y2 = d_test['Y_test_plot']\n",
    "        y5 = d_test['mu1_test']\n",
    "        y3 = d_test['mu1_test_plot']\n",
    "        y7 = d_test['mu2_test']\n",
    "        y6 = d_test['mu2_test_plot']\n",
    "        y8 = d_test['sigma2_1_test']\n",
    "        y9 = d_test['sigma2_2_test']\n",
    "        y10 = d_test['sigma_1_test']\n",
    "        y11 = d_test['sigma_2_test']\n",
    "        y12 = d_test['w1_test']\n",
    "        y13 = d_test['varY_test']\n",
    "#         # create figure and axis objects with subplots()\n",
    "#         fig,ax = plt.subplots(figsize = (23,8))\n",
    "#         # make a plot\n",
    "#         ax.plot(x, y1, label='$Y_Test$', color = 'g')\n",
    "#         # set x-axis label\n",
    "#         ax.set_xlabel(f'exp1-{k} Testing Period', fontsize = 14)\n",
    "#         # set y-axis label\n",
    "#         ax.set_ylabel(f'exp1-{k}_CCMP INDEX Log Return', fontsize=14)\n",
    "#         plt.legend()\n",
    "#         # twin object for two different y-axis on the sample plot\n",
    "#         ax2=ax.twinx()\n",
    "#         # make a plot with different y-axis using second axis object\n",
    "#         # ax2.plot(x, y5, label='$\\mu1_t$', color = 'b', linestyle='-')\n",
    "#         # ax2.plot(x, y7, label='$\\mu2_t$', color = 'r', linestyle='-')\n",
    "#         ax2.plot(x, y10, label='$\\sigma1^2_t$', color = 'b', linestyle='--')\n",
    "#         ax2.plot(x, y11, label='$\\sigma2^2_t$', color = 'r', linestyle='--')\n",
    "#         # ax2.plot(x, y12, label='$w1$', color = 'b', linestyle='--')\n",
    "\n",
    "#         ax2.set_ylabel(f'_{j}exp1-{k}_Estimated Value',fontsize=14)\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "        # save the plot as a file\n",
    "        # Negative log likelihood in testing period\n",
    "        nll_test = np.mean(\n",
    "            (-np.log(w1_test)+0.5*(np.log(sigma2_1_test)+np.square(Y_rtest-mu1_test)/sigma2_1_test)\n",
    "                   -np.log(1-w1_test)+0.5*(np.log(sigma2_2_test)+np.square(Y_rtest-mu2_test)/sigma2_2_test)))\n",
    "\n",
    "#             # create figure and axis objects with subplots()\n",
    "#         fig,ax = plt.subplots(figsize = (23,8))\n",
    "#         # make a plot\n",
    "#         ax.plot(x, y1, label='$Y_test$', color = 'g')\n",
    "#         # set x-axis label\n",
    "#         ax.set_xlabel(f'_{j}exp1-{k} Testing Period', fontsize = 14)\n",
    "#         # set y-axis label\n",
    "#         ax.set_ylabel(f'_{j}exp1-{k}_CCMP INDEX LOG DIFF', fontsize=14)\n",
    "#         plt.legend()\n",
    "#         # twin object for two different y-axis on the sample plot\n",
    "#         ax2=ax.twinx()\n",
    "#         # make a plot with different y-axis using second axis object\n",
    "#         ax2.plot(x, y5, label='$\\mu1_t$', color = 'b', linestyle='-')\n",
    "#         ax2.plot(x, y7, label='$\\mu2_t$', color = 'r', linestyle='-')\n",
    "        print(\"The negative log-likelihood of the model in the testing period is:\",nll_test)    \n",
    "        print(\"The Deviation Measure the Test period is:\",'{:.4f}'.format(np.mean((y2-y12*y5-(1-y12)*y7)/(y12*y8+(1-y12)*y9))))\n",
    "        print(f'Pearson corr coef of expected return of the Gaussian Mixture with Y_Test during Test Period:', '{:.2f}'.format(np.corrcoef(y1, y12*y5+(1-y12)*y7)[0,1]))\n",
    "        print(f'Pearson corr coef of mu2 with Y_Test during Test Period:', '{:.2f}'.format(np.corrcoef(y1, y7)[0,1]))\n",
    "        print(f'Pearson corr coef of var of Y_Test vs weighted sum of var of sigma1^2 and sigma2^2 of the Gaussian Mixture during Test Period:', '{:.2f}'.format(np.corrcoef(y13, np.square(y12)*y8+np.square(1-y12)*y9)[0,1]))\n",
    "        print(f'var of Y_test is calculated based on the var of previous 59 days')\n",
    "\n",
    "        corr,pval = spearmanr(y1, y12*y5+(1-y12)*y7)\n",
    "        print(f\"TESTING PERIOD Spearman rank correlation coefficient: {corr:.2f}\")\n",
    "        print(f\"TESTING PERIOD p-val:{pval:.2f}\")\n",
    "        Test_NLL.append('{:.2f}'.format(nll_test))\n",
    "        Deviation_Measure_Test.append('{:.4f}'.format(np.mean((y2-y12*y5-(1-y12)*y7)/(y12*y8+(1-y12)*y9))))\n",
    "        Test_Pearson_Corr_Coeff.append('{:.2f}'.format(np.corrcoef(y1, y12*y5+(1-y12)*y7)[0,1]))\n",
    "        Test_Spearman_rank_Corr_Coeff.append('{:.2f}'.format(corr))\n",
    "        Test_Pearson_Corr_Coeff_Var.append('{:.2f}'.format(np.corrcoef(y13, y12*y8+(1-y12)*y9)[0,1]))\n",
    "\n",
    "#         ax2.set_ylabel(f'_{j}exp1-{k}_Estimated Value',fontsize=14)\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "\n",
    "\n",
    "#         #TESTING PERIOD UNDERLYING ASSET LOG RETURN vs w1#############################\n",
    "#             #TESTING PERIOD UNDERLYING ASSET LOG RETURN vs w1#############################\n",
    "#                 #TESTING PERIOD UNDERLYING ASSET LOG RETURN vs w1#############################\n",
    "#                     #TESTING PERIOD UNDERLYING ASSET LOG RETURN vs w1#############################\n",
    "#                         #TESTING PERIOD UNDERLYING ASSET LOG RETURN vs w1#############################\n",
    "#         # create figure and axis objects with subplots()\n",
    "#         fig,ax = plt.subplots(figsize = (23,8))\n",
    "#         # make a plot\n",
    "#         ax.plot(x, y1, label='$Y_test$', color = 'g')\n",
    "#         # set x-axis label\n",
    "#         ax.set_xlabel(f'_{j}exp1-{k}_Testing Period', fontsize = 14)\n",
    "#         # set y-axis label\n",
    "#         ax.set_ylabel(f'_{j}exp1-{k}_CCMP US Equity LOG DIFF', fontsize=14)\n",
    "#         plt.legend()\n",
    "#         # twin object for two different y-axis on the sample plot\n",
    "#         ax2=ax.twinx()\n",
    "#         # make a plot with different y-axis using second axis object\n",
    "#         # ax2.plot(x, y5, label='$\\mu1_t$', color = 'b', linestyle='-')\n",
    "\n",
    "#         ax2.plot(x, y12, label='$w1$', color = 'k', linestyle='--')\n",
    "\n",
    "#         ax2.set_ylabel(\"w1\",fontsize=14)\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "#         # save the plot as a file\n",
    "\n",
    "    d_report[f'{loop}_Train_nll'] = Train_NLL\n",
    "    d_report[f'{loop}_Mean Train_Pearson Corr Coeff'] = Train_Pearson_Corr_Coeff\n",
    "    d_report[f'{loop}_Mean Train_Spearman rank Corr Coeff'] = Train_Spearman_rank_Corr_Coeff\n",
    "    d_report[f'{loop}_Variance Train Pear Corr Coeff'] = Train_Pearson_Corr_Coeff_Var\n",
    "    d_report[f'{loop}_Deviation Measure Train'] = Deviation_Measure_Train\n",
    "\n",
    "    d_report[f'{loop}_Test_nll'] = Test_NLL\n",
    "    d_report[f'{loop}_Mean Test_Pearson Corr Coeff'] = Test_Pearson_Corr_Coeff\n",
    "    d_report[f'{loop}_Mean Test_Spearman rank Corr Coeff'] = Test_Spearman_rank_Corr_Coeff\n",
    "    d_report[f'{loop}_Variance Test Pear Corr Coeff'] = Test_Pearson_Corr_Coeff_Var\n",
    "    d_report[f'{loop}_Deviation Measure Test'] = Deviation_Measure_Test\n",
    "\n",
    "\n",
    "d_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "177a0600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1_Train_nll</th>\n",
       "      <th>1_Mean Train_Pearson Corr Coeff</th>\n",
       "      <th>1_Mean Train_Spearman rank Corr Coeff</th>\n",
       "      <th>1_Variance Train Pear Corr Coeff</th>\n",
       "      <th>1_Deviation Measure Train</th>\n",
       "      <th>1_Test_nll</th>\n",
       "      <th>1_Mean Test_Pearson Corr Coeff</th>\n",
       "      <th>1_Mean Test_Spearman rank Corr Coeff</th>\n",
       "      <th>1_Variance Test Pear Corr Coeff</th>\n",
       "      <th>1_Deviation Measure Test</th>\n",
       "      <th>2_Train_nll</th>\n",
       "      <th>2_Mean Train_Pearson Corr Coeff</th>\n",
       "      <th>2_Mean Train_Spearman rank Corr Coeff</th>\n",
       "      <th>2_Variance Train Pear Corr Coeff</th>\n",
       "      <th>2_Deviation Measure Train</th>\n",
       "      <th>2_Test_nll</th>\n",
       "      <th>2_Mean Test_Pearson Corr Coeff</th>\n",
       "      <th>2_Mean Test_Spearman rank Corr Coeff</th>\n",
       "      <th>2_Variance Test Pear Corr Coeff</th>\n",
       "      <th>2_Deviation Measure Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.05</th>\n",
       "      <td>3.69</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.0091</td>\n",
       "      <td>6.20</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>3.69</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>4.94</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>-0.0019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.06</th>\n",
       "      <td>3.62</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>7.88</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0286</td>\n",
       "      <td>3.64</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>8.98</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>0.0069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.07</th>\n",
       "      <td>3.54</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.0127</td>\n",
       "      <td>6.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>3.58</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.0139</td>\n",
       "      <td>5.89</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.0157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.08</th>\n",
       "      <td>3.51</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>4.03</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.0147</td>\n",
       "      <td>6.45</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.0067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.09</th>\n",
       "      <td>3.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>6.00</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.0203</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.0170</td>\n",
       "      <td>4.97</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.0207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.10</th>\n",
       "      <td>3.45</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0175</td>\n",
       "      <td>6.34</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>3.43</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>3.34</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.0191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.11</th>\n",
       "      <td>3.56</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>3.71</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.0123</td>\n",
       "      <td>3.40</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.0182</td>\n",
       "      <td>7.67</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.0238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.12</th>\n",
       "      <td>3.34</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.0216</td>\n",
       "      <td>9.16</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>3.40</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0163</td>\n",
       "      <td>5.09</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.0152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.13</th>\n",
       "      <td>3.35</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.0234</td>\n",
       "      <td>4.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0137</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.0219</td>\n",
       "      <td>8.85</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.0112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.14</th>\n",
       "      <td>3.27</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.0216</td>\n",
       "      <td>4.12</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.1839</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.0238</td>\n",
       "      <td>4.87</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>0.0212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.15</th>\n",
       "      <td>3.29</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.0251</td>\n",
       "      <td>7.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.0221</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0287</td>\n",
       "      <td>6.12</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.0195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.16</th>\n",
       "      <td>3.21</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>3.73</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-0.0078</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0247</td>\n",
       "      <td>4.18</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.17</th>\n",
       "      <td>3.22</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>6.64</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>3.22</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0247</td>\n",
       "      <td>5.32</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.18</th>\n",
       "      <td>3.18</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.0281</td>\n",
       "      <td>5.14</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.0297</td>\n",
       "      <td>3.79</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-0.0196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.19</th>\n",
       "      <td>3.15</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>3.79</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.0288</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.0349</td>\n",
       "      <td>6.40</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.0310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     1_Train_nll 1_Mean Train_Pearson Corr Coeff  \\\n",
       "0.05        3.69                           -0.03   \n",
       "0.06        3.62                            0.03   \n",
       "0.07        3.54                            0.03   \n",
       "0.08        3.51                            0.00   \n",
       "0.09        3.46                            0.00   \n",
       "0.10        3.45                            0.02   \n",
       "0.11        3.56                           -0.01   \n",
       "0.12        3.34                            0.05   \n",
       "0.13        3.35                            0.05   \n",
       "0.14        3.27                            0.03   \n",
       "0.15        3.29                            0.04   \n",
       "0.16        3.21                            0.03   \n",
       "0.17        3.22                            0.03   \n",
       "0.18        3.18                            0.03   \n",
       "0.19        3.15                            0.04   \n",
       "\n",
       "     1_Mean Train_Spearman rank Corr Coeff 1_Variance Train Pear Corr Coeff  \\\n",
       "0.05                                 -0.02                             0.66   \n",
       "0.06                                  0.02                             0.72   \n",
       "0.07                                  0.00                             0.73   \n",
       "0.08                                  0.01                             0.56   \n",
       "0.09                                  0.01                             0.76   \n",
       "0.10                                  0.01                             0.65   \n",
       "0.11                                  0.01                             0.48   \n",
       "0.12                                  0.01                             0.72   \n",
       "0.13                                  0.01                             0.74   \n",
       "0.14                                  0.01                             0.72   \n",
       "0.15                                  0.02                             0.74   \n",
       "0.16                                  0.01                             0.72   \n",
       "0.17                                  0.01                             0.78   \n",
       "0.18                                  0.01                             0.70   \n",
       "0.19                                  0.03                             0.73   \n",
       "\n",
       "     1_Deviation Measure Train 1_Test_nll 1_Mean Test_Pearson Corr Coeff  \\\n",
       "0.05                    0.0091       6.20                           0.04   \n",
       "0.06                    0.0111       7.88                           0.06   \n",
       "0.07                    0.0127       6.27                           0.00   \n",
       "0.08                    0.0135       4.03                           0.07   \n",
       "0.09                    0.0129       6.00                          -0.07   \n",
       "0.10                    0.0175       6.34                          -0.05   \n",
       "0.11                    0.0191       3.71                          -0.10   \n",
       "0.12                    0.0216       9.16                           0.01   \n",
       "0.13                    0.0234       4.09                           0.02   \n",
       "0.14                    0.0216       4.12                          -0.06   \n",
       "0.15                    0.0251       7.02                           0.03   \n",
       "0.16                    0.0250       3.73                          -0.03   \n",
       "0.17                    0.0250       6.64                           0.09   \n",
       "0.18                    0.0281       5.14                          -0.08   \n",
       "0.19                    0.0310       3.79                           0.07   \n",
       "\n",
       "     1_Mean Test_Spearman rank Corr Coeff 1_Variance Test Pear Corr Coeff  \\\n",
       "0.05                                 0.03                           -0.18   \n",
       "0.06                                 0.10                            0.02   \n",
       "0.07                                 0.04                            0.04   \n",
       "0.08                                 0.04                            0.14   \n",
       "0.09                                -0.06                            0.28   \n",
       "0.10                                -0.08                           -0.25   \n",
       "0.11                                -0.13                           -0.16   \n",
       "0.12                                 0.06                            0.18   \n",
       "0.13                                 0.05                            0.02   \n",
       "0.14                                -0.02                            0.22   \n",
       "0.15                                 0.02                            0.53   \n",
       "0.16                                -0.04                            0.31   \n",
       "0.17                                 0.12                            0.43   \n",
       "0.18                                -0.11                            0.59   \n",
       "0.19                                 0.11                            0.34   \n",
       "\n",
       "     1_Deviation Measure Test 2_Train_nll 2_Mean Train_Pearson Corr Coeff  \\\n",
       "0.05                   0.0026        3.69                            0.02   \n",
       "0.06                   0.0286        3.64                            0.01   \n",
       "0.07                   0.0151        3.58                            0.03   \n",
       "0.08                   0.0167        3.49                            0.03   \n",
       "0.09                   0.0203        3.50                            0.06   \n",
       "0.10                   0.0059        3.43                            0.03   \n",
       "0.11                   0.0123        3.40                            0.01   \n",
       "0.12                   0.0116        3.40                            0.06   \n",
       "0.13                   0.0137        3.30                            0.03   \n",
       "0.14                   0.1839        3.32                            0.03   \n",
       "0.15                   0.0221        3.26                            0.02   \n",
       "0.16                  -0.0078        3.20                            0.04   \n",
       "0.17                   0.0180        3.22                            0.03   \n",
       "0.18                   0.0168        3.17                            0.05   \n",
       "0.19                   0.0288        3.17                            0.05   \n",
       "\n",
       "     2_Mean Train_Spearman rank Corr Coeff 2_Variance Train Pear Corr Coeff  \\\n",
       "0.05                                  0.01                             0.74   \n",
       "0.06                                  0.01                             0.65   \n",
       "0.07                                  0.01                             0.72   \n",
       "0.08                                  0.01                             0.73   \n",
       "0.09                                  0.02                             0.73   \n",
       "0.10                                  0.01                             0.70   \n",
       "0.11                                 -0.00                             0.67   \n",
       "0.12                                  0.03                             0.71   \n",
       "0.13                                  0.01                             0.72   \n",
       "0.14                                  0.02                             0.76   \n",
       "0.15                                 -0.01                             0.75   \n",
       "0.16                                  0.02                             0.75   \n",
       "0.17                                  0.02                             0.71   \n",
       "0.18                                  0.01                             0.74   \n",
       "0.19                                 -0.00                             0.74   \n",
       "\n",
       "     2_Deviation Measure Train 2_Test_nll 2_Mean Test_Pearson Corr Coeff  \\\n",
       "0.05                    0.0103       4.94                           0.02   \n",
       "0.06                    0.0115       8.98                           0.01   \n",
       "0.07                    0.0139       5.89                          -0.05   \n",
       "0.08                    0.0147       6.45                           0.06   \n",
       "0.09                    0.0170       4.97                          -0.01   \n",
       "0.10                    0.0185       3.34                          -0.05   \n",
       "0.11                    0.0182       7.67                           0.02   \n",
       "0.12                    0.0163       5.09                           0.04   \n",
       "0.13                    0.0219       8.85                           0.08   \n",
       "0.14                    0.0238       4.87                          -0.05   \n",
       "0.15                    0.0287       6.12                          -0.07   \n",
       "0.16                    0.0247       4.18                           0.02   \n",
       "0.17                    0.0247       5.32                           0.03   \n",
       "0.18                    0.0297       3.79                           0.01   \n",
       "0.19                    0.0349       6.40                          -0.09   \n",
       "\n",
       "     2_Mean Test_Spearman rank Corr Coeff 2_Variance Test Pear Corr Coeff  \\\n",
       "0.05                                 0.07                           -0.63   \n",
       "0.06                                -0.06                           -0.31   \n",
       "0.07                                -0.08                           -0.22   \n",
       "0.08                                 0.08                            0.17   \n",
       "0.09                                 0.02                            0.54   \n",
       "0.10                                -0.04                            0.17   \n",
       "0.11                                 0.04                            0.27   \n",
       "0.12                                 0.05                            0.41   \n",
       "0.13                                 0.10                            0.39   \n",
       "0.14                                -0.08                           -0.31   \n",
       "0.15                                -0.08                            0.32   \n",
       "0.16                                 0.06                            0.64   \n",
       "0.17                                 0.07                            0.04   \n",
       "0.18                                -0.04                            0.31   \n",
       "0.19                                -0.09                            0.31   \n",
       "\n",
       "     2_Deviation Measure Test  \n",
       "0.05                  -0.0019  \n",
       "0.06                   0.0069  \n",
       "0.07                   0.0157  \n",
       "0.08                   0.0067  \n",
       "0.09                   0.0207  \n",
       "0.10                   0.0191  \n",
       "0.11                   0.0238  \n",
       "0.12                   0.0152  \n",
       "0.13                   0.0112  \n",
       "0.14                   0.0212  \n",
       "0.15                   0.0195  \n",
       "0.16                   0.0286  \n",
       "0.17                   0.0312  \n",
       "0.18                  -0.0196  \n",
       "0.19                   0.0310  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "29ee4b27",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [55]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_report\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1_Train_nll\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_report\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2_Train_nll\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36msum\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2259\u001b[0m, in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2256\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   2257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[1;32m-> 2259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2260\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "np.sum(np.array(d_report['1_Train_nll']),np.array(d_report['2_Train_nll']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3680644c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1_Train_nll</th>\n",
       "      <th>1_Mean Train_Pearson Corr Coeff</th>\n",
       "      <th>1_Mean Train_Spearman rank Corr Coeff</th>\n",
       "      <th>1_Variance Train Pear Corr Coeff</th>\n",
       "      <th>1_Test_nll</th>\n",
       "      <th>1_Mean Test_Pearson Corr Coeff</th>\n",
       "      <th>1_Mean Test_Spearman rank Corr Coeff</th>\n",
       "      <th>1_Variance Test Pear Corr Coeff</th>\n",
       "      <th>2_Train_nll</th>\n",
       "      <th>2_Mean Train_Pearson Corr Coeff</th>\n",
       "      <th>2_Mean Train_Spearman rank Corr Coeff</th>\n",
       "      <th>2_Variance Train Pear Corr Coeff</th>\n",
       "      <th>2_Test_nll</th>\n",
       "      <th>2_Mean Test_Pearson Corr Coeff</th>\n",
       "      <th>2_Mean Test_Spearman rank Corr Coeff</th>\n",
       "      <th>2_Variance Test Pear Corr Coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.05</th>\n",
       "      <td>3.69</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.71</td>\n",
       "      <td>4.19</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.19</td>\n",
       "      <td>3.69</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.71</td>\n",
       "      <td>4.19</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.06</th>\n",
       "      <td>3.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.72</td>\n",
       "      <td>5.80</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.72</td>\n",
       "      <td>5.80</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.07</th>\n",
       "      <td>3.55</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.72</td>\n",
       "      <td>6.90</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.23</td>\n",
       "      <td>3.55</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.72</td>\n",
       "      <td>6.90</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.08</th>\n",
       "      <td>3.53</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.70</td>\n",
       "      <td>5.85</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.53</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.70</td>\n",
       "      <td>5.85</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.09</th>\n",
       "      <td>3.44</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.66</td>\n",
       "      <td>5.13</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>3.44</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.66</td>\n",
       "      <td>5.13</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.10</th>\n",
       "      <td>3.39</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.73</td>\n",
       "      <td>8.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.21</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.73</td>\n",
       "      <td>8.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.11</th>\n",
       "      <td>3.37</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.73</td>\n",
       "      <td>5.93</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.39</td>\n",
       "      <td>3.37</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.73</td>\n",
       "      <td>5.93</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.12</th>\n",
       "      <td>3.34</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.71</td>\n",
       "      <td>4.62</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.22</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.71</td>\n",
       "      <td>4.62</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.13</th>\n",
       "      <td>3.31</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.23</td>\n",
       "      <td>3.31</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.14</th>\n",
       "      <td>3.29</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.75</td>\n",
       "      <td>6.45</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.37</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.75</td>\n",
       "      <td>6.45</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.15</th>\n",
       "      <td>3.26</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.74</td>\n",
       "      <td>5.13</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.51</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.74</td>\n",
       "      <td>5.13</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.16</th>\n",
       "      <td>3.23</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.73</td>\n",
       "      <td>6.78</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.23</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.73</td>\n",
       "      <td>6.78</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.17</th>\n",
       "      <td>3.17</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.74</td>\n",
       "      <td>3.96</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.44</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.74</td>\n",
       "      <td>3.96</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.18</th>\n",
       "      <td>3.20</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.73</td>\n",
       "      <td>6.24</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.14</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.73</td>\n",
       "      <td>6.24</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.19</th>\n",
       "      <td>3.18</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.76</td>\n",
       "      <td>6.47</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.20</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.76</td>\n",
       "      <td>6.47</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      1_Train_nll 1_Mean Train_Pearson Corr Coeff  \\\n",
       "0.05         3.69                            0.04   \n",
       "0.06         3.64                            0.00   \n",
       "0.07         3.55                            0.04   \n",
       "0.08         3.53                            0.01   \n",
       "0.09         3.44                            0.02   \n",
       "0.10         3.39                            0.03   \n",
       "0.11         3.37                            0.05   \n",
       "0.12         3.34                            0.02   \n",
       "0.13         3.31                            0.02   \n",
       "0.14         3.29                            0.06   \n",
       "0.15         3.26                            0.04   \n",
       "0.16         3.23                            0.03   \n",
       "0.17         3.17                            0.04   \n",
       "0.18         3.20                            0.03   \n",
       "0.19         3.18                            0.02   \n",
       "\n",
       "     1_Mean Train_Spearman rank Corr Coeff 1_Variance Train Pear Corr Coeff  \\\n",
       "0.05                                  0.03                             0.71   \n",
       "0.06                                 -0.00                             0.72   \n",
       "0.07                                  0.01                             0.72   \n",
       "0.08                                 -0.01                             0.70   \n",
       "0.09                                 -0.00                             0.66   \n",
       "0.10                                  0.01                             0.73   \n",
       "0.11                                  0.01                             0.73   \n",
       "0.12                                  0.01                             0.71   \n",
       "0.13                                  0.00                             0.70   \n",
       "0.14                                  0.02                             0.75   \n",
       "0.15                                  0.01                             0.74   \n",
       "0.16                                  0.02                             0.73   \n",
       "0.17                                  0.01                             0.74   \n",
       "0.18                                  0.02                             0.73   \n",
       "0.19                                  0.00                             0.76   \n",
       "\n",
       "     1_Test_nll 1_Mean Test_Pearson Corr Coeff  \\\n",
       "0.05       4.19                          -0.01   \n",
       "0.06       5.80                          -0.06   \n",
       "0.07       6.90                          -0.09   \n",
       "0.08       5.85                          -0.04   \n",
       "0.09       5.13                           0.05   \n",
       "0.10       8.91                           0.00   \n",
       "0.11       5.93                          -0.07   \n",
       "0.12       4.62                           0.05   \n",
       "0.13       3.45                           0.07   \n",
       "0.14       6.45                           0.06   \n",
       "0.15       5.13                           0.02   \n",
       "0.16       6.78                          -0.00   \n",
       "0.17       3.96                          -0.03   \n",
       "0.18       6.24                          -0.03   \n",
       "0.19       6.47                          -0.12   \n",
       "\n",
       "     1_Mean Test_Spearman rank Corr Coeff 1_Variance Test Pear Corr Coeff  \\\n",
       "0.05                                -0.03                            0.19   \n",
       "0.06                                -0.07                            0.33   \n",
       "0.07                                -0.12                            0.23   \n",
       "0.08                                -0.04                            0.01   \n",
       "0.09                                 0.08                           -0.17   \n",
       "0.10                                -0.01                            0.21   \n",
       "0.11                                -0.04                            0.39   \n",
       "0.12                                 0.03                            0.22   \n",
       "0.13                                 0.08                            0.23   \n",
       "0.14                                 0.06                            0.37   \n",
       "0.15                                 0.03                            0.51   \n",
       "0.16                                 0.05                            0.03   \n",
       "0.17                                -0.02                            0.44   \n",
       "0.18                                -0.02                            0.14   \n",
       "0.19                                -0.12                            0.20   \n",
       "\n",
       "      2_Train_nll 2_Mean Train_Pearson Corr Coeff  \\\n",
       "0.05         3.69                            0.04   \n",
       "0.06         3.64                            0.00   \n",
       "0.07         3.55                            0.04   \n",
       "0.08         3.53                            0.01   \n",
       "0.09         3.44                            0.02   \n",
       "0.10         3.39                            0.03   \n",
       "0.11         3.37                            0.05   \n",
       "0.12         3.34                            0.02   \n",
       "0.13         3.31                            0.02   \n",
       "0.14         3.29                            0.06   \n",
       "0.15         3.26                            0.04   \n",
       "0.16         3.23                            0.03   \n",
       "0.17         3.17                            0.04   \n",
       "0.18         3.20                            0.03   \n",
       "0.19         3.18                            0.02   \n",
       "\n",
       "     2_Mean Train_Spearman rank Corr Coeff 2_Variance Train Pear Corr Coeff  \\\n",
       "0.05                                  0.03                             0.71   \n",
       "0.06                                 -0.00                             0.72   \n",
       "0.07                                  0.01                             0.72   \n",
       "0.08                                 -0.01                             0.70   \n",
       "0.09                                 -0.00                             0.66   \n",
       "0.10                                  0.01                             0.73   \n",
       "0.11                                  0.01                             0.73   \n",
       "0.12                                  0.01                             0.71   \n",
       "0.13                                  0.00                             0.70   \n",
       "0.14                                  0.02                             0.75   \n",
       "0.15                                  0.01                             0.74   \n",
       "0.16                                  0.02                             0.73   \n",
       "0.17                                  0.01                             0.74   \n",
       "0.18                                  0.02                             0.73   \n",
       "0.19                                  0.00                             0.76   \n",
       "\n",
       "     2_Test_nll 2_Mean Test_Pearson Corr Coeff  \\\n",
       "0.05       4.19                          -0.01   \n",
       "0.06       5.80                          -0.06   \n",
       "0.07       6.90                          -0.09   \n",
       "0.08       5.85                          -0.04   \n",
       "0.09       5.13                           0.05   \n",
       "0.10       8.91                           0.00   \n",
       "0.11       5.93                          -0.07   \n",
       "0.12       4.62                           0.05   \n",
       "0.13       3.45                           0.07   \n",
       "0.14       6.45                           0.06   \n",
       "0.15       5.13                           0.02   \n",
       "0.16       6.78                          -0.00   \n",
       "0.17       3.96                          -0.03   \n",
       "0.18       6.24                          -0.03   \n",
       "0.19       6.47                          -0.12   \n",
       "\n",
       "     2_Mean Test_Spearman rank Corr Coeff 2_Variance Test Pear Corr Coeff  \n",
       "0.05                                -0.03                            0.19  \n",
       "0.06                                -0.07                            0.33  \n",
       "0.07                                -0.12                            0.23  \n",
       "0.08                                -0.04                            0.01  \n",
       "0.09                                 0.08                           -0.17  \n",
       "0.10                                -0.01                            0.21  \n",
       "0.11                                -0.04                            0.39  \n",
       "0.12                                 0.03                            0.22  \n",
       "0.13                                 0.08                            0.23  \n",
       "0.14                                 0.06                            0.37  \n",
       "0.15                                 0.03                            0.51  \n",
       "0.16                                 0.05                            0.03  \n",
       "0.17                                -0.02                            0.44  \n",
       "0.18                                -0.02                            0.14  \n",
       "0.19                                -0.12                            0.20  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0644dbcf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot perform reduce with flexible type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [83]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m d_report[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTest_NLL\u001b[49m\u001b[43m,\u001b[49m\u001b[43mTest_NLL\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36msum\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2259\u001b[0m, in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2256\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   2257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[1;32m-> 2259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2260\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot perform reduce with flexible type"
     ]
    }
   ],
   "source": [
    "d_report['sum'] = np.sum(Test_NLL,Test_NLL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "539a9c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex Lam\\AppData\\Local\\Temp\\ipykernel_16636\\301123308.py:1: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  d_report['1_Train_nll'] = d_report['1_Train_nll'].astype(np.float)\n"
     ]
    }
   ],
   "source": [
    "d_report['1_Train_nll'] = d_report['1_Train_nll'].astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c681d19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex Lam\\AppData\\Local\\Temp\\ipykernel_16636\\3222756506.py:1: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  d_report['2_Train_nll']=d_report['2_Train_nll'].astype(np.float)\n"
     ]
    }
   ],
   "source": [
    "d_report['2_Train_nll']=d_report['2_Train_nll'].astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5c2990e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [77]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_report\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2_Train_nll\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_report\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1_Train_nll\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36msum\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2259\u001b[0m, in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2256\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   2257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[1;32m-> 2259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2260\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "np.sum(np.array(d_report['2_Train_nll'].astype(float)),np.array(d_report['1_Train_nll'].astype(float)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3821f5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_report.to_csv('report_t+1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b633e13a",
   "metadata": {},
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f1493d",
   "metadata": {},
   "source": [
    "# HYPERPARAMETER TUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e8918e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(layer, self).__init__()\n",
    "        \n",
    "        \n",
    "    def Normal_Dist_Layer_2(x):\n",
    "        \"\"\"\n",
    "        Lambda function for generating Normal Distribution parameters\n",
    "        mu and sigma2 from a Dense(2) output.\n",
    "        Assumes tensorflow 2 backend.\n",
    "\n",
    "        Usage\n",
    "        -----\n",
    "        outputs = Dense(4)(final_layer)\n",
    "        distribution_outputs = Lambda(Normal_Dist_Layer_2)(outputs)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : tf.Tensor\n",
    "            output tensor of Dense layer\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        out_tensor : tf.Tensor\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the number of dimensions of the input\n",
    "        num_dims = len(x.get_shape())\n",
    "    #     print('x.get_shape(): ',x.get_shape(),'num_dims: ',num_dims)\n",
    "        # Separate the parameters\n",
    "        mu1,sigma2_1,mu2,sigma2_2, w1 = tf.unstack(x, num=5, axis=-1)\n",
    "\n",
    "        # Add one dimension to make the right shape\n",
    "\n",
    "        mu1 = tf.expand_dims(mu1, -1)\n",
    "        sigma2_1 = tf.expand_dims(sigma2_1, -1)\n",
    "        mu2 = tf.expand_dims(mu2, -1)\n",
    "        sigma2_2 = tf.expand_dims(sigma2_2, -1)    \n",
    "        w1 = tf.expand_dims(w1, -1)\n",
    "\n",
    "        # Apply a softplus to make variance 1 and variance 2 both positive\n",
    "        # But other than that, there are no other restrictions\n",
    "        sigma2_1 = tf.keras.activations.softplus(sigma2_1)\n",
    "        sigma2_2 = tf.keras.activations.softplus(sigma2_2)\n",
    "\n",
    "        # We restrict mu1 to be very close to the simple mean return of the whole training period. \n",
    "        # We aim to find the 2nd component, which could explain the abnormal log return.\n",
    "        # Would it make more sense to convert this to a more dynamic process?\n",
    "        # May be exponentially weighted? Consider the mean return of the window that we are observing?\n",
    "\n",
    "        mu1 = 0.009+0.0009*tf.keras.activations.sigmoid(mu1)*1.2\n",
    "    #     mu2 = -tf.keras.activations.softplus(mu2)\n",
    "        w1 = 0.8+0.2*tf.keras.activations.sigmoid(w1)\n",
    "\n",
    "        # Join back together again\n",
    "        out_tensor = tf.stack((mu1,sigma2_1,mu2,sigma2_2, w1), axis=num_dims-1)\n",
    "\n",
    "        return out_tensor\n",
    "    \n",
    "    def negative_log_likelihood_loss_2(y_true,y_pred):\n",
    "        \"\"\"\n",
    "        Negative log-likelihood loss function.\n",
    "        Assumes tensorflow backend.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : tf.Tensor\n",
    "            Ground truth values of predicted variable.\n",
    "        y_pred : tf.Tensor\n",
    "            mu and sigma2 values of predicted distribution.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        nll : tf.Tensor\n",
    "            Negative log likelihood.\n",
    "        \"\"\"\n",
    "\n",
    "        # Separate the parameters\n",
    "        mu1, sigma2_1, mu2, sigma2_2, w1 = tf.unstack(y_pred, num=5, axis=-1)\n",
    "\n",
    "        # Add one dimension to make the right shape\n",
    "        mu1 = tf.expand_dims(mu1, -1)\n",
    "        sigma2_1 = tf.expand_dims(sigma2_1, -1)\n",
    "        mu2 = tf.expand_dims(mu2, -1)\n",
    "        sigma2_2 = tf.expand_dims(sigma2_2, -1)\n",
    "        w1 = tf.expand_dims(w1, -1)\n",
    "\n",
    "        #     print('mu1:',mu1,'sigma2:',sigma2_1)\n",
    "\n",
    "        # Calculate the negative log likelihood\n",
    "        nll = (-tf.math.log(w1)\n",
    "               +0.5*(tf.math.log(sigma2_1)\n",
    "                     +tf.math.square(y_true-mu1)/sigma2_1)\n",
    "               -tf.math.log(1-w1)\n",
    "               +0.5*(tf.math.log(sigma2_2)\n",
    "                     +tf.math.square(y_true-mu2)/sigma2_2)\n",
    "              )\n",
    "        -tf.math.log(sigma2_2/sigma2_1)+0.09*sigma2_2\n",
    "    #     print('This is nll_value:',nll)\n",
    "        return tf.reduce_mean(nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c62b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hp.Int('Input_Layer_LSTM_unit',min_value=8,max_value=32,step=8),\n",
    "                   return_sequences=True, \n",
    "                   input_shape=(X_train.shape[1],X_train.shape[2]),\n",
    "                  ))\n",
    "    model.add(Dropout(hp.Float('First_Dropout',min_value=0,max_value=0.5,step=0.1)))\n",
    "    for i in range(hp.Int('n_layers', 0, 3)):\n",
    "        model.add(LSTM(hp.Int(f'LSTM_{i}_units',min_value=8,max_value=32,step=8),kernel_initializer=glorot_uniform(),\n",
    "                       return_sequences=True))\n",
    "        model.add(Dropout(hp.Float(f'Dropout_{i}',min_value=0,max_value=0.5,step=0.1)))\n",
    "    \n",
    "    model.add(LSTM(hp.Int('Final_Lstm_layer',min_value=8,max_value=32,step=8)))\n",
    "    model.add(Dense(hp.Int('Final_Dense',min_value=16,max_value=64,step=8), activation='relu'))\n",
    "    model.add(Dense(5))\n",
    "    model.add(Lambda(Normal_Dist_Layer_2))\n",
    "\n",
    "    model.compile(loss = layer.negative_log_likelihood_loss_2, \n",
    "                   optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n",
    "                   )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca194a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_path = r\"C:\\Users\\Alex Lam\\OneDrive\\Documents\\Demonstration\\Tensorflow\\CCMP\\exp1\"\n",
    "\n",
    "print(f\"The tuning process will be stored at {tuning_path}\")\n",
    "\n",
    "exp1_tuner= kt.RandomSearch(\n",
    "        build_model,\n",
    "        objective='val_loss', #val_loss\n",
    "        max_trials=200,\n",
    "        executions_per_trial=3,\n",
    "        directory=tuning_path\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af55f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)  #val_loss\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\",factor=0.99, patience = 1, verbose=1) #val_loss\n",
    "callbacks_list = [early_stop, reduce_lr]\n",
    "\n",
    "exp1_tuner.search(\n",
    "        x=X_train,\n",
    "        y=Y_train,\n",
    "        epochs=200,\n",
    "        batch_size=128,\n",
    "        validation_data=(X_val,Y_val),\n",
    "        callbacks=callbacks_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20c07c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140be2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc4f91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b311ac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1926da28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ab61b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
